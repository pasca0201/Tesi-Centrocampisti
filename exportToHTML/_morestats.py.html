<html>
<head>
<title>_morestats.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #7a7e85;}
.s4 { color: #6aab73;}
.s5 { color: #2aacb8;}
.s6 { color: #5f826b; font-style: italic;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_morestats.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">annotations</span>
<span class="s0">import </span><span class="s1">math</span>
<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">collections </span><span class="s0">import </span><span class="s1">namedtuple</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">from </span><span class="s1">numpy </span><span class="s0">import </span><span class="s2">(</span><span class="s1">isscalar</span><span class="s2">, </span><span class="s1">r_</span><span class="s2">, </span><span class="s1">log</span><span class="s2">, </span><span class="s1">around</span><span class="s2">, </span><span class="s1">unique</span><span class="s2">, </span><span class="s1">asarray</span><span class="s2">, </span><span class="s1">zeros</span><span class="s2">,</span>
                   <span class="s1">arange</span><span class="s2">, </span><span class="s1">sort</span><span class="s2">, </span><span class="s1">amin</span><span class="s2">, </span><span class="s1">amax</span><span class="s2">, </span><span class="s1">sqrt</span><span class="s2">, </span><span class="s1">array</span><span class="s2">,</span>
                   <span class="s1">pi</span><span class="s2">, </span><span class="s1">exp</span><span class="s2">, </span><span class="s1">ravel</span><span class="s2">, </span><span class="s1">count_nonzero</span><span class="s2">)</span>

<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">optimize</span><span class="s2">, </span><span class="s1">special</span><span class="s2">, </span><span class="s1">interpolate</span><span class="s2">, </span><span class="s1">stats</span>
<span class="s0">from </span><span class="s1">scipy</span><span class="s2">.</span><span class="s1">_lib</span><span class="s2">.</span><span class="s1">_bunch </span><span class="s0">import </span><span class="s1">_make_tuple_bunch</span>
<span class="s0">from </span><span class="s1">scipy</span><span class="s2">.</span><span class="s1">_lib</span><span class="s2">.</span><span class="s1">_util </span><span class="s0">import </span><span class="s1">_rename_parameter</span><span class="s2">, </span><span class="s1">_contains_nan</span><span class="s2">, </span><span class="s1">_get_nan</span>
<span class="s0">from </span><span class="s1">scipy</span><span class="s2">.</span><span class="s1">_lib</span><span class="s2">.</span><span class="s1">_array_api </span><span class="s0">import </span><span class="s2">(</span><span class="s1">array_namespace</span><span class="s2">, </span><span class="s1">xp_minimum</span><span class="s2">, </span><span class="s1">size </span><span class="s0">as </span><span class="s1">xp_size</span><span class="s2">,</span>
                                   <span class="s1">xp_moveaxis_to_end</span><span class="s2">)</span>

<span class="s0">from </span><span class="s2">.</span><span class="s1">_ansari_swilk_statistics </span><span class="s0">import </span><span class="s1">gscale</span><span class="s2">, </span><span class="s1">swilk</span>
<span class="s0">from </span><span class="s2">. </span><span class="s0">import </span><span class="s1">_stats_py</span><span class="s2">, </span><span class="s1">_wilcoxon</span>
<span class="s0">from </span><span class="s2">.</span><span class="s1">_fit </span><span class="s0">import </span><span class="s1">FitResult</span>
<span class="s0">from </span><span class="s2">.</span><span class="s1">_stats_py </span><span class="s0">import </span><span class="s2">(</span><span class="s1">find_repeats</span><span class="s2">, </span><span class="s1">_get_pvalue</span><span class="s2">, </span><span class="s1">SignificanceResult</span><span class="s2">,  </span><span class="s3"># noqa:F401</span>
                        <span class="s1">_SimpleNormal</span><span class="s2">, </span><span class="s1">_SimpleChi2</span><span class="s2">)</span>
<span class="s0">from </span><span class="s2">.</span><span class="s1">contingency </span><span class="s0">import </span><span class="s1">chi2_contingency</span>
<span class="s0">from </span><span class="s2">. </span><span class="s0">import </span><span class="s1">distributions</span>
<span class="s0">from </span><span class="s2">.</span><span class="s1">_distn_infrastructure </span><span class="s0">import </span><span class="s1">rv_generic</span>
<span class="s0">from </span><span class="s2">.</span><span class="s1">_axis_nan_policy </span><span class="s0">import </span><span class="s1">_axis_nan_policy_factory</span><span class="s2">, </span><span class="s1">_broadcast_arrays</span>


<span class="s1">__all__ </span><span class="s2">= [</span><span class="s4">'mvsdist'</span><span class="s2">,</span>
           <span class="s4">'bayes_mvs'</span><span class="s2">, </span><span class="s4">'kstat'</span><span class="s2">, </span><span class="s4">'kstatvar'</span><span class="s2">, </span><span class="s4">'probplot'</span><span class="s2">, </span><span class="s4">'ppcc_max'</span><span class="s2">, </span><span class="s4">'ppcc_plot'</span><span class="s2">,</span>
           <span class="s4">'boxcox_llf'</span><span class="s2">, </span><span class="s4">'boxcox'</span><span class="s2">, </span><span class="s4">'boxcox_normmax'</span><span class="s2">, </span><span class="s4">'boxcox_normplot'</span><span class="s2">,</span>
           <span class="s4">'shapiro'</span><span class="s2">, </span><span class="s4">'anderson'</span><span class="s2">, </span><span class="s4">'ansari'</span><span class="s2">, </span><span class="s4">'bartlett'</span><span class="s2">, </span><span class="s4">'levene'</span><span class="s2">,</span>
           <span class="s4">'fligner'</span><span class="s2">, </span><span class="s4">'mood'</span><span class="s2">, </span><span class="s4">'wilcoxon'</span><span class="s2">, </span><span class="s4">'median_test'</span><span class="s2">,</span>
           <span class="s4">'circmean'</span><span class="s2">, </span><span class="s4">'circvar'</span><span class="s2">, </span><span class="s4">'circstd'</span><span class="s2">, </span><span class="s4">'anderson_ksamp'</span><span class="s2">,</span>
           <span class="s4">'yeojohnson_llf'</span><span class="s2">, </span><span class="s4">'yeojohnson'</span><span class="s2">, </span><span class="s4">'yeojohnson_normmax'</span><span class="s2">,</span>
           <span class="s4">'yeojohnson_normplot'</span><span class="s2">, </span><span class="s4">'directional_stats'</span><span class="s2">,</span>
           <span class="s4">'false_discovery_control'</span>
           <span class="s2">]</span>


<span class="s1">Mean </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'Mean'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'minmax'</span><span class="s2">))</span>
<span class="s1">Variance </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'Variance'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'minmax'</span><span class="s2">))</span>
<span class="s1">Std_dev </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'Std_dev'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'minmax'</span><span class="s2">))</span>


<span class="s0">def </span><span class="s1">bayes_mvs</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">=</span><span class="s5">0.90</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot; 
    Bayesian confidence intervals for the mean, var, and std. 
 
    Parameters 
    ---------- 
    data : array_like 
        Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`. 
        Requires 2 or more data points. 
    alpha : float, optional 
        Probability that the returned confidence interval contains 
        the true parameter. 
 
    Returns 
    ------- 
    mean_cntr, var_cntr, std_cntr : tuple 
        The three results are for the mean, variance and standard deviation, 
        respectively.  Each result is a tuple of the form:: 
 
            (center, (lower, upper)) 
 
        with `center` the mean of the conditional pdf of the value given the 
        data, and `(lower, upper)` a confidence interval, centered on the 
        median, containing the estimate to a probability ``alpha``. 
 
    See Also 
    -------- 
    mvsdist 
 
    Notes 
    ----- 
    Each tuple of mean, variance, and standard deviation estimates represent 
    the (center, (lower, upper)) with center the mean of the conditional pdf 
    of the value given the data and (lower, upper) is a confidence interval 
    centered on the median, containing the estimate to a probability 
    ``alpha``. 
 
    Converts data to 1-D and assumes all data has the same mean and variance. 
    Uses Jeffrey's prior for variance and std. 
 
    Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))`` 
 
    References 
    ---------- 
    T.E. Oliphant, &quot;A Bayesian perspective on estimating mean, variance, and 
    standard-deviation from data&quot;, https://scholarsarchive.byu.edu/facpub/278, 
    2006. 
 
    Examples 
    -------- 
    First a basic example to demonstrate the outputs: 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; data = [6, 9, 12, 7, 8, 8, 13] 
    &gt;&gt;&gt; mean, var, std = stats.bayes_mvs(data) 
    &gt;&gt;&gt; mean 
    Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467)) 
    &gt;&gt;&gt; var 
    Variance(statistic=10.0, minmax=(3.176724206, 24.45910382)) 
    &gt;&gt;&gt; std 
    Std_dev(statistic=2.9724954732045084, 
            minmax=(1.7823367265645143, 4.945614605014631)) 
 
    Now we generate some normally distributed random data, and get estimates of 
    mean and standard deviation with 95% confidence intervals for those 
    estimates: 
 
    &gt;&gt;&gt; n_samples = 100000 
    &gt;&gt;&gt; data = stats.norm.rvs(size=n_samples) 
    &gt;&gt;&gt; res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95) 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; ax.hist(data, bins=100, density=True, label='Histogram of data') 
    &gt;&gt;&gt; ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean') 
    &gt;&gt;&gt; ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r', 
    ...            alpha=0.2, label=r'Estimated mean (95% limits)') 
    &gt;&gt;&gt; ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale') 
    &gt;&gt;&gt; ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2, 
    ...            label=r'Estimated scale (95% limits)') 
 
    &gt;&gt;&gt; ax.legend(fontsize=10) 
    &gt;&gt;&gt; ax.set_xlim([-4, 4]) 
    &gt;&gt;&gt; ax.set_ylim([0, 0.5]) 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">m</span><span class="s2">, </span><span class="s1">v</span><span class="s2">, </span><span class="s1">s </span><span class="s2">= </span><span class="s1">mvsdist</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">alpha </span><span class="s2">&gt;= </span><span class="s5">1 </span><span class="s0">or </span><span class="s1">alpha </span><span class="s2">&lt;= </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;0 &lt; alpha &lt; 1 is required, but </span><span class="s0">{</span><span class="s1">alpha</span><span class="s2">=</span><span class="s0">} </span><span class="s4">was given.&quot;</span><span class="s2">)</span>

    <span class="s1">m_res </span><span class="s2">= </span><span class="s1">Mean</span><span class="s2">(</span><span class="s1">m</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(), </span><span class="s1">m</span><span class="s2">.</span><span class="s1">interval</span><span class="s2">(</span><span class="s1">alpha</span><span class="s2">))</span>
    <span class="s1">v_res </span><span class="s2">= </span><span class="s1">Variance</span><span class="s2">(</span><span class="s1">v</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(), </span><span class="s1">v</span><span class="s2">.</span><span class="s1">interval</span><span class="s2">(</span><span class="s1">alpha</span><span class="s2">))</span>
    <span class="s1">s_res </span><span class="s2">= </span><span class="s1">Std_dev</span><span class="s2">(</span><span class="s1">s</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(), </span><span class="s1">s</span><span class="s2">.</span><span class="s1">interval</span><span class="s2">(</span><span class="s1">alpha</span><span class="s2">))</span>

    <span class="s0">return </span><span class="s1">m_res</span><span class="s2">, </span><span class="s1">v_res</span><span class="s2">, </span><span class="s1">s_res</span>


<span class="s0">def </span><span class="s1">mvsdist</span><span class="s2">(</span><span class="s1">data</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot; 
    'Frozen' distributions for mean, variance, and standard deviation of data. 
 
    Parameters 
    ---------- 
    data : array_like 
        Input array. Converted to 1-D using ravel. 
        Requires 2 or more data-points. 
 
    Returns 
    ------- 
    mdist : &quot;frozen&quot; distribution object 
        Distribution object representing the mean of the data. 
    vdist : &quot;frozen&quot; distribution object 
        Distribution object representing the variance of the data. 
    sdist : &quot;frozen&quot; distribution object 
        Distribution object representing the standard deviation of the data. 
 
    See Also 
    -------- 
    bayes_mvs 
 
    Notes 
    ----- 
    The return values from ``bayes_mvs(data)`` is equivalent to 
    ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``. 
 
    In other words, calling ``&lt;dist&gt;.mean()`` and ``&lt;dist&gt;.interval(0.90)`` 
    on the three distribution objects returned from this function will give 
    the same results that are returned from `bayes_mvs`. 
 
    References 
    ---------- 
    T.E. Oliphant, &quot;A Bayesian perspective on estimating mean, variance, and 
    standard-deviation from data&quot;, https://scholarsarchive.byu.edu/facpub/278, 
    2006. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; data = [6, 9, 12, 7, 8, 8, 13] 
    &gt;&gt;&gt; mean, var, std = stats.mvsdist(data) 
 
    We now have frozen distribution objects &quot;mean&quot;, &quot;var&quot; and &quot;std&quot; that we can 
    examine: 
 
    &gt;&gt;&gt; mean.mean() 
    9.0 
    &gt;&gt;&gt; mean.interval(0.95) 
    (6.6120585482655692, 11.387941451734431) 
    &gt;&gt;&gt; mean.std() 
    1.1952286093343936 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">ravel</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s1">n </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">n </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Need at least 2 data-points.&quot;</span><span class="s2">)</span>
    <span class="s1">xbar </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">()</span>
    <span class="s1">C </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">var</span><span class="s2">()</span>
    <span class="s0">if </span><span class="s1">n </span><span class="s2">&gt; </span><span class="s5">1000</span><span class="s2">:  </span><span class="s3"># gaussian approximations for large n</span>
        <span class="s1">mdist </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">(</span><span class="s1">loc</span><span class="s2">=</span><span class="s1">xbar</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">C </span><span class="s2">/ </span><span class="s1">n</span><span class="s2">))</span>
        <span class="s1">sdist </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">(</span><span class="s1">loc</span><span class="s2">=</span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">C</span><span class="s2">), </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">C </span><span class="s2">/ (</span><span class="s5">2. </span><span class="s2">* </span><span class="s1">n</span><span class="s2">)))</span>
        <span class="s1">vdist </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">(</span><span class="s1">loc</span><span class="s2">=</span><span class="s1">C</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s5">2.0 </span><span class="s2">/ </span><span class="s1">n</span><span class="s2">) * </span><span class="s1">C</span><span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">nm1 </span><span class="s2">= </span><span class="s1">n </span><span class="s2">- </span><span class="s5">1</span>
        <span class="s1">fac </span><span class="s2">= </span><span class="s1">n </span><span class="s2">* </span><span class="s1">C </span><span class="s2">/ </span><span class="s5">2.</span>
        <span class="s1">val </span><span class="s2">= </span><span class="s1">nm1 </span><span class="s2">/ </span><span class="s5">2.</span>
        <span class="s1">mdist </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">t</span><span class="s2">(</span><span class="s1">nm1</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">=</span><span class="s1">xbar</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">C </span><span class="s2">/ </span><span class="s1">nm1</span><span class="s2">))</span>
        <span class="s1">sdist </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gengamma</span><span class="s2">(</span><span class="s1">val</span><span class="s2">, -</span><span class="s5">2</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">fac</span><span class="s2">))</span>
        <span class="s1">vdist </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">invgamma</span><span class="s2">(</span><span class="s1">val</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">fac</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">mdist</span><span class="s2">, </span><span class="s1">vdist</span><span class="s2">, </span><span class="s1">sdist</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span>
    <span class="s0">lambda </span><span class="s1">x</span><span class="s2">: </span><span class="s1">x</span><span class="s2">, </span><span class="s1">result_to_tuple</span><span class="s2">=</span><span class="s0">lambda </span><span class="s1">x</span><span class="s2">: (</span><span class="s1">x</span><span class="s2">,), </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">default_axis</span><span class="s2">=</span><span class="s0">None</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">kstat</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">n</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, *, </span><span class="s1">axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot; 
    Return the `n` th k-statistic ( ``1&lt;=n&lt;=4`` so far). 
 
    The `n` th k-statistic ``k_n`` is the unique symmetric unbiased estimator of the 
    `n` th cumulant :math:`\kappa_n` [1]_ [2]_. 
 
    Parameters 
    ---------- 
    data : array_like 
        Input array. 
    n : int, {1, 2, 3, 4}, optional 
        Default is equal to 2. 
    axis : int or None, default: None 
        If an int, the axis of the input along which to compute the statistic. 
        The statistic of each axis-slice (e.g. row) of the input will appear 
        in a corresponding element of the output. If ``None``, the input will 
        be raveled before computing the statistic. 
 
    Returns 
    ------- 
    kstat : float 
        The `n` th k-statistic. 
 
    See Also 
    -------- 
    kstatvar : Returns an unbiased estimator of the variance of the k-statistic 
    moment : Returns the n-th central moment about the mean for a sample. 
 
    Notes 
    ----- 
    For a sample size :math:`n`, the first few k-statistics are given by 
 
    .. math:: 
 
        k_1 &amp;= \frac{S_1}{n}, \\ 
        k_2 &amp;= \frac{nS_2 - S_1^2}{n(n-1)}, \\ 
        k_3 &amp;= \frac{2S_1^3 - 3nS_1S_2 + n^2S_3}{n(n-1)(n-2)}, \\ 
        k_4 &amp;= \frac{-6S_1^4 + 12nS_1^2S_2 - 3n(n-1)S_2^2 - 4n(n+1)S_1S_3 
        + n^2(n+1)S_4}{n (n-1)(n-2)(n-3)}, 
 
    where 
 
    .. math:: 
 
        S_r \equiv \sum_{i=1}^n X_i^r, 
 
    and :math:`X_i` is the :math:`i` th data point. 
 
    References 
    ---------- 
    .. [1] http://mathworld.wolfram.com/k-Statistic.html 
 
    .. [2] http://mathworld.wolfram.com/Cumulant.html 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; from numpy.random import default_rng 
    &gt;&gt;&gt; rng = default_rng() 
 
    As sample size increases, `n`-th moment and `n`-th k-statistic converge to the 
    same number (although they aren't identical). In the case of the normal 
    distribution, they converge to zero. 
 
    &gt;&gt;&gt; for i in range(2,8): 
    ...     x = rng.normal(size=10**i) 
    ...     m, k = stats.moment(x, 3), stats.kstat(x, 3) 
    ...     print(f&quot;{i=}: {m=:.3g}, {k=:.3g}, {(m-k)=:.3g}&quot;) 
    i=2: m=-0.631, k=-0.651, (m-k)=0.0194  # random 
    i=3: m=0.0282, k=0.0283, (m-k)=-8.49e-05 
    i=4: m=-0.0454, k=-0.0454, (m-k)=1.36e-05 
    i=6: m=7.53e-05, k=7.53e-05, (m-k)=-2.26e-09 
    i=7: m=0.00166, k=0.00166, (m-k)=-4.99e-09 
    i=8: m=-2.88e-06 k=-2.88e-06, (m-k)=8.63e-13 
    &quot;&quot;&quot;</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s1">data </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">n </span><span class="s2">&gt; </span><span class="s5">4 </span><span class="s0">or </span><span class="s1">n </span><span class="s2">&lt; </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;k-statistics only supported for 1&lt;=n&lt;=4&quot;</span><span class="s2">)</span>
    <span class="s1">n </span><span class="s2">= </span><span class="s1">int</span><span class="s2">(</span><span class="s1">n</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">data </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, (-</span><span class="s5">1</span><span class="s2">,))</span>
        <span class="s1">axis </span><span class="s2">= </span><span class="s5">0</span>

    <span class="s1">N </span><span class="s2">= </span><span class="s1">data</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span>

    <span class="s1">S </span><span class="s2">= [</span><span class="s0">None</span><span class="s2">] + [</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">data</span><span class="s2">**</span><span class="s1">k</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">) </span><span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">n </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">)]</span>
    <span class="s0">if </span><span class="s1">n </span><span class="s2">== </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">] * </span><span class="s5">1.0</span><span class="s2">/</span><span class="s1">N</span>
    <span class="s0">elif </span><span class="s1">n </span><span class="s2">== </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s1">N</span><span class="s2">*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">2</span><span class="s2">] - </span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]**</span><span class="s5">2.0</span><span class="s2">) / (</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N </span><span class="s2">- </span><span class="s5">1.0</span><span class="s2">))</span>
    <span class="s0">elif </span><span class="s1">n </span><span class="s2">== </span><span class="s5">3</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s5">2</span><span class="s2">*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]**</span><span class="s5">3 </span><span class="s2">- </span><span class="s5">3</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">2</span><span class="s2">] + </span><span class="s1">N</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">3</span><span class="s2">]) / (</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N </span><span class="s2">- </span><span class="s5">1.0</span><span class="s2">)*(</span><span class="s1">N </span><span class="s2">- </span><span class="s5">2.0</span><span class="s2">))</span>
    <span class="s0">elif </span><span class="s1">n </span><span class="s2">== </span><span class="s5">4</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s2">((-</span><span class="s5">6</span><span class="s2">*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]**</span><span class="s5">4 </span><span class="s2">+ </span><span class="s5">12</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]**</span><span class="s5">2 </span><span class="s2">* </span><span class="s1">S</span><span class="s2">[</span><span class="s5">2</span><span class="s2">] - </span><span class="s5">3</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N</span><span class="s2">-</span><span class="s5">1.0</span><span class="s2">)*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">2</span><span class="s2">]**</span><span class="s5">2 </span><span class="s2">-</span>
                 <span class="s5">4</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N</span><span class="s2">+</span><span class="s5">1</span><span class="s2">)*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">3</span><span class="s2">] + </span><span class="s1">N</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N</span><span class="s2">+</span><span class="s5">1</span><span class="s2">)*</span><span class="s1">S</span><span class="s2">[</span><span class="s5">4</span><span class="s2">]) /</span>
                <span class="s2">(</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N</span><span class="s2">-</span><span class="s5">1.0</span><span class="s2">)*(</span><span class="s1">N</span><span class="s2">-</span><span class="s5">2.0</span><span class="s2">)*(</span><span class="s1">N</span><span class="s2">-</span><span class="s5">3.0</span><span class="s2">)))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Should not be here.&quot;</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span>
    <span class="s0">lambda </span><span class="s1">x</span><span class="s2">: </span><span class="s1">x</span><span class="s2">, </span><span class="s1">result_to_tuple</span><span class="s2">=</span><span class="s0">lambda </span><span class="s1">x</span><span class="s2">: (</span><span class="s1">x</span><span class="s2">,), </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">default_axis</span><span class="s2">=</span><span class="s0">None</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">kstatvar</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">n</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, *, </span><span class="s1">axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s4">r&quot;&quot;&quot;Return an unbiased estimator of the variance of the k-statistic. 
 
    See `kstat` and [1]_ for more details about the k-statistic. 
 
    Parameters 
    ---------- 
    data : array_like 
        Input array. 
    n : int, {1, 2}, optional 
        Default is equal to 2. 
    axis : int or None, default: None 
        If an int, the axis of the input along which to compute the statistic. 
        The statistic of each axis-slice (e.g. row) of the input will appear 
        in a corresponding element of the output. If ``None``, the input will 
        be raveled before computing the statistic. 
 
    Returns 
    ------- 
    kstatvar : float 
        The `n` th k-statistic variance. 
 
    See Also 
    -------- 
    kstat : Returns the n-th k-statistic. 
    moment : Returns the n-th central moment about the mean for a sample. 
 
    Notes 
    ----- 
    Unbiased estimators of the variances of the first two k-statistics are given by 
 
    .. math:: 
 
        \mathrm{var}(k_1) &amp;= \frac{k_2}{n}, \\ 
        \mathrm{var}(k_2) &amp;= \frac{2k_2^2n + (n-1)k_4}{n(n - 1)}. 
 
    References 
    ---------- 
    .. [1] http://mathworld.wolfram.com/k-Statistic.html 
 
    &quot;&quot;&quot;  </span><span class="s3"># noqa: E501</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s1">data </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">data </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, (-</span><span class="s5">1</span><span class="s2">,))</span>
        <span class="s1">axis </span><span class="s2">= </span><span class="s5">0</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s1">data</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span>

    <span class="s0">if </span><span class="s1">n </span><span class="s2">== </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">kstat</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">n</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">_no_deco</span><span class="s2">=</span><span class="s0">True</span><span class="s2">) * </span><span class="s5">1.0</span><span class="s2">/</span><span class="s1">N</span>
    <span class="s0">elif </span><span class="s1">n </span><span class="s2">== </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s1">k2 </span><span class="s2">= </span><span class="s1">kstat</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">n</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">_no_deco</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
        <span class="s1">k4 </span><span class="s2">= </span><span class="s1">kstat</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">n</span><span class="s2">=</span><span class="s5">4</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">_no_deco</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s5">2</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*</span><span class="s1">k2</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">+ (</span><span class="s1">N</span><span class="s2">-</span><span class="s5">1</span><span class="s2">)*</span><span class="s1">k4</span><span class="s2">) / (</span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N</span><span class="s2">+</span><span class="s5">1</span><span class="s2">))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Only n=1 or n=2 supported.&quot;</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">_calc_uniform_order_statistic_medians</span><span class="s2">(</span><span class="s1">n</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Approximations of uniform order statistic medians. 
 
    Parameters 
    ---------- 
    n : int 
        Sample size. 
 
    Returns 
    ------- 
    v : 1d float array 
        Approximations of the order statistic medians. 
 
    References 
    ---------- 
    .. [1] James J. Filliben, &quot;The Probability Plot Correlation Coefficient 
           Test for Normality&quot;, Technometrics, Vol. 17, pp. 111-117, 1975. 
 
    Examples 
    -------- 
    Order statistics of the uniform distribution on the unit interval 
    are marginally distributed according to beta distributions. 
    The expectations of these order statistic are evenly spaced across 
    the interval, but the distributions are skewed in a way that 
    pushes the medians slightly towards the endpoints of the unit interval: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; n = 4 
    &gt;&gt;&gt; k = np.arange(1, n+1) 
    &gt;&gt;&gt; from scipy.stats import beta 
    &gt;&gt;&gt; a = k 
    &gt;&gt;&gt; b = n-k+1 
    &gt;&gt;&gt; beta.mean(a, b) 
    array([0.2, 0.4, 0.6, 0.8]) 
    &gt;&gt;&gt; beta.median(a, b) 
    array([0.15910358, 0.38572757, 0.61427243, 0.84089642]) 
 
    The Filliben approximation uses the exact medians of the smallest 
    and greatest order statistics, and the remaining medians are approximated 
    by points spread evenly across a sub-interval of the unit interval: 
 
    &gt;&gt;&gt; from scipy.stats._morestats import _calc_uniform_order_statistic_medians 
    &gt;&gt;&gt; _calc_uniform_order_statistic_medians(n) 
    array([0.15910358, 0.38545246, 0.61454754, 0.84089642]) 
 
    This plot shows the skewed distributions of the order statistics 
    of a sample of size four from a uniform distribution on the unit interval: 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, num=50, endpoint=True) 
    &gt;&gt;&gt; pdfs = [beta.pdf(x, a[i], b[i]) for i in range(n)] 
    &gt;&gt;&gt; plt.figure() 
    &gt;&gt;&gt; plt.plot(x, pdfs[0], x, pdfs[1], x, pdfs[2], x, pdfs[3]) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">v </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">)</span>
    <span class="s1">v</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">] = </span><span class="s5">0.5</span><span class="s2">**(</span><span class="s5">1.0 </span><span class="s2">/ </span><span class="s1">n</span><span class="s2">)</span>
    <span class="s1">v</span><span class="s2">[</span><span class="s5">0</span><span class="s2">] = </span><span class="s5">1 </span><span class="s2">- </span><span class="s1">v</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span>
    <span class="s1">i </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s5">2</span><span class="s2">, </span><span class="s1">n</span><span class="s2">)</span>
    <span class="s1">v</span><span class="s2">[</span><span class="s5">1</span><span class="s2">:-</span><span class="s5">1</span><span class="s2">] = (</span><span class="s1">i </span><span class="s2">- </span><span class="s5">0.3175</span><span class="s2">) / (</span><span class="s1">n </span><span class="s2">+ </span><span class="s5">0.365</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">v</span>


<span class="s0">def </span><span class="s1">_parse_dist_kw</span><span class="s2">(</span><span class="s1">dist</span><span class="s2">, </span><span class="s1">enforce_subclass</span><span class="s2">=</span><span class="s0">True</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Parse `dist` keyword. 
 
    Parameters 
    ---------- 
    dist : str or stats.distributions instance. 
        Several functions take `dist` as a keyword, hence this utility 
        function. 
    enforce_subclass : bool, optional 
        If True (default), `dist` needs to be a 
        `_distn_infrastructure.rv_generic` instance. 
        It can sometimes be useful to set this keyword to False, if a function 
        wants to accept objects that just look somewhat like such an instance 
        (for example, they have a ``ppf`` method). 
 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">dist</span><span class="s2">, </span><span class="s1">rv_generic</span><span class="s2">):</span>
        <span class="s0">pass</span>
    <span class="s0">elif </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">dist</span><span class="s2">, </span><span class="s1">str</span><span class="s2">):</span>
        <span class="s0">try</span><span class="s2">:</span>
            <span class="s1">dist </span><span class="s2">= </span><span class="s1">getattr</span><span class="s2">(</span><span class="s1">distributions</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">)</span>
        <span class="s0">except </span><span class="s1">AttributeError </span><span class="s0">as </span><span class="s1">e</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">dist</span><span class="s0">} </span><span class="s4">is not a valid distribution name&quot;</span><span class="s2">) </span><span class="s0">from </span><span class="s1">e</span>
    <span class="s0">elif </span><span class="s1">enforce_subclass</span><span class="s2">:</span>
        <span class="s1">msg </span><span class="s2">= (</span><span class="s4">&quot;`dist` should be a stats.distributions instance or a string &quot;</span>
               <span class="s4">&quot;with the name of such a distribution.&quot;</span><span class="s2">)</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">msg</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">dist</span>


<span class="s0">def </span><span class="s1">_add_axis_labels_title</span><span class="s2">(</span><span class="s1">plot</span><span class="s2">, </span><span class="s1">xlabel</span><span class="s2">, </span><span class="s1">ylabel</span><span class="s2">, </span><span class="s1">title</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Helper function to add axes labels and a title to stats plots.&quot;&quot;&quot;</span>
    <span class="s0">try</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">plot</span><span class="s2">, </span><span class="s4">'set_title'</span><span class="s2">):</span>
            <span class="s3"># Matplotlib Axes instance or something that looks like it</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">set_title</span><span class="s2">(</span><span class="s1">title</span><span class="s2">)</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">set_xlabel</span><span class="s2">(</span><span class="s1">xlabel</span><span class="s2">)</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">set_ylabel</span><span class="s2">(</span><span class="s1">ylabel</span><span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s3"># matplotlib.pyplot module</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">title</span><span class="s2">(</span><span class="s1">title</span><span class="s2">)</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">xlabel</span><span class="s2">(</span><span class="s1">xlabel</span><span class="s2">)</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">ylabel</span><span class="s2">(</span><span class="s1">ylabel</span><span class="s2">)</span>
    <span class="s0">except </span><span class="s1">Exception</span><span class="s2">:</span>
        <span class="s3"># Not an MPL object or something that looks (enough) like it.</span>
        <span class="s3"># Don't crash on adding labels or title</span>
        <span class="s0">pass</span>


<span class="s0">def </span><span class="s1">probplot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">sparams</span><span class="s2">=(), </span><span class="s1">dist</span><span class="s2">=</span><span class="s4">'norm'</span><span class="s2">, </span><span class="s1">fit</span><span class="s2">=</span><span class="s0">True</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">rvalue</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot; 
    Calculate quantiles for a probability plot, and optionally show the plot. 
 
    Generates a probability plot of sample data against the quantiles of a 
    specified theoretical distribution (the normal distribution by default). 
    `probplot` optionally calculates a best-fit line for the data and plots the 
    results using Matplotlib or a given plot function. 
 
    Parameters 
    ---------- 
    x : array_like 
        Sample/response data from which `probplot` creates the plot. 
    sparams : tuple, optional 
        Distribution-specific shape parameters (shape parameters plus location 
        and scale). 
    dist : str or stats.distributions instance, optional 
        Distribution or distribution function name. The default is 'norm' for a 
        normal probability plot.  Objects that look enough like a 
        stats.distributions instance (i.e. they have a ``ppf`` method) are also 
        accepted. 
    fit : bool, optional 
        Fit a least-squares regression (best-fit) line to the sample data if 
        True (default). 
    plot : object, optional 
        If given, plots the quantiles. 
        If given and `fit` is True, also plots the least squares fit. 
        `plot` is an object that has to have methods &quot;plot&quot; and &quot;text&quot;. 
        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used, 
        or a custom object with the same methods. 
        Default is None, which means that no plot is created. 
    rvalue : bool, optional 
        If `plot` is provided and `fit` is True, setting `rvalue` to True 
        includes the coefficient of determination on the plot. 
        Default is False. 
 
    Returns 
    ------- 
    (osm, osr) : tuple of ndarrays 
        Tuple of theoretical quantiles (osm, or order statistic medians) and 
        ordered responses (osr).  `osr` is simply sorted input `x`. 
        For details on how `osm` is calculated see the Notes section. 
    (slope, intercept, r) : tuple of floats, optional 
        Tuple  containing the result of the least-squares fit, if that is 
        performed by `probplot`. `r` is the square root of the coefficient of 
        determination.  If ``fit=False`` and ``plot=None``, this tuple is not 
        returned. 
 
    Notes 
    ----- 
    Even if `plot` is given, the figure is not shown or saved by `probplot`; 
    ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after 
    calling `probplot`. 
 
    `probplot` generates a probability plot, which should not be confused with 
    a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this 
    type, see ``statsmodels.api.ProbPlot``. 
 
    The formula used for the theoretical quantiles (horizontal axis of the 
    probability plot) is Filliben's estimate:: 
 
        quantiles = dist.ppf(val), for 
 
                0.5**(1/n),                  for i = n 
          val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1 
                1 - 0.5**(1/n),              for i = 1 
 
    where ``i`` indicates the i-th ordered value and ``n`` is the total number 
    of values. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; nsample = 100 
    &gt;&gt;&gt; rng = np.random.default_rng() 
 
    A t distribution with small degrees of freedom: 
 
    &gt;&gt;&gt; ax1 = plt.subplot(221) 
    &gt;&gt;&gt; x = stats.t.rvs(3, size=nsample, random_state=rng) 
    &gt;&gt;&gt; res = stats.probplot(x, plot=plt) 
 
    A t distribution with larger degrees of freedom: 
 
    &gt;&gt;&gt; ax2 = plt.subplot(222) 
    &gt;&gt;&gt; x = stats.t.rvs(25, size=nsample, random_state=rng) 
    &gt;&gt;&gt; res = stats.probplot(x, plot=plt) 
 
    A mixture of two normal distributions with broadcasting: 
 
    &gt;&gt;&gt; ax3 = plt.subplot(223) 
    &gt;&gt;&gt; x = stats.norm.rvs(loc=[0,5], scale=[1,1.5], 
    ...                    size=(nsample//2,2), random_state=rng).ravel() 
    &gt;&gt;&gt; res = stats.probplot(x, plot=plt) 
 
    A standard normal distribution: 
 
    &gt;&gt;&gt; ax4 = plt.subplot(224) 
    &gt;&gt;&gt; x = stats.norm.rvs(loc=0, scale=1, size=nsample, random_state=rng) 
    &gt;&gt;&gt; res = stats.probplot(x, plot=plt) 
 
    Produce a new figure with a loggamma distribution, using the ``dist`` and 
    ``sparams`` keywords: 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; x = stats.loggamma.rvs(c=2.5, size=500, random_state=rng) 
    &gt;&gt;&gt; res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax) 
    &gt;&gt;&gt; ax.set_title(&quot;Probplot for loggamma dist with shape parameter 2.5&quot;) 
 
    Show the results with Matplotlib: 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">x</span><span class="s2">.</span><span class="s1">size </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">fit</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">x</span><span class="s2">), (</span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">, </span><span class="s5">0.0</span><span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">x</span><span class="s2">, </span><span class="s1">x</span>

    <span class="s1">osm_uniform </span><span class="s2">= </span><span class="s1">_calc_uniform_order_statistic_medians</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">))</span>
    <span class="s1">dist </span><span class="s2">= </span><span class="s1">_parse_dist_kw</span><span class="s2">(</span><span class="s1">dist</span><span class="s2">, </span><span class="s1">enforce_subclass</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">sparams </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">sparams </span><span class="s2">= ()</span>
    <span class="s0">if </span><span class="s1">isscalar</span><span class="s2">(</span><span class="s1">sparams</span><span class="s2">):</span>
        <span class="s1">sparams </span><span class="s2">= (</span><span class="s1">sparams</span><span class="s2">,)</span>
    <span class="s0">if not </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">sparams</span><span class="s2">, </span><span class="s1">tuple</span><span class="s2">):</span>
        <span class="s1">sparams </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">sparams</span><span class="s2">)</span>

    <span class="s1">osm </span><span class="s2">= </span><span class="s1">dist</span><span class="s2">.</span><span class="s1">ppf</span><span class="s2">(</span><span class="s1">osm_uniform</span><span class="s2">, *</span><span class="s1">sparams</span><span class="s2">)</span>
    <span class="s1">osr </span><span class="s2">= </span><span class="s1">sort</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">fit</span><span class="s2">:</span>
        <span class="s3"># perform a linear least squares fit.</span>
        <span class="s1">slope</span><span class="s2">, </span><span class="s1">intercept</span><span class="s2">, </span><span class="s1">r</span><span class="s2">, </span><span class="s1">prob</span><span class="s2">, </span><span class="s1">_ </span><span class="s2">= </span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">linregress</span><span class="s2">(</span><span class="s1">osm</span><span class="s2">, </span><span class="s1">osr</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">plot </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s1">plot</span><span class="s2">.</span><span class="s1">plot</span><span class="s2">(</span><span class="s1">osm</span><span class="s2">, </span><span class="s1">osr</span><span class="s2">, </span><span class="s4">'bo'</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">fit</span><span class="s2">:</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">plot</span><span class="s2">(</span><span class="s1">osm</span><span class="s2">, </span><span class="s1">slope</span><span class="s2">*</span><span class="s1">osm </span><span class="s2">+ </span><span class="s1">intercept</span><span class="s2">, </span><span class="s4">'r-'</span><span class="s2">)</span>
        <span class="s1">_add_axis_labels_title</span><span class="s2">(</span><span class="s1">plot</span><span class="s2">, </span><span class="s1">xlabel</span><span class="s2">=</span><span class="s4">'Theoretical quantiles'</span><span class="s2">,</span>
                               <span class="s1">ylabel</span><span class="s2">=</span><span class="s4">'Ordered Values'</span><span class="s2">,</span>
                               <span class="s1">title</span><span class="s2">=</span><span class="s4">'Probability Plot'</span><span class="s2">)</span>

        <span class="s3"># Add R^2 value to the plot as text</span>
        <span class="s0">if </span><span class="s1">fit </span><span class="s0">and </span><span class="s1">rvalue</span><span class="s2">:</span>
            <span class="s1">xmin </span><span class="s2">= </span><span class="s1">amin</span><span class="s2">(</span><span class="s1">osm</span><span class="s2">)</span>
            <span class="s1">xmax </span><span class="s2">= </span><span class="s1">amax</span><span class="s2">(</span><span class="s1">osm</span><span class="s2">)</span>
            <span class="s1">ymin </span><span class="s2">= </span><span class="s1">amin</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
            <span class="s1">ymax </span><span class="s2">= </span><span class="s1">amax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
            <span class="s1">posx </span><span class="s2">= </span><span class="s1">xmin </span><span class="s2">+ </span><span class="s5">0.70 </span><span class="s2">* (</span><span class="s1">xmax </span><span class="s2">- </span><span class="s1">xmin</span><span class="s2">)</span>
            <span class="s1">posy </span><span class="s2">= </span><span class="s1">ymin </span><span class="s2">+ </span><span class="s5">0.01 </span><span class="s2">* (</span><span class="s1">ymax </span><span class="s2">- </span><span class="s1">ymin</span><span class="s2">)</span>
            <span class="s1">plot</span><span class="s2">.</span><span class="s1">text</span><span class="s2">(</span><span class="s1">posx</span><span class="s2">, </span><span class="s1">posy</span><span class="s2">, </span><span class="s4">&quot;$R^2=%1.4f$&quot; </span><span class="s2">% </span><span class="s1">r</span><span class="s2">**</span><span class="s5">2</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">fit</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s1">osm</span><span class="s2">, </span><span class="s1">osr</span><span class="s2">), (</span><span class="s1">slope</span><span class="s2">, </span><span class="s1">intercept</span><span class="s2">, </span><span class="s1">r</span><span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">osm</span><span class="s2">, </span><span class="s1">osr</span>


<span class="s0">def </span><span class="s1">ppcc_max</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">brack</span><span class="s2">=(</span><span class="s5">0.0</span><span class="s2">, </span><span class="s5">1.0</span><span class="s2">), </span><span class="s1">dist</span><span class="s2">=</span><span class="s4">'tukeylambda'</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Calculate the shape parameter that maximizes the PPCC. 
 
    The probability plot correlation coefficient (PPCC) plot can be used 
    to determine the optimal shape parameter for a one-parameter family 
    of distributions. ``ppcc_max`` returns the shape parameter that would 
    maximize the probability plot correlation coefficient for the given 
    data to a one-parameter family of distributions. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    brack : tuple, optional 
        Triple (a,b,c) where (a&lt;b&lt;c). If bracket consists of two numbers (a, c) 
        then they are assumed to be a starting interval for a downhill bracket 
        search (see `scipy.optimize.brent`). 
    dist : str or stats.distributions instance, optional 
        Distribution or distribution function name.  Objects that look enough 
        like a stats.distributions instance (i.e. they have a ``ppf`` method) 
        are also accepted.  The default is ``'tukeylambda'``. 
 
    Returns 
    ------- 
    shape_value : float 
        The shape parameter at which the probability plot correlation 
        coefficient reaches its max value. 
 
    See Also 
    -------- 
    ppcc_plot, probplot, boxcox 
 
    Notes 
    ----- 
    The brack keyword serves as a starting point which is useful in corner 
    cases. One can use a plot to obtain a rough visual estimate of the location 
    for the maximum to start the search near it. 
 
    References 
    ---------- 
    .. [1] J.J. Filliben, &quot;The Probability Plot Correlation Coefficient Test 
           for Normality&quot;, Technometrics, Vol. 17, pp. 111-117, 1975. 
    .. [2] Engineering Statistics Handbook, NIST/SEMATEC, 
           https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm 
 
    Examples 
    -------- 
    First we generate some random data from a Weibull distribution 
    with shape parameter 2.5: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; c = 2.5 
    &gt;&gt;&gt; x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng) 
 
    Generate the PPCC plot for this data with the Weibull distribution. 
 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 6)) 
    &gt;&gt;&gt; res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax) 
 
    We calculate the value where the shape should reach its maximum and a 
    red line is drawn there. The line should coincide with the highest 
    point in the PPCC graph. 
 
    &gt;&gt;&gt; cmax = stats.ppcc_max(x, brack=(c/2, 2*c), dist='weibull_min') 
    &gt;&gt;&gt; ax.axvline(cmax, color='r') 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">dist </span><span class="s2">= </span><span class="s1">_parse_dist_kw</span><span class="s2">(</span><span class="s1">dist</span><span class="s2">)</span>
    <span class="s1">osm_uniform </span><span class="s2">= </span><span class="s1">_calc_uniform_order_statistic_medians</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">))</span>
    <span class="s1">osr </span><span class="s2">= </span><span class="s1">sort</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>

    <span class="s3"># this function computes the x-axis values of the probability plot</span>
    <span class="s3">#  and computes a linear regression (including the correlation)</span>
    <span class="s3">#  and returns 1-r so that a minimization function maximizes the</span>
    <span class="s3">#  correlation</span>
    <span class="s0">def </span><span class="s1">tempfunc</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">mi</span><span class="s2">, </span><span class="s1">yvals</span><span class="s2">, </span><span class="s1">func</span><span class="s2">):</span>
        <span class="s1">xvals </span><span class="s2">= </span><span class="s1">func</span><span class="s2">(</span><span class="s1">mi</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s1">r</span><span class="s2">, </span><span class="s1">prob </span><span class="s2">= </span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">pearsonr</span><span class="s2">(</span><span class="s1">xvals</span><span class="s2">, </span><span class="s1">yvals</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s5">1 </span><span class="s2">- </span><span class="s1">r</span>

    <span class="s0">return </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">brent</span><span class="s2">(</span><span class="s1">tempfunc</span><span class="s2">, </span><span class="s1">brack</span><span class="s2">=</span><span class="s1">brack</span><span class="s2">,</span>
                          <span class="s1">args</span><span class="s2">=(</span><span class="s1">osm_uniform</span><span class="s2">, </span><span class="s1">osr</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">.</span><span class="s1">ppf</span><span class="s2">))</span>


<span class="s0">def </span><span class="s1">ppcc_plot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">=</span><span class="s4">'tukeylambda'</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">N</span><span class="s2">=</span><span class="s5">80</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Calculate and optionally plot probability plot correlation coefficient. 
 
    The probability plot correlation coefficient (PPCC) plot can be used to 
    determine the optimal shape parameter for a one-parameter family of 
    distributions.  It cannot be used for distributions without shape 
    parameters 
    (like the normal distribution) or with multiple shape parameters. 
 
    By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A 
    Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed 
    distributions via an approximately normal one, and is therefore 
    particularly useful in practice. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    a, b : scalar 
        Lower and upper bounds of the shape parameter to use. 
    dist : str or stats.distributions instance, optional 
        Distribution or distribution function name.  Objects that look enough 
        like a stats.distributions instance (i.e. they have a ``ppf`` method) 
        are also accepted.  The default is ``'tukeylambda'``. 
    plot : object, optional 
        If given, plots PPCC against the shape parameter. 
        `plot` is an object that has to have methods &quot;plot&quot; and &quot;text&quot;. 
        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used, 
        or a custom object with the same methods. 
        Default is None, which means that no plot is created. 
    N : int, optional 
        Number of points on the horizontal axis (equally distributed from 
        `a` to `b`). 
 
    Returns 
    ------- 
    svals : ndarray 
        The shape values for which `ppcc` was calculated. 
    ppcc : ndarray 
        The calculated probability plot correlation coefficient values. 
 
    See Also 
    -------- 
    ppcc_max, probplot, boxcox_normplot, tukeylambda 
 
    References 
    ---------- 
    J.J. Filliben, &quot;The Probability Plot Correlation Coefficient Test for 
    Normality&quot;, Technometrics, Vol. 17, pp. 111-117, 1975. 
 
    Examples 
    -------- 
    First we generate some random data from a Weibull distribution 
    with shape parameter 2.5, and plot the histogram of the data: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; c = 2.5 
    &gt;&gt;&gt; x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng) 
 
    Take a look at the histogram of the data. 
 
    &gt;&gt;&gt; fig1, ax = plt.subplots(figsize=(9, 4)) 
    &gt;&gt;&gt; ax.hist(x, bins=50) 
    &gt;&gt;&gt; ax.set_title('Histogram of x') 
    &gt;&gt;&gt; plt.show() 
 
    Now we explore this data with a PPCC plot as well as the related 
    probability plot and Box-Cox normplot.  A red line is drawn where we 
    expect the PPCC value to be maximal (at the shape parameter ``c`` 
    used above): 
 
    &gt;&gt;&gt; fig2 = plt.figure(figsize=(12, 4)) 
    &gt;&gt;&gt; ax1 = fig2.add_subplot(1, 3, 1) 
    &gt;&gt;&gt; ax2 = fig2.add_subplot(1, 3, 2) 
    &gt;&gt;&gt; ax3 = fig2.add_subplot(1, 3, 3) 
    &gt;&gt;&gt; res = stats.probplot(x, plot=ax1) 
    &gt;&gt;&gt; res = stats.boxcox_normplot(x, -4, 4, plot=ax2) 
    &gt;&gt;&gt; res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax3) 
    &gt;&gt;&gt; ax3.axvline(c, color='r') 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">b </span><span class="s2">&lt;= </span><span class="s1">a</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`b` has to be larger than `a`.&quot;</span><span class="s2">)</span>

    <span class="s1">svals </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">linspace</span><span class="s2">(</span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">num</span><span class="s2">=</span><span class="s1">N</span><span class="s2">)</span>
    <span class="s1">ppcc </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty_like</span><span class="s2">(</span><span class="s1">svals</span><span class="s2">)</span>
    <span class="s0">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">sval </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">svals</span><span class="s2">):</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">r2 </span><span class="s2">= </span><span class="s1">probplot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">sval</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">=</span><span class="s1">dist</span><span class="s2">, </span><span class="s1">fit</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
        <span class="s1">ppcc</span><span class="s2">[</span><span class="s1">k</span><span class="s2">] = </span><span class="s1">r2</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span>

    <span class="s0">if </span><span class="s1">plot </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s1">plot</span><span class="s2">.</span><span class="s1">plot</span><span class="s2">(</span><span class="s1">svals</span><span class="s2">, </span><span class="s1">ppcc</span><span class="s2">, </span><span class="s4">'x'</span><span class="s2">)</span>
        <span class="s1">_add_axis_labels_title</span><span class="s2">(</span><span class="s1">plot</span><span class="s2">, </span><span class="s1">xlabel</span><span class="s2">=</span><span class="s4">'Shape Values'</span><span class="s2">,</span>
                               <span class="s1">ylabel</span><span class="s2">=</span><span class="s4">'Prob Plot Corr. Coef.'</span><span class="s2">,</span>
                               <span class="s1">title</span><span class="s2">=</span><span class="s4">f'(</span><span class="s0">{</span><span class="s1">dist</span><span class="s0">}</span><span class="s4">) PPCC Plot'</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">svals</span><span class="s2">, </span><span class="s1">ppcc</span>


<span class="s0">def </span><span class="s1">_log_mean</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">):</span>
    <span class="s3"># compute log of mean of x from log(x)</span>
    <span class="s0">return </span><span class="s1">special</span><span class="s2">.</span><span class="s1">logsumexp</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">) - </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">))</span>


<span class="s0">def </span><span class="s1">_log_var</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">):</span>
    <span class="s3"># compute log of variance of x from log(x)</span>
    <span class="s1">logmean </span><span class="s2">= </span><span class="s1">_log_mean</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">)</span>
    <span class="s1">pij </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">full_like</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">pi </span><span class="s2">* </span><span class="s5">1j</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">np</span><span class="s2">.</span><span class="s1">complex128</span><span class="s2">)</span>
    <span class="s1">logxmu </span><span class="s2">= </span><span class="s1">special</span><span class="s2">.</span><span class="s1">logsumexp</span><span class="s2">([</span><span class="s1">logx</span><span class="s2">, </span><span class="s1">logmean </span><span class="s2">+ </span><span class="s1">pij</span><span class="s2">], </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">real</span><span class="s2">(</span><span class="s1">special</span><span class="s2">.</span><span class="s1">logsumexp</span><span class="s2">(</span><span class="s5">2 </span><span class="s2">* </span><span class="s1">logxmu</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)) - </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">))</span>


<span class="s0">def </span><span class="s1">boxcox_llf</span><span class="s2">(</span><span class="s1">lmb</span><span class="s2">, </span><span class="s1">data</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;The boxcox log-likelihood function. 
 
    Parameters 
    ---------- 
    lmb : scalar 
        Parameter for Box-Cox transformation.  See `boxcox` for details. 
    data : array_like 
        Data to calculate Box-Cox log-likelihood for.  If `data` is 
        multi-dimensional, the log-likelihood is calculated along the first 
        axis. 
 
    Returns 
    ------- 
    llf : float or ndarray 
        Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`, 
        an array otherwise. 
 
    See Also 
    -------- 
    boxcox, probplot, boxcox_normplot, boxcox_normmax 
 
    Notes 
    ----- 
    The Box-Cox log-likelihood function is defined here as 
 
    .. math:: 
 
        llf = (\lambda - 1) \sum_i(\log(x_i)) - 
              N/2 \log(\sum_i (y_i - \bar{y})^2 / N), 
 
    where ``y`` is the Box-Cox transformed input data ``x``. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; from mpl_toolkits.axes_grid1.inset_locator import inset_axes 
 
    Generate some random variates and calculate Box-Cox log-likelihood values 
    for them for a range of ``lmbda`` values: 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, loc=10, size=1000, random_state=rng) 
    &gt;&gt;&gt; lmbdas = np.linspace(-2, 10) 
    &gt;&gt;&gt; llf = np.zeros(lmbdas.shape, dtype=float) 
    &gt;&gt;&gt; for ii, lmbda in enumerate(lmbdas): 
    ...     llf[ii] = stats.boxcox_llf(lmbda, x) 
 
    Also find the optimal lmbda value with `boxcox`: 
 
    &gt;&gt;&gt; x_most_normal, lmbda_optimal = stats.boxcox(x) 
 
    Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a 
    horizontal line to check that that's really the optimum: 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; ax.plot(lmbdas, llf, 'b.-') 
    &gt;&gt;&gt; ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r') 
    &gt;&gt;&gt; ax.set_xlabel('lmbda parameter') 
    &gt;&gt;&gt; ax.set_ylabel('Box-Cox log-likelihood') 
 
    Now add some probability plots to show that where the log-likelihood is 
    maximized the data transformed with `boxcox` looks closest to normal: 
 
    &gt;&gt;&gt; locs = [3, 10, 4]  # 'lower left', 'center', 'lower right' 
    &gt;&gt;&gt; for lmbda, loc in zip([-1, lmbda_optimal, 9], locs): 
    ...     xt = stats.boxcox(x, lmbda=lmbda) 
    ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt) 
    ...     ax_inset = inset_axes(ax, width=&quot;20%&quot;, height=&quot;20%&quot;, loc=loc) 
    ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-') 
    ...     ax_inset.set_xticklabels([]) 
    ...     ax_inset.set_yticklabels([]) 
    ...     ax_inset.set_title(r'$\lambda=%1.2f$' % lmbda) 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">data </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s1">data</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">0</span><span class="s2">]</span>
    <span class="s0">if </span><span class="s1">N </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

    <span class="s1">logdata </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>

    <span class="s3"># Compute the variance of the transformed data.</span>
    <span class="s0">if </span><span class="s1">lmb </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s1">logvar </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">logdata</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s3"># Transform without the constant offset 1/lmb.  The offset does</span>
        <span class="s3"># not affect the variance, and the subtraction of the offset can</span>
        <span class="s3"># lead to loss of precision.</span>
        <span class="s3"># Division by lmb can be factored out to enhance numerical stability.</span>
        <span class="s1">logx </span><span class="s2">= </span><span class="s1">lmb </span><span class="s2">* </span><span class="s1">logdata</span>
        <span class="s1">logvar </span><span class="s2">= </span><span class="s1">_log_var</span><span class="s2">(</span><span class="s1">logx</span><span class="s2">) - </span><span class="s5">2 </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">abs</span><span class="s2">(</span><span class="s1">lmb</span><span class="s2">))</span>

    <span class="s0">return </span><span class="s2">(</span><span class="s1">lmb </span><span class="s2">- </span><span class="s5">1</span><span class="s2">) * </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">logdata</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">) - </span><span class="s1">N</span><span class="s2">/</span><span class="s5">2 </span><span class="s2">* </span><span class="s1">logvar</span>


<span class="s0">def </span><span class="s1">_boxcox_conf_interval</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">):</span>
    <span class="s3"># Need to find the lambda for which</span>
    <span class="s3">#  f(x,lmbda) &gt;= f(x,lmax) - 0.5*chi^2_alpha;1</span>
    <span class="s1">fac </span><span class="s2">= </span><span class="s5">0.5 </span><span class="s2">* </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">chi2</span><span class="s2">.</span><span class="s1">ppf</span><span class="s2">(</span><span class="s5">1 </span><span class="s2">- </span><span class="s1">alpha</span><span class="s2">, </span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">target </span><span class="s2">= </span><span class="s1">boxcox_llf</span><span class="s2">(</span><span class="s1">lmax</span><span class="s2">, </span><span class="s1">x</span><span class="s2">) - </span><span class="s1">fac</span>

    <span class="s0">def </span><span class="s1">rootfunc</span><span class="s2">(</span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">target</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">boxcox_llf</span><span class="s2">(</span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">data</span><span class="s2">) - </span><span class="s1">target</span>

    <span class="s3"># Find positive endpoint of interval in which answer is to be found</span>
    <span class="s1">newlm </span><span class="s2">= </span><span class="s1">lmax </span><span class="s2">+ </span><span class="s5">0.5</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s5">0</span>
    <span class="s0">while </span><span class="s2">(</span><span class="s1">rootfunc</span><span class="s2">(</span><span class="s1">newlm</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">target</span><span class="s2">) &gt; </span><span class="s5">0.0</span><span class="s2">) </span><span class="s0">and </span><span class="s2">(</span><span class="s1">N </span><span class="s2">&lt; </span><span class="s5">500</span><span class="s2">):</span>
        <span class="s1">newlm </span><span class="s2">+= </span><span class="s5">0.1</span>
        <span class="s1">N </span><span class="s2">+= </span><span class="s5">1</span>

    <span class="s0">if </span><span class="s1">N </span><span class="s2">== </span><span class="s5">500</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">RuntimeError</span><span class="s2">(</span><span class="s4">&quot;Could not find endpoint.&quot;</span><span class="s2">)</span>

    <span class="s1">lmplus </span><span class="s2">= </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">brentq</span><span class="s2">(</span><span class="s1">rootfunc</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">, </span><span class="s1">newlm</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">target</span><span class="s2">))</span>

    <span class="s3"># Now find negative interval in the same way</span>
    <span class="s1">newlm </span><span class="s2">= </span><span class="s1">lmax </span><span class="s2">- </span><span class="s5">0.5</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s5">0</span>
    <span class="s0">while </span><span class="s2">(</span><span class="s1">rootfunc</span><span class="s2">(</span><span class="s1">newlm</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">target</span><span class="s2">) &gt; </span><span class="s5">0.0</span><span class="s2">) </span><span class="s0">and </span><span class="s2">(</span><span class="s1">N </span><span class="s2">&lt; </span><span class="s5">500</span><span class="s2">):</span>
        <span class="s1">newlm </span><span class="s2">-= </span><span class="s5">0.1</span>
        <span class="s1">N </span><span class="s2">+= </span><span class="s5">1</span>

    <span class="s0">if </span><span class="s1">N </span><span class="s2">== </span><span class="s5">500</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">RuntimeError</span><span class="s2">(</span><span class="s4">&quot;Could not find endpoint.&quot;</span><span class="s2">)</span>

    <span class="s1">lmminus </span><span class="s2">= </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">brentq</span><span class="s2">(</span><span class="s1">rootfunc</span><span class="s2">, </span><span class="s1">newlm</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">target</span><span class="s2">))</span>
    <span class="s0">return </span><span class="s1">lmminus</span><span class="s2">, </span><span class="s1">lmplus</span>


<span class="s0">def </span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">optimizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Return a dataset transformed by a Box-Cox power transformation. 
 
    Parameters 
    ---------- 
    x : ndarray 
        Input array to be transformed. 
 
        If `lmbda` is not None, this is an alias of 
        `scipy.special.boxcox`. 
        Returns nan if ``x &lt; 0``; returns -inf if ``x == 0 and lmbda &lt; 0``. 
 
        If `lmbda` is None, array must be positive, 1-dimensional, and 
        non-constant. 
 
    lmbda : scalar, optional 
        If `lmbda` is None (default), find the value of `lmbda` that maximizes 
        the log-likelihood function and return it as the second output 
        argument. 
 
        If `lmbda` is not None, do the transformation for that value. 
 
    alpha : float, optional 
        If `lmbda` is None and `alpha` is not None (default), return the 
        ``100 * (1-alpha)%`` confidence  interval for `lmbda` as the third 
        output argument. Must be between 0.0 and 1.0. 
 
        If `lmbda` is not None, `alpha` is ignored. 
    optimizer : callable, optional 
        If `lmbda` is None, `optimizer` is the scalar optimizer used to find 
        the value of `lmbda` that minimizes the negative log-likelihood 
        function. `optimizer` is a callable that accepts one argument: 
 
        fun : callable 
            The objective function, which evaluates the negative 
            log-likelihood function at a provided value of `lmbda` 
 
        and returns an object, such as an instance of 
        `scipy.optimize.OptimizeResult`, which holds the optimal value of 
        `lmbda` in an attribute `x`. 
 
        See the example in `boxcox_normmax` or the documentation of 
        `scipy.optimize.minimize_scalar` for more information. 
 
        If `lmbda` is not None, `optimizer` is ignored. 
 
    Returns 
    ------- 
    boxcox : ndarray 
        Box-Cox power transformed array. 
    maxlog : float, optional 
        If the `lmbda` parameter is None, the second returned argument is 
        the `lmbda` that maximizes the log-likelihood function. 
    (min_ci, max_ci) : tuple of float, optional 
        If `lmbda` parameter is None and `alpha` is not None, this returned 
        tuple of floats represents the minimum and maximum confidence limits 
        given `alpha`. 
 
    See Also 
    -------- 
    probplot, boxcox_normplot, boxcox_normmax, boxcox_llf 
 
    Notes 
    ----- 
    The Box-Cox transform is given by:: 
 
        y = (x**lmbda - 1) / lmbda,  for lmbda != 0 
            log(x),                  for lmbda = 0 
 
    `boxcox` requires the input data to be positive.  Sometimes a Box-Cox 
    transformation provides a shift parameter to achieve this; `boxcox` does 
    not.  Such a shift parameter is equivalent to adding a positive constant to 
    `x` before calling `boxcox`. 
 
    The confidence limits returned when `alpha` is provided give the interval 
    where: 
 
    .. math:: 
 
        llf(\hat{\lambda}) - llf(\lambda) &lt; \frac{1}{2}\chi^2(1 - \alpha, 1), 
 
    with ``llf`` the log-likelihood function and :math:`\chi^2` the chi-squared 
    function. 
 
    References 
    ---------- 
    G.E.P. Box and D.R. Cox, &quot;An Analysis of Transformations&quot;, Journal of the 
    Royal Statistical Society B, 26, 211-252 (1964). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
 
    We generate some random variates from a non-normal distribution and make a 
    probability plot for it, to show it is non-normal in the tails: 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax1 = fig.add_subplot(211) 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, size=500) + 5 
    &gt;&gt;&gt; prob = stats.probplot(x, dist=stats.norm, plot=ax1) 
    &gt;&gt;&gt; ax1.set_xlabel('') 
    &gt;&gt;&gt; ax1.set_title('Probplot against normal distribution') 
 
    We now use `boxcox` to transform the data so it's closest to normal: 
 
    &gt;&gt;&gt; ax2 = fig.add_subplot(212) 
    &gt;&gt;&gt; xt, _ = stats.boxcox(x) 
    &gt;&gt;&gt; prob = stats.probplot(xt, dist=stats.norm, plot=ax2) 
    &gt;&gt;&gt; ax2.set_title('Probplot after Box-Cox transformation') 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">lmbda </span><span class="s0">is not None</span><span class="s2">:  </span><span class="s3"># single transformation</span>
        <span class="s0">return </span><span class="s1">special</span><span class="s2">.</span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">x</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">!= </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Data must be 1-dimensional.&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">x</span><span class="s2">.</span><span class="s1">size </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span><span class="s1">x </span><span class="s2">== </span><span class="s1">x</span><span class="s2">[</span><span class="s5">0</span><span class="s2">]):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Data must not be constant.&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">any</span><span class="s2">(</span><span class="s1">x </span><span class="s2">&lt;= </span><span class="s5">0</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Data must be positive.&quot;</span><span class="s2">)</span>

    <span class="s3"># If lmbda=None, find the lmbda that maximizes the log-likelihood function.</span>
    <span class="s1">lmax </span><span class="s2">= </span><span class="s1">boxcox_normmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">method</span><span class="s2">=</span><span class="s4">'mle'</span><span class="s2">, </span><span class="s1">optimizer</span><span class="s2">=</span><span class="s1">optimizer</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">alpha </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">y</span><span class="s2">, </span><span class="s1">lmax</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s3"># Find confidence interval</span>
        <span class="s1">interval </span><span class="s2">= </span><span class="s1">_boxcox_conf_interval</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">y</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">, </span><span class="s1">interval</span>


<span class="s0">def </span><span class="s1">_boxcox_inv_lmbda</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">):</span>
    <span class="s3"># compute lmbda given x and y for Box-Cox transformation</span>
    <span class="s1">num </span><span class="s2">= </span><span class="s1">special</span><span class="s2">.</span><span class="s1">lambertw</span><span class="s2">(-(</span><span class="s1">x </span><span class="s2">** (-</span><span class="s5">1 </span><span class="s2">/ </span><span class="s1">y</span><span class="s2">)) * </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">x</span><span class="s2">) / </span><span class="s1">y</span><span class="s2">, </span><span class="s1">k</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">real</span><span class="s2">(-</span><span class="s1">num </span><span class="s2">/ </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">x</span><span class="s2">) - </span><span class="s5">1 </span><span class="s2">/ </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">class </span><span class="s1">_BigFloat</span><span class="s2">:</span>
    <span class="s0">def </span><span class="s1">__repr__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s4">&quot;BIG_FLOAT&quot;</span>


<span class="s0">def </span><span class="s1">boxcox_normmax</span><span class="s2">(</span>
    <span class="s1">x</span><span class="s2">, </span><span class="s1">brack</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">method</span><span class="s2">=</span><span class="s4">'pearsonr'</span><span class="s2">, </span><span class="s1">optimizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, *, </span><span class="s1">ymax</span><span class="s2">=</span><span class="s1">_BigFloat</span><span class="s2">()</span>
<span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute optimal Box-Cox transform parameter for input data. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. All entries must be positive, finite, real numbers. 
    brack : 2-tuple, optional, default (-2.0, 2.0) 
         The starting interval for a downhill bracket search for the default 
         `optimize.brent` solver. Note that this is in most cases not 
         critical; the final result is allowed to be outside this bracket. 
         If `optimizer` is passed, `brack` must be None. 
    method : str, optional 
        The method to determine the optimal transform parameter (`boxcox` 
        ``lmbda`` parameter). Options are: 
 
        'pearsonr'  (default) 
            Maximizes the Pearson correlation coefficient between 
            ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be 
            normally-distributed. 
 
        'mle' 
            Maximizes the log-likelihood `boxcox_llf`.  This is the method used 
            in `boxcox`. 
 
        'all' 
            Use all optimization methods available, and return all results. 
            Useful to compare different methods. 
    optimizer : callable, optional 
        `optimizer` is a callable that accepts one argument: 
 
        fun : callable 
            The objective function to be minimized. `fun` accepts one argument, 
            the Box-Cox transform parameter `lmbda`, and returns the value of 
            the function (e.g., the negative log-likelihood) at the provided 
            argument. The job of `optimizer` is to find the value of `lmbda` 
            that *minimizes* `fun`. 
 
        and returns an object, such as an instance of 
        `scipy.optimize.OptimizeResult`, which holds the optimal value of 
        `lmbda` in an attribute `x`. 
 
        See the example below or the documentation of 
        `scipy.optimize.minimize_scalar` for more information. 
    ymax : float, optional 
        The unconstrained optimal transform parameter may cause Box-Cox 
        transformed data to have extreme magnitude or even overflow. 
        This parameter constrains MLE optimization such that the magnitude 
        of the transformed `x` does not exceed `ymax`. The default is 
        the maximum value of the input dtype. If set to infinity, 
        `boxcox_normmax` returns the unconstrained optimal lambda. 
        Ignored when ``method='pearsonr'``. 
 
    Returns 
    ------- 
    maxlog : float or ndarray 
        The optimal transform parameter found.  An array instead of a scalar 
        for ``method='all'``. 
 
    See Also 
    -------- 
    boxcox, boxcox_llf, boxcox_normplot, scipy.optimize.minimize_scalar 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
 
    We can generate some data and determine the optimal ``lmbda`` in various 
    ways: 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5 
    &gt;&gt;&gt; y, lmax_mle = stats.boxcox(x) 
    &gt;&gt;&gt; lmax_pearsonr = stats.boxcox_normmax(x) 
 
    &gt;&gt;&gt; lmax_mle 
    2.217563431465757 
    &gt;&gt;&gt; lmax_pearsonr 
    2.238318660200961 
    &gt;&gt;&gt; stats.boxcox_normmax(x, method='all') 
    array([2.23831866, 2.21756343]) 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; prob = stats.boxcox_normplot(x, -10, 10, plot=ax) 
    &gt;&gt;&gt; ax.axvline(lmax_mle, color='r') 
    &gt;&gt;&gt; ax.axvline(lmax_pearsonr, color='g', ls='--') 
 
    &gt;&gt;&gt; plt.show() 
 
    Alternatively, we can define our own `optimizer` function. Suppose we 
    are only interested in values of `lmbda` on the interval [6, 7], we 
    want to use `scipy.optimize.minimize_scalar` with ``method='bounded'``, 
    and we want to use tighter tolerances when optimizing the log-likelihood 
    function. To do this, we define a function that accepts positional argument 
    `fun` and uses `scipy.optimize.minimize_scalar` to minimize `fun` subject 
    to the provided bounds and tolerances: 
 
    &gt;&gt;&gt; from scipy import optimize 
    &gt;&gt;&gt; options = {'xatol': 1e-12}  # absolute tolerance on `x` 
    &gt;&gt;&gt; def optimizer(fun): 
    ...     return optimize.minimize_scalar(fun, bounds=(6, 7), 
    ...                                     method=&quot;bounded&quot;, options=options) 
    &gt;&gt;&gt; stats.boxcox_normmax(x, optimizer=optimizer) 
    6.000000000 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>

    <span class="s0">if not </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">isfinite</span><span class="s2">(</span><span class="s1">x</span><span class="s2">) &amp; (</span><span class="s1">x </span><span class="s2">&gt;= </span><span class="s5">0</span><span class="s2">)):</span>
        <span class="s1">message </span><span class="s2">= (</span><span class="s4">&quot;The `x` argument of `boxcox_normmax` must contain &quot;</span>
                   <span class="s4">&quot;only positive, finite, real numbers.&quot;</span><span class="s2">)</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">message</span><span class="s2">)</span>

    <span class="s1">end_msg </span><span class="s2">= </span><span class="s4">&quot;exceed specified `ymax`.&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">ymax</span><span class="s2">, </span><span class="s1">_BigFloat</span><span class="s2">):</span>
        <span class="s1">dtype </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype </span><span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">floating</span><span class="s2">) </span><span class="s0">else </span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span>
        <span class="s3"># 10000 is a safety factor because `special.boxcox` overflows prematurely.</span>
        <span class="s1">ymax </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s1">dtype</span><span class="s2">).</span><span class="s1">max </span><span class="s2">/ </span><span class="s5">10000</span>
        <span class="s1">end_msg </span><span class="s2">= </span><span class="s4">f&quot;overflow in </span><span class="s0">{</span><span class="s1">dtype</span><span class="s0">}</span><span class="s4">.&quot;</span>
    <span class="s0">elif </span><span class="s1">ymax </span><span class="s2">&lt;= </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`ymax` must be strictly positive&quot;</span><span class="s2">)</span>

    <span class="s3"># If optimizer is not given, define default 'brent' optimizer.</span>
    <span class="s0">if </span><span class="s1">optimizer </span><span class="s0">is None</span><span class="s2">:</span>

        <span class="s3"># Set default value for `brack`.</span>
        <span class="s0">if </span><span class="s1">brack </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">brack </span><span class="s2">= (-</span><span class="s5">2.0</span><span class="s2">, </span><span class="s5">2.0</span><span class="s2">)</span>

        <span class="s0">def </span><span class="s1">_optimizer</span><span class="s2">(</span><span class="s1">func</span><span class="s2">, </span><span class="s1">args</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">brent</span><span class="s2">(</span><span class="s1">func</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=</span><span class="s1">args</span><span class="s2">, </span><span class="s1">brack</span><span class="s2">=</span><span class="s1">brack</span><span class="s2">)</span>

    <span class="s3"># Otherwise check optimizer.</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">if not </span><span class="s1">callable</span><span class="s2">(</span><span class="s1">optimizer</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`optimizer` must be a callable&quot;</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">brack </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`brack` must be None if `optimizer` is given&quot;</span><span class="s2">)</span>

        <span class="s3"># `optimizer` is expected to return a `OptimizeResult` object, we here</span>
        <span class="s3"># get the solution to the optimization problem.</span>
        <span class="s0">def </span><span class="s1">_optimizer</span><span class="s2">(</span><span class="s1">func</span><span class="s2">, </span><span class="s1">args</span><span class="s2">):</span>
            <span class="s0">def </span><span class="s1">func_wrapped</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
                <span class="s0">return </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, *</span><span class="s1">args</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">getattr</span><span class="s2">(</span><span class="s1">optimizer</span><span class="s2">(</span><span class="s1">func_wrapped</span><span class="s2">), </span><span class="s4">'x'</span><span class="s2">, </span><span class="s0">None</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_pearsonr</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
        <span class="s1">osm_uniform </span><span class="s2">= </span><span class="s1">_calc_uniform_order_statistic_medians</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">))</span>
        <span class="s1">xvals </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">.</span><span class="s1">ppf</span><span class="s2">(</span><span class="s1">osm_uniform</span><span class="s2">)</span>

        <span class="s0">def </span><span class="s1">_eval_pearsonr</span><span class="s2">(</span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">xvals</span><span class="s2">, </span><span class="s1">samps</span><span class="s2">):</span>
            <span class="s3"># This function computes the x-axis values of the probability plot</span>
            <span class="s3"># and computes a linear regression (including the correlation) and</span>
            <span class="s3"># returns ``1 - r`` so that a minimization function maximizes the</span>
            <span class="s3"># correlation.</span>
            <span class="s1">y </span><span class="s2">= </span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">samps</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">)</span>
            <span class="s1">yvals </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
            <span class="s1">r</span><span class="s2">, </span><span class="s1">prob </span><span class="s2">= </span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">pearsonr</span><span class="s2">(</span><span class="s1">xvals</span><span class="s2">, </span><span class="s1">yvals</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s5">1 </span><span class="s2">- </span><span class="s1">r</span>

        <span class="s0">return </span><span class="s1">_optimizer</span><span class="s2">(</span><span class="s1">_eval_pearsonr</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">xvals</span><span class="s2">, </span><span class="s1">x</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">_mle</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">def </span><span class="s1">_eval_mle</span><span class="s2">(</span><span class="s1">lmb</span><span class="s2">, </span><span class="s1">data</span><span class="s2">):</span>
            <span class="s3"># function to minimize</span>
            <span class="s0">return </span><span class="s2">-</span><span class="s1">boxcox_llf</span><span class="s2">(</span><span class="s1">lmb</span><span class="s2">, </span><span class="s1">data</span><span class="s2">)</span>

        <span class="s0">return </span><span class="s1">_optimizer</span><span class="s2">(</span><span class="s1">_eval_mle</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">x</span><span class="s2">,))</span>

    <span class="s0">def </span><span class="s1">_all</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
        <span class="s1">maxlog </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty</span><span class="s2">(</span><span class="s5">2</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">float</span><span class="s2">)</span>
        <span class="s1">maxlog</span><span class="s2">[</span><span class="s5">0</span><span class="s2">] = </span><span class="s1">_pearsonr</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s1">maxlog</span><span class="s2">[</span><span class="s5">1</span><span class="s2">] = </span><span class="s1">_mle</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">maxlog</span>

    <span class="s1">methods </span><span class="s2">= {</span><span class="s4">'pearsonr'</span><span class="s2">: </span><span class="s1">_pearsonr</span><span class="s2">,</span>
               <span class="s4">'mle'</span><span class="s2">: </span><span class="s1">_mle</span><span class="s2">,</span>
               <span class="s4">'all'</span><span class="s2">: </span><span class="s1">_all</span><span class="s2">}</span>
    <span class="s0">if </span><span class="s1">method </span><span class="s0">not in </span><span class="s1">methods</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">():</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;Method </span><span class="s0">{</span><span class="s1">method</span><span class="s0">} </span><span class="s4">not recognized.&quot;</span><span class="s2">)</span>

    <span class="s1">optimfunc </span><span class="s2">= </span><span class="s1">methods</span><span class="s2">[</span><span class="s1">method</span><span class="s2">]</span>

    <span class="s1">res </span><span class="s2">= </span><span class="s1">optimfunc</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">res </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">message </span><span class="s2">= (</span><span class="s4">&quot;The `optimizer` argument of `boxcox_normmax` must return &quot;</span>
                   <span class="s4">&quot;an object containing the optimal `lmbda` in attribute `x`.&quot;</span><span class="s2">)</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">message</span><span class="s2">)</span>
    <span class="s0">elif not </span><span class="s1">np</span><span class="s2">.</span><span class="s1">isinf</span><span class="s2">(</span><span class="s1">ymax</span><span class="s2">):  </span><span class="s3"># adjust the final lambda</span>
        <span class="s3"># x &gt; 1, boxcox(x) &gt; 0; x &lt; 1, boxcox(x) &lt; 0</span>
        <span class="s1">xmax</span><span class="s2">, </span><span class="s1">xmin </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">max</span><span class="s2">(</span><span class="s1">x</span><span class="s2">), </span><span class="s1">np</span><span class="s2">.</span><span class="s1">min</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">xmin </span><span class="s2">&gt;= </span><span class="s5">1</span><span class="s2">:</span>
            <span class="s1">x_treme </span><span class="s2">= </span><span class="s1">xmax</span>
        <span class="s0">elif </span><span class="s1">xmax </span><span class="s2">&lt;= </span><span class="s5">1</span><span class="s2">:</span>
            <span class="s1">x_treme </span><span class="s2">= </span><span class="s1">xmin</span>
        <span class="s0">else</span><span class="s2">:  </span><span class="s3"># xmin &lt; 1 &lt; xmax</span>
            <span class="s1">indicator </span><span class="s2">= </span><span class="s1">special</span><span class="s2">.</span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">xmax</span><span class="s2">, </span><span class="s1">res</span><span class="s2">) &gt; </span><span class="s1">abs</span><span class="s2">(</span><span class="s1">special</span><span class="s2">.</span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">xmin</span><span class="s2">, </span><span class="s1">res</span><span class="s2">))</span>
            <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">res</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ndarray</span><span class="s2">):</span>
                <span class="s1">indicator </span><span class="s2">= </span><span class="s1">indicator</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]  </span><span class="s3"># select corresponds with 'mle'</span>
            <span class="s1">x_treme </span><span class="s2">= </span><span class="s1">xmax </span><span class="s0">if </span><span class="s1">indicator </span><span class="s0">else </span><span class="s1">xmin</span>

        <span class="s1">mask </span><span class="s2">= </span><span class="s1">abs</span><span class="s2">(</span><span class="s1">special</span><span class="s2">.</span><span class="s1">boxcox</span><span class="s2">(</span><span class="s1">x_treme</span><span class="s2">, </span><span class="s1">res</span><span class="s2">)) &gt; </span><span class="s1">ymax</span>
        <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">any</span><span class="s2">(</span><span class="s1">mask</span><span class="s2">):</span>
            <span class="s1">message </span><span class="s2">= (</span>
                <span class="s4">f&quot;The optimal lambda is </span><span class="s0">{</span><span class="s1">res</span><span class="s0">}</span><span class="s4">, but the returned lambda is the &quot;</span>
                <span class="s4">f&quot;constrained optimum to ensure that the maximum or the minimum &quot;</span>
                <span class="s4">f&quot;of the transformed data does not &quot; </span><span class="s2">+ </span><span class="s1">end_msg</span>
            <span class="s2">)</span>
            <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s1">message</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>

            <span class="s3"># Return the constrained lambda to ensure the transformation</span>
            <span class="s3"># does not cause overflow or exceed specified `ymax`</span>
            <span class="s1">constrained_res </span><span class="s2">= </span><span class="s1">_boxcox_inv_lmbda</span><span class="s2">(</span><span class="s1">x_treme</span><span class="s2">, </span><span class="s1">ymax </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sign</span><span class="s2">(</span><span class="s1">x_treme </span><span class="s2">- </span><span class="s5">1</span><span class="s2">))</span>

            <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">res</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ndarray</span><span class="s2">):</span>
                <span class="s1">res</span><span class="s2">[</span><span class="s1">mask</span><span class="s2">] = </span><span class="s1">constrained_res</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">res </span><span class="s2">= </span><span class="s1">constrained_res</span>
    <span class="s0">return </span><span class="s1">res</span>


<span class="s0">def </span><span class="s1">_normplot</span><span class="s2">(</span><span class="s1">method</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">N</span><span class="s2">=</span><span class="s5">80</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute parameters for a Box-Cox or Yeo-Johnson normality plot, 
    optionally show it. 
 
    See `boxcox_normplot` or `yeojohnson_normplot` for details. 
    &quot;&quot;&quot;</span>

    <span class="s0">if </span><span class="s1">method </span><span class="s2">== </span><span class="s4">'boxcox'</span><span class="s2">:</span>
        <span class="s1">title </span><span class="s2">= </span><span class="s4">'Box-Cox Normality Plot'</span>
        <span class="s1">transform_func </span><span class="s2">= </span><span class="s1">boxcox</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">title </span><span class="s2">= </span><span class="s4">'Yeo-Johnson Normality Plot'</span>
        <span class="s1">transform_func </span><span class="s2">= </span><span class="s1">yeojohnson</span>

    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">x</span><span class="s2">.</span><span class="s1">size </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">if </span><span class="s1">lb </span><span class="s2">&lt;= </span><span class="s1">la</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`lb` has to be larger than `la`.&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">method </span><span class="s2">== </span><span class="s4">'boxcox' </span><span class="s0">and </span><span class="s1">np</span><span class="s2">.</span><span class="s1">any</span><span class="s2">(</span><span class="s1">x </span><span class="s2">&lt;= </span><span class="s5">0</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Data must be positive.&quot;</span><span class="s2">)</span>

    <span class="s1">lmbdas </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">linspace</span><span class="s2">(</span><span class="s1">la</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">num</span><span class="s2">=</span><span class="s1">N</span><span class="s2">)</span>
    <span class="s1">ppcc </span><span class="s2">= </span><span class="s1">lmbdas </span><span class="s2">* </span><span class="s5">0.0</span>
    <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">val </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">lmbdas</span><span class="s2">):</span>
        <span class="s3"># Determine for each lmbda the square root of correlation coefficient</span>
        <span class="s3"># of transformed x</span>
        <span class="s1">z </span><span class="s2">= </span><span class="s1">transform_func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">=</span><span class="s1">val</span><span class="s2">)</span>
        <span class="s1">_</span><span class="s2">, (</span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">r</span><span class="s2">) = </span><span class="s1">probplot</span><span class="s2">(</span><span class="s1">z</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">=</span><span class="s4">'norm'</span><span class="s2">, </span><span class="s1">fit</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
        <span class="s1">ppcc</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] = </span><span class="s1">r</span>

    <span class="s0">if </span><span class="s1">plot </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s1">plot</span><span class="s2">.</span><span class="s1">plot</span><span class="s2">(</span><span class="s1">lmbdas</span><span class="s2">, </span><span class="s1">ppcc</span><span class="s2">, </span><span class="s4">'x'</span><span class="s2">)</span>
        <span class="s1">_add_axis_labels_title</span><span class="s2">(</span><span class="s1">plot</span><span class="s2">, </span><span class="s1">xlabel</span><span class="s2">=</span><span class="s4">'$</span><span class="s0">\\</span><span class="s4">lambda$'</span><span class="s2">,</span>
                               <span class="s1">ylabel</span><span class="s2">=</span><span class="s4">'Prob Plot Corr. Coef.'</span><span class="s2">,</span>
                               <span class="s1">title</span><span class="s2">=</span><span class="s1">title</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">lmbdas</span><span class="s2">, </span><span class="s1">ppcc</span>


<span class="s0">def </span><span class="s1">boxcox_normplot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">N</span><span class="s2">=</span><span class="s5">80</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute parameters for a Box-Cox normality plot, optionally show it. 
 
    A Box-Cox normality plot shows graphically what the best transformation 
    parameter is to use in `boxcox` to obtain a distribution that is close 
    to normal. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    la, lb : scalar 
        The lower and upper bounds for the ``lmbda`` values to pass to `boxcox` 
        for Box-Cox transformations.  These are also the limits of the 
        horizontal axis of the plot if that is generated. 
    plot : object, optional 
        If given, plots the quantiles and least squares fit. 
        `plot` is an object that has to have methods &quot;plot&quot; and &quot;text&quot;. 
        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used, 
        or a custom object with the same methods. 
        Default is None, which means that no plot is created. 
    N : int, optional 
        Number of points on the horizontal axis (equally distributed from 
        `la` to `lb`). 
 
    Returns 
    ------- 
    lmbdas : ndarray 
        The ``lmbda`` values for which a Box-Cox transform was done. 
    ppcc : ndarray 
        Probability Plot Correlelation Coefficient, as obtained from `probplot` 
        when fitting the Box-Cox transformed input `x` against a normal 
        distribution. 
 
    See Also 
    -------- 
    probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max 
 
    Notes 
    ----- 
    Even if `plot` is given, the figure is not shown or saved by 
    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')`` 
    should be used after calling `probplot`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
 
    Generate some non-normally distributed data, and create a Box-Cox plot: 
 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, size=500) + 5 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; prob = stats.boxcox_normplot(x, -20, 20, plot=ax) 
 
    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in 
    the same plot: 
 
    &gt;&gt;&gt; _, maxlog = stats.boxcox(x) 
    &gt;&gt;&gt; ax.axvline(maxlog, color='r') 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">_normplot</span><span class="s2">(</span><span class="s4">'boxcox'</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">, </span><span class="s1">N</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">yeojohnson</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Return a dataset transformed by a Yeo-Johnson power transformation. 
 
    Parameters 
    ---------- 
    x : ndarray 
        Input array.  Should be 1-dimensional. 
    lmbda : float, optional 
        If ``lmbda`` is ``None``, find the lambda that maximizes the 
        log-likelihood function and return it as the second output argument. 
        Otherwise the transformation is done for the given value. 
 
    Returns 
    ------- 
    yeojohnson: ndarray 
        Yeo-Johnson power transformed array. 
    maxlog : float, optional 
        If the `lmbda` parameter is None, the second returned argument is 
        the lambda that maximizes the log-likelihood function. 
 
    See Also 
    -------- 
    probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox 
 
    Notes 
    ----- 
    The Yeo-Johnson transform is given by:: 
 
        y = ((x + 1)**lmbda - 1) / lmbda,                for x &gt;= 0, lmbda != 0 
            log(x + 1),                                  for x &gt;= 0, lmbda = 0 
            -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x &lt; 0, lmbda != 2 
            -log(-x + 1),                                for x &lt; 0, lmbda = 2 
 
    Unlike `boxcox`, `yeojohnson` does not require the input data to be 
    positive. 
 
    .. versionadded:: 1.2.0 
 
 
    References 
    ---------- 
    I. Yeo and R.A. Johnson, &quot;A New Family of Power Transformations to 
    Improve Normality or Symmetry&quot;, Biometrika 87.4 (2000): 
 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
 
    We generate some random variates from a non-normal distribution and make a 
    probability plot for it, to show it is non-normal in the tails: 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax1 = fig.add_subplot(211) 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, size=500) + 5 
    &gt;&gt;&gt; prob = stats.probplot(x, dist=stats.norm, plot=ax1) 
    &gt;&gt;&gt; ax1.set_xlabel('') 
    &gt;&gt;&gt; ax1.set_title('Probplot against normal distribution') 
 
    We now use `yeojohnson` to transform the data so it's closest to normal: 
 
    &gt;&gt;&gt; ax2 = fig.add_subplot(212) 
    &gt;&gt;&gt; xt, lmbda = stats.yeojohnson(x) 
    &gt;&gt;&gt; prob = stats.probplot(xt, dist=stats.norm, plot=ax2) 
    &gt;&gt;&gt; ax2.set_title('Probplot after Yeo-Johnson transformation') 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">x</span><span class="s2">.</span><span class="s1">size </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">complexfloating</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">'Yeo-Johnson transformation is not defined for '</span>
                         <span class="s4">'complex numbers.'</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">integer</span><span class="s2">):</span>
        <span class="s1">x </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">, </span><span class="s1">copy</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">lmbda </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">_yeojohnson_transform</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">)</span>

    <span class="s3"># if lmbda=None, find the lmbda that maximizes the log-likelihood function.</span>
    <span class="s1">lmax </span><span class="s2">= </span><span class="s1">yeojohnson_normmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">_yeojohnson_transform</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmax</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">y</span><span class="s2">, </span><span class="s1">lmax</span>


<span class="s0">def </span><span class="s1">_yeojohnson_transform</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lmbda</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Returns `x` transformed by the Yeo-Johnson power transform with given 
    parameter `lmbda`. 
    &quot;&quot;&quot;</span>
    <span class="s1">dtype </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype </span><span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">floating</span><span class="s2">) </span><span class="s0">else </span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span>
    <span class="s1">out </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros_like</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
    <span class="s1">pos </span><span class="s2">= </span><span class="s1">x </span><span class="s2">&gt;= </span><span class="s5">0  </span><span class="s3"># binary mask</span>

    <span class="s3"># when x &gt;= 0</span>
    <span class="s0">if </span><span class="s1">abs</span><span class="s2">(</span><span class="s1">lmbda</span><span class="s2">) &lt; </span><span class="s1">np</span><span class="s2">.</span><span class="s1">spacing</span><span class="s2">(</span><span class="s5">1.</span><span class="s2">):</span>
        <span class="s1">out</span><span class="s2">[</span><span class="s1">pos</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log1p</span><span class="s2">(</span><span class="s1">x</span><span class="s2">[</span><span class="s1">pos</span><span class="s2">])</span>
    <span class="s0">else</span><span class="s2">:  </span><span class="s3"># lmbda != 0</span>
        <span class="s3"># more stable version of: ((x + 1) ** lmbda - 1) / lmbda</span>
        <span class="s1">out</span><span class="s2">[</span><span class="s1">pos</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">expm1</span><span class="s2">(</span><span class="s1">lmbda </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log1p</span><span class="s2">(</span><span class="s1">x</span><span class="s2">[</span><span class="s1">pos</span><span class="s2">])) / </span><span class="s1">lmbda</span>

    <span class="s3"># when x &lt; 0</span>
    <span class="s0">if </span><span class="s1">abs</span><span class="s2">(</span><span class="s1">lmbda </span><span class="s2">- </span><span class="s5">2</span><span class="s2">) &gt; </span><span class="s1">np</span><span class="s2">.</span><span class="s1">spacing</span><span class="s2">(</span><span class="s5">1.</span><span class="s2">):</span>
        <span class="s1">out</span><span class="s2">[~</span><span class="s1">pos</span><span class="s2">] = -</span><span class="s1">np</span><span class="s2">.</span><span class="s1">expm1</span><span class="s2">((</span><span class="s5">2 </span><span class="s2">- </span><span class="s1">lmbda</span><span class="s2">) * </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log1p</span><span class="s2">(-</span><span class="s1">x</span><span class="s2">[~</span><span class="s1">pos</span><span class="s2">])) / (</span><span class="s5">2 </span><span class="s2">- </span><span class="s1">lmbda</span><span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:  </span><span class="s3"># lmbda == 2</span>
        <span class="s1">out</span><span class="s2">[~</span><span class="s1">pos</span><span class="s2">] = -</span><span class="s1">np</span><span class="s2">.</span><span class="s1">log1p</span><span class="s2">(-</span><span class="s1">x</span><span class="s2">[~</span><span class="s1">pos</span><span class="s2">])</span>

    <span class="s0">return </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">yeojohnson_llf</span><span class="s2">(</span><span class="s1">lmb</span><span class="s2">, </span><span class="s1">data</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;The yeojohnson log-likelihood function. 
 
    Parameters 
    ---------- 
    lmb : scalar 
        Parameter for Yeo-Johnson transformation. See `yeojohnson` for 
        details. 
    data : array_like 
        Data to calculate Yeo-Johnson log-likelihood for. If `data` is 
        multi-dimensional, the log-likelihood is calculated along the first 
        axis. 
 
    Returns 
    ------- 
    llf : float 
        Yeo-Johnson log-likelihood of `data` given `lmb`. 
 
    See Also 
    -------- 
    yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax 
 
    Notes 
    ----- 
    The Yeo-Johnson log-likelihood function is defined here as 
 
    .. math:: 
 
        llf = -N/2 \log(\hat{\sigma}^2) + (\lambda - 1) 
              \sum_i \text{ sign }(x_i)\log(|x_i| + 1) 
 
    where :math:`\hat{\sigma}^2` is estimated variance of the Yeo-Johnson 
    transformed input data ``x``. 
 
    .. versionadded:: 1.2.0 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; from mpl_toolkits.axes_grid1.inset_locator import inset_axes 
 
    Generate some random variates and calculate Yeo-Johnson log-likelihood 
    values for them for a range of ``lmbda`` values: 
 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, loc=10, size=1000) 
    &gt;&gt;&gt; lmbdas = np.linspace(-2, 10) 
    &gt;&gt;&gt; llf = np.zeros(lmbdas.shape, dtype=float) 
    &gt;&gt;&gt; for ii, lmbda in enumerate(lmbdas): 
    ...     llf[ii] = stats.yeojohnson_llf(lmbda, x) 
 
    Also find the optimal lmbda value with `yeojohnson`: 
 
    &gt;&gt;&gt; x_most_normal, lmbda_optimal = stats.yeojohnson(x) 
 
    Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a 
    horizontal line to check that that's really the optimum: 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; ax.plot(lmbdas, llf, 'b.-') 
    &gt;&gt;&gt; ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r') 
    &gt;&gt;&gt; ax.set_xlabel('lmbda parameter') 
    &gt;&gt;&gt; ax.set_ylabel('Yeo-Johnson log-likelihood') 
 
    Now add some probability plots to show that where the log-likelihood is 
    maximized the data transformed with `yeojohnson` looks closest to normal: 
 
    &gt;&gt;&gt; locs = [3, 10, 4]  # 'lower left', 'center', 'lower right' 
    &gt;&gt;&gt; for lmbda, loc in zip([-1, lmbda_optimal, 9], locs): 
    ...     xt = stats.yeojohnson(x, lmbda=lmbda) 
    ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt) 
    ...     ax_inset = inset_axes(ax, width=&quot;20%&quot;, height=&quot;20%&quot;, loc=loc) 
    ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-') 
    ...     ax_inset.set_xticklabels([]) 
    ...     ax_inset.set_yticklabels([]) 
    ...     ax_inset.set_title(r'$\lambda=%1.2f$' % lmbda) 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">data </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s1">data</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">0</span><span class="s2">]</span>

    <span class="s0">if </span><span class="s1">n_samples </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

    <span class="s1">trans </span><span class="s2">= </span><span class="s1">_yeojohnson_transform</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">lmb</span><span class="s2">)</span>
    <span class="s1">trans_var </span><span class="s2">= </span><span class="s1">trans</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">loglike </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty_like</span><span class="s2">(</span><span class="s1">trans_var</span><span class="s2">)</span>

    <span class="s3"># Avoid RuntimeWarning raised by np.log when the variance is too low</span>
    <span class="s1">tiny_variance </span><span class="s2">= </span><span class="s1">trans_var </span><span class="s2">&lt; </span><span class="s1">np</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s1">trans_var</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">).</span><span class="s1">tiny</span>
    <span class="s1">loglike</span><span class="s2">[</span><span class="s1">tiny_variance</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span>

    <span class="s1">loglike</span><span class="s2">[~</span><span class="s1">tiny_variance</span><span class="s2">] = (</span>
        <span class="s2">-</span><span class="s1">n_samples </span><span class="s2">/ </span><span class="s5">2 </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">trans_var</span><span class="s2">[~</span><span class="s1">tiny_variance</span><span class="s2">]))</span>
    <span class="s1">loglike</span><span class="s2">[~</span><span class="s1">tiny_variance</span><span class="s2">] += (</span>
        <span class="s2">(</span><span class="s1">lmb </span><span class="s2">- </span><span class="s5">1</span><span class="s2">) * (</span><span class="s1">np</span><span class="s2">.</span><span class="s1">sign</span><span class="s2">(</span><span class="s1">data</span><span class="s2">) * </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log1p</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">abs</span><span class="s2">(</span><span class="s1">data</span><span class="s2">))).</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">))</span>
    <span class="s0">return </span><span class="s1">loglike</span>


<span class="s0">def </span><span class="s1">yeojohnson_normmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">brack</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute optimal Yeo-Johnson transform parameter. 
 
    Compute optimal Yeo-Johnson transform parameter for input data, using 
    maximum likelihood estimation. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    brack : 2-tuple, optional 
        The starting interval for a downhill bracket search with 
        `optimize.brent`. Note that this is in most cases not critical; the 
        final result is allowed to be outside this bracket. If None, 
        `optimize.fminbound` is used with bounds that avoid overflow. 
 
    Returns 
    ------- 
    maxlog : float 
        The optimal transform parameter found. 
 
    See Also 
    -------- 
    yeojohnson, yeojohnson_llf, yeojohnson_normplot 
 
    Notes 
    ----- 
    .. versionadded:: 1.2.0 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
 
    Generate some data and determine optimal ``lmbda`` 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5 
    &gt;&gt;&gt; lmax = stats.yeojohnson_normmax(x) 
 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax) 
    &gt;&gt;&gt; ax.axvline(lmax, color='r') 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s0">def </span><span class="s1">_neg_llf</span><span class="s2">(</span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">data</span><span class="s2">):</span>
        <span class="s1">llf </span><span class="s2">= </span><span class="s1">yeojohnson_llf</span><span class="s2">(</span><span class="s1">lmbda</span><span class="s2">, </span><span class="s1">data</span><span class="s2">)</span>
        <span class="s3"># reject likelihoods that are inf which are likely due to small</span>
        <span class="s3"># variance in the transformed space</span>
        <span class="s1">llf</span><span class="s2">[</span><span class="s1">np</span><span class="s2">.</span><span class="s1">isinf</span><span class="s2">(</span><span class="s1">llf</span><span class="s2">)] = -</span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span>
        <span class="s0">return </span><span class="s2">-</span><span class="s1">llf</span>

    <span class="s0">with </span><span class="s1">np</span><span class="s2">.</span><span class="s1">errstate</span><span class="s2">(</span><span class="s1">invalid</span><span class="s2">=</span><span class="s4">'ignore'</span><span class="s2">):</span>
        <span class="s0">if not </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">isfinite</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">'Yeo-Johnson input must be finite.'</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span><span class="s1">x </span><span class="s2">== </span><span class="s5">0</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s5">1.0</span>
        <span class="s0">if </span><span class="s1">brack </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">brent</span><span class="s2">(</span><span class="s1">_neg_llf</span><span class="s2">, </span><span class="s1">brack</span><span class="s2">=</span><span class="s1">brack</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">x</span><span class="s2">,))</span>
        <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s1">dtype </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype </span><span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">floating</span><span class="s2">) </span><span class="s0">else </span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span>
        <span class="s3"># Allow values up to 20 times the maximum observed value to be safely</span>
        <span class="s3"># transformed without over- or underflow.</span>
        <span class="s1">log1p_max_x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log1p</span><span class="s2">(</span><span class="s5">20 </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">max</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">abs</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)))</span>
        <span class="s3"># Use half of floating point's exponent range to allow safe computation</span>
        <span class="s3"># of the variance of the transformed data.</span>
        <span class="s1">log_eps </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s1">dtype</span><span class="s2">).</span><span class="s1">eps</span><span class="s2">)</span>
        <span class="s1">log_tiny_float </span><span class="s2">= (</span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s1">dtype</span><span class="s2">).</span><span class="s1">tiny</span><span class="s2">) - </span><span class="s1">log_eps</span><span class="s2">) / </span><span class="s5">2</span>
        <span class="s1">log_max_float </span><span class="s2">= (</span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s1">dtype</span><span class="s2">).</span><span class="s1">max</span><span class="s2">) + </span><span class="s1">log_eps</span><span class="s2">) / </span><span class="s5">2</span>
        <span class="s3"># Compute the bounds by approximating the inverse of the Yeo-Johnson</span>
        <span class="s3"># transform on the smallest and largest floating point exponents, given</span>
        <span class="s3"># the largest data we expect to observe. See [1] for further details.</span>
        <span class="s3"># [1] https://github.com/scipy/scipy/pull/18852#issuecomment-1630286174</span>
        <span class="s1">lb </span><span class="s2">= </span><span class="s1">log_tiny_float </span><span class="s2">/ </span><span class="s1">log1p_max_x</span>
        <span class="s1">ub </span><span class="s2">= </span><span class="s1">log_max_float </span><span class="s2">/ </span><span class="s1">log1p_max_x</span>
        <span class="s3"># Convert the bounds if all or some of the data is negative.</span>
        <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span><span class="s1">x </span><span class="s2">&lt; </span><span class="s5">0</span><span class="s2">):</span>
            <span class="s1">lb</span><span class="s2">, </span><span class="s1">ub </span><span class="s2">= </span><span class="s5">2 </span><span class="s2">- </span><span class="s1">ub</span><span class="s2">, </span><span class="s5">2 </span><span class="s2">- </span><span class="s1">lb</span>
        <span class="s0">elif </span><span class="s1">np</span><span class="s2">.</span><span class="s1">any</span><span class="s2">(</span><span class="s1">x </span><span class="s2">&lt; </span><span class="s5">0</span><span class="s2">):</span>
            <span class="s1">lb</span><span class="s2">, </span><span class="s1">ub </span><span class="s2">= </span><span class="s1">max</span><span class="s2">(</span><span class="s5">2 </span><span class="s2">- </span><span class="s1">ub</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">), </span><span class="s1">min</span><span class="s2">(</span><span class="s5">2 </span><span class="s2">- </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub</span><span class="s2">)</span>
        <span class="s3"># Match `optimize.brent`'s tolerance.</span>
        <span class="s1">tol_brent </span><span class="s2">= </span><span class="s5">1.48e-08</span>
        <span class="s0">return </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">fminbound</span><span class="s2">(</span><span class="s1">_neg_llf</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">ub</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">x</span><span class="s2">,), </span><span class="s1">xtol</span><span class="s2">=</span><span class="s1">tol_brent</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">yeojohnson_normplot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">N</span><span class="s2">=</span><span class="s5">80</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute parameters for a Yeo-Johnson normality plot, optionally show it. 
 
    A Yeo-Johnson normality plot shows graphically what the best 
    transformation parameter is to use in `yeojohnson` to obtain a 
    distribution that is close to normal. 
 
    Parameters 
    ---------- 
    x : array_like 
        Input array. 
    la, lb : scalar 
        The lower and upper bounds for the ``lmbda`` values to pass to 
        `yeojohnson` for Yeo-Johnson transformations. These are also the 
        limits of the horizontal axis of the plot if that is generated. 
    plot : object, optional 
        If given, plots the quantiles and least squares fit. 
        `plot` is an object that has to have methods &quot;plot&quot; and &quot;text&quot;. 
        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used, 
        or a custom object with the same methods. 
        Default is None, which means that no plot is created. 
    N : int, optional 
        Number of points on the horizontal axis (equally distributed from 
        `la` to `lb`). 
 
    Returns 
    ------- 
    lmbdas : ndarray 
        The ``lmbda`` values for which a Yeo-Johnson transform was done. 
    ppcc : ndarray 
        Probability Plot Correlelation Coefficient, as obtained from `probplot` 
        when fitting the Box-Cox transformed input `x` against a normal 
        distribution. 
 
    See Also 
    -------- 
    probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max 
 
    Notes 
    ----- 
    Even if `plot` is given, the figure is not shown or saved by 
    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')`` 
    should be used after calling `probplot`. 
 
    .. versionadded:: 1.2.0 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
 
    Generate some non-normally distributed data, and create a Yeo-Johnson plot: 
 
    &gt;&gt;&gt; x = stats.loggamma.rvs(5, size=500) + 5 
    &gt;&gt;&gt; fig = plt.figure() 
    &gt;&gt;&gt; ax = fig.add_subplot(111) 
    &gt;&gt;&gt; prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax) 
 
    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in 
    the same plot: 
 
    &gt;&gt;&gt; _, maxlog = stats.yeojohnson(x) 
    &gt;&gt;&gt; ax.axvline(maxlog, color='r') 
 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">_normplot</span><span class="s2">(</span><span class="s4">'yeojohnson'</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">lb</span><span class="s2">, </span><span class="s1">plot</span><span class="s2">, </span><span class="s1">N</span><span class="s2">)</span>


<span class="s1">ShapiroResult </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'ShapiroResult'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s1">ShapiroResult</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">too_small</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">default_axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">shapiro</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Perform the Shapiro-Wilk test for normality. 
 
    The Shapiro-Wilk test tests the null hypothesis that the 
    data was drawn from a normal distribution. 
 
    Parameters 
    ---------- 
    x : array_like 
        Array of sample data. Must contain at least three observations. 
 
    Returns 
    ------- 
    statistic : float 
        The test statistic. 
    p-value : float 
        The p-value for the hypothesis test. 
 
    See Also 
    -------- 
    anderson : The Anderson-Darling test for normality 
    kstest : The Kolmogorov-Smirnov test for goodness of fit. 
 
    Notes 
    ----- 
    The algorithm used is described in [4]_ but censoring parameters as 
    described are not implemented. For N &gt; 5000 the W test statistic is 
    accurate, but the p-value may not be. 
 
    References 
    ---------- 
    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm 
           :doi:`10.18434/M32189` 
    .. [2] Shapiro, S. S. &amp; Wilk, M.B, &quot;An analysis of variance test for 
           normality (complete samples)&quot;, Biometrika, 1965, Vol. 52, 
           pp. 591-611, :doi:`10.2307/2333709` 
    .. [3] Razali, N. M. &amp; Wah, Y. B., &quot;Power comparisons of Shapiro-Wilk, 
           Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests&quot;, Journal 
           of Statistical Modeling and Analytics, 2011, Vol. 2, pp. 21-33. 
    .. [4] Royston P., &quot;Remark AS R94: A Remark on Algorithm AS 181: The 
           W-test for Normality&quot;, 1995, Applied Statistics, Vol. 44, 
           :doi:`10.2307/2986146` 
    .. [5] Phipson B., and Smyth, G. K., &quot;Permutation P-values Should Never Be 
           Zero: Calculating Exact P-values When Permutations Are Randomly 
           Drawn&quot;, Statistical Applications in Genetics and Molecular Biology, 
           2010, Vol.9, :doi:`10.2202/1544-6115.1585` 
    .. [6] Panagiotakos, D. B., &quot;The value of p-value in biomedical 
           research&quot;, The Open Cardiovascular Medicine Journal, 2008, Vol.2, 
           pp. 97-99, :doi:`10.2174/1874192400802010097` 
 
    Examples 
    -------- 
    Suppose we wish to infer from measurements whether the weights of adult 
    human males in a medical study are not normally distributed [2]_. 
    The weights (lbs) are recorded in the array ``x`` below. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236]) 
 
    The normality test of [1]_ and [2]_ begins by computing a statistic based 
    on the relationship between the observations and the expected order 
    statistics of a normal distribution. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; res = stats.shapiro(x) 
    &gt;&gt;&gt; res.statistic 
    0.7888147830963135 
 
    The value of this statistic tends to be high (close to 1) for samples drawn 
    from a normal distribution. 
 
    The test is performed by comparing the observed value of the statistic 
    against the null distribution: the distribution of statistic values formed 
    under the null hypothesis that the weights were drawn from a normal 
    distribution. For this normality test, the null distribution is not easy to 
    calculate exactly, so it is usually approximated by Monte Carlo methods, 
    that is, drawing many samples of the same size as ``x`` from a normal 
    distribution and computing the values of the statistic for each. 
 
    &gt;&gt;&gt; def statistic(x): 
    ...     # Get only the `shapiro` statistic; ignore its p-value 
    ...     return stats.shapiro(x).statistic 
    &gt;&gt;&gt; ref = stats.monte_carlo_test(x, stats.norm.rvs, statistic, 
    ...                              alternative='less') 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; bins = np.linspace(0.65, 1, 50) 
    &gt;&gt;&gt; def plot(ax):  # we'll reuse this 
    ...     ax.hist(ref.null_distribution, density=True, bins=bins) 
    ...     ax.set_title(&quot;Shapiro-Wilk Test Null Distribution \n&quot; 
    ...                  &quot;(Monte Carlo Approximation, 11 Observations)&quot;) 
    ...     ax.set_xlabel(&quot;statistic&quot;) 
    ...     ax.set_ylabel(&quot;probability density&quot;) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    The comparison is quantified by the p-value: the proportion of values in 
    the null distribution less than or equal to the observed value of the 
    statistic. 
 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; annotation = (f'p-value={res.pvalue:.6f}\n(highlighted area)') 
    &gt;&gt;&gt; props = dict(facecolor='black', width=1, headwidth=5, headlength=8) 
    &gt;&gt;&gt; _ = ax.annotate(annotation, (0.75, 0.1), (0.68, 0.7), arrowprops=props) 
    &gt;&gt;&gt; i_extreme = np.where(bins &lt;= res.statistic)[0] 
    &gt;&gt;&gt; for i in i_extreme: 
    ...     ax.patches[i].set_color('C1') 
    &gt;&gt;&gt; plt.xlim(0.65, 0.9) 
    &gt;&gt;&gt; plt.ylim(0, 4) 
    &gt;&gt;&gt; plt.show 
    &gt;&gt;&gt; res.pvalue 
    0.006703833118081093 
 
    If the p-value is &quot;small&quot; - that is, if there is a low probability of 
    sampling data from a normally distributed population that produces such an 
    extreme value of the statistic - this may be taken as evidence against 
    the null hypothesis in favor of the alternative: the weights were not 
    drawn from a normal distribution. Note that: 
 
    - The inverse is not true; that is, the test is not used to provide 
      evidence *for* the null hypothesis. 
    - The threshold for values that will be considered &quot;small&quot; is a choice that 
      should be made before the data is analyzed [5]_ with consideration of the 
      risks of both false positives (incorrectly rejecting the null hypothesis) 
      and false negatives (failure to reject a false null hypothesis). 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ravel</span><span class="s2">(</span><span class="s1">x</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">)</span>

    <span class="s1">N </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">N </span><span class="s2">&lt; </span><span class="s5">3</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Data must be at least length 3.&quot;</span><span class="s2">)</span>

    <span class="s1">a </span><span class="s2">= </span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">N</span><span class="s2">//</span><span class="s5">2</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">)</span>
    <span class="s1">init </span><span class="s2">= </span><span class="s5">0</span>

    <span class="s1">y </span><span class="s2">= </span><span class="s1">sort</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">-= </span><span class="s1">x</span><span class="s2">[</span><span class="s1">N</span><span class="s2">//</span><span class="s5">2</span><span class="s2">]  </span><span class="s3"># subtract the median (or a nearby value); see gh-15777</span>

    <span class="s1">w</span><span class="s2">, </span><span class="s1">pw</span><span class="s2">, </span><span class="s1">ifault </span><span class="s2">= </span><span class="s1">swilk</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">init</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">ifault </span><span class="s0">not in </span><span class="s2">[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">2</span><span class="s2">]:</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s4">&quot;scipy.stats.shapiro: Input data has range zero. The&quot;</span>
                      <span class="s4">&quot; results may not be accurate.&quot;</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">N </span><span class="s2">&gt; </span><span class="s5">5000</span><span class="s2">:</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s4">&quot;scipy.stats.shapiro: For N &gt; 5000, computed p-value &quot;</span>
                      <span class="s4">f&quot;may not be accurate. Current N is </span><span class="s0">{</span><span class="s1">N</span><span class="s0">}</span><span class="s4">.&quot;</span><span class="s2">,</span>
                      <span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>

    <span class="s3"># `w` and `pw` are always Python floats, which are double precision.</span>
    <span class="s3"># We want to ensure that they are NumPy floats, so until dtypes are</span>
    <span class="s3"># respected, we can explicitly convert each to float64 (faster than</span>
    <span class="s3"># `np.array([w, pw])`).</span>
    <span class="s0">return </span><span class="s1">ShapiroResult</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">(</span><span class="s1">w</span><span class="s2">), </span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">(</span><span class="s1">pw</span><span class="s2">))</span>


<span class="s3"># Values from Stephens, M A, &quot;EDF Statistics for Goodness of Fit and</span>
<span class="s3">#             Some Comparisons&quot;, Journal of the American Statistical</span>
<span class="s3">#             Association, Vol. 69, Issue 347, Sept. 1974, pp 730-737</span>
<span class="s1">_Avals_norm </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.576</span><span class="s2">, </span><span class="s5">0.656</span><span class="s2">, </span><span class="s5">0.787</span><span class="s2">, </span><span class="s5">0.918</span><span class="s2">, </span><span class="s5">1.092</span><span class="s2">])</span>
<span class="s1">_Avals_expon </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.922</span><span class="s2">, </span><span class="s5">1.078</span><span class="s2">, </span><span class="s5">1.341</span><span class="s2">, </span><span class="s5">1.606</span><span class="s2">, </span><span class="s5">1.957</span><span class="s2">])</span>
<span class="s3"># From Stephens, M A, &quot;Goodness of Fit for the Extreme Value Distribution&quot;,</span>
<span class="s3">#             Biometrika, Vol. 64, Issue 3, Dec. 1977, pp 583-588.</span>
<span class="s1">_Avals_gumbel </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.474</span><span class="s2">, </span><span class="s5">0.637</span><span class="s2">, </span><span class="s5">0.757</span><span class="s2">, </span><span class="s5">0.877</span><span class="s2">, </span><span class="s5">1.038</span><span class="s2">])</span>
<span class="s3"># From Stephens, M A, &quot;Tests of Fit for the Logistic Distribution Based</span>
<span class="s3">#             on the Empirical Distribution Function.&quot;, Biometrika,</span>
<span class="s3">#             Vol. 66, Issue 3, Dec. 1979, pp 591-595.</span>
<span class="s1">_Avals_logistic </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.426</span><span class="s2">, </span><span class="s5">0.563</span><span class="s2">, </span><span class="s5">0.660</span><span class="s2">, </span><span class="s5">0.769</span><span class="s2">, </span><span class="s5">0.906</span><span class="s2">, </span><span class="s5">1.010</span><span class="s2">])</span>
<span class="s3"># From Richard A. Lockhart and Michael A. Stephens &quot;Estimation and Tests of</span>
<span class="s3">#             Fit for the Three-Parameter Weibull Distribution&quot;</span>
<span class="s3">#             Journal of the Royal Statistical Society.Series B(Methodological)</span>
<span class="s3">#             Vol. 56, No. 3 (1994), pp. 491-500, table 1. Keys are c*100</span>
<span class="s1">_Avals_weibull </span><span class="s2">= [[</span><span class="s5">0.292</span><span class="s2">, </span><span class="s5">0.395</span><span class="s2">, </span><span class="s5">0.467</span><span class="s2">, </span><span class="s5">0.522</span><span class="s2">, </span><span class="s5">0.617</span><span class="s2">, </span><span class="s5">0.711</span><span class="s2">, </span><span class="s5">0.836</span><span class="s2">, </span><span class="s5">0.931</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.295</span><span class="s2">, </span><span class="s5">0.399</span><span class="s2">, </span><span class="s5">0.471</span><span class="s2">, </span><span class="s5">0.527</span><span class="s2">, </span><span class="s5">0.623</span><span class="s2">, </span><span class="s5">0.719</span><span class="s2">, </span><span class="s5">0.845</span><span class="s2">, </span><span class="s5">0.941</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.298</span><span class="s2">, </span><span class="s5">0.403</span><span class="s2">, </span><span class="s5">0.476</span><span class="s2">, </span><span class="s5">0.534</span><span class="s2">, </span><span class="s5">0.631</span><span class="s2">, </span><span class="s5">0.728</span><span class="s2">, </span><span class="s5">0.856</span><span class="s2">, </span><span class="s5">0.954</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.301</span><span class="s2">, </span><span class="s5">0.408</span><span class="s2">, </span><span class="s5">0.483</span><span class="s2">, </span><span class="s5">0.541</span><span class="s2">, </span><span class="s5">0.640</span><span class="s2">, </span><span class="s5">0.738</span><span class="s2">, </span><span class="s5">0.869</span><span class="s2">, </span><span class="s5">0.969</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.305</span><span class="s2">, </span><span class="s5">0.414</span><span class="s2">, </span><span class="s5">0.490</span><span class="s2">, </span><span class="s5">0.549</span><span class="s2">, </span><span class="s5">0.650</span><span class="s2">, </span><span class="s5">0.751</span><span class="s2">, </span><span class="s5">0.885</span><span class="s2">, </span><span class="s5">0.986</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.309</span><span class="s2">, </span><span class="s5">0.421</span><span class="s2">, </span><span class="s5">0.498</span><span class="s2">, </span><span class="s5">0.559</span><span class="s2">, </span><span class="s5">0.662</span><span class="s2">, </span><span class="s5">0.765</span><span class="s2">, </span><span class="s5">0.902</span><span class="s2">, </span><span class="s5">1.007</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.314</span><span class="s2">, </span><span class="s5">0.429</span><span class="s2">, </span><span class="s5">0.508</span><span class="s2">, </span><span class="s5">0.570</span><span class="s2">, </span><span class="s5">0.676</span><span class="s2">, </span><span class="s5">0.782</span><span class="s2">, </span><span class="s5">0.923</span><span class="s2">, </span><span class="s5">1.030</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.320</span><span class="s2">, </span><span class="s5">0.438</span><span class="s2">, </span><span class="s5">0.519</span><span class="s2">, </span><span class="s5">0.583</span><span class="s2">, </span><span class="s5">0.692</span><span class="s2">, </span><span class="s5">0.802</span><span class="s2">, </span><span class="s5">0.947</span><span class="s2">, </span><span class="s5">1.057</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.327</span><span class="s2">, </span><span class="s5">0.448</span><span class="s2">, </span><span class="s5">0.532</span><span class="s2">, </span><span class="s5">0.598</span><span class="s2">, </span><span class="s5">0.711</span><span class="s2">, </span><span class="s5">0.824</span><span class="s2">, </span><span class="s5">0.974</span><span class="s2">, </span><span class="s5">1.089</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.334</span><span class="s2">, </span><span class="s5">0.469</span><span class="s2">, </span><span class="s5">0.547</span><span class="s2">, </span><span class="s5">0.615</span><span class="s2">, </span><span class="s5">0.732</span><span class="s2">, </span><span class="s5">0.850</span><span class="s2">, </span><span class="s5">1.006</span><span class="s2">, </span><span class="s5">1.125</span><span class="s2">],</span>
                  <span class="s2">[</span><span class="s5">0.342</span><span class="s2">, </span><span class="s5">0.472</span><span class="s2">, </span><span class="s5">0.563</span><span class="s2">, </span><span class="s5">0.636</span><span class="s2">, </span><span class="s5">0.757</span><span class="s2">, </span><span class="s5">0.879</span><span class="s2">, </span><span class="s5">1.043</span><span class="s2">, </span><span class="s5">1.167</span><span class="s2">]]</span>
<span class="s1">_Avals_weibull </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">(</span><span class="s1">_Avals_weibull</span><span class="s2">)</span>
<span class="s1">_cvals_weibull </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">linspace</span><span class="s2">(</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">11</span><span class="s2">)</span>
<span class="s1">_get_As_weibull </span><span class="s2">= </span><span class="s1">interpolate</span><span class="s2">.</span><span class="s1">interp1d</span><span class="s2">(</span><span class="s1">_cvals_weibull</span><span class="s2">, </span><span class="s1">_Avals_weibull</span><span class="s2">.</span><span class="s1">T</span><span class="s2">,</span>
                                       <span class="s1">kind</span><span class="s2">=</span><span class="s4">'linear'</span><span class="s2">, </span><span class="s1">bounds_error</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
                                       <span class="s1">fill_value</span><span class="s2">=</span><span class="s1">_Avals_weibull</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">])</span>


<span class="s0">def </span><span class="s1">_weibull_fit_check</span><span class="s2">(</span><span class="s1">params</span><span class="s2">, </span><span class="s1">x</span><span class="s2">):</span>
    <span class="s3"># Refine the fit returned by `weibull_min.fit` to ensure that the first</span>
    <span class="s3"># order necessary conditions are satisfied. If not, raise an error.</span>
    <span class="s3"># Here, use `m` for the shape parameter to be consistent with [7]</span>
    <span class="s3"># and avoid confusion with `c` as defined in [7].</span>
    <span class="s1">n </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s1">m</span><span class="s2">, </span><span class="s1">u</span><span class="s2">, </span><span class="s1">s </span><span class="s2">= </span><span class="s1">params</span>

    <span class="s0">def </span><span class="s1">dnllf_dm</span><span class="s2">(</span><span class="s1">m</span><span class="s2">, </span><span class="s1">u</span><span class="s2">):</span>
        <span class="s3"># Partial w.r.t. shape w/ optimal scale. See [7] Equation 5.</span>
        <span class="s1">xu </span><span class="s2">= </span><span class="s1">x</span><span class="s2">-</span><span class="s1">u</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s5">1</span><span class="s2">/</span><span class="s1">m </span><span class="s2">- (</span><span class="s1">xu</span><span class="s2">**</span><span class="s1">m</span><span class="s2">*</span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">xu</span><span class="s2">)).</span><span class="s1">sum</span><span class="s2">()/(</span><span class="s1">xu</span><span class="s2">**</span><span class="s1">m</span><span class="s2">).</span><span class="s1">sum</span><span class="s2">()</span>
                <span class="s2">+ </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">xu</span><span class="s2">).</span><span class="s1">sum</span><span class="s2">()/</span><span class="s1">n</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">dnllf_du</span><span class="s2">(</span><span class="s1">m</span><span class="s2">, </span><span class="s1">u</span><span class="s2">):</span>
        <span class="s3"># Partial w.r.t. loc w/ optimal scale. See [7] Equation 6.</span>
        <span class="s1">xu </span><span class="s2">= </span><span class="s1">x</span><span class="s2">-</span><span class="s1">u</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s1">m</span><span class="s2">-</span><span class="s5">1</span><span class="s2">)/</span><span class="s1">m</span><span class="s2">*(</span><span class="s1">xu</span><span class="s2">**-</span><span class="s5">1</span><span class="s2">).</span><span class="s1">sum</span><span class="s2">() - </span><span class="s1">n</span><span class="s2">*(</span><span class="s1">xu</span><span class="s2">**(</span><span class="s1">m</span><span class="s2">-</span><span class="s5">1</span><span class="s2">)).</span><span class="s1">sum</span><span class="s2">()/(</span><span class="s1">xu</span><span class="s2">**</span><span class="s1">m</span><span class="s2">).</span><span class="s1">sum</span><span class="s2">()</span>

    <span class="s0">def </span><span class="s1">get_scale</span><span class="s2">(</span><span class="s1">m</span><span class="s2">, </span><span class="s1">u</span><span class="s2">):</span>
        <span class="s3"># Partial w.r.t. scale solved in terms of shape and location.</span>
        <span class="s3"># See [7] Equation 7.</span>
        <span class="s0">return </span><span class="s2">((</span><span class="s1">x</span><span class="s2">-</span><span class="s1">u</span><span class="s2">)**</span><span class="s1">m</span><span class="s2">/</span><span class="s1">n</span><span class="s2">).</span><span class="s1">sum</span><span class="s2">()**(</span><span class="s5">1</span><span class="s2">/</span><span class="s1">m</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">dnllf</span><span class="s2">(</span><span class="s1">params</span><span class="s2">):</span>
        <span class="s3"># Partial derivatives of the NLLF w.r.t. parameters, i.e.</span>
        <span class="s3"># first order necessary conditions for MLE fit.</span>
        <span class="s0">return </span><span class="s2">[</span><span class="s1">dnllf_dm</span><span class="s2">(*</span><span class="s1">params</span><span class="s2">), </span><span class="s1">dnllf_du</span><span class="s2">(*</span><span class="s1">params</span><span class="s2">)]</span>

    <span class="s1">suggestion </span><span class="s2">= (</span><span class="s4">&quot;Maximum likelihood estimation is known to be challenging &quot;</span>
                  <span class="s4">&quot;for the three-parameter Weibull distribution. Consider &quot;</span>
                  <span class="s4">&quot;performing a custom goodness-of-fit test using &quot;</span>
                  <span class="s4">&quot;`scipy.stats.monte_carlo_test`.&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">allclose</span><span class="s2">(</span><span class="s1">u</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">min</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)) </span><span class="s0">or </span><span class="s1">m </span><span class="s2">&lt; </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s3"># The critical values provided by [7] don't seem to control the</span>
        <span class="s3"># Type I error rate in this case. Error out.</span>
        <span class="s1">message </span><span class="s2">= (</span><span class="s4">&quot;Maximum likelihood estimation has converged to &quot;</span>
                   <span class="s4">&quot;a solution in which the location is equal to the minimum &quot;</span>
                   <span class="s4">&quot;of the data, the shape parameter is less than 2, or both. &quot;</span>
                   <span class="s4">&quot;The table of critical values in [7] does not &quot;</span>
                   <span class="s4">&quot;include this case. &quot; </span><span class="s2">+ </span><span class="s1">suggestion</span><span class="s2">)</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">message</span><span class="s2">)</span>

    <span class="s0">try</span><span class="s2">:</span>
        <span class="s3"># Refine the MLE / verify that first-order necessary conditions are</span>
        <span class="s3"># satisfied. If so, the critical values provided in [7] seem reliable.</span>
        <span class="s0">with </span><span class="s1">np</span><span class="s2">.</span><span class="s1">errstate</span><span class="s2">(</span><span class="s1">over</span><span class="s2">=</span><span class="s4">'raise'</span><span class="s2">, </span><span class="s1">invalid</span><span class="s2">=</span><span class="s4">'raise'</span><span class="s2">):</span>
            <span class="s1">res </span><span class="s2">= </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">root</span><span class="s2">(</span><span class="s1">dnllf</span><span class="s2">, </span><span class="s1">params</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">])</span>

        <span class="s1">message </span><span class="s2">= (</span><span class="s4">&quot;Solution of MLE first-order conditions failed: &quot;</span>
                   <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">res</span><span class="s2">.</span><span class="s1">message</span><span class="s0">}</span><span class="s4">. `anderson` cannot continue. &quot; </span><span class="s2">+ </span><span class="s1">suggestion</span><span class="s2">)</span>
        <span class="s0">if not </span><span class="s1">res</span><span class="s2">.</span><span class="s1">success</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">message</span><span class="s2">)</span>

    <span class="s0">except </span><span class="s2">(</span><span class="s1">FloatingPointError</span><span class="s2">, </span><span class="s1">ValueError</span><span class="s2">) </span><span class="s0">as </span><span class="s1">e</span><span class="s2">:</span>
        <span class="s1">message </span><span class="s2">= (</span><span class="s4">&quot;An error occurred while fitting the Weibull distribution &quot;</span>
                   <span class="s4">&quot;to the data, so `anderson` cannot continue. &quot; </span><span class="s2">+ </span><span class="s1">suggestion</span><span class="s2">)</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">message</span><span class="s2">) </span><span class="s0">from </span><span class="s1">e</span>

    <span class="s1">m</span><span class="s2">, </span><span class="s1">u </span><span class="s2">= </span><span class="s1">res</span><span class="s2">.</span><span class="s1">x</span>
    <span class="s1">s </span><span class="s2">= </span><span class="s1">get_scale</span><span class="s2">(</span><span class="s1">m</span><span class="s2">, </span><span class="s1">u</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">m</span><span class="s2">, </span><span class="s1">u</span><span class="s2">, </span><span class="s1">s</span>


<span class="s1">AndersonResult </span><span class="s2">= </span><span class="s1">_make_tuple_bunch</span><span class="s2">(</span><span class="s4">'AndersonResult'</span><span class="s2">,</span>
                                   <span class="s2">[</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'critical_values'</span><span class="s2">,</span>
                                    <span class="s4">'significance_level'</span><span class="s2">], [</span><span class="s4">'fit_result'</span><span class="s2">])</span>


<span class="s0">def </span><span class="s1">anderson</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">=</span><span class="s4">'norm'</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Anderson-Darling test for data coming from a particular distribution. 
 
    The Anderson-Darling test tests the null hypothesis that a sample is 
    drawn from a population that follows a particular distribution. 
    For the Anderson-Darling test, the critical values depend on 
    which distribution is being tested against.  This function works 
    for normal, exponential, logistic, weibull_min, or Gumbel (Extreme Value 
    Type I) distributions. 
 
    Parameters 
    ---------- 
    x : array_like 
        Array of sample data. 
    dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r', 'extreme1', 'weibull_min'}, optional 
        The type of distribution to test against.  The default is 'norm'. 
        The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the 
        same distribution. 
 
    Returns 
    ------- 
    result : AndersonResult 
        An object with the following attributes: 
 
        statistic : float 
            The Anderson-Darling test statistic. 
        critical_values : list 
            The critical values for this distribution. 
        significance_level : list 
            The significance levels for the corresponding critical values 
            in percents.  The function returns critical values for a 
            differing set of significance levels depending on the 
            distribution that is being tested against. 
        fit_result : `~scipy.stats._result_classes.FitResult` 
            An object containing the results of fitting the distribution to 
            the data. 
 
    See Also 
    -------- 
    kstest : The Kolmogorov-Smirnov test for goodness-of-fit. 
 
    Notes 
    ----- 
    Critical values provided are for the following significance levels: 
 
    normal/exponential 
        15%, 10%, 5%, 2.5%, 1% 
    logistic 
        25%, 10%, 5%, 2.5%, 1%, 0.5% 
    gumbel_l / gumbel_r 
        25%, 10%, 5%, 2.5%, 1% 
    weibull_min 
        50%, 25%, 15%, 10%, 5%, 2.5%, 1%, 0.5% 
 
    If the returned statistic is larger than these critical values then 
    for the corresponding significance level, the null hypothesis that 
    the data come from the chosen distribution can be rejected. 
    The returned statistic is referred to as 'A2' in the references. 
 
    For `weibull_min`, maximum likelihood estimation is known to be 
    challenging. If the test returns successfully, then the first order 
    conditions for a maximum likehood estimate have been verified and 
    the critical values correspond relatively well to the significance levels, 
    provided that the sample is sufficiently large (&gt;10 observations [7]). 
    However, for some data - especially data with no left tail - `anderson` 
    is likely to result in an error message. In this case, consider 
    performing a custom goodness of fit test using 
    `scipy.stats.monte_carlo_test`. 
 
    References 
    ---------- 
    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm 
    .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and 
           Some Comparisons, Journal of the American Statistical Association, 
           Vol. 69, pp. 730-737. 
    .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit 
           Statistics with Unknown Parameters, Annals of Statistics, Vol. 4, 
           pp. 357-369. 
    .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value 
           Distribution, Biometrika, Vol. 64, pp. 583-588. 
    .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference 
           to Tests for Exponentiality , Technical Report No. 262, 
           Department of Statistics, Stanford University, Stanford, CA. 
    .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution 
           Based on the Empirical Distribution Function, Biometrika, Vol. 66, 
           pp. 591-595. 
    .. [7] Richard A. Lockhart and Michael A. Stephens &quot;Estimation and Tests of 
           Fit for the Three-Parameter Weibull Distribution&quot; 
           Journal of the Royal Statistical Society.Series B(Methodological) 
           Vol. 56, No. 3 (1994), pp. 491-500, Table 0. 
 
    Examples 
    -------- 
    Test the null hypothesis that a random sample was drawn from a normal 
    distribution (with unspecified mean and standard deviation). 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import anderson 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; data = rng.random(size=35) 
    &gt;&gt;&gt; res = anderson(data) 
    &gt;&gt;&gt; res.statistic 
    0.8398018749744764 
    &gt;&gt;&gt; res.critical_values 
    array([0.527, 0.6  , 0.719, 0.839, 0.998]) 
    &gt;&gt;&gt; res.significance_level 
    array([15. , 10. ,  5. ,  2.5,  1. ]) 
 
    The value of the statistic (barely) exceeds the critical value associated 
    with a significance level of 2.5%, so the null hypothesis may be rejected 
    at a significance level of 2.5%, but not at a significance level of 1%. 
 
    &quot;&quot;&quot; </span><span class="s3"># numpy/numpydoc#87  # noqa: E501</span>
    <span class="s1">dist </span><span class="s2">= </span><span class="s1">dist</span><span class="s2">.</span><span class="s1">lower</span><span class="s2">()</span>
    <span class="s0">if </span><span class="s1">dist </span><span class="s0">in </span><span class="s2">{</span><span class="s4">'extreme1'</span><span class="s2">, </span><span class="s4">'gumbel'</span><span class="s2">}:</span>
        <span class="s1">dist </span><span class="s2">= </span><span class="s4">'gumbel_l'</span>
    <span class="s1">dists </span><span class="s2">= {</span><span class="s4">'norm'</span><span class="s2">, </span><span class="s4">'expon'</span><span class="s2">, </span><span class="s4">'gumbel_l'</span><span class="s2">,</span>
             <span class="s4">'gumbel_r'</span><span class="s2">, </span><span class="s4">'logistic'</span><span class="s2">, </span><span class="s4">'weibull_min'</span><span class="s2">}</span>

    <span class="s0">if </span><span class="s1">dist </span><span class="s0">not in </span><span class="s1">dists</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;Invalid distribution; dist must be in </span><span class="s0">{</span><span class="s1">dists</span><span class="s0">}</span><span class="s4">.&quot;</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">sort</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s1">xbar </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">dist </span><span class="s2">== </span><span class="s4">'norm'</span><span class="s2">:</span>
        <span class="s1">s </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">std</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">ddof</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s1">w </span><span class="s2">= (</span><span class="s1">y </span><span class="s2">- </span><span class="s1">xbar</span><span class="s2">) / </span><span class="s1">s</span>
        <span class="s1">fit_params </span><span class="s2">= </span><span class="s1">xbar</span><span class="s2">, </span><span class="s1">s</span>
        <span class="s1">logcdf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">.</span><span class="s1">logcdf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">logsf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">.</span><span class="s1">logsf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">sig </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">15</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">2.5</span><span class="s2">, </span><span class="s5">1</span><span class="s2">])</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">around</span><span class="s2">(</span><span class="s1">_Avals_norm </span><span class="s2">/ (</span><span class="s5">1.0 </span><span class="s2">+ </span><span class="s5">4.0</span><span class="s2">/</span><span class="s1">N </span><span class="s2">- </span><span class="s5">25.0</span><span class="s2">/</span><span class="s1">N</span><span class="s2">/</span><span class="s1">N</span><span class="s2">), </span><span class="s5">3</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">dist </span><span class="s2">== </span><span class="s4">'expon'</span><span class="s2">:</span>
        <span class="s1">w </span><span class="s2">= </span><span class="s1">y </span><span class="s2">/ </span><span class="s1">xbar</span>
        <span class="s1">fit_params </span><span class="s2">= </span><span class="s5">0</span><span class="s2">, </span><span class="s1">xbar</span>
        <span class="s1">logcdf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">expon</span><span class="s2">.</span><span class="s1">logcdf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">logsf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">expon</span><span class="s2">.</span><span class="s1">logsf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">sig </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">15</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">2.5</span><span class="s2">, </span><span class="s5">1</span><span class="s2">])</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">around</span><span class="s2">(</span><span class="s1">_Avals_expon </span><span class="s2">/ (</span><span class="s5">1.0 </span><span class="s2">+ </span><span class="s5">0.6</span><span class="s2">/</span><span class="s1">N</span><span class="s2">), </span><span class="s5">3</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">dist </span><span class="s2">== </span><span class="s4">'logistic'</span><span class="s2">:</span>
        <span class="s0">def </span><span class="s1">rootfunc</span><span class="s2">(</span><span class="s1">ab</span><span class="s2">, </span><span class="s1">xj</span><span class="s2">, </span><span class="s1">N</span><span class="s2">):</span>
            <span class="s1">a</span><span class="s2">, </span><span class="s1">b </span><span class="s2">= </span><span class="s1">ab</span>
            <span class="s1">tmp </span><span class="s2">= (</span><span class="s1">xj </span><span class="s2">- </span><span class="s1">a</span><span class="s2">) / </span><span class="s1">b</span>
            <span class="s1">tmp2 </span><span class="s2">= </span><span class="s1">exp</span><span class="s2">(</span><span class="s1">tmp</span><span class="s2">)</span>
            <span class="s1">val </span><span class="s2">= [</span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s5">1.0</span><span class="s2">/(</span><span class="s5">1</span><span class="s2">+</span><span class="s1">tmp2</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">) - </span><span class="s5">0.5</span><span class="s2">*</span><span class="s1">N</span><span class="s2">,</span>
                   <span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">tmp</span><span class="s2">*(</span><span class="s5">1.0</span><span class="s2">-</span><span class="s1">tmp2</span><span class="s2">)/(</span><span class="s5">1</span><span class="s2">+</span><span class="s1">tmp2</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">) + </span><span class="s1">N</span><span class="s2">]</span>
            <span class="s0">return </span><span class="s1">array</span><span class="s2">(</span><span class="s1">val</span><span class="s2">)</span>

        <span class="s1">sol0 </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s1">xbar</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">std</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">ddof</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)])</span>
        <span class="s1">sol </span><span class="s2">= </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">fsolve</span><span class="s2">(</span><span class="s1">rootfunc</span><span class="s2">, </span><span class="s1">sol0</span><span class="s2">, </span><span class="s1">args</span><span class="s2">=(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">N</span><span class="s2">), </span><span class="s1">xtol</span><span class="s2">=</span><span class="s5">1e-5</span><span class="s2">)</span>
        <span class="s1">w </span><span class="s2">= (</span><span class="s1">y </span><span class="s2">- </span><span class="s1">sol</span><span class="s2">[</span><span class="s5">0</span><span class="s2">]) / </span><span class="s1">sol</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]</span>
        <span class="s1">fit_params </span><span class="s2">= </span><span class="s1">sol</span>
        <span class="s1">logcdf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">logistic</span><span class="s2">.</span><span class="s1">logcdf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">logsf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">logistic</span><span class="s2">.</span><span class="s1">logsf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">sig </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">25</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">2.5</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">0.5</span><span class="s2">])</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">around</span><span class="s2">(</span><span class="s1">_Avals_logistic </span><span class="s2">/ (</span><span class="s5">1.0 </span><span class="s2">+ </span><span class="s5">0.25</span><span class="s2">/</span><span class="s1">N</span><span class="s2">), </span><span class="s5">3</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">dist </span><span class="s2">== </span><span class="s4">'gumbel_r'</span><span class="s2">:</span>
        <span class="s1">xbar</span><span class="s2">, </span><span class="s1">s </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gumbel_r</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s1">w </span><span class="s2">= (</span><span class="s1">y </span><span class="s2">- </span><span class="s1">xbar</span><span class="s2">) / </span><span class="s1">s</span>
        <span class="s1">fit_params </span><span class="s2">= </span><span class="s1">xbar</span><span class="s2">, </span><span class="s1">s</span>
        <span class="s1">logcdf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gumbel_r</span><span class="s2">.</span><span class="s1">logcdf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">logsf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gumbel_r</span><span class="s2">.</span><span class="s1">logsf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">sig </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">25</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">2.5</span><span class="s2">, </span><span class="s5">1</span><span class="s2">])</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">around</span><span class="s2">(</span><span class="s1">_Avals_gumbel </span><span class="s2">/ (</span><span class="s5">1.0 </span><span class="s2">+ </span><span class="s5">0.2</span><span class="s2">/</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">N</span><span class="s2">)), </span><span class="s5">3</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">dist </span><span class="s2">== </span><span class="s4">'gumbel_l'</span><span class="s2">:</span>
        <span class="s1">xbar</span><span class="s2">, </span><span class="s1">s </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gumbel_l</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s1">w </span><span class="s2">= (</span><span class="s1">y </span><span class="s2">- </span><span class="s1">xbar</span><span class="s2">) / </span><span class="s1">s</span>
        <span class="s1">fit_params </span><span class="s2">= </span><span class="s1">xbar</span><span class="s2">, </span><span class="s1">s</span>
        <span class="s1">logcdf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gumbel_l</span><span class="s2">.</span><span class="s1">logcdf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">logsf </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">gumbel_l</span><span class="s2">.</span><span class="s1">logsf</span><span class="s2">(</span><span class="s1">w</span><span class="s2">)</span>
        <span class="s1">sig </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">25</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s5">5</span><span class="s2">, </span><span class="s5">2.5</span><span class="s2">, </span><span class="s5">1</span><span class="s2">])</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">around</span><span class="s2">(</span><span class="s1">_Avals_gumbel </span><span class="s2">/ (</span><span class="s5">1.0 </span><span class="s2">+ </span><span class="s5">0.2</span><span class="s2">/</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">N</span><span class="s2">)), </span><span class="s5">3</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">dist </span><span class="s2">== </span><span class="s4">'weibull_min'</span><span class="s2">:</span>
        <span class="s1">message </span><span class="s2">= (</span><span class="s4">&quot;Critical values of the test statistic are given for the &quot;</span>
                   <span class="s4">&quot;asymptotic distribution. These may not be accurate for &quot;</span>
                   <span class="s4">&quot;samples with fewer than 10 observations. Consider using &quot;</span>
                   <span class="s4">&quot;`scipy.stats.monte_carlo_test`.&quot;</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">N </span><span class="s2">&lt; </span><span class="s5">10</span><span class="s2">:</span>
            <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s1">message</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
        <span class="s3"># [7] writes our 'c' as 'm', and they write `c = 1/m`. Use their names.</span>
        <span class="s1">m</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">scale </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">weibull_min</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
        <span class="s1">m</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">scale </span><span class="s2">= </span><span class="s1">_weibull_fit_check</span><span class="s2">((</span><span class="s1">m</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">), </span><span class="s1">y</span><span class="s2">)</span>
        <span class="s1">fit_params </span><span class="s2">= </span><span class="s1">m</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">scale</span>
        <span class="s1">logcdf </span><span class="s2">= </span><span class="s1">stats</span><span class="s2">.</span><span class="s1">weibull_min</span><span class="s2">(*</span><span class="s1">fit_params</span><span class="s2">).</span><span class="s1">logcdf</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
        <span class="s1">logsf </span><span class="s2">= </span><span class="s1">stats</span><span class="s2">.</span><span class="s1">weibull_min</span><span class="s2">(*</span><span class="s1">fit_params</span><span class="s2">).</span><span class="s1">logsf</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
        <span class="s1">c </span><span class="s2">= </span><span class="s5">1 </span><span class="s2">/ </span><span class="s1">m  </span><span class="s3"># m and c are as used in [7]</span>
        <span class="s1">sig </span><span class="s2">= </span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.5</span><span class="s2">, </span><span class="s5">0.75</span><span class="s2">, </span><span class="s5">0.85</span><span class="s2">, </span><span class="s5">0.9</span><span class="s2">, </span><span class="s5">0.95</span><span class="s2">, </span><span class="s5">0.975</span><span class="s2">, </span><span class="s5">0.99</span><span class="s2">, </span><span class="s5">0.995</span><span class="s2">])</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">_get_As_weibull</span><span class="s2">(</span><span class="s1">c</span><span class="s2">)</span>
        <span class="s3"># Goodness-of-fit tests should only be used to provide evidence</span>
        <span class="s3"># _against_ the null hypothesis. Be conservative and round up.</span>
        <span class="s1">critical </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">round</span><span class="s2">(</span><span class="s1">critical </span><span class="s2">+ </span><span class="s5">0.0005</span><span class="s2">, </span><span class="s1">decimals</span><span class="s2">=</span><span class="s5">3</span><span class="s2">)</span>

    <span class="s1">i </span><span class="s2">= </span><span class="s1">arange</span><span class="s2">(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">N </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">A2 </span><span class="s2">= -</span><span class="s1">N </span><span class="s2">- </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">((</span><span class="s5">2</span><span class="s2">*</span><span class="s1">i </span><span class="s2">- </span><span class="s5">1.0</span><span class="s2">) / </span><span class="s1">N </span><span class="s2">* (</span><span class="s1">logcdf </span><span class="s2">+ </span><span class="s1">logsf</span><span class="s2">[::-</span><span class="s5">1</span><span class="s2">]), </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s3"># FitResult initializer expects an optimize result, so let's work with it</span>
    <span class="s1">message </span><span class="s2">= </span><span class="s4">'`anderson` successfully fit the distribution to the data.'</span>
    <span class="s1">res </span><span class="s2">= </span><span class="s1">optimize</span><span class="s2">.</span><span class="s1">OptimizeResult</span><span class="s2">(</span><span class="s1">success</span><span class="s2">=</span><span class="s0">True</span><span class="s2">, </span><span class="s1">message</span><span class="s2">=</span><span class="s1">message</span><span class="s2">)</span>
    <span class="s1">res</span><span class="s2">.</span><span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">(</span><span class="s1">fit_params</span><span class="s2">)</span>
    <span class="s1">fit_result </span><span class="s2">= </span><span class="s1">FitResult</span><span class="s2">(</span><span class="s1">getattr</span><span class="s2">(</span><span class="s1">distributions</span><span class="s2">, </span><span class="s1">dist</span><span class="s2">), </span><span class="s1">y</span><span class="s2">,</span>
                           <span class="s1">discrete</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, </span><span class="s1">res</span><span class="s2">=</span><span class="s1">res</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">AndersonResult</span><span class="s2">(</span><span class="s1">A2</span><span class="s2">, </span><span class="s1">critical</span><span class="s2">, </span><span class="s1">sig</span><span class="s2">, </span><span class="s1">fit_result</span><span class="s2">=</span><span class="s1">fit_result</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">_anderson_ksamp_midrank</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">Z</span><span class="s2">, </span><span class="s1">Zstar</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">N</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute A2akN equation 7 of Scholz and Stephens. 
 
    Parameters 
    ---------- 
    samples : sequence of 1-D array_like 
        Array of sample arrays. 
    Z : array_like 
        Sorted array of all observations. 
    Zstar : array_like 
        Sorted array of unique observations. 
    k : int 
        Number of samples. 
    n : array_like 
        Number of observations in each sample. 
    N : int 
        Total number of observations. 
 
    Returns 
    ------- 
    A2aKN : float 
        The A2aKN statistics of Scholz and Stephens 1987. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">A2akN </span><span class="s2">= </span><span class="s5">0.</span>
    <span class="s1">Z_ssorted_left </span><span class="s2">= </span><span class="s1">Z</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">, </span><span class="s4">'left'</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">N </span><span class="s2">== </span><span class="s1">Zstar</span><span class="s2">.</span><span class="s1">size</span><span class="s2">:</span>
        <span class="s1">lj </span><span class="s2">= </span><span class="s5">1.</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">lj </span><span class="s2">= </span><span class="s1">Z</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">, </span><span class="s4">'right'</span><span class="s2">) - </span><span class="s1">Z_ssorted_left</span>
    <span class="s1">Bj </span><span class="s2">= </span><span class="s1">Z_ssorted_left </span><span class="s2">+ </span><span class="s1">lj </span><span class="s2">/ </span><span class="s5">2.</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">arange</span><span class="s2">(</span><span class="s5">0</span><span class="s2">, </span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">s </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">i</span><span class="s2">])</span>
        <span class="s1">s_ssorted_right </span><span class="s2">= </span><span class="s1">s</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">, </span><span class="s1">side</span><span class="s2">=</span><span class="s4">'right'</span><span class="s2">)</span>
        <span class="s1">Mij </span><span class="s2">= </span><span class="s1">s_ssorted_right</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">float</span><span class="s2">)</span>
        <span class="s1">fij </span><span class="s2">= </span><span class="s1">s_ssorted_right </span><span class="s2">- </span><span class="s1">s</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">, </span><span class="s4">'left'</span><span class="s2">)</span>
        <span class="s1">Mij </span><span class="s2">-= </span><span class="s1">fij </span><span class="s2">/ </span><span class="s5">2.</span>
        <span class="s1">inner </span><span class="s2">= </span><span class="s1">lj </span><span class="s2">/ </span><span class="s1">float</span><span class="s2">(</span><span class="s1">N</span><span class="s2">) * (</span><span class="s1">N</span><span class="s2">*</span><span class="s1">Mij </span><span class="s2">- </span><span class="s1">Bj</span><span class="s2">*</span><span class="s1">n</span><span class="s2">[</span><span class="s1">i</span><span class="s2">])**</span><span class="s5">2 </span><span class="s2">/ (</span><span class="s1">Bj</span><span class="s2">*(</span><span class="s1">N </span><span class="s2">- </span><span class="s1">Bj</span><span class="s2">) - </span><span class="s1">N</span><span class="s2">*</span><span class="s1">lj</span><span class="s2">/</span><span class="s5">4.</span><span class="s2">)</span>
        <span class="s1">A2akN </span><span class="s2">+= </span><span class="s1">inner</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">() / </span><span class="s1">n</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]</span>
    <span class="s1">A2akN </span><span class="s2">*= (</span><span class="s1">N </span><span class="s2">- </span><span class="s5">1.</span><span class="s2">) / </span><span class="s1">N</span>
    <span class="s0">return </span><span class="s1">A2akN</span>


<span class="s0">def </span><span class="s1">_anderson_ksamp_right</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">Z</span><span class="s2">, </span><span class="s1">Zstar</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">N</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Compute A2akN equation 6 of Scholz &amp; Stephens. 
 
    Parameters 
    ---------- 
    samples : sequence of 1-D array_like 
        Array of sample arrays. 
    Z : array_like 
        Sorted array of all observations. 
    Zstar : array_like 
        Sorted array of unique observations. 
    k : int 
        Number of samples. 
    n : array_like 
        Number of observations in each sample. 
    N : int 
        Total number of observations. 
 
    Returns 
    ------- 
    A2KN : float 
        The A2KN statistics of Scholz and Stephens 1987. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">A2kN </span><span class="s2">= </span><span class="s5">0.</span>
    <span class="s1">lj </span><span class="s2">= </span><span class="s1">Z</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">], </span><span class="s4">'right'</span><span class="s2">) - </span><span class="s1">Z</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">],</span>
                                                              <span class="s4">'left'</span><span class="s2">)</span>
    <span class="s1">Bj </span><span class="s2">= </span><span class="s1">lj</span><span class="s2">.</span><span class="s1">cumsum</span><span class="s2">()</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">arange</span><span class="s2">(</span><span class="s5">0</span><span class="s2">, </span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">s </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">i</span><span class="s2">])</span>
        <span class="s1">Mij </span><span class="s2">= </span><span class="s1">s</span><span class="s2">.</span><span class="s1">searchsorted</span><span class="s2">(</span><span class="s1">Zstar</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">], </span><span class="s1">side</span><span class="s2">=</span><span class="s4">'right'</span><span class="s2">)</span>
        <span class="s1">inner </span><span class="s2">= </span><span class="s1">lj </span><span class="s2">/ </span><span class="s1">float</span><span class="s2">(</span><span class="s1">N</span><span class="s2">) * (</span><span class="s1">N </span><span class="s2">* </span><span class="s1">Mij </span><span class="s2">- </span><span class="s1">Bj </span><span class="s2">* </span><span class="s1">n</span><span class="s2">[</span><span class="s1">i</span><span class="s2">])**</span><span class="s5">2 </span><span class="s2">/ (</span><span class="s1">Bj </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">- </span><span class="s1">Bj</span><span class="s2">))</span>
        <span class="s1">A2kN </span><span class="s2">+= </span><span class="s1">inner</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">() / </span><span class="s1">n</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]</span>
    <span class="s0">return </span><span class="s1">A2kN</span>


<span class="s1">Anderson_ksampResult </span><span class="s2">= </span><span class="s1">_make_tuple_bunch</span><span class="s2">(</span>
    <span class="s4">'Anderson_ksampResult'</span><span class="s2">,</span>
    <span class="s2">[</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'critical_values'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">], []</span>
<span class="s2">)</span>


<span class="s0">def </span><span class="s1">anderson_ksamp</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">midrank</span><span class="s2">=</span><span class="s0">True</span><span class="s2">, *, </span><span class="s1">method</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;The Anderson-Darling test for k-samples. 
 
    The k-sample Anderson-Darling test is a modification of the 
    one-sample Anderson-Darling test. It tests the null hypothesis 
    that k-samples are drawn from the same population without having 
    to specify the distribution function of that population. The 
    critical values depend on the number of samples. 
 
    Parameters 
    ---------- 
    samples : sequence of 1-D array_like 
        Array of sample data in arrays. 
    midrank : bool, optional 
        Type of Anderson-Darling test which is computed. Default 
        (True) is the midrank test applicable to continuous and 
        discrete populations. If False, the right side empirical 
        distribution is used. 
    method : PermutationMethod, optional 
        Defines the method used to compute the p-value. If `method` is an 
        instance of `PermutationMethod`, the p-value is computed using 
        `scipy.stats.permutation_test` with the provided configuration options 
        and other appropriate settings. Otherwise, the p-value is interpolated 
        from tabulated values. 
 
    Returns 
    ------- 
    res : Anderson_ksampResult 
        An object containing attributes: 
 
        statistic : float 
            Normalized k-sample Anderson-Darling test statistic. 
        critical_values : array 
            The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%, 
            0.5%, 0.1%. 
        pvalue : float 
            The approximate p-value of the test. If `method` is not 
            provided, the value is floored / capped at 0.1% / 25%. 
 
    Raises 
    ------ 
    ValueError 
        If fewer than 2 samples are provided, a sample is empty, or no 
        distinct observations are in the samples. 
 
    See Also 
    -------- 
    ks_2samp : 2 sample Kolmogorov-Smirnov test 
    anderson : 1 sample Anderson-Darling test 
 
    Notes 
    ----- 
    [1]_ defines three versions of the k-sample Anderson-Darling test: 
    one for continuous distributions and two for discrete 
    distributions, in which ties between samples may occur. The 
    default of this routine is to compute the version based on the 
    midrank empirical distribution function. This test is applicable 
    to continuous and discrete data. If midrank is set to False, the 
    right side empirical distribution is used for a test for discrete 
    data. According to [1]_, the two discrete test statistics differ 
    only slightly if a few collisions due to round-off errors occur in 
    the test not adjusted for ties between samples. 
 
    The critical values corresponding to the significance levels from 0.01 
    to 0.25 are taken from [1]_. p-values are floored / capped 
    at 0.1% / 25%. Since the range of critical values might be extended in 
    future releases, it is recommended not to test ``p == 0.25``, but rather 
    ``p &gt;= 0.25`` (analogously for the lower bound). 
 
    .. versionadded:: 0.14.0 
 
    References 
    ---------- 
    .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample 
           Anderson-Darling Tests, Journal of the American Statistical 
           Association, Vol. 82, pp. 918-924. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; res = stats.anderson_ksamp([rng.normal(size=50), 
    ... rng.normal(loc=0.5, size=30)]) 
    &gt;&gt;&gt; res.statistic, res.pvalue 
    (1.974403288713695, 0.04991293614572478) 
    &gt;&gt;&gt; res.critical_values 
    array([0.325, 1.226, 1.961, 2.718, 3.752, 4.592, 6.546]) 
 
    The null hypothesis that the two random samples come from the same 
    distribution can be rejected at the 5% level because the returned 
    test value is greater than the critical value for 5% (1.961) but 
    not at the 2.5% level. The interpolation gives an approximate 
    p-value of 4.99%. 
 
    &gt;&gt;&gt; samples = [rng.normal(size=50), rng.normal(size=30), 
    ...            rng.normal(size=20)] 
    &gt;&gt;&gt; res = stats.anderson_ksamp(samples) 
    &gt;&gt;&gt; res.statistic, res.pvalue 
    (-0.29103725200789504, 0.25) 
    &gt;&gt;&gt; res.critical_values 
    array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856, 
      4.07210043, 5.56419101]) 
 
    The null hypothesis cannot be rejected for three samples from an 
    identical distribution. The reported p-value (25%) has been capped and 
    may not be very accurate (since it corresponds to the value 0.449 
    whereas the statistic is -0.291). 
 
    In such cases where the p-value is capped or when sample sizes are 
    small, a permutation test may be more accurate. 
 
    &gt;&gt;&gt; method = stats.PermutationMethod(n_resamples=9999, random_state=rng) 
    &gt;&gt;&gt; res = stats.anderson_ksamp(samples, method=method) 
    &gt;&gt;&gt; res.pvalue 
    0.5254 
 
    &quot;&quot;&quot;</span>
    <span class="s1">k </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s2">(</span><span class="s1">k </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;anderson_ksamp needs at least two samples&quot;</span><span class="s2">)</span>

    <span class="s1">samples </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">map</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">, </span><span class="s1">samples</span><span class="s2">))</span>
    <span class="s1">Z </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">hstack</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">))</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s1">Z</span><span class="s2">.</span><span class="s1">size</span>
    <span class="s1">Zstar </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">unique</span><span class="s2">(</span><span class="s1">Z</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">Zstar</span><span class="s2">.</span><span class="s1">size </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;anderson_ksamp needs more than one distinct &quot;</span>
                         <span class="s4">&quot;observation&quot;</span><span class="s2">)</span>

    <span class="s1">n </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s1">sample</span><span class="s2">.</span><span class="s1">size </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">])</span>
    <span class="s0">if </span><span class="s1">np</span><span class="s2">.</span><span class="s1">any</span><span class="s2">(</span><span class="s1">n </span><span class="s2">== </span><span class="s5">0</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;anderson_ksamp encountered sample without &quot;</span>
                         <span class="s4">&quot;observations&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">midrank</span><span class="s2">:</span>
        <span class="s1">A2kN_fun </span><span class="s2">= </span><span class="s1">_anderson_ksamp_midrank</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">A2kN_fun </span><span class="s2">= </span><span class="s1">_anderson_ksamp_right</span>
    <span class="s1">A2kN </span><span class="s2">= </span><span class="s1">A2kN_fun</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">Z</span><span class="s2">, </span><span class="s1">Zstar</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">N</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">statistic</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">A2kN_fun</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">Z</span><span class="s2">, </span><span class="s1">Zstar</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">N</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">method </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s1">res </span><span class="s2">= </span><span class="s1">stats</span><span class="s2">.</span><span class="s1">permutation_test</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">statistic</span><span class="s2">, **</span><span class="s1">method</span><span class="s2">.</span><span class="s1">_asdict</span><span class="s2">(),</span>
                                     <span class="s1">alternative</span><span class="s2">=</span><span class="s4">'greater'</span><span class="s2">)</span>

    <span class="s1">H </span><span class="s2">= (</span><span class="s5">1. </span><span class="s2">/ </span><span class="s1">n</span><span class="s2">).</span><span class="s1">sum</span><span class="s2">()</span>
    <span class="s1">hs_cs </span><span class="s2">= (</span><span class="s5">1. </span><span class="s2">/ </span><span class="s1">arange</span><span class="s2">(</span><span class="s1">N </span><span class="s2">- </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, -</span><span class="s5">1</span><span class="s2">)).</span><span class="s1">cumsum</span><span class="s2">()</span>
    <span class="s1">h </span><span class="s2">= </span><span class="s1">hs_cs</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">] + </span><span class="s5">1</span>
    <span class="s1">g </span><span class="s2">= (</span><span class="s1">hs_cs </span><span class="s2">/ </span><span class="s1">arange</span><span class="s2">(</span><span class="s5">2</span><span class="s2">, </span><span class="s1">N</span><span class="s2">)).</span><span class="s1">sum</span><span class="s2">()</span>

    <span class="s1">a </span><span class="s2">= (</span><span class="s5">4</span><span class="s2">*</span><span class="s1">g </span><span class="s2">- </span><span class="s5">6</span><span class="s2">) * (</span><span class="s1">k </span><span class="s2">- </span><span class="s5">1</span><span class="s2">) + (</span><span class="s5">10 </span><span class="s2">- </span><span class="s5">6</span><span class="s2">*</span><span class="s1">g</span><span class="s2">)*</span><span class="s1">H</span>
    <span class="s1">b </span><span class="s2">= (</span><span class="s5">2</span><span class="s2">*</span><span class="s1">g </span><span class="s2">- </span><span class="s5">4</span><span class="s2">)*</span><span class="s1">k</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">+ </span><span class="s5">8</span><span class="s2">*</span><span class="s1">h</span><span class="s2">*</span><span class="s1">k </span><span class="s2">+ (</span><span class="s5">2</span><span class="s2">*</span><span class="s1">g </span><span class="s2">- </span><span class="s5">14</span><span class="s2">*</span><span class="s1">h </span><span class="s2">- </span><span class="s5">4</span><span class="s2">)*</span><span class="s1">H </span><span class="s2">- </span><span class="s5">8</span><span class="s2">*</span><span class="s1">h </span><span class="s2">+ </span><span class="s5">4</span><span class="s2">*</span><span class="s1">g </span><span class="s2">- </span><span class="s5">6</span>
    <span class="s1">c </span><span class="s2">= (</span><span class="s5">6</span><span class="s2">*</span><span class="s1">h </span><span class="s2">+ </span><span class="s5">2</span><span class="s2">*</span><span class="s1">g </span><span class="s2">- </span><span class="s5">2</span><span class="s2">)*</span><span class="s1">k</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">+ (</span><span class="s5">4</span><span class="s2">*</span><span class="s1">h </span><span class="s2">- </span><span class="s5">4</span><span class="s2">*</span><span class="s1">g </span><span class="s2">+ </span><span class="s5">6</span><span class="s2">)*</span><span class="s1">k </span><span class="s2">+ (</span><span class="s5">2</span><span class="s2">*</span><span class="s1">h </span><span class="s2">- </span><span class="s5">6</span><span class="s2">)*</span><span class="s1">H </span><span class="s2">+ </span><span class="s5">4</span><span class="s2">*</span><span class="s1">h</span>
    <span class="s1">d </span><span class="s2">= (</span><span class="s5">2</span><span class="s2">*</span><span class="s1">h </span><span class="s2">+ </span><span class="s5">6</span><span class="s2">)*</span><span class="s1">k</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">- </span><span class="s5">4</span><span class="s2">*</span><span class="s1">h</span><span class="s2">*</span><span class="s1">k</span>
    <span class="s1">sigmasq </span><span class="s2">= (</span><span class="s1">a</span><span class="s2">*</span><span class="s1">N</span><span class="s2">**</span><span class="s5">3 </span><span class="s2">+ </span><span class="s1">b</span><span class="s2">*</span><span class="s1">N</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">+ </span><span class="s1">c</span><span class="s2">*</span><span class="s1">N </span><span class="s2">+ </span><span class="s1">d</span><span class="s2">) / ((</span><span class="s1">N </span><span class="s2">- </span><span class="s5">1.</span><span class="s2">) * (</span><span class="s1">N </span><span class="s2">- </span><span class="s5">2.</span><span class="s2">) * (</span><span class="s1">N </span><span class="s2">- </span><span class="s5">3.</span><span class="s2">))</span>
    <span class="s1">m </span><span class="s2">= </span><span class="s1">k </span><span class="s2">- </span><span class="s5">1</span>
    <span class="s1">A2 </span><span class="s2">= (</span><span class="s1">A2kN </span><span class="s2">- </span><span class="s1">m</span><span class="s2">) / </span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">sigmasq</span><span class="s2">)</span>

    <span class="s3"># The b_i values are the interpolation coefficients from Table 2</span>
    <span class="s3"># of Scholz and Stephens 1987</span>
    <span class="s1">b0 </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.675</span><span class="s2">, </span><span class="s5">1.281</span><span class="s2">, </span><span class="s5">1.645</span><span class="s2">, </span><span class="s5">1.96</span><span class="s2">, </span><span class="s5">2.326</span><span class="s2">, </span><span class="s5">2.573</span><span class="s2">, </span><span class="s5">3.085</span><span class="s2">])</span>
    <span class="s1">b1 </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([-</span><span class="s5">0.245</span><span class="s2">, </span><span class="s5">0.25</span><span class="s2">, </span><span class="s5">0.678</span><span class="s2">, </span><span class="s5">1.149</span><span class="s2">, </span><span class="s5">1.822</span><span class="s2">, </span><span class="s5">2.364</span><span class="s2">, </span><span class="s5">3.615</span><span class="s2">])</span>
    <span class="s1">b2 </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([-</span><span class="s5">0.105</span><span class="s2">, -</span><span class="s5">0.305</span><span class="s2">, -</span><span class="s5">0.362</span><span class="s2">, -</span><span class="s5">0.391</span><span class="s2">, -</span><span class="s5">0.396</span><span class="s2">, -</span><span class="s5">0.345</span><span class="s2">, -</span><span class="s5">0.154</span><span class="s2">])</span>
    <span class="s1">critical </span><span class="s2">= </span><span class="s1">b0 </span><span class="s2">+ </span><span class="s1">b1 </span><span class="s2">/ </span><span class="s1">math</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">m</span><span class="s2">) + </span><span class="s1">b2 </span><span class="s2">/ </span><span class="s1">m</span>

    <span class="s1">sig </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s5">0.25</span><span class="s2">, </span><span class="s5">0.1</span><span class="s2">, </span><span class="s5">0.05</span><span class="s2">, </span><span class="s5">0.025</span><span class="s2">, </span><span class="s5">0.01</span><span class="s2">, </span><span class="s5">0.005</span><span class="s2">, </span><span class="s5">0.001</span><span class="s2">])</span>

    <span class="s0">if </span><span class="s1">A2 </span><span class="s2">&lt; </span><span class="s1">critical</span><span class="s2">.</span><span class="s1">min</span><span class="s2">() </span><span class="s0">and </span><span class="s1">method </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">p </span><span class="s2">= </span><span class="s1">sig</span><span class="s2">.</span><span class="s1">max</span><span class="s2">()</span>
        <span class="s1">msg </span><span class="s2">= (</span><span class="s4">f&quot;p-value capped: true value larger than </span><span class="s0">{</span><span class="s1">p</span><span class="s0">}</span><span class="s4">. Consider &quot;</span>
               <span class="s4">&quot;specifying `method` &quot;</span>
               <span class="s4">&quot;(e.g. `method=stats.PermutationMethod()`.)&quot;</span><span class="s2">)</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s1">msg</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">A2 </span><span class="s2">&gt; </span><span class="s1">critical</span><span class="s2">.</span><span class="s1">max</span><span class="s2">() </span><span class="s0">and </span><span class="s1">method </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">p </span><span class="s2">= </span><span class="s1">sig</span><span class="s2">.</span><span class="s1">min</span><span class="s2">()</span>
        <span class="s1">msg </span><span class="s2">= (</span><span class="s4">f&quot;p-value floored: true value smaller than </span><span class="s0">{</span><span class="s1">p</span><span class="s0">}</span><span class="s4">. Consider &quot;</span>
               <span class="s4">&quot;specifying `method` &quot;</span>
               <span class="s4">&quot;(e.g. `method=stats.PermutationMethod()`.)&quot;</span><span class="s2">)</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s1">msg</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">method </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s3"># interpolation of probit of significance level</span>
        <span class="s1">pf </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">polyfit</span><span class="s2">(</span><span class="s1">critical</span><span class="s2">, </span><span class="s1">log</span><span class="s2">(</span><span class="s1">sig</span><span class="s2">), </span><span class="s5">2</span><span class="s2">)</span>
        <span class="s1">p </span><span class="s2">= </span><span class="s1">math</span><span class="s2">.</span><span class="s1">exp</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">polyval</span><span class="s2">(</span><span class="s1">pf</span><span class="s2">, </span><span class="s1">A2</span><span class="s2">))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">p </span><span class="s2">= </span><span class="s1">res</span><span class="s2">.</span><span class="s1">pvalue </span><span class="s0">if </span><span class="s1">method </span><span class="s0">is not None else </span><span class="s1">p</span>

    <span class="s3"># create result object with alias for backward compatibility</span>
    <span class="s1">res </span><span class="s2">= </span><span class="s1">Anderson_ksampResult</span><span class="s2">(</span><span class="s1">A2</span><span class="s2">, </span><span class="s1">critical</span><span class="s2">, </span><span class="s1">p</span><span class="s2">)</span>
    <span class="s1">res</span><span class="s2">.</span><span class="s1">significance_level </span><span class="s2">= </span><span class="s1">p</span>
    <span class="s0">return </span><span class="s1">res</span>


<span class="s1">AnsariResult </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'AnsariResult'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">))</span>


<span class="s0">class </span><span class="s1">_ABW</span><span class="s2">:</span>
    <span class="s6">&quot;&quot;&quot;Distribution of Ansari-Bradley W-statistic under the null hypothesis.&quot;&quot;&quot;</span>
    <span class="s3"># TODO: calculate exact distribution considering ties</span>
    <span class="s3"># We could avoid summing over more than half the frequencies,</span>
    <span class="s3"># but initially it doesn't seem worth the extra complexity</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Minimal initializer.&quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">m </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">n </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">astart </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">total </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">freqs </span><span class="s2">= </span><span class="s0">None</span>

    <span class="s0">def </span><span class="s1">_recalc</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;When necessary, recalculate exact distribution.&quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">n </span><span class="s2">!= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">n </span><span class="s0">or </span><span class="s1">m </span><span class="s2">!= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">m</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">n</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">m </span><span class="s2">= </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span>
            <span class="s3"># distribution is NOT symmetric when m + n is odd</span>
            <span class="s3"># n is len(x), m is len(y), and ratio of scales is defined x/y</span>
            <span class="s1">astart</span><span class="s2">, </span><span class="s1">a1</span><span class="s2">, </span><span class="s1">_ </span><span class="s2">= </span><span class="s1">gscale</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">astart </span><span class="s2">= </span><span class="s1">astart  </span><span class="s3"># minimum value of statistic</span>
            <span class="s3"># Exact distribution of test statistic under null hypothesis</span>
            <span class="s3"># expressed as frequencies/counts/integers to maintain precision.</span>
            <span class="s3"># Stored as floats to avoid overflow of sums.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">freqs </span><span class="s2">= </span><span class="s1">a1</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">float64</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">total </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">freqs</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">()  </span><span class="s3"># could calculate from m and n</span>
            <span class="s3"># probability mass is self.freqs / self.total;</span>

    <span class="s0">def </span><span class="s1">pmf</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Probability mass function.&quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_recalc</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">)</span>
        <span class="s3"># The convention here is that PMF at k = 12.5 is the same as at k = 12,</span>
        <span class="s3"># -&gt; use `floor` in case of ties.</span>
        <span class="s1">ind </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">floor</span><span class="s2">(</span><span class="s1">k </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">astart</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">int</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">freqs</span><span class="s2">[</span><span class="s1">ind</span><span class="s2">] / </span><span class="s1">self</span><span class="s2">.</span><span class="s1">total</span>

    <span class="s0">def </span><span class="s1">cdf</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Cumulative distribution function.&quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_recalc</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">)</span>
        <span class="s3"># Null distribution derived without considering ties is</span>
        <span class="s3"># approximate. Round down to avoid Type I error.</span>
        <span class="s1">ind </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ceil</span><span class="s2">(</span><span class="s1">k </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">astart</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">int</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">freqs</span><span class="s2">[:</span><span class="s1">ind</span><span class="s2">+</span><span class="s5">1</span><span class="s2">].</span><span class="s1">sum</span><span class="s2">() / </span><span class="s1">self</span><span class="s2">.</span><span class="s1">total</span>

    <span class="s0">def </span><span class="s1">sf</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Survival function.&quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_recalc</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">)</span>
        <span class="s3"># Null distribution derived without considering ties is</span>
        <span class="s3"># approximate. Round down to avoid Type I error.</span>
        <span class="s1">ind </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">floor</span><span class="s2">(</span><span class="s1">k </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">astart</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">int</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">freqs</span><span class="s2">[</span><span class="s1">ind</span><span class="s2">:].</span><span class="s1">sum</span><span class="s2">() / </span><span class="s1">self</span><span class="s2">.</span><span class="s1">total</span>


<span class="s3"># Maintain state for faster repeat calls to ansari w/ method='exact'</span>
<span class="s1">_abw_state </span><span class="s2">= </span><span class="s1">_ABW</span><span class="s2">()</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s1">AnsariResult</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">ansari</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">alternative</span><span class="s2">=</span><span class="s4">'two-sided'</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Perform the Ansari-Bradley test for equal scale parameters. 
 
    The Ansari-Bradley test ([1]_, [2]_) is a non-parametric test 
    for the equality of the scale parameter of the distributions 
    from which two samples were drawn. The null hypothesis states that 
    the ratio of the scale of the distribution underlying `x` to the scale 
    of the distribution underlying `y` is 1. 
 
    Parameters 
    ---------- 
    x, y : array_like 
        Arrays of sample data. 
    alternative : {'two-sided', 'less', 'greater'}, optional 
        Defines the alternative hypothesis. Default is 'two-sided'. 
        The following options are available: 
 
        * 'two-sided': the ratio of scales is not equal to 1. 
        * 'less': the ratio of scales is less than 1. 
        * 'greater': the ratio of scales is greater than 1. 
 
        .. versionadded:: 1.7.0 
 
    Returns 
    ------- 
    statistic : float 
        The Ansari-Bradley test statistic. 
    pvalue : float 
        The p-value of the hypothesis test. 
 
    See Also 
    -------- 
    fligner : A non-parametric test for the equality of k variances 
    mood : A non-parametric test for the equality of two scale parameters 
 
    Notes 
    ----- 
    The p-value given is exact when the sample sizes are both less than 
    55 and there are no ties, otherwise a normal approximation for the 
    p-value is used. 
 
    References 
    ---------- 
    .. [1] Ansari, A. R. and Bradley, R. A. (1960) Rank-sum tests for 
           dispersions, Annals of Mathematical Statistics, 31, 1174-1189. 
    .. [2] Sprent, Peter and N.C. Smeeton.  Applied nonparametric 
           statistical methods.  3rd ed. Chapman and Hall/CRC. 2001. 
           Section 5.8.2. 
    .. [3] Nathaniel E. Helwig &quot;Nonparametric Dispersion and Equality 
           Tests&quot; at http://users.stat.umn.edu/~helwig/notes/npde-Notes.pdf 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import ansari 
    &gt;&gt;&gt; rng = np.random.default_rng() 
 
    For these examples, we'll create three random data sets.  The first 
    two, with sizes 35 and 25, are drawn from a normal distribution with 
    mean 0 and standard deviation 2.  The third data set has size 25 and 
    is drawn from a normal distribution with standard deviation 1.25. 
 
    &gt;&gt;&gt; x1 = rng.normal(loc=0, scale=2, size=35) 
    &gt;&gt;&gt; x2 = rng.normal(loc=0, scale=2, size=25) 
    &gt;&gt;&gt; x3 = rng.normal(loc=0, scale=1.25, size=25) 
 
    First we apply `ansari` to `x1` and `x2`.  These samples are drawn 
    from the same distribution, so we expect the Ansari-Bradley test 
    should not lead us to conclude that the scales of the distributions 
    are different. 
 
    &gt;&gt;&gt; ansari(x1, x2) 
    AnsariResult(statistic=541.0, pvalue=0.9762532927399098) 
 
    With a p-value close to 1, we cannot conclude that there is a 
    significant difference in the scales (as expected). 
 
    Now apply the test to `x1` and `x3`: 
 
    &gt;&gt;&gt; ansari(x1, x3) 
    AnsariResult(statistic=425.0, pvalue=0.0003087020407974518) 
 
    The probability of observing such an extreme value of the statistic 
    under the null hypothesis of equal scales is only 0.03087%. We take this 
    as evidence against the null hypothesis in favor of the alternative: 
    the scales of the distributions from which the samples were drawn 
    are not equal. 
 
    We can use the `alternative` parameter to perform a one-tailed test. 
    In the above example, the scale of `x1` is greater than `x3` and so 
    the ratio of scales of `x1` and `x3` is greater than 1. This means 
    that the p-value when ``alternative='greater'`` should be near 0 and 
    hence we should be able to reject the null hypothesis: 
 
    &gt;&gt;&gt; ansari(x1, x3, alternative='greater') 
    AnsariResult(statistic=425.0, pvalue=0.0001543510203987259) 
 
    As we can see, the p-value is indeed quite low. Use of 
    ``alternative='less'`` should thus yield a large p-value: 
 
    &gt;&gt;&gt; ansari(x1, x3, alternative='less') 
    AnsariResult(statistic=425.0, pvalue=0.9998643258449039) 
 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">alternative </span><span class="s0">not in </span><span class="s2">{</span><span class="s4">'two-sided'</span><span class="s2">, </span><span class="s4">'greater'</span><span class="s2">, </span><span class="s4">'less'</span><span class="s2">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;'alternative' must be 'two-sided',&quot;</span>
                         <span class="s4">&quot; 'greater', or 'less'.&quot;</span><span class="s2">)</span>
    <span class="s1">x</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">), </span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">n </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s1">m </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">m </span><span class="s2">&lt; </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Not enough other observations.&quot;</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">n </span><span class="s2">&lt; </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Not enough test observations.&quot;</span><span class="s2">)</span>

    <span class="s1">N </span><span class="s2">= </span><span class="s1">m </span><span class="s2">+ </span><span class="s1">n</span>
    <span class="s1">xy </span><span class="s2">= </span><span class="s1">r_</span><span class="s2">[</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">]  </span><span class="s3"># combine</span>
    <span class="s1">rank </span><span class="s2">= </span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">rankdata</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">)</span>
    <span class="s1">symrank </span><span class="s2">= </span><span class="s1">amin</span><span class="s2">(</span><span class="s1">array</span><span class="s2">((</span><span class="s1">rank</span><span class="s2">, </span><span class="s1">N </span><span class="s2">- </span><span class="s1">rank </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">)), </span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">AB </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">symrank</span><span class="s2">[:</span><span class="s1">n</span><span class="s2">], </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">uxy </span><span class="s2">= </span><span class="s1">unique</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">)</span>
    <span class="s1">repeats </span><span class="s2">= (</span><span class="s1">len</span><span class="s2">(</span><span class="s1">uxy</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">))</span>
    <span class="s1">exact </span><span class="s2">= ((</span><span class="s1">m </span><span class="s2">&lt; </span><span class="s5">55</span><span class="s2">) </span><span class="s0">and </span><span class="s2">(</span><span class="s1">n </span><span class="s2">&lt; </span><span class="s5">55</span><span class="s2">) </span><span class="s0">and not </span><span class="s1">repeats</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">repeats </span><span class="s0">and </span><span class="s2">(</span><span class="s1">m </span><span class="s2">&lt; </span><span class="s5">55 </span><span class="s0">or </span><span class="s1">n </span><span class="s2">&lt; </span><span class="s5">55</span><span class="s2">):</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s4">&quot;Ties preclude use of exact statistic.&quot;</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">exact</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">alternative </span><span class="s2">== </span><span class="s4">'two-sided'</span><span class="s2">:</span>
            <span class="s1">pval </span><span class="s2">= </span><span class="s5">2.0 </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">minimum</span><span class="s2">(</span><span class="s1">_abw_state</span><span class="s2">.</span><span class="s1">cdf</span><span class="s2">(</span><span class="s1">AB</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">),</span>
                                    <span class="s1">_abw_state</span><span class="s2">.</span><span class="s1">sf</span><span class="s2">(</span><span class="s1">AB</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">))</span>
        <span class="s0">elif </span><span class="s1">alternative </span><span class="s2">== </span><span class="s4">'greater'</span><span class="s2">:</span>
            <span class="s3"># AB statistic is _smaller_ when ratio of scales is larger,</span>
            <span class="s3"># so this is the opposite of the usual calculation</span>
            <span class="s1">pval </span><span class="s2">= </span><span class="s1">_abw_state</span><span class="s2">.</span><span class="s1">cdf</span><span class="s2">(</span><span class="s1">AB</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">pval </span><span class="s2">= </span><span class="s1">_abw_state</span><span class="s2">.</span><span class="s1">sf</span><span class="s2">(</span><span class="s1">AB</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">AnsariResult</span><span class="s2">(</span><span class="s1">AB</span><span class="s2">, </span><span class="s1">min</span><span class="s2">(</span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">))</span>

    <span class="s3"># otherwise compute normal approximation</span>
    <span class="s0">if </span><span class="s1">N </span><span class="s2">% </span><span class="s5">2</span><span class="s2">:  </span><span class="s3"># N odd</span>
        <span class="s1">mnAB </span><span class="s2">= </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N</span><span class="s2">+</span><span class="s5">1.0</span><span class="s2">)**</span><span class="s5">2 </span><span class="s2">/ </span><span class="s5">4.0 </span><span class="s2">/ </span><span class="s1">N</span>
        <span class="s1">varAB </span><span class="s2">= </span><span class="s1">n </span><span class="s2">* </span><span class="s1">m </span><span class="s2">* (</span><span class="s1">N</span><span class="s2">+</span><span class="s5">1.0</span><span class="s2">) * (</span><span class="s5">3</span><span class="s2">+</span><span class="s1">N</span><span class="s2">**</span><span class="s5">2</span><span class="s2">) / (</span><span class="s5">48.0 </span><span class="s2">* </span><span class="s1">N</span><span class="s2">**</span><span class="s5">2</span><span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">mnAB </span><span class="s2">= </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N</span><span class="s2">+</span><span class="s5">2.0</span><span class="s2">) / </span><span class="s5">4.0</span>
        <span class="s1">varAB </span><span class="s2">= </span><span class="s1">m </span><span class="s2">* </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N</span><span class="s2">+</span><span class="s5">2</span><span class="s2">) * (</span><span class="s1">N</span><span class="s2">-</span><span class="s5">2.0</span><span class="s2">) / </span><span class="s5">48 </span><span class="s2">/ (</span><span class="s1">N</span><span class="s2">-</span><span class="s5">1.0</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">repeats</span><span class="s2">:   </span><span class="s3"># adjust variance estimates</span>
        <span class="s3"># compute np.sum(tj * rj**2,axis=0)</span>
        <span class="s1">fac </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">symrank</span><span class="s2">**</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">N </span><span class="s2">% </span><span class="s5">2</span><span class="s2">:  </span><span class="s3"># N odd</span>
            <span class="s1">varAB </span><span class="s2">= </span><span class="s1">m </span><span class="s2">* </span><span class="s1">n </span><span class="s2">* (</span><span class="s5">16</span><span class="s2">*</span><span class="s1">N</span><span class="s2">*</span><span class="s1">fac </span><span class="s2">- (</span><span class="s1">N</span><span class="s2">+</span><span class="s5">1</span><span class="s2">)**</span><span class="s5">4</span><span class="s2">) / (</span><span class="s5">16.0 </span><span class="s2">* </span><span class="s1">N</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">* (</span><span class="s1">N</span><span class="s2">-</span><span class="s5">1</span><span class="s2">))</span>
        <span class="s0">else</span><span class="s2">:  </span><span class="s3"># N even</span>
            <span class="s1">varAB </span><span class="s2">= </span><span class="s1">m </span><span class="s2">* </span><span class="s1">n </span><span class="s2">* (</span><span class="s5">16</span><span class="s2">*</span><span class="s1">fac </span><span class="s2">- </span><span class="s1">N</span><span class="s2">*(</span><span class="s1">N</span><span class="s2">+</span><span class="s5">2</span><span class="s2">)**</span><span class="s5">2</span><span class="s2">) / (</span><span class="s5">16.0 </span><span class="s2">* </span><span class="s1">N </span><span class="s2">* (</span><span class="s1">N</span><span class="s2">-</span><span class="s5">1</span><span class="s2">))</span>

    <span class="s3"># Small values of AB indicate larger dispersion for the x sample.</span>
    <span class="s3"># Large values of AB indicate larger dispersion for the y sample.</span>
    <span class="s3"># This is opposite to the way we define the ratio of scales. see [1]_.</span>
    <span class="s1">z </span><span class="s2">= (</span><span class="s1">mnAB </span><span class="s2">- </span><span class="s1">AB</span><span class="s2">) / </span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">varAB</span><span class="s2">)</span>
    <span class="s1">pvalue </span><span class="s2">= </span><span class="s1">_get_pvalue</span><span class="s2">(</span><span class="s1">z</span><span class="s2">, </span><span class="s1">_SimpleNormal</span><span class="s2">(), </span><span class="s1">alternative</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">np</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">AnsariResult</span><span class="s2">(</span><span class="s1">AB</span><span class="s2">[()], </span><span class="s1">pvalue</span><span class="s2">[()])</span>


<span class="s1">BartlettResult </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'BartlettResult'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s1">BartlettResult</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s0">None</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">bartlett</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Perform Bartlett's test for equal variances. 
 
    Bartlett's test tests the null hypothesis that all input samples 
    are from populations with equal variances.  For samples 
    from significantly non-normal populations, Levene's test 
    `levene` is more robust. 
 
    Parameters 
    ---------- 
    sample1, sample2, ... : array_like 
        arrays of sample data.  Only 1d arrays are accepted, they may have 
        different lengths. 
 
    Returns 
    ------- 
    statistic : float 
        The test statistic. 
    pvalue : float 
        The p-value of the test. 
 
    See Also 
    -------- 
    fligner : A non-parametric test for the equality of k variances 
    levene : A robust parametric test for equality of k variances 
 
    Notes 
    ----- 
    Conover et al. (1981) examine many of the existing parametric and 
    nonparametric tests by extensive simulations and they conclude that the 
    tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be 
    superior in terms of robustness of departures from normality and power 
    ([3]_). 
 
    References 
    ---------- 
    .. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm 
    .. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical 
              Methods, Eighth Edition, Iowa State University Press. 
    .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and 
           Hypothesis Testing based on Quadratic Inference Function. Technical 
           Report #99-03, Center for Likelihood Studies, Pennsylvania State 
           University. 
    .. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical 
           Tests. Proceedings of the Royal Society of London. Series A, 
           Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282. 
    .. [5] C.I. BLISS (1952), The Statistics of Bioassay: With Special 
           Reference to the Vitamins, pp 499-503, 
           :doi:`10.1016/C2013-0-12584-6`. 
    .. [6] B. Phipson and G. K. Smyth. &quot;Permutation P-values Should Never Be 
           Zero: Calculating Exact P-values When Permutations Are Randomly 
           Drawn.&quot; Statistical Applications in Genetics and Molecular Biology 
           9.1 (2010). 
    .. [7] Ludbrook, J., &amp; Dudley, H. (1998). Why permutation tests are 
           superior to t and F tests in biomedical research. The American 
           Statistician, 52(2), 127-132. 
 
    Examples 
    -------- 
    In [5]_, the influence of vitamin C on the tooth growth of guinea pigs 
    was investigated. In a control study, 60 subjects were divided into 
    small dose, medium dose, and large dose groups that received 
    daily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively. 
    After 42 days, the tooth growth was measured. 
 
    The ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record 
    tooth growth measurements of the three groups in microns. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; small_dose = np.array([ 
    ...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7, 
    ...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7 
    ... ]) 
    &gt;&gt;&gt; medium_dose = np.array([ 
    ...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5, 
    ...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3 
    ... ]) 
    &gt;&gt;&gt; large_dose = np.array([ 
    ...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5, 
    ...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23 
    ... ]) 
 
    The `bartlett` statistic is sensitive to differences in variances 
    between the samples. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; res = stats.bartlett(small_dose, medium_dose, large_dose) 
    &gt;&gt;&gt; res.statistic 
    0.6654670663030519 
 
    The value of the statistic tends to be high when there is a large 
    difference in variances. 
 
    We can test for inequality of variance among the groups by comparing the 
    observed value of the statistic against the null distribution: the 
    distribution of statistic values derived under the null hypothesis that 
    the population variances of the three groups are equal. 
 
    For this test, the null distribution follows the chi-square distribution 
    as shown below. 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; k = 3  # number of samples 
    &gt;&gt;&gt; dist = stats.chi2(df=k-1) 
    &gt;&gt;&gt; val = np.linspace(0, 5, 100) 
    &gt;&gt;&gt; pdf = dist.pdf(val) 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; def plot(ax):  # we'll reuse this 
    ...     ax.plot(val, pdf, color='C0') 
    ...     ax.set_title(&quot;Bartlett Test Null Distribution&quot;) 
    ...     ax.set_xlabel(&quot;statistic&quot;) 
    ...     ax.set_ylabel(&quot;probability density&quot;) 
    ...     ax.set_xlim(0, 5) 
    ...     ax.set_ylim(0, 1) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    The comparison is quantified by the p-value: the proportion of values in 
    the null distribution greater than or equal to the observed value of the 
    statistic. 
 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; pvalue = dist.sf(res.statistic) 
    &gt;&gt;&gt; annotation = (f'p-value={pvalue:.3f}\n(shaded area)') 
    &gt;&gt;&gt; props = dict(facecolor='black', width=1, headwidth=5, headlength=8) 
    &gt;&gt;&gt; _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props) 
    &gt;&gt;&gt; i = val &gt;= res.statistic 
    &gt;&gt;&gt; ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0') 
    &gt;&gt;&gt; plt.show() 
 
    &gt;&gt;&gt; res.pvalue 
    0.71696121509966 
 
    If the p-value is &quot;small&quot; - that is, if there is a low probability of 
    sampling data from distributions with identical variances that produces 
    such an extreme value of the statistic - this may be taken as evidence 
    against the null hypothesis in favor of the alternative: the variances of 
    the groups are not equal. Note that: 
 
    - The inverse is not true; that is, the test is not used to provide 
      evidence for the null hypothesis. 
    - The threshold for values that will be considered &quot;small&quot; is a choice that 
      should be made before the data is analyzed [6]_ with consideration of the 
      risks of both false positives (incorrectly rejecting the null hypothesis) 
      and false negatives (failure to reject a false null hypothesis). 
    - Small p-values are not evidence for a *large* effect; rather, they can 
      only provide evidence for a &quot;significant&quot; effect, meaning that they are 
      unlikely to have occurred under the null hypothesis. 
 
    Note that the chi-square distribution provides the null distribution 
    when the observations are normally distributed. For small samples 
    drawn from non-normal populations, it may be more appropriate to 
    perform a 
    permutation test: Under the null hypothesis that all three samples were 
    drawn from the same population, each of the measurements is equally likely 
    to have been observed in any of the three samples. Therefore, we can form 
    a randomized null distribution by calculating the statistic under many 
    randomly-generated partitionings of the observations into the three 
    samples. 
 
    &gt;&gt;&gt; def statistic(*samples): 
    ...     return stats.bartlett(*samples).statistic 
    &gt;&gt;&gt; ref = stats.permutation_test( 
    ...     (small_dose, medium_dose, large_dose), statistic, 
    ...     permutation_type='independent', alternative='greater' 
    ... ) 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; bins = np.linspace(0, 5, 25) 
    &gt;&gt;&gt; ax.hist( 
    ...     ref.null_distribution, bins=bins, density=True, facecolor=&quot;C1&quot; 
    ... ) 
    &gt;&gt;&gt; ax.legend(['aymptotic approximation\n(many observations)', 
    ...            'randomized null distribution']) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    &gt;&gt;&gt; ref.pvalue  # randomized test p-value 
    0.5387  # may vary 
 
    Note that there is significant disagreement between the p-value calculated 
    here and the asymptotic approximation returned by `bartlett` above. 
    The statistical inferences that can be drawn rigorously from a permutation 
    test are limited; nonetheless, they may be the preferred approach in many 
    circumstances [7]_. 
 
    Following is another generic example where the null hypothesis would be 
    rejected. 
 
    Test whether the lists `a`, `b` and `c` come from populations 
    with equal variances. 
 
    &gt;&gt;&gt; a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99] 
    &gt;&gt;&gt; b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05] 
    &gt;&gt;&gt; c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98] 
    &gt;&gt;&gt; stat, p = stats.bartlett(a, b, c) 
    &gt;&gt;&gt; p 
    1.1254782518834628e-05 
 
    The very small p-value suggests that the populations do not have equal 
    variances. 
 
    This is not surprising, given that the sample variance of `b` is much 
    larger than that of `a` and `c`: 
 
    &gt;&gt;&gt; [np.var(x, ddof=1) for x in [a, b, c]] 
    [0.007054444444444413, 0.13073888888888888, 0.008890000000000002] 
 
    &quot;&quot;&quot;</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">)</span>

    <span class="s1">k </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">k </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Must enter at least two input sample vectors.&quot;</span><span class="s2">)</span>

    <span class="s1">samples </span><span class="s2">= </span><span class="s1">_broadcast_arrays</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">xp</span><span class="s2">)</span>
    <span class="s1">samples </span><span class="s2">= [</span><span class="s1">xp_moveaxis_to_end</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">xp</span><span class="s2">) </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">]</span>

    <span class="s1">Ni </span><span class="s2">= [</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">], </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">sample</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">) </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">]</span>
    <span class="s1">Ni </span><span class="s2">= [</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">broadcast_to</span><span class="s2">(</span><span class="s1">N</span><span class="s2">, </span><span class="s1">samples</span><span class="s2">[</span><span class="s5">0</span><span class="s2">].</span><span class="s1">shape</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">]) </span><span class="s0">for </span><span class="s1">N </span><span class="s0">in </span><span class="s1">Ni</span><span class="s2">]</span>
    <span class="s1">ssq </span><span class="s2">= [</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">, </span><span class="s1">correction</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">) </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">]</span>
    <span class="s1">Ni </span><span class="s2">= [</span><span class="s1">arr</span><span class="s2">[</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">newaxis</span><span class="s2">, ...] </span><span class="s0">for </span><span class="s1">arr </span><span class="s0">in </span><span class="s1">Ni</span><span class="s2">]</span>
    <span class="s1">ssq </span><span class="s2">= [</span><span class="s1">arr</span><span class="s2">[</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">newaxis</span><span class="s2">, ...] </span><span class="s0">for </span><span class="s1">arr </span><span class="s0">in </span><span class="s1">ssq</span><span class="s2">]</span>
    <span class="s1">Ni </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">concat</span><span class="s2">(</span><span class="s1">Ni</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">ssq </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">concat</span><span class="s2">(</span><span class="s1">ssq</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">Ntot </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">Ni</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">spsq </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">((</span><span class="s1">Ni </span><span class="s2">- </span><span class="s5">1</span><span class="s2">)*</span><span class="s1">ssq</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">) / (</span><span class="s1">Ntot </span><span class="s2">- </span><span class="s1">k</span><span class="s2">)</span>
    <span class="s1">numer </span><span class="s2">= (</span><span class="s1">Ntot </span><span class="s2">- </span><span class="s1">k</span><span class="s2">) * </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">spsq</span><span class="s2">) - </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">((</span><span class="s1">Ni </span><span class="s2">- </span><span class="s5">1</span><span class="s2">)*</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">ssq</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">denom </span><span class="s2">= </span><span class="s5">1 </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">/(</span><span class="s5">3</span><span class="s2">*(</span><span class="s1">k </span><span class="s2">- </span><span class="s5">1</span><span class="s2">)) * ((</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s5">1</span><span class="s2">/(</span><span class="s1">Ni </span><span class="s2">- </span><span class="s5">1</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)) - </span><span class="s5">1</span><span class="s2">/(</span><span class="s1">Ntot </span><span class="s2">- </span><span class="s1">k</span><span class="s2">))</span>
    <span class="s1">T </span><span class="s2">= </span><span class="s1">numer </span><span class="s2">/ </span><span class="s1">denom</span>

    <span class="s1">chi2 </span><span class="s2">= </span><span class="s1">_SimpleChi2</span><span class="s2">(</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">k</span><span class="s2">-</span><span class="s5">1</span><span class="s2">))</span>
    <span class="s1">pvalue </span><span class="s2">= </span><span class="s1">_get_pvalue</span><span class="s2">(</span><span class="s1">T</span><span class="s2">, </span><span class="s1">chi2</span><span class="s2">, </span><span class="s1">alternative</span><span class="s2">=</span><span class="s4">'greater'</span><span class="s2">, </span><span class="s1">symmetric</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">xp</span><span class="s2">)</span>

    <span class="s1">T </span><span class="s2">= </span><span class="s1">T</span><span class="s2">[()] </span><span class="s0">if </span><span class="s1">T</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">== </span><span class="s5">0 </span><span class="s0">else </span><span class="s1">T</span>
    <span class="s1">pvalue </span><span class="s2">= </span><span class="s1">pvalue</span><span class="s2">[()] </span><span class="s0">if </span><span class="s1">pvalue</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">== </span><span class="s5">0 </span><span class="s0">else </span><span class="s1">pvalue</span>

    <span class="s0">return </span><span class="s1">BartlettResult</span><span class="s2">(</span><span class="s1">T</span><span class="s2">, </span><span class="s1">pvalue</span><span class="s2">)</span>


<span class="s1">LeveneResult </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'LeveneResult'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s1">LeveneResult</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s0">None</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">levene</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">center</span><span class="s2">=</span><span class="s4">'median'</span><span class="s2">, </span><span class="s1">proportiontocut</span><span class="s2">=</span><span class="s5">0.05</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Perform Levene test for equal variances. 
 
    The Levene test tests the null hypothesis that all input samples 
    are from populations with equal variances.  Levene's test is an 
    alternative to Bartlett's test `bartlett` in the case where 
    there are significant deviations from normality. 
 
    Parameters 
    ---------- 
    sample1, sample2, ... : array_like 
        The sample data, possibly with different lengths. Only one-dimensional 
        samples are accepted. 
    center : {'mean', 'median', 'trimmed'}, optional 
        Which function of the data to use in the test.  The default 
        is 'median'. 
    proportiontocut : float, optional 
        When `center` is 'trimmed', this gives the proportion of data points 
        to cut from each end. (See `scipy.stats.trim_mean`.) 
        Default is 0.05. 
 
    Returns 
    ------- 
    statistic : float 
        The test statistic. 
    pvalue : float 
        The p-value for the test. 
 
    See Also 
    -------- 
    fligner : A non-parametric test for the equality of k variances 
    bartlett : A parametric test for equality of k variances in normal samples 
 
    Notes 
    ----- 
    Three variations of Levene's test are possible.  The possibilities 
    and their recommended usages are: 
 
      * 'median' : Recommended for skewed (non-normal) distributions&gt; 
      * 'mean' : Recommended for symmetric, moderate-tailed distributions. 
      * 'trimmed' : Recommended for heavy-tailed distributions. 
 
    The test version using the mean was proposed in the original article 
    of Levene ([2]_) while the median and trimmed mean have been studied by 
    Brown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe 
    test. 
 
    References 
    ---------- 
    .. [1] https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm 
    .. [2] Levene, H. (1960). In Contributions to Probability and Statistics: 
           Essays in Honor of Harold Hotelling, I. Olkin et al. eds., 
           Stanford University Press, pp. 278-292. 
    .. [3] Brown, M. B. and Forsythe, A. B. (1974), Journal of the American 
           Statistical Association, 69, 364-367 
    .. [4] C.I. BLISS (1952), The Statistics of Bioassay: With Special 
           Reference to the Vitamins, pp 499-503, 
           :doi:`10.1016/C2013-0-12584-6`. 
    .. [5] B. Phipson and G. K. Smyth. &quot;Permutation P-values Should Never Be 
           Zero: Calculating Exact P-values When Permutations Are Randomly 
           Drawn.&quot; Statistical Applications in Genetics and Molecular Biology 
           9.1 (2010). 
    .. [6] Ludbrook, J., &amp; Dudley, H. (1998). Why permutation tests are 
           superior to t and F tests in biomedical research. The American 
           Statistician, 52(2), 127-132. 
 
    Examples 
    -------- 
    In [4]_, the influence of vitamin C on the tooth growth of guinea pigs 
    was investigated. In a control study, 60 subjects were divided into 
    small dose, medium dose, and large dose groups that received 
    daily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively. 
    After 42 days, the tooth growth was measured. 
 
    The ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record 
    tooth growth measurements of the three groups in microns. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; small_dose = np.array([ 
    ...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7, 
    ...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7 
    ... ]) 
    &gt;&gt;&gt; medium_dose = np.array([ 
    ...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5, 
    ...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3 
    ... ]) 
    &gt;&gt;&gt; large_dose = np.array([ 
    ...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5, 
    ...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23 
    ... ]) 
 
    The `levene` statistic is sensitive to differences in variances 
    between the samples. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; res = stats.levene(small_dose, medium_dose, large_dose) 
    &gt;&gt;&gt; res.statistic 
    0.6457341109631506 
 
    The value of the statistic tends to be high when there is a large 
    difference in variances. 
 
    We can test for inequality of variance among the groups by comparing the 
    observed value of the statistic against the null distribution: the 
    distribution of statistic values derived under the null hypothesis that 
    the population variances of the three groups are equal. 
 
    For this test, the null distribution follows the F distribution as shown 
    below. 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; k, n = 3, 60   # number of samples, total number of observations 
    &gt;&gt;&gt; dist = stats.f(dfn=k-1, dfd=n-k) 
    &gt;&gt;&gt; val = np.linspace(0, 5, 100) 
    &gt;&gt;&gt; pdf = dist.pdf(val) 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; def plot(ax):  # we'll reuse this 
    ...     ax.plot(val, pdf, color='C0') 
    ...     ax.set_title(&quot;Levene Test Null Distribution&quot;) 
    ...     ax.set_xlabel(&quot;statistic&quot;) 
    ...     ax.set_ylabel(&quot;probability density&quot;) 
    ...     ax.set_xlim(0, 5) 
    ...     ax.set_ylim(0, 1) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    The comparison is quantified by the p-value: the proportion of values in 
    the null distribution greater than or equal to the observed value of the 
    statistic. 
 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; pvalue = dist.sf(res.statistic) 
    &gt;&gt;&gt; annotation = (f'p-value={pvalue:.3f}\n(shaded area)') 
    &gt;&gt;&gt; props = dict(facecolor='black', width=1, headwidth=5, headlength=8) 
    &gt;&gt;&gt; _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props) 
    &gt;&gt;&gt; i = val &gt;= res.statistic 
    &gt;&gt;&gt; ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0') 
    &gt;&gt;&gt; plt.show() 
 
    &gt;&gt;&gt; res.pvalue 
    0.5280694573759905 
 
    If the p-value is &quot;small&quot; - that is, if there is a low probability of 
    sampling data from distributions with identical variances that produces 
    such an extreme value of the statistic - this may be taken as evidence 
    against the null hypothesis in favor of the alternative: the variances of 
    the groups are not equal. Note that: 
 
    - The inverse is not true; that is, the test is not used to provide 
      evidence for the null hypothesis. 
    - The threshold for values that will be considered &quot;small&quot; is a choice that 
      should be made before the data is analyzed [5]_ with consideration of the 
      risks of both false positives (incorrectly rejecting the null hypothesis) 
      and false negatives (failure to reject a false null hypothesis). 
    - Small p-values are not evidence for a *large* effect; rather, they can 
      only provide evidence for a &quot;significant&quot; effect, meaning that they are 
      unlikely to have occurred under the null hypothesis. 
 
    Note that the F distribution provides an asymptotic approximation of the 
    null distribution. 
    For small samples, it may be more appropriate to perform a permutation 
    test: Under the null hypothesis that all three samples were drawn from 
    the same population, each of the measurements is equally likely to have 
    been observed in any of the three samples. Therefore, we can form a 
    randomized null distribution by calculating the statistic under many 
    randomly-generated partitionings of the observations into the three 
    samples. 
 
    &gt;&gt;&gt; def statistic(*samples): 
    ...     return stats.levene(*samples).statistic 
    &gt;&gt;&gt; ref = stats.permutation_test( 
    ...     (small_dose, medium_dose, large_dose), statistic, 
    ...     permutation_type='independent', alternative='greater' 
    ... ) 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; bins = np.linspace(0, 5, 25) 
    &gt;&gt;&gt; ax.hist( 
    ...     ref.null_distribution, bins=bins, density=True, facecolor=&quot;C1&quot; 
    ... ) 
    &gt;&gt;&gt; ax.legend(['aymptotic approximation\n(many observations)', 
    ...            'randomized null distribution']) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    &gt;&gt;&gt; ref.pvalue  # randomized test p-value 
    0.4559  # may vary 
 
    Note that there is significant disagreement between the p-value calculated 
    here and the asymptotic approximation returned by `levene` above. 
    The statistical inferences that can be drawn rigorously from a permutation 
    test are limited; nonetheless, they may be the preferred approach in many 
    circumstances [6]_. 
 
    Following is another generic example where the null hypothesis would be 
    rejected. 
 
    Test whether the lists `a`, `b` and `c` come from populations 
    with equal variances. 
 
    &gt;&gt;&gt; a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99] 
    &gt;&gt;&gt; b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05] 
    &gt;&gt;&gt; c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98] 
    &gt;&gt;&gt; stat, p = stats.levene(a, b, c) 
    &gt;&gt;&gt; p 
    0.002431505967249681 
 
    The small p-value suggests that the populations do not have equal 
    variances. 
 
    This is not surprising, given that the sample variance of `b` is much 
    larger than that of `a` and `c`: 
 
    &gt;&gt;&gt; [np.var(x, ddof=1) for x in [a, b, c]] 
    [0.007054444444444413, 0.13073888888888888, 0.008890000000000002] 
 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">center </span><span class="s0">not in </span><span class="s2">[</span><span class="s4">'mean'</span><span class="s2">, </span><span class="s4">'median'</span><span class="s2">, </span><span class="s4">'trimmed'</span><span class="s2">]:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;center must be 'mean', 'median' or 'trimmed'.&quot;</span><span class="s2">)</span>

    <span class="s1">k </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">k </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Must enter at least two input sample vectors.&quot;</span><span class="s2">)</span>

    <span class="s1">Ni </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty</span><span class="s2">(</span><span class="s1">k</span><span class="s2">)</span>
    <span class="s1">Yci </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty</span><span class="s2">(</span><span class="s1">k</span><span class="s2">, </span><span class="s4">'d'</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">center </span><span class="s2">== </span><span class="s4">'median'</span><span class="s2">:</span>

        <span class="s0">def </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">median</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s0">elif </span><span class="s1">center </span><span class="s2">== </span><span class="s4">'mean'</span><span class="s2">:</span>

        <span class="s0">def </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s0">else</span><span class="s2">:  </span><span class="s3"># center == 'trimmed'</span>
        <span class="s1">samples </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">trimboth</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">), </span><span class="s1">proportiontocut</span><span class="s2">)</span>
                        <span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">)</span>

        <span class="s0">def </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">Ni</span><span class="s2">[</span><span class="s1">j</span><span class="s2">] = </span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">j</span><span class="s2">])</span>
        <span class="s1">Yci</span><span class="s2">[</span><span class="s1">j</span><span class="s2">] = </span><span class="s1">func</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">j</span><span class="s2">])</span>
    <span class="s1">Ntot </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">Ni</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s3"># compute Zij's</span>
    <span class="s1">Zij </span><span class="s2">= [</span><span class="s0">None</span><span class="s2">] * </span><span class="s1">k</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">Zij</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] = </span><span class="s1">abs</span><span class="s2">(</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]) - </span><span class="s1">Yci</span><span class="s2">[</span><span class="s1">i</span><span class="s2">])</span>

    <span class="s3"># compute Zbari</span>
    <span class="s1">Zbari </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty</span><span class="s2">(</span><span class="s1">k</span><span class="s2">, </span><span class="s4">'d'</span><span class="s2">)</span>
    <span class="s1">Zbar </span><span class="s2">= </span><span class="s5">0.0</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">Zbari</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">Zij</span><span class="s2">[</span><span class="s1">i</span><span class="s2">], </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s1">Zbar </span><span class="s2">+= </span><span class="s1">Zbari</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] * </span><span class="s1">Ni</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]</span>

    <span class="s1">Zbar </span><span class="s2">/= </span><span class="s1">Ntot</span>
    <span class="s1">numer </span><span class="s2">= (</span><span class="s1">Ntot </span><span class="s2">- </span><span class="s1">k</span><span class="s2">) * </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">Ni </span><span class="s2">* (</span><span class="s1">Zbari </span><span class="s2">- </span><span class="s1">Zbar</span><span class="s2">)**</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s3"># compute denom_variance</span>
    <span class="s1">dvar </span><span class="s2">= </span><span class="s5">0.0</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">dvar </span><span class="s2">+= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">((</span><span class="s1">Zij</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] - </span><span class="s1">Zbari</span><span class="s2">[</span><span class="s1">i</span><span class="s2">])**</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s1">denom </span><span class="s2">= (</span><span class="s1">k </span><span class="s2">- </span><span class="s5">1.0</span><span class="s2">) * </span><span class="s1">dvar</span>

    <span class="s1">W </span><span class="s2">= </span><span class="s1">numer </span><span class="s2">/ </span><span class="s1">denom</span>
    <span class="s1">pval </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">f</span><span class="s2">.</span><span class="s1">sf</span><span class="s2">(</span><span class="s1">W</span><span class="s2">, </span><span class="s1">k</span><span class="s2">-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">Ntot</span><span class="s2">-</span><span class="s1">k</span><span class="s2">)  </span><span class="s3"># 1 - cdf</span>
    <span class="s0">return </span><span class="s1">LeveneResult</span><span class="s2">(</span><span class="s1">W</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">_apply_func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">func</span><span class="s2">):</span>
    <span class="s3"># g is list of indices into x</span>
    <span class="s3">#  separating x into different groups</span>
    <span class="s3">#  func should be applied over the groups</span>
    <span class="s1">g </span><span class="s2">= </span><span class="s1">unique</span><span class="s2">(</span><span class="s1">r_</span><span class="s2">[</span><span class="s5">0</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)])</span>
    <span class="s1">output </span><span class="s2">= [</span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">[</span><span class="s1">g</span><span class="s2">[</span><span class="s1">k</span><span class="s2">]:</span><span class="s1">g</span><span class="s2">[</span><span class="s1">k</span><span class="s2">+</span><span class="s5">1</span><span class="s2">]]) </span><span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">g</span><span class="s2">) - </span><span class="s5">1</span><span class="s2">)]</span>

    <span class="s0">return </span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>


<span class="s1">FlignerResult </span><span class="s2">= </span><span class="s1">namedtuple</span><span class="s2">(</span><span class="s4">'FlignerResult'</span><span class="s2">, (</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s1">FlignerResult</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s0">None</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">fligner</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">center</span><span class="s2">=</span><span class="s4">'median'</span><span class="s2">, </span><span class="s1">proportiontocut</span><span class="s2">=</span><span class="s5">0.05</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Perform Fligner-Killeen test for equality of variance. 
 
    Fligner's test tests the null hypothesis that all input samples 
    are from populations with equal variances.  Fligner-Killeen's test is 
    distribution free when populations are identical [2]_. 
 
    Parameters 
    ---------- 
    sample1, sample2, ... : array_like 
        Arrays of sample data.  Need not be the same length. 
    center : {'mean', 'median', 'trimmed'}, optional 
        Keyword argument controlling which function of the data is used in 
        computing the test statistic.  The default is 'median'. 
    proportiontocut : float, optional 
        When `center` is 'trimmed', this gives the proportion of data points 
        to cut from each end. (See `scipy.stats.trim_mean`.) 
        Default is 0.05. 
 
    Returns 
    ------- 
    statistic : float 
        The test statistic. 
    pvalue : float 
        The p-value for the hypothesis test. 
 
    See Also 
    -------- 
    bartlett : A parametric test for equality of k variances in normal samples 
    levene : A robust parametric test for equality of k variances 
 
    Notes 
    ----- 
    As with Levene's test there are three variants of Fligner's test that 
    differ by the measure of central tendency used in the test.  See `levene` 
    for more information. 
 
    Conover et al. (1981) examine many of the existing parametric and 
    nonparametric tests by extensive simulations and they conclude that the 
    tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be 
    superior in terms of robustness of departures from normality and power 
    [3]_. 
 
    References 
    ---------- 
    .. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and 
           Hypothesis Testing based on Quadratic Inference Function. Technical 
           Report #99-03, Center for Likelihood Studies, Pennsylvania State 
           University. 
           https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf 
    .. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample 
           tests for scale. 'Journal of the American Statistical Association.' 
           71(353), 210-213. 
    .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and 
           Hypothesis Testing based on Quadratic Inference Function. Technical 
           Report #99-03, Center for Likelihood Studies, Pennsylvania State 
           University. 
    .. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A 
           comparative study of tests for homogeneity of variances, with 
           applications to the outer continental shelf bidding data. 
           Technometrics, 23(4), 351-361. 
    .. [5] C.I. BLISS (1952), The Statistics of Bioassay: With Special 
           Reference to the Vitamins, pp 499-503, 
           :doi:`10.1016/C2013-0-12584-6`. 
    .. [6] B. Phipson and G. K. Smyth. &quot;Permutation P-values Should Never Be 
           Zero: Calculating Exact P-values When Permutations Are Randomly 
           Drawn.&quot; Statistical Applications in Genetics and Molecular Biology 
           9.1 (2010). 
    .. [7] Ludbrook, J., &amp; Dudley, H. (1998). Why permutation tests are 
           superior to t and F tests in biomedical research. The American 
           Statistician, 52(2), 127-132. 
 
    Examples 
    -------- 
    In [5]_, the influence of vitamin C on the tooth growth of guinea pigs 
    was investigated. In a control study, 60 subjects were divided into 
    small dose, medium dose, and large dose groups that received 
    daily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively. 
    After 42 days, the tooth growth was measured. 
 
    The ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record 
    tooth growth measurements of the three groups in microns. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; small_dose = np.array([ 
    ...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7, 
    ...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7 
    ... ]) 
    &gt;&gt;&gt; medium_dose = np.array([ 
    ...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5, 
    ...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3 
    ... ]) 
    &gt;&gt;&gt; large_dose = np.array([ 
    ...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5, 
    ...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23 
    ... ]) 
 
    The `fligner` statistic is sensitive to differences in variances 
    between the samples. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; res = stats.fligner(small_dose, medium_dose, large_dose) 
    &gt;&gt;&gt; res.statistic 
    1.3878943408857916 
 
    The value of the statistic tends to be high when there is a large 
    difference in variances. 
 
    We can test for inequality of variance among the groups by comparing the 
    observed value of the statistic against the null distribution: the 
    distribution of statistic values derived under the null hypothesis that 
    the population variances of the three groups are equal. 
 
    For this test, the null distribution follows the chi-square distribution 
    as shown below. 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; k = 3  # number of samples 
    &gt;&gt;&gt; dist = stats.chi2(df=k-1) 
    &gt;&gt;&gt; val = np.linspace(0, 8, 100) 
    &gt;&gt;&gt; pdf = dist.pdf(val) 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; def plot(ax):  # we'll reuse this 
    ...     ax.plot(val, pdf, color='C0') 
    ...     ax.set_title(&quot;Fligner Test Null Distribution&quot;) 
    ...     ax.set_xlabel(&quot;statistic&quot;) 
    ...     ax.set_ylabel(&quot;probability density&quot;) 
    ...     ax.set_xlim(0, 8) 
    ...     ax.set_ylim(0, 0.5) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    The comparison is quantified by the p-value: the proportion of values in 
    the null distribution greater than or equal to the observed value of the 
    statistic. 
 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; pvalue = dist.sf(res.statistic) 
    &gt;&gt;&gt; annotation = (f'p-value={pvalue:.4f}\n(shaded area)') 
    &gt;&gt;&gt; props = dict(facecolor='black', width=1, headwidth=5, headlength=8) 
    &gt;&gt;&gt; _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props) 
    &gt;&gt;&gt; i = val &gt;= res.statistic 
    &gt;&gt;&gt; ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0') 
    &gt;&gt;&gt; plt.show() 
 
    &gt;&gt;&gt; res.pvalue 
    0.49960016501182125 
 
    If the p-value is &quot;small&quot; - that is, if there is a low probability of 
    sampling data from distributions with identical variances that produces 
    such an extreme value of the statistic - this may be taken as evidence 
    against the null hypothesis in favor of the alternative: the variances of 
    the groups are not equal. Note that: 
 
    - The inverse is not true; that is, the test is not used to provide 
      evidence for the null hypothesis. 
    - The threshold for values that will be considered &quot;small&quot; is a choice that 
      should be made before the data is analyzed [6]_ with consideration of the 
      risks of both false positives (incorrectly rejecting the null hypothesis) 
      and false negatives (failure to reject a false null hypothesis). 
    - Small p-values are not evidence for a *large* effect; rather, they can 
      only provide evidence for a &quot;significant&quot; effect, meaning that they are 
      unlikely to have occurred under the null hypothesis. 
 
    Note that the chi-square distribution provides an asymptotic approximation 
    of the null distribution. 
    For small samples, it may be more appropriate to perform a 
    permutation test: Under the null hypothesis that all three samples were 
    drawn from the same population, each of the measurements is equally likely 
    to have been observed in any of the three samples. Therefore, we can form 
    a randomized null distribution by calculating the statistic under many 
    randomly-generated partitionings of the observations into the three 
    samples. 
 
    &gt;&gt;&gt; def statistic(*samples): 
    ...     return stats.fligner(*samples).statistic 
    &gt;&gt;&gt; ref = stats.permutation_test( 
    ...     (small_dose, medium_dose, large_dose), statistic, 
    ...     permutation_type='independent', alternative='greater' 
    ... ) 
    &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(8, 5)) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; bins = np.linspace(0, 8, 25) 
    &gt;&gt;&gt; ax.hist( 
    ...     ref.null_distribution, bins=bins, density=True, facecolor=&quot;C1&quot; 
    ... ) 
    &gt;&gt;&gt; ax.legend(['aymptotic approximation\n(many observations)', 
    ...            'randomized null distribution']) 
    &gt;&gt;&gt; plot(ax) 
    &gt;&gt;&gt; plt.show() 
 
    &gt;&gt;&gt; ref.pvalue  # randomized test p-value 
    0.4332  # may vary 
 
    Note that there is significant disagreement between the p-value calculated 
    here and the asymptotic approximation returned by `fligner` above. 
    The statistical inferences that can be drawn rigorously from a permutation 
    test are limited; nonetheless, they may be the preferred approach in many 
    circumstances [7]_. 
 
    Following is another generic example where the null hypothesis would be 
    rejected. 
 
    Test whether the lists `a`, `b` and `c` come from populations 
    with equal variances. 
 
    &gt;&gt;&gt; a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99] 
    &gt;&gt;&gt; b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05] 
    &gt;&gt;&gt; c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98] 
    &gt;&gt;&gt; stat, p = stats.fligner(a, b, c) 
    &gt;&gt;&gt; p 
    0.00450826080004775 
 
    The small p-value suggests that the populations do not have equal 
    variances. 
 
    This is not surprising, given that the sample variance of `b` is much 
    larger than that of `a` and `c`: 
 
    &gt;&gt;&gt; [np.var(x, ddof=1) for x in [a, b, c]] 
    [0.007054444444444413, 0.13073888888888888, 0.008890000000000002] 
 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">center </span><span class="s0">not in </span><span class="s2">[</span><span class="s4">'mean'</span><span class="s2">, </span><span class="s4">'median'</span><span class="s2">, </span><span class="s4">'trimmed'</span><span class="s2">]:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;center must be 'mean', 'median' or 'trimmed'.&quot;</span><span class="s2">)</span>

    <span class="s1">k </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">k </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Must enter at least two input sample vectors.&quot;</span><span class="s2">)</span>

    <span class="s3"># Handle empty input</span>
    <span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">sample</span><span class="s2">.</span><span class="s1">size </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
            <span class="s1">NaN </span><span class="s2">= </span><span class="s1">_get_nan</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">FlignerResult</span><span class="s2">(</span><span class="s1">NaN</span><span class="s2">, </span><span class="s1">NaN</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">center </span><span class="s2">== </span><span class="s4">'median'</span><span class="s2">:</span>

        <span class="s0">def </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">median</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s0">elif </span><span class="s1">center </span><span class="s2">== </span><span class="s4">'mean'</span><span class="s2">:</span>

        <span class="s0">def </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s0">else</span><span class="s2">:  </span><span class="s3"># center == 'trimmed'</span>
        <span class="s1">samples </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">trimboth</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">, </span><span class="s1">proportiontocut</span><span class="s2">)</span>
                        <span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">)</span>

        <span class="s0">def </span><span class="s1">func</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>

    <span class="s1">Ni </span><span class="s2">= </span><span class="s1">asarray</span><span class="s2">([</span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">j</span><span class="s2">]) </span><span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">)])</span>
    <span class="s1">Yci </span><span class="s2">= </span><span class="s1">asarray</span><span class="s2">([</span><span class="s1">func</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">j</span><span class="s2">]) </span><span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">)])</span>
    <span class="s1">Ntot </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">Ni</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s3"># compute Zij's</span>
    <span class="s1">Zij </span><span class="s2">= [</span><span class="s1">abs</span><span class="s2">(</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]) - </span><span class="s1">Yci</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">)]</span>
    <span class="s1">allZij </span><span class="s2">= []</span>
    <span class="s1">g </span><span class="s2">= [</span><span class="s5">0</span><span class="s2">]</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">):</span>
        <span class="s1">allZij</span><span class="s2">.</span><span class="s1">extend</span><span class="s2">(</span><span class="s1">list</span><span class="s2">(</span><span class="s1">Zij</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]))</span>
        <span class="s1">g</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">allZij</span><span class="s2">))</span>

    <span class="s1">ranks </span><span class="s2">= </span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">rankdata</span><span class="s2">(</span><span class="s1">allZij</span><span class="s2">)</span>
    <span class="s1">sample </span><span class="s2">= </span><span class="s1">distributions</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">.</span><span class="s1">ppf</span><span class="s2">(</span><span class="s1">ranks </span><span class="s2">/ (</span><span class="s5">2</span><span class="s2">*(</span><span class="s1">Ntot </span><span class="s2">+ </span><span class="s5">1.0</span><span class="s2">)) + </span><span class="s5">0.5</span><span class="s2">)</span>

    <span class="s3"># compute Aibar</span>
    <span class="s1">Aibar </span><span class="s2">= </span><span class="s1">_apply_func</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">, </span><span class="s1">g</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">) / </span><span class="s1">Ni</span>
    <span class="s1">anbar </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">varsq </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">ddof</span><span class="s2">=</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">statistic </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">Ni </span><span class="s2">* (</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">Aibar</span><span class="s2">) - </span><span class="s1">anbar</span><span class="s2">)**</span><span class="s5">2.0</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">) / </span><span class="s1">varsq</span>
    <span class="s1">chi2 </span><span class="s2">= </span><span class="s1">_SimpleChi2</span><span class="s2">(</span><span class="s1">k</span><span class="s2">-</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">pval </span><span class="s2">= </span><span class="s1">_get_pvalue</span><span class="s2">(</span><span class="s1">statistic</span><span class="s2">, </span><span class="s1">chi2</span><span class="s2">, </span><span class="s1">alternative</span><span class="s2">=</span><span class="s4">'greater'</span><span class="s2">, </span><span class="s1">symmetric</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">np</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">FlignerResult</span><span class="s2">(</span><span class="s1">statistic</span><span class="s2">, </span><span class="s1">pval</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s0">lambda </span><span class="s1">x1</span><span class="s2">: (</span><span class="s1">x1</span><span class="s2">,), </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s5">4</span><span class="s2">, </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s5">1</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">_mood_inner_lc</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">diffs</span><span class="s2">, </span><span class="s1">sorted_xy</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">N</span><span class="s2">) </span><span class="s1">-&gt; float</span><span class="s2">:</span>
    <span class="s3"># Obtain the unique values and their frequencies from the pooled samples.</span>
    <span class="s3"># &quot;a_j, + b_j, = t_j, for j = 1, ... k&quot; where `k` is the number of unique</span>
    <span class="s3"># classes, and &quot;[t]he number of values associated with the x's and y's in</span>
    <span class="s3"># the jth class will be denoted by a_j, and b_j respectively.&quot;</span>
    <span class="s3"># (Mielke, 312)</span>
    <span class="s3"># Reuse previously computed sorted array and `diff` arrays to obtain the</span>
    <span class="s3"># unique values and counts. Prepend `diffs` with a non-zero to indicate</span>
    <span class="s3"># that the first element should be marked as not matching what preceded it.</span>
    <span class="s1">diffs_prep </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(([</span><span class="s5">1</span><span class="s2">], </span><span class="s1">diffs</span><span class="s2">))</span>
    <span class="s3"># Unique elements are where the was a difference between elements in the</span>
    <span class="s3"># sorted array</span>
    <span class="s1">uniques </span><span class="s2">= </span><span class="s1">sorted_xy</span><span class="s2">[</span><span class="s1">diffs_prep </span><span class="s2">!= </span><span class="s5">0</span><span class="s2">]</span>
    <span class="s3"># The count of each element is the bin size for each set of consecutive</span>
    <span class="s3"># differences where the difference is zero. Replace nonzero differences</span>
    <span class="s3"># with 1 and then use the cumulative sum to count the indices.</span>
    <span class="s1">t </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">bincount</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">cumsum</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">diffs_prep </span><span class="s2">!= </span><span class="s5">0</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">int</span><span class="s2">)))[</span><span class="s5">1</span><span class="s2">:]</span>
    <span class="s1">k </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">uniques</span><span class="s2">)</span>
    <span class="s1">js </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">k </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">int</span><span class="s2">)</span>
    <span class="s3"># the `b` array mentioned in the paper is not used, outside of the</span>
    <span class="s3"># calculation of `t`, so we do not need to calculate it separately. Here</span>
    <span class="s3"># we calculate `a`. In plain language, `a[j]` is the number of values in</span>
    <span class="s3"># `x` that equal `uniques[j]`.</span>
    <span class="s1">sorted_xyx </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">((</span><span class="s1">xy</span><span class="s2">, </span><span class="s1">x</span><span class="s2">)))</span>
    <span class="s1">diffs </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">diff</span><span class="s2">(</span><span class="s1">sorted_xyx</span><span class="s2">)</span>
    <span class="s1">diffs_prep </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(([</span><span class="s5">1</span><span class="s2">], </span><span class="s1">diffs</span><span class="s2">))</span>
    <span class="s1">diff_is_zero </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">diffs_prep </span><span class="s2">!= </span><span class="s5">0</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">int</span><span class="s2">)</span>
    <span class="s1">xyx_counts </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">bincount</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">cumsum</span><span class="s2">(</span><span class="s1">diff_is_zero</span><span class="s2">))[</span><span class="s5">1</span><span class="s2">:]</span>
    <span class="s1">a </span><span class="s2">= </span><span class="s1">xyx_counts </span><span class="s2">- </span><span class="s1">t</span>
    <span class="s3"># &quot;Define .. a_0 = b_0 = t_0 = S_0 = 0&quot; (Mielke 312) so we shift  `a`</span>
    <span class="s3"># and `t` arrays over 1 to allow a first element of 0 to accommodate this</span>
    <span class="s3"># indexing.</span>
    <span class="s1">t </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(([</span><span class="s5">0</span><span class="s2">], </span><span class="s1">t</span><span class="s2">))</span>
    <span class="s1">a </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(([</span><span class="s5">0</span><span class="s2">], </span><span class="s1">a</span><span class="s2">))</span>
    <span class="s3"># S is built from `t`, so it does not need a preceding zero added on.</span>
    <span class="s1">S </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">cumsum</span><span class="s2">(</span><span class="s1">t</span><span class="s2">)</span>
    <span class="s3"># define a copy of `S` with a prepending zero for later use to avoid</span>
    <span class="s3"># the need for indexing.</span>
    <span class="s1">S_i_m1 </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(([</span><span class="s5">0</span><span class="s2">], </span><span class="s1">S</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">]))</span>

    <span class="s3"># Psi, as defined by the 6th unnumbered equation on page 313 (Mielke).</span>
    <span class="s3"># Note that in the paper there is an error where the denominator `2` is</span>
    <span class="s3"># squared when it should be the entire equation.</span>
    <span class="s0">def </span><span class="s1">psi</span><span class="s2">(</span><span class="s1">indicator</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s1">indicator </span><span class="s2">- (</span><span class="s1">N </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">)/</span><span class="s5">2</span><span class="s2">)**</span><span class="s5">2</span>

    <span class="s3"># define summation range for use in calculation of phi, as seen in sum</span>
    <span class="s3"># in the unnumbered equation on the bottom of page 312 (Mielke).</span>
    <span class="s1">s_lower </span><span class="s2">= </span><span class="s1">S</span><span class="s2">[</span><span class="s1">js </span><span class="s2">- </span><span class="s5">1</span><span class="s2">] + </span><span class="s5">1</span>
    <span class="s1">s_upper </span><span class="s2">= </span><span class="s1">S</span><span class="s2">[</span><span class="s1">js</span><span class="s2">] + </span><span class="s5">1</span>
    <span class="s1">phi_J </span><span class="s2">= [</span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s1">s_lower</span><span class="s2">[</span><span class="s1">idx</span><span class="s2">], </span><span class="s1">s_upper</span><span class="s2">[</span><span class="s1">idx</span><span class="s2">]) </span><span class="s0">for </span><span class="s1">idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">k</span><span class="s2">)]</span>

    <span class="s3"># for every range in the above array, determine the sum of psi(I) for</span>
    <span class="s3"># every element in the range. Divide all the sums by `t`. Following the</span>
    <span class="s3"># last unnumbered equation on page 312.</span>
    <span class="s1">phis </span><span class="s2">= [</span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">psi</span><span class="s2">(</span><span class="s1">I_j</span><span class="s2">)) </span><span class="s0">for </span><span class="s1">I_j </span><span class="s0">in </span><span class="s1">phi_J</span><span class="s2">] / </span><span class="s1">t</span><span class="s2">[</span><span class="s1">js</span><span class="s2">]</span>

    <span class="s3"># `T` is equal to a[j] * phi[j], per the first unnumbered equation on</span>
    <span class="s3"># page 312. `phis` is already in the order based on `js`, so we index</span>
    <span class="s3"># into `a` with `js` as well.</span>
    <span class="s1">T </span><span class="s2">= </span><span class="s1">sum</span><span class="s2">(</span><span class="s1">phis </span><span class="s2">* </span><span class="s1">a</span><span class="s2">[</span><span class="s1">js</span><span class="s2">])</span>

    <span class="s3"># The approximate statistic</span>
    <span class="s1">E_0_T </span><span class="s2">= </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">* </span><span class="s1">N </span><span class="s2">- </span><span class="s5">1</span><span class="s2">) / </span><span class="s5">12</span>

    <span class="s1">varM </span><span class="s2">= (</span><span class="s1">m </span><span class="s2">* </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">+ </span><span class="s5">1.0</span><span class="s2">) * (</span><span class="s1">N </span><span class="s2">** </span><span class="s5">2 </span><span class="s2">- </span><span class="s5">4</span><span class="s2">) / </span><span class="s5">180 </span><span class="s2">-</span>
            <span class="s1">m </span><span class="s2">* </span><span class="s1">n </span><span class="s2">/ (</span><span class="s5">180 </span><span class="s2">* </span><span class="s1">N </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">- </span><span class="s5">1</span><span class="s2">)) * </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span>
                <span class="s1">t </span><span class="s2">* (</span><span class="s1">t</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">- </span><span class="s5">1</span><span class="s2">) * (</span><span class="s1">t</span><span class="s2">**</span><span class="s5">2 </span><span class="s2">- </span><span class="s5">4 </span><span class="s2">+ (</span><span class="s5">15 </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">- </span><span class="s1">S </span><span class="s2">- </span><span class="s1">S_i_m1</span><span class="s2">) ** </span><span class="s5">2</span><span class="s2">))</span>
            <span class="s2">))</span>

    <span class="s0">return </span><span class="s2">((</span><span class="s1">T </span><span class="s2">- </span><span class="s1">E_0_T</span><span class="s2">) / </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">varM</span><span class="s2">),)</span>


<span class="s0">def </span><span class="s1">_mood_too_small</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">kwargs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">):</span>
    <span class="s1">x</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">samples</span>
    <span class="s1">n </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span>
    <span class="s1">m </span><span class="s2">= </span><span class="s1">y</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s1">m </span><span class="s2">+ </span><span class="s1">n</span>
    <span class="s0">return </span><span class="s1">N </span><span class="s2">&lt; </span><span class="s5">3</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span><span class="s1">SignificanceResult</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">too_small</span><span class="s2">=</span><span class="s1">_mood_too_small</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">mood</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">alternative</span><span class="s2">=</span><span class="s4">&quot;two-sided&quot;</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Perform Mood's test for equal scale parameters. 
 
    Mood's two-sample test for scale parameters is a non-parametric 
    test for the null hypothesis that two samples are drawn from the 
    same distribution with the same scale parameter. 
 
    Parameters 
    ---------- 
    x, y : array_like 
        Arrays of sample data. There must be at least three observations 
        total. 
    axis : int, optional 
        The axis along which the samples are tested.  `x` and `y` can be of 
        different length along `axis`. 
        If `axis` is None, `x` and `y` are flattened and the test is done on 
        all values in the flattened arrays. 
    alternative : {'two-sided', 'less', 'greater'}, optional 
        Defines the alternative hypothesis. Default is 'two-sided'. 
        The following options are available: 
 
        * 'two-sided': the scales of the distributions underlying `x` and `y` 
          are different. 
        * 'less': the scale of the distribution underlying `x` is less than 
          the scale of the distribution underlying `y`. 
        * 'greater': the scale of the distribution underlying `x` is greater 
          than the scale of the distribution underlying `y`. 
 
        .. versionadded:: 1.7.0 
 
    Returns 
    ------- 
    res : SignificanceResult 
        An object containing attributes: 
 
        statistic : scalar or ndarray 
            The z-score for the hypothesis test.  For 1-D inputs a scalar is 
            returned. 
        pvalue : scalar ndarray 
            The p-value for the hypothesis test. 
 
    See Also 
    -------- 
    fligner : A non-parametric test for the equality of k variances 
    ansari : A non-parametric test for the equality of 2 variances 
    bartlett : A parametric test for equality of k variances in normal samples 
    levene : A parametric test for equality of k variances 
 
    Notes 
    ----- 
    The data are assumed to be drawn from probability distributions ``f(x)`` 
    and ``f(x/s) / s`` respectively, for some probability density function f. 
    The null hypothesis is that ``s == 1``. 
 
    For multi-dimensional arrays, if the inputs are of shapes 
    ``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the 
    resulting z and p values will have shape ``(n0, n2, n3)``.  Note that 
    ``n1`` and ``m1`` don't have to be equal, but the other dimensions do. 
 
    References 
    ---------- 
    [1] Mielke, Paul W. &quot;Note on Some Squared Rank Tests with Existing Ties.&quot; 
        Technometrics, vol. 9, no. 2, 1967, pp. 312-14. JSTOR, 
        https://doi.org/10.2307/1266427. Accessed 18 May 2022. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; x2 = rng.standard_normal((2, 45, 6, 7)) 
    &gt;&gt;&gt; x1 = rng.standard_normal((2, 30, 6, 7)) 
    &gt;&gt;&gt; res = stats.mood(x1, x2, axis=1) 
    &gt;&gt;&gt; res.pvalue.shape 
    (2, 6, 7) 
 
    Find the number of points where the difference in scale is not significant: 
 
    &gt;&gt;&gt; (res.pvalue &gt; 0.1).sum() 
    78 
 
    Perform the test with different scales: 
 
    &gt;&gt;&gt; x1 = rng.standard_normal((2, 30)) 
    &gt;&gt;&gt; x2 = rng.standard_normal((2, 35)) * 10.0 
    &gt;&gt;&gt; stats.mood(x1, x2, axis=1) 
    SignificanceResult(statistic=array([-5.76174136, -6.12650783]), 
                       pvalue=array([8.32505043e-09, 8.98287869e-10])) 
 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">float</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">float</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">axis </span><span class="s2">&lt; </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s1">axis </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">+ </span><span class="s1">axis</span>

    <span class="s3"># Determine shape of the result arrays</span>
    <span class="s1">res_shape </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">([</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">ax</span><span class="s2">] </span><span class="s0">for </span><span class="s1">ax </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)) </span><span class="s0">if </span><span class="s1">ax </span><span class="s2">!= </span><span class="s1">axis</span><span class="s2">])</span>
    <span class="s0">if not </span><span class="s2">(</span><span class="s1">res_shape </span><span class="s2">== </span><span class="s1">tuple</span><span class="s2">([</span><span class="s1">y</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">ax</span><span class="s2">] </span><span class="s0">for </span><span class="s1">ax </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">y</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)) </span><span class="s0">if</span>
                                <span class="s1">ax </span><span class="s2">!= </span><span class="s1">axis</span><span class="s2">])):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Dimensions of x and y on all axes except `axis` &quot;</span>
                         <span class="s4">&quot;should match&quot;</span><span class="s2">)</span>

    <span class="s1">n </span><span class="s2">= </span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span>
    <span class="s1">m </span><span class="s2">= </span><span class="s1">y</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span>
    <span class="s1">N </span><span class="s2">= </span><span class="s1">m </span><span class="s2">+ </span><span class="s1">n</span>
    <span class="s0">if </span><span class="s1">N </span><span class="s2">&lt; </span><span class="s5">3</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Not enough observations.&quot;</span><span class="s2">)</span>

    <span class="s1">xy </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">((</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s3"># determine if any of the samples contain ties</span>
    <span class="s1">sorted_xy </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sort</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">diffs </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">diff</span><span class="s2">(</span><span class="s1">sorted_xy</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s5">0 </span><span class="s0">in </span><span class="s1">diffs</span><span class="s2">:</span>
        <span class="s1">z </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">_mood_inner_lc</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">diffs</span><span class="s2">, </span><span class="s1">sorted_xy</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">N</span><span class="s2">,</span>
                                      <span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">axis </span><span class="s2">!= </span><span class="s5">0</span><span class="s2">:</span>
            <span class="s1">xy </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">moveaxis</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s5">0</span><span class="s2">)</span>

        <span class="s1">xy </span><span class="s2">= </span><span class="s1">xy</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">0</span><span class="s2">], -</span><span class="s5">1</span><span class="s2">)</span>
        <span class="s3"># Generalized to the n-dimensional case by adding the axis argument,</span>
        <span class="s3"># and using for loops, since rankdata is not vectorized.  For improving</span>
        <span class="s3"># performance consider vectorizing rankdata function.</span>
        <span class="s1">all_ranks </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">empty_like</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">)</span>
        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]):</span>
            <span class="s1">all_ranks</span><span class="s2">[:, </span><span class="s1">j</span><span class="s2">] = </span><span class="s1">_stats_py</span><span class="s2">.</span><span class="s1">rankdata</span><span class="s2">(</span><span class="s1">xy</span><span class="s2">[:, </span><span class="s1">j</span><span class="s2">])</span>

        <span class="s1">Ri </span><span class="s2">= </span><span class="s1">all_ranks</span><span class="s2">[:</span><span class="s1">n</span><span class="s2">]</span>
        <span class="s1">M </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">((</span><span class="s1">Ri </span><span class="s2">- (</span><span class="s1">N </span><span class="s2">+ </span><span class="s5">1.0</span><span class="s2">) / </span><span class="s5">2</span><span class="s2">) ** </span><span class="s5">2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s3"># Approx stat.</span>
        <span class="s1">mnM </span><span class="s2">= </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">* </span><span class="s1">N </span><span class="s2">- </span><span class="s5">1.0</span><span class="s2">) / </span><span class="s5">12</span>
        <span class="s1">varM </span><span class="s2">= </span><span class="s1">m </span><span class="s2">* </span><span class="s1">n </span><span class="s2">* (</span><span class="s1">N </span><span class="s2">+ </span><span class="s5">1.0</span><span class="s2">) * (</span><span class="s1">N </span><span class="s2">+ </span><span class="s5">2</span><span class="s2">) * (</span><span class="s1">N </span><span class="s2">- </span><span class="s5">2</span><span class="s2">) / </span><span class="s5">180</span>
        <span class="s1">z </span><span class="s2">= (</span><span class="s1">M </span><span class="s2">- </span><span class="s1">mnM</span><span class="s2">) / </span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">varM</span><span class="s2">)</span>
    <span class="s1">pval </span><span class="s2">= </span><span class="s1">_get_pvalue</span><span class="s2">(</span><span class="s1">z</span><span class="s2">, </span><span class="s1">_SimpleNormal</span><span class="s2">(), </span><span class="s1">alternative</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">np</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">res_shape </span><span class="s2">== ():</span>
        <span class="s3"># Return scalars, not 0-D arrays</span>
        <span class="s1">z </span><span class="s2">= </span><span class="s1">z</span><span class="s2">[</span><span class="s5">0</span><span class="s2">]</span>
        <span class="s1">pval </span><span class="s2">= </span><span class="s1">pval</span><span class="s2">[</span><span class="s5">0</span><span class="s2">]</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">z</span><span class="s2">.</span><span class="s1">shape </span><span class="s2">= </span><span class="s1">res_shape</span>
        <span class="s1">pval</span><span class="s2">.</span><span class="s1">shape </span><span class="s2">= </span><span class="s1">res_shape</span>
    <span class="s0">return </span><span class="s1">SignificanceResult</span><span class="s2">(</span><span class="s1">z</span><span class="s2">[()], </span><span class="s1">pval</span><span class="s2">[()])</span>


<span class="s1">WilcoxonResult </span><span class="s2">= </span><span class="s1">_make_tuple_bunch</span><span class="s2">(</span><span class="s4">'WilcoxonResult'</span><span class="s2">, [</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">])</span>


<span class="s0">def </span><span class="s1">wilcoxon_result_unpacker</span><span class="s2">(</span><span class="s1">res</span><span class="s2">):</span>
    <span class="s0">if </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">res</span><span class="s2">, </span><span class="s4">'zstatistic'</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">res</span><span class="s2">.</span><span class="s1">statistic</span><span class="s2">, </span><span class="s1">res</span><span class="s2">.</span><span class="s1">pvalue</span><span class="s2">, </span><span class="s1">res</span><span class="s2">.</span><span class="s1">zstatistic</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">res</span><span class="s2">.</span><span class="s1">statistic</span><span class="s2">, </span><span class="s1">res</span><span class="s2">.</span><span class="s1">pvalue</span>


<span class="s0">def </span><span class="s1">wilcoxon_result_object</span><span class="s2">(</span><span class="s1">statistic</span><span class="s2">, </span><span class="s1">pvalue</span><span class="s2">, </span><span class="s1">zstatistic</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s1">res </span><span class="s2">= </span><span class="s1">WilcoxonResult</span><span class="s2">(</span><span class="s1">statistic</span><span class="s2">, </span><span class="s1">pvalue</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">zstatistic </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s1">res</span><span class="s2">.</span><span class="s1">zstatistic </span><span class="s2">= </span><span class="s1">zstatistic</span>
    <span class="s0">return </span><span class="s1">res</span>


<span class="s0">def </span><span class="s1">wilcoxon_outputs</span><span class="s2">(</span><span class="s1">kwds</span><span class="s2">):</span>
    <span class="s1">method </span><span class="s2">= </span><span class="s1">kwds</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s4">'method'</span><span class="s2">, </span><span class="s4">'auto'</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">method </span><span class="s2">== </span><span class="s4">'approx'</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s5">3</span>
    <span class="s0">return </span><span class="s5">2</span>


<span class="s2">@</span><span class="s1">_rename_parameter</span><span class="s2">(</span><span class="s4">&quot;mode&quot;</span><span class="s2">, </span><span class="s4">&quot;method&quot;</span><span class="s2">)</span>
<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span>
    <span class="s1">wilcoxon_result_object</span><span class="s2">, </span><span class="s1">paired</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
    <span class="s1">n_samples</span><span class="s2">=</span><span class="s0">lambda </span><span class="s1">kwds</span><span class="s2">: </span><span class="s5">2 </span><span class="s0">if </span><span class="s1">kwds</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s4">'y'</span><span class="s2">, </span><span class="s0">None</span><span class="s2">) </span><span class="s0">is not None else </span><span class="s5">1</span><span class="s2">,</span>
    <span class="s1">result_to_tuple</span><span class="s2">=</span><span class="s1">wilcoxon_result_unpacker</span><span class="s2">, </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s1">wilcoxon_outputs</span><span class="s2">,</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">wilcoxon</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">zero_method</span><span class="s2">=</span><span class="s4">&quot;wilcox&quot;</span><span class="s2">, </span><span class="s1">correction</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
             <span class="s1">alternative</span><span class="s2">=</span><span class="s4">&quot;two-sided&quot;</span><span class="s2">, </span><span class="s1">method</span><span class="s2">=</span><span class="s4">'auto'</span><span class="s2">, *, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Calculate the Wilcoxon signed-rank test. 
 
    The Wilcoxon signed-rank test tests the null hypothesis that two 
    related paired samples come from the same distribution. In particular, 
    it tests whether the distribution of the differences ``x - y`` is symmetric 
    about zero. It is a non-parametric version of the paired T-test. 
 
    Parameters 
    ---------- 
    x : array_like 
        Either the first set of measurements (in which case ``y`` is the second 
        set of measurements), or the differences between two sets of 
        measurements (in which case ``y`` is not to be specified.)  Must be 
        one-dimensional. 
    y : array_like, optional 
        Either the second set of measurements (if ``x`` is the first set of 
        measurements), or not specified (if ``x`` is the differences between 
        two sets of measurements.)  Must be one-dimensional. 
 
        .. warning:: 
            When `y` is provided, `wilcoxon` calculates the test statistic 
            based on the ranks of the absolute values of ``d = x - y``. 
            Roundoff error in the subtraction can result in elements of ``d`` 
            being assigned different ranks even when they would be tied with 
            exact arithmetic. Rather than passing `x` and `y` separately, 
            consider computing the difference ``x - y``, rounding as needed to 
            ensure that only truly unique elements are numerically distinct, 
            and passing the result as `x`, leaving `y` at the default (None). 
 
    zero_method : {&quot;wilcox&quot;, &quot;pratt&quot;, &quot;zsplit&quot;}, optional 
        There are different conventions for handling pairs of observations 
        with equal values (&quot;zero-differences&quot;, or &quot;zeros&quot;). 
 
        * &quot;wilcox&quot;: Discards all zero-differences (default); see [4]_. 
        * &quot;pratt&quot;: Includes zero-differences in the ranking process, 
          but drops the ranks of the zeros (more conservative); see [3]_. 
          In this case, the normal approximation is adjusted as in [5]_. 
        * &quot;zsplit&quot;: Includes zero-differences in the ranking process and 
          splits the zero rank between positive and negative ones. 
 
    correction : bool, optional 
        If True, apply continuity correction by adjusting the Wilcoxon rank 
        statistic by 0.5 towards the mean value when computing the 
        z-statistic if a normal approximation is used.  Default is False. 
    alternative : {&quot;two-sided&quot;, &quot;greater&quot;, &quot;less&quot;}, optional 
        Defines the alternative hypothesis. Default is 'two-sided'. 
        In the following, let ``d`` represent the difference between the paired 
        samples: ``d = x - y`` if both ``x`` and ``y`` are provided, or 
        ``d = x`` otherwise. 
 
        * 'two-sided': the distribution underlying ``d`` is not symmetric 
          about zero. 
        * 'less': the distribution underlying ``d`` is stochastically less 
          than a distribution symmetric about zero. 
        * 'greater': the distribution underlying ``d`` is stochastically 
          greater than a distribution symmetric about zero. 
 
    method : {&quot;auto&quot;, &quot;exact&quot;, &quot;approx&quot;} or `PermutationMethod` instance, optional 
        Method to calculate the p-value, see Notes. Default is &quot;auto&quot;. 
 
    axis : int or None, default: 0 
        If an int, the axis of the input along which to compute the statistic. 
        The statistic of each axis-slice (e.g. row) of the input will appear 
        in a corresponding element of the output. If ``None``, the input will 
        be raveled before computing the statistic. 
 
    Returns 
    ------- 
    An object with the following attributes. 
 
    statistic : array_like 
        If `alternative` is &quot;two-sided&quot;, the sum of the ranks of the 
        differences above or below zero, whichever is smaller. 
        Otherwise the sum of the ranks of the differences above zero. 
    pvalue : array_like 
        The p-value for the test depending on `alternative` and `method`. 
    zstatistic : array_like 
        When ``method = 'approx'``, this is the normalized z-statistic:: 
 
            z = (T - mn - d) / se 
 
        where ``T`` is `statistic` as defined above, ``mn`` is the mean of the 
        distribution under the null hypothesis, ``d`` is a continuity 
        correction, and ``se`` is the standard error. 
        When ``method != 'approx'``, this attribute is not available. 
 
    See Also 
    -------- 
    kruskal, mannwhitneyu 
 
    Notes 
    ----- 
    In the following, let ``d`` represent the difference between the paired 
    samples: ``d = x - y`` if both ``x`` and ``y`` are provided, or ``d = x`` 
    otherwise. Assume that all elements of ``d`` are independent and 
    identically distributed observations, and all are distinct and nonzero. 
 
    - When ``len(d)`` is sufficiently large, the null distribution of the 
      normalized test statistic (`zstatistic` above) is approximately normal, 
      and ``method = 'approx'`` can be used to compute the p-value. 
 
    - When ``len(d)`` is small, the normal approximation may not be accurate, 
      and ``method='exact'`` is preferred (at the cost of additional 
      execution time). 
 
    - The default, ``method='auto'``, selects between the two: when 
      ``len(d) &lt;= 50`` and there are no zeros, the exact method is used; 
      otherwise, the approximate method is used. 
 
    The presence of &quot;ties&quot; (i.e. not all elements of ``d`` are unique) or 
    &quot;zeros&quot; (i.e. elements of ``d`` are zero) changes the null distribution 
    of the test statistic, and ``method='exact'`` no longer calculates 
    the exact p-value. If ``method='approx'``, the z-statistic is adjusted 
    for more accurate comparison against the standard normal, but still, 
    for finite sample sizes, the standard normal is only an approximation of 
    the true null distribution of the z-statistic. For such situations, the 
    `method` parameter also accepts instances `PermutationMethod`. In this 
    case, the p-value is computed using `permutation_test` with the provided 
    configuration options and other appropriate settings. 
 
    References 
    ---------- 
    .. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test 
    .. [2] Conover, W.J., Practical Nonparametric Statistics, 1971. 
    .. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed 
       Rank Procedures, Journal of the American Statistical Association, 
       Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526` 
    .. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods, 
       Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968` 
    .. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank 
       Sampling Distribution When Zero Differences are Present, 
       Journal of the American Statistical Association, Vol. 62, 1967, 
       pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917` 
 
    Examples 
    -------- 
    In [4]_, the differences in height between cross- and self-fertilized 
    corn plants is given as follows: 
 
    &gt;&gt;&gt; d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75] 
 
    Cross-fertilized plants appear to be higher. To test the null 
    hypothesis that there is no height difference, we can apply the 
    two-sided test: 
 
    &gt;&gt;&gt; from scipy.stats import wilcoxon 
    &gt;&gt;&gt; res = wilcoxon(d) 
    &gt;&gt;&gt; res.statistic, res.pvalue 
    (24.0, 0.041259765625) 
 
    Hence, we would reject the null hypothesis at a confidence level of 5%, 
    concluding that there is a difference in height between the groups. 
    To confirm that the median of the differences can be assumed to be 
    positive, we use: 
 
    &gt;&gt;&gt; res = wilcoxon(d, alternative='greater') 
    &gt;&gt;&gt; res.statistic, res.pvalue 
    (96.0, 0.0206298828125) 
 
    This shows that the null hypothesis that the median is negative can be 
    rejected at a confidence level of 5% in favor of the alternative that 
    the median is greater than zero. The p-values above are exact. Using the 
    normal approximation gives very similar values: 
 
    &gt;&gt;&gt; res = wilcoxon(d, method='approx') 
    &gt;&gt;&gt; res.statistic, res.pvalue 
    (24.0, 0.04088813291185591) 
 
    Note that the statistic changed to 96 in the one-sided case (the sum 
    of ranks of positive differences) whereas it is 24 in the two-sided 
    case (the minimum of sum of ranks above and below zero). 
 
    In the example above, the differences in height between paired plants are 
    provided to `wilcoxon` directly. Alternatively, `wilcoxon` accepts two 
    samples of equal length, calculates the differences between paired 
    elements, then performs the test. Consider the samples ``x`` and ``y``: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; x = np.array([0.5, 0.825, 0.375, 0.5]) 
    &gt;&gt;&gt; y = np.array([0.525, 0.775, 0.325, 0.55]) 
    &gt;&gt;&gt; res = wilcoxon(x, y, alternative='greater') 
    &gt;&gt;&gt; res 
    WilcoxonResult(statistic=5.0, pvalue=0.5625) 
 
    Note that had we calculated the differences by hand, the test would have 
    produced different results: 
 
    &gt;&gt;&gt; d = [-0.025, 0.05, 0.05, -0.05] 
    &gt;&gt;&gt; ref = wilcoxon(d, alternative='greater') 
    &gt;&gt;&gt; ref 
    WilcoxonResult(statistic=6.0, pvalue=0.4375) 
 
    The substantial difference is due to roundoff error in the results of 
    ``x-y``: 
 
    &gt;&gt;&gt; d - (x-y) 
    array([2.08166817e-17, 6.93889390e-17, 1.38777878e-17, 4.16333634e-17]) 
 
    Even though we expected all the elements of ``(x-y)[1:]`` to have the same 
    magnitude ``0.05``, they have slightly different magnitudes in practice, 
    and therefore are assigned different ranks in the test. Before performing 
    the test, consider calculating ``d`` and adjusting it as necessary to 
    ensure that theoretically identically values are not numerically distinct. 
    For example: 
 
    &gt;&gt;&gt; d2 = np.around(x - y, decimals=3) 
    &gt;&gt;&gt; wilcoxon(d2, alternative='greater') 
    WilcoxonResult(statistic=6.0, pvalue=0.4375) 
 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">_wilcoxon</span><span class="s2">.</span><span class="s1">_wilcoxon_nd</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">zero_method</span><span class="s2">, </span><span class="s1">correction</span><span class="s2">, </span><span class="s1">alternative</span><span class="s2">,</span>
                                  <span class="s1">method</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">)</span>


<span class="s1">MedianTestResult </span><span class="s2">= </span><span class="s1">_make_tuple_bunch</span><span class="s2">(</span>
    <span class="s4">'MedianTestResult'</span><span class="s2">,</span>
    <span class="s2">[</span><span class="s4">'statistic'</span><span class="s2">, </span><span class="s4">'pvalue'</span><span class="s2">, </span><span class="s4">'median'</span><span class="s2">, </span><span class="s4">'table'</span><span class="s2">], []</span>
<span class="s2">)</span>


<span class="s0">def </span><span class="s1">median_test</span><span class="s2">(*</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">ties</span><span class="s2">=</span><span class="s4">'below'</span><span class="s2">, </span><span class="s1">correction</span><span class="s2">=</span><span class="s0">True</span><span class="s2">, </span><span class="s1">lambda_</span><span class="s2">=</span><span class="s5">1</span><span class="s2">,</span>
                <span class="s1">nan_policy</span><span class="s2">=</span><span class="s4">'propagate'</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Perform a Mood's median test. 
 
    Test that two or more samples come from populations with the same median. 
 
    Let ``n = len(samples)`` be the number of samples.  The &quot;grand median&quot; of 
    all the data is computed, and a contingency table is formed by 
    classifying the values in each sample as being above or below the grand 
    median.  The contingency table, along with `correction` and `lambda_`, 
    are passed to `scipy.stats.chi2_contingency` to compute the test statistic 
    and p-value. 
 
    Parameters 
    ---------- 
    sample1, sample2, ... : array_like 
        The set of samples.  There must be at least two samples. 
        Each sample must be a one-dimensional sequence containing at least 
        one value.  The samples are not required to have the same length. 
    ties : str, optional 
        Determines how values equal to the grand median are classified in 
        the contingency table.  The string must be one of:: 
 
            &quot;below&quot;: 
                Values equal to the grand median are counted as &quot;below&quot;. 
            &quot;above&quot;: 
                Values equal to the grand median are counted as &quot;above&quot;. 
            &quot;ignore&quot;: 
                Values equal to the grand median are not counted. 
 
        The default is &quot;below&quot;. 
    correction : bool, optional 
        If True, *and* there are just two samples, apply Yates' correction 
        for continuity when computing the test statistic associated with 
        the contingency table.  Default is True. 
    lambda_ : float or str, optional 
        By default, the statistic computed in this test is Pearson's 
        chi-squared statistic.  `lambda_` allows a statistic from the 
        Cressie-Read power divergence family to be used instead.  See 
        `power_divergence` for details. 
        Default is 1 (Pearson's chi-squared statistic). 
    nan_policy : {'propagate', 'raise', 'omit'}, optional 
        Defines how to handle when input contains nan. 'propagate' returns nan, 
        'raise' throws an error, 'omit' performs the calculations ignoring nan 
        values. Default is 'propagate'. 
 
    Returns 
    ------- 
    res : MedianTestResult 
        An object containing attributes: 
 
        statistic : float 
            The test statistic.  The statistic that is returned is determined 
            by `lambda_`.  The default is Pearson's chi-squared statistic. 
        pvalue : float 
            The p-value of the test. 
        median : float 
            The grand median. 
        table : ndarray 
            The contingency table.  The shape of the table is (2, n), where 
            n is the number of samples.  The first row holds the counts of the 
            values above the grand median, and the second row holds the counts 
            of the values below the grand median.  The table allows further 
            analysis with, for example, `scipy.stats.chi2_contingency`, or with 
            `scipy.stats.fisher_exact` if there are two samples, without having 
            to recompute the table.  If ``nan_policy`` is &quot;propagate&quot; and there 
            are nans in the input, the return value for ``table`` is ``None``. 
 
    See Also 
    -------- 
    kruskal : Compute the Kruskal-Wallis H-test for independent samples. 
    mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y. 
 
    Notes 
    ----- 
    .. versionadded:: 0.15.0 
 
    References 
    ---------- 
    .. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill 
        (1950), pp. 394-399. 
    .. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010). 
        See Sections 8.12 and 10.15. 
 
    Examples 
    -------- 
    A biologist runs an experiment in which there are three groups of plants. 
    Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants. 
    Each plant produces a number of seeds.  The seed counts for each group 
    are:: 
 
        Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49 
        Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99 
        Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84 
 
    The following code applies Mood's median test to these samples. 
 
    &gt;&gt;&gt; g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49] 
    &gt;&gt;&gt; g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99] 
    &gt;&gt;&gt; g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84] 
    &gt;&gt;&gt; from scipy.stats import median_test 
    &gt;&gt;&gt; res = median_test(g1, g2, g3) 
 
    The median is 
 
    &gt;&gt;&gt; res.median 
    34.0 
 
    and the contingency table is 
 
    &gt;&gt;&gt; res.table 
    array([[ 5, 10,  7], 
           [11,  5, 10]]) 
 
    `p` is too large to conclude that the medians are not the same: 
 
    &gt;&gt;&gt; res.pvalue 
    0.12609082774093244 
 
    The &quot;G-test&quot; can be performed by passing ``lambda_=&quot;log-likelihood&quot;`` to 
    `median_test`. 
 
    &gt;&gt;&gt; res = median_test(g1, g2, g3, lambda_=&quot;log-likelihood&quot;) 
    &gt;&gt;&gt; res.pvalue 
    0.12224779737117837 
 
    The median occurs several times in the data, so we'll get a different 
    result if, for example, ``ties=&quot;above&quot;`` is used: 
 
    &gt;&gt;&gt; res = median_test(g1, g2, g3, ties=&quot;above&quot;) 
    &gt;&gt;&gt; res.pvalue 
    0.063873276069553273 
 
    &gt;&gt;&gt; res.table 
    array([[ 5, 11,  9], 
           [11,  4,  8]]) 
 
    This example demonstrates that if the data set is not large and there 
    are values equal to the median, the p-value can be sensitive to the 
    choice of `ties`. 
 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">) &lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">'median_test requires two or more samples.'</span><span class="s2">)</span>

    <span class="s1">ties_options </span><span class="s2">= [</span><span class="s4">'below'</span><span class="s2">, </span><span class="s4">'above'</span><span class="s2">, </span><span class="s4">'ignore'</span><span class="s2">]</span>
    <span class="s0">if </span><span class="s1">ties </span><span class="s0">not in </span><span class="s1">ties_options</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;invalid 'ties' option '</span><span class="s0">{</span><span class="s1">ties</span><span class="s0">}</span><span class="s4">'; 'ties' must be one &quot;</span>
                         <span class="s4">f&quot;of: </span><span class="s0">{</span><span class="s1">str</span><span class="s2">(</span><span class="s1">ties_options</span><span class="s2">)[</span><span class="s5">1</span><span class="s2">:-</span><span class="s5">1</span><span class="s2">]</span><span class="s0">}</span><span class="s4">&quot;</span><span class="s2">)</span>

    <span class="s1">data </span><span class="s2">= [</span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">) </span><span class="s0">for </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">samples</span><span class="s2">]</span>

    <span class="s3"># Validate the sizes and shapes of the arguments.</span>
    <span class="s0">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">d </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">data</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">d</span><span class="s2">.</span><span class="s1">size </span><span class="s2">== </span><span class="s5">0</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Sample %d is empty. All samples must &quot;</span>
                             <span class="s4">&quot;contain at least one value.&quot; </span><span class="s2">% (</span><span class="s1">k </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">))</span>
        <span class="s0">if </span><span class="s1">d</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">!= </span><span class="s5">1</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;Sample %d has %d dimensions.  All &quot;</span>
                             <span class="s4">&quot;samples must be one-dimensional sequences.&quot; </span><span class="s2">%</span>
                             <span class="s2">(</span><span class="s1">k </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">, </span><span class="s1">d</span><span class="s2">.</span><span class="s1">ndim</span><span class="s2">))</span>

    <span class="s1">cdata </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)</span>
    <span class="s1">contains_nan</span><span class="s2">, </span><span class="s1">nan_policy </span><span class="s2">= </span><span class="s1">_contains_nan</span><span class="s2">(</span><span class="s1">cdata</span><span class="s2">, </span><span class="s1">nan_policy</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">contains_nan </span><span class="s0">and </span><span class="s1">nan_policy </span><span class="s2">== </span><span class="s4">'propagate'</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">MedianTestResult</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">, </span><span class="s0">None</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">contains_nan</span><span class="s2">:</span>
        <span class="s1">grand_median </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">median</span><span class="s2">(</span><span class="s1">cdata</span><span class="s2">[~</span><span class="s1">np</span><span class="s2">.</span><span class="s1">isnan</span><span class="s2">(</span><span class="s1">cdata</span><span class="s2">)])</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">grand_median </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">median</span><span class="s2">(</span><span class="s1">cdata</span><span class="s2">)</span>
    <span class="s3"># When the minimum version of numpy supported by scipy is 1.9.0,</span>
    <span class="s3"># the above if/else statement can be replaced by the single line:</span>
    <span class="s3">#     grand_median = np.nanmedian(cdata)</span>

    <span class="s3"># Create the contingency table.</span>
    <span class="s1">table </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">((</span><span class="s5">2</span><span class="s2">, </span><span class="s1">len</span><span class="s2">(</span><span class="s1">data</span><span class="s2">)), </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">np</span><span class="s2">.</span><span class="s1">int64</span><span class="s2">)</span>
    <span class="s0">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">sample </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">data</span><span class="s2">):</span>
        <span class="s1">sample </span><span class="s2">= </span><span class="s1">sample</span><span class="s2">[~</span><span class="s1">np</span><span class="s2">.</span><span class="s1">isnan</span><span class="s2">(</span><span class="s1">sample</span><span class="s2">)]</span>

        <span class="s1">nabove </span><span class="s2">= </span><span class="s1">count_nonzero</span><span class="s2">(</span><span class="s1">sample </span><span class="s2">&gt; </span><span class="s1">grand_median</span><span class="s2">)</span>
        <span class="s1">nbelow </span><span class="s2">= </span><span class="s1">count_nonzero</span><span class="s2">(</span><span class="s1">sample </span><span class="s2">&lt; </span><span class="s1">grand_median</span><span class="s2">)</span>
        <span class="s1">nequal </span><span class="s2">= </span><span class="s1">sample</span><span class="s2">.</span><span class="s1">size </span><span class="s2">- (</span><span class="s1">nabove </span><span class="s2">+ </span><span class="s1">nbelow</span><span class="s2">)</span>
        <span class="s1">table</span><span class="s2">[</span><span class="s5">0</span><span class="s2">, </span><span class="s1">k</span><span class="s2">] += </span><span class="s1">nabove</span>
        <span class="s1">table</span><span class="s2">[</span><span class="s5">1</span><span class="s2">, </span><span class="s1">k</span><span class="s2">] += </span><span class="s1">nbelow</span>
        <span class="s0">if </span><span class="s1">ties </span><span class="s2">== </span><span class="s4">&quot;below&quot;</span><span class="s2">:</span>
            <span class="s1">table</span><span class="s2">[</span><span class="s5">1</span><span class="s2">, </span><span class="s1">k</span><span class="s2">] += </span><span class="s1">nequal</span>
        <span class="s0">elif </span><span class="s1">ties </span><span class="s2">== </span><span class="s4">&quot;above&quot;</span><span class="s2">:</span>
            <span class="s1">table</span><span class="s2">[</span><span class="s5">0</span><span class="s2">, </span><span class="s1">k</span><span class="s2">] += </span><span class="s1">nequal</span>

    <span class="s3"># Check that no row or column of the table is all zero.</span>
    <span class="s3"># Such a table can not be given to chi2_contingency, because it would have</span>
    <span class="s3"># a zero in the table of expected frequencies.</span>
    <span class="s1">rowsums </span><span class="s2">= </span><span class="s1">table</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">rowsums</span><span class="s2">[</span><span class="s5">0</span><span class="s2">] == </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;All values are below the grand median (</span><span class="s0">{</span><span class="s1">grand_median</span><span class="s0">}</span><span class="s4">).&quot;</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">rowsums</span><span class="s2">[</span><span class="s5">1</span><span class="s2">] == </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;All values are above the grand median (</span><span class="s0">{</span><span class="s1">grand_median</span><span class="s0">}</span><span class="s4">).&quot;</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">ties </span><span class="s2">== </span><span class="s4">&quot;ignore&quot;</span><span class="s2">:</span>
        <span class="s3"># We already checked that each sample has at least one value, but it</span>
        <span class="s3"># is possible that all those values equal the grand median.  If `ties`</span>
        <span class="s3"># is &quot;ignore&quot;, that would result in a column of zeros in `table`.  We</span>
        <span class="s3"># check for that case here.</span>
        <span class="s1">zero_cols </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nonzero</span><span class="s2">((</span><span class="s1">table </span><span class="s2">== </span><span class="s5">0</span><span class="s2">).</span><span class="s1">all</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">))[</span><span class="s5">0</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">zero_cols</span><span class="s2">) &gt; </span><span class="s5">0</span><span class="s2">:</span>
            <span class="s1">msg </span><span class="s2">= (</span><span class="s4">&quot;All values in sample %d are equal to the grand &quot;</span>
                   <span class="s4">&quot;median (%r), so they are ignored, resulting in an &quot;</span>
                   <span class="s4">&quot;empty sample.&quot; </span><span class="s2">% (</span><span class="s1">zero_cols</span><span class="s2">[</span><span class="s5">0</span><span class="s2">] + </span><span class="s5">1</span><span class="s2">, </span><span class="s1">grand_median</span><span class="s2">))</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s1">msg</span><span class="s2">)</span>

    <span class="s1">stat</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">dof</span><span class="s2">, </span><span class="s1">expected </span><span class="s2">= </span><span class="s1">chi2_contingency</span><span class="s2">(</span><span class="s1">table</span><span class="s2">, </span><span class="s1">lambda_</span><span class="s2">=</span><span class="s1">lambda_</span><span class="s2">,</span>
                                              <span class="s1">correction</span><span class="s2">=</span><span class="s1">correction</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">MedianTestResult</span><span class="s2">(</span><span class="s1">stat</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">grand_median</span><span class="s2">, </span><span class="s1">table</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">_circfuncs_common</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">) </span><span class="s0">if </span><span class="s1">xp </span><span class="s0">is None else </span><span class="s1">xp</span>

    <span class="s0">if </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">isdtype</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s4">'integral'</span><span class="s2">):</span>
        <span class="s1">dtype </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s5">1.</span><span class="s2">).</span><span class="s1">dtype  </span><span class="s3"># get default float type</span>
        <span class="s1">samples </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>

    <span class="s3"># Recast samples as radians that range between 0 and 2 pi and calculate</span>
    <span class="s3"># the sine and cosine</span>
    <span class="s1">sin_samp </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sin</span><span class="s2">((</span><span class="s1">samples </span><span class="s2">- </span><span class="s1">low</span><span class="s2">)*</span><span class="s5">2.</span><span class="s2">*</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">pi </span><span class="s2">/ (</span><span class="s1">high </span><span class="s2">- </span><span class="s1">low</span><span class="s2">))</span>
    <span class="s1">cos_samp </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">cos</span><span class="s2">((</span><span class="s1">samples </span><span class="s2">- </span><span class="s1">low</span><span class="s2">)*</span><span class="s5">2.</span><span class="s2">*</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">pi </span><span class="s2">/ (</span><span class="s1">high </span><span class="s2">- </span><span class="s1">low</span><span class="s2">))</span>

    <span class="s0">return </span><span class="s1">samples</span><span class="s2">, </span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">cos_samp</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span>
    <span class="s0">lambda </span><span class="s1">x</span><span class="s2">: </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">default_axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">result_to_tuple</span><span class="s2">=</span><span class="s0">lambda </span><span class="s1">x</span><span class="s2">: (</span><span class="s1">x</span><span class="s2">,)</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">circmean</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s5">2</span><span class="s2">*</span><span class="s1">pi</span><span class="s2">, </span><span class="s1">low</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">nan_policy</span><span class="s2">=</span><span class="s4">'propagate'</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Compute the circular mean of a sample of angle observations. 
 
    Given :math:`n` angle observations :math:`x_1, \cdots, x_n` measured in 
    radians, their `circular mean` is defined by ([1]_, Eq. 2.2.4) 
 
    .. math:: 
 
       \mathrm{Arg} \left( \frac{1}{n} \sum_{k=1}^n e^{i x_k} \right) 
 
    where :math:`i` is the imaginary unit and :math:`\mathop{\mathrm{Arg}} z` 
    gives the principal value of the argument of complex number :math:`z`, 
    restricted to the range :math:`[0,2\pi]` by default.  :math:`z` in the 
    above expression is known as the `mean resultant vector`. 
 
    Parameters 
    ---------- 
    samples : array_like 
        Input array of angle observations.  The value of a full angle is 
        equal to ``(high - low)``. 
    high : float, optional 
        Upper boundary of the principal value of an angle.  Default is ``2*pi``. 
    low : float, optional 
        Lower boundary of the principal value of an angle.  Default is ``0``. 
 
    Returns 
    ------- 
    circmean : float 
        Circular mean, restricted to the range ``[low, high]``. 
 
        If the mean resultant vector is zero, an input-dependent, 
        implementation-defined number between ``[low, high]`` is returned. 
        If the input array is empty, ``np.nan`` is returned. 
 
    See Also 
    -------- 
    circstd : Circular standard deviation. 
    circvar : Circular variance. 
 
    References 
    ---------- 
    .. [1] Mardia, K. V. and Jupp, P. E. *Directional Statistics*. 
           John Wiley &amp; Sons, 1999. 
 
    Examples 
    -------- 
    For readability, all angles are printed out in degrees. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import circmean 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; angles = np.deg2rad(np.array([20, 30, 330])) 
    &gt;&gt;&gt; circmean = circmean(angles) 
    &gt;&gt;&gt; np.rad2deg(circmean) 
    7.294976657784009 
 
    &gt;&gt;&gt; mean = angles.mean() 
    &gt;&gt;&gt; np.rad2deg(mean) 
    126.66666666666666 
 
    Plot and compare the circular mean against the arithmetic mean. 
 
    &gt;&gt;&gt; plt.plot(np.cos(np.linspace(0, 2*np.pi, 500)), 
    ...          np.sin(np.linspace(0, 2*np.pi, 500)), 
    ...          c='k') 
    &gt;&gt;&gt; plt.scatter(np.cos(angles), np.sin(angles), c='k') 
    &gt;&gt;&gt; plt.scatter(np.cos(circmean), np.sin(circmean), c='b', 
    ...             label='circmean') 
    &gt;&gt;&gt; plt.scatter(np.cos(mean), np.sin(mean), c='r', label='mean') 
    &gt;&gt;&gt; plt.legend() 
    &gt;&gt;&gt; plt.axis('equal') 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s3"># Needed for non-NumPy arrays to get appropriate NaN result</span>
    <span class="s3"># Apparently atan2(0, 0) is 0, even though it is mathematically undefined</span>
    <span class="s0">if </span><span class="s1">xp_size</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">) == </span><span class="s5">0</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">samples</span><span class="s2">, </span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">cos_samp </span><span class="s2">= </span><span class="s1">_circfuncs_common</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">xp</span><span class="s2">)</span>
    <span class="s1">sin_sum </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">cos_sum </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">cos_samp</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">res </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">atan2</span><span class="s2">(</span><span class="s1">sin_sum</span><span class="s2">, </span><span class="s1">cos_sum</span><span class="s2">) % (</span><span class="s5">2</span><span class="s2">*</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">pi</span><span class="s2">)</span>

    <span class="s1">res </span><span class="s2">= </span><span class="s1">res</span><span class="s2">[()] </span><span class="s0">if </span><span class="s1">res</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">== </span><span class="s5">0 </span><span class="s0">else </span><span class="s1">res</span>
    <span class="s0">return </span><span class="s1">res</span><span class="s2">*(</span><span class="s1">high </span><span class="s2">- </span><span class="s1">low</span><span class="s2">)/</span><span class="s5">2.0</span><span class="s2">/</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">pi </span><span class="s2">+ </span><span class="s1">low</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span>
    <span class="s0">lambda </span><span class="s1">x</span><span class="s2">: </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">default_axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">result_to_tuple</span><span class="s2">=</span><span class="s0">lambda </span><span class="s1">x</span><span class="s2">: (</span><span class="s1">x</span><span class="s2">,)</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">circvar</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s5">2</span><span class="s2">*</span><span class="s1">pi</span><span class="s2">, </span><span class="s1">low</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">nan_policy</span><span class="s2">=</span><span class="s4">'propagate'</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot;Compute the circular variance of a sample of angle observations. 
 
    Given :math:`n` angle observations :math:`x_1, \cdots, x_n` measured in 
    radians, their `circular variance` is defined by ([2]_, Eq. 2.3.3) 
 
    .. math:: 
 
       1 - \left| \frac{1}{n} \sum_{k=1}^n e^{i x_k} \right| 
 
    where :math:`i` is the imaginary unit and :math:`|z|` gives the length 
    of the complex number :math:`z`.  :math:`|z|` in the above expression 
    is known as the `mean resultant length`. 
 
    Parameters 
    ---------- 
    samples : array_like 
        Input array of angle observations.  The value of a full angle is 
        equal to ``(high - low)``. 
    high : float, optional 
        Upper boundary of the principal value of an angle.  Default is ``2*pi``. 
    low : float, optional 
        Lower boundary of the principal value of an angle.  Default is ``0``. 
 
    Returns 
    ------- 
    circvar : float 
        Circular variance.  The returned value is in the range ``[0, 1]``, 
        where ``0`` indicates no variance and ``1`` indicates large variance. 
 
        If the input array is empty, ``np.nan`` is returned. 
 
    See Also 
    -------- 
    circmean : Circular mean. 
    circstd : Circular standard deviation. 
 
    Notes 
    ----- 
    In the limit of small angles, the circular variance is close to 
    half the 'linear' variance if measured in radians. 
 
    References 
    ---------- 
    .. [1] Fisher, N.I. *Statistical analysis of circular data*. Cambridge 
           University Press, 1993. 
    .. [2] Mardia, K. V. and Jupp, P. E. *Directional Statistics*. 
           John Wiley &amp; Sons, 1999. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import circvar 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; samples_1 = np.array([0.072, -0.158, 0.077, 0.108, 0.286, 
    ...                       0.133, -0.473, -0.001, -0.348, 0.131]) 
    &gt;&gt;&gt; samples_2 = np.array([0.111, -0.879, 0.078, 0.733, 0.421, 
    ...                       0.104, -0.136, -0.867,  0.012,  0.105]) 
    &gt;&gt;&gt; circvar_1 = circvar(samples_1) 
    &gt;&gt;&gt; circvar_2 = circvar(samples_2) 
 
    Plot the samples. 
 
    &gt;&gt;&gt; fig, (left, right) = plt.subplots(ncols=2) 
    &gt;&gt;&gt; for image in (left, right): 
    ...     image.plot(np.cos(np.linspace(0, 2*np.pi, 500)), 
    ...                np.sin(np.linspace(0, 2*np.pi, 500)), 
    ...                c='k') 
    ...     image.axis('equal') 
    ...     image.axis('off') 
    &gt;&gt;&gt; left.scatter(np.cos(samples_1), np.sin(samples_1), c='k', s=15) 
    &gt;&gt;&gt; left.set_title(f&quot;circular variance: {np.round(circvar_1, 2)!r}&quot;) 
    &gt;&gt;&gt; right.scatter(np.cos(samples_2), np.sin(samples_2), c='k', s=15) 
    &gt;&gt;&gt; right.set_title(f&quot;circular variance: {np.round(circvar_2, 2)!r}&quot;) 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s1">samples</span><span class="s2">, </span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">cos_samp </span><span class="s2">= </span><span class="s1">_circfuncs_common</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">xp</span><span class="s2">)</span>
    <span class="s1">sin_mean </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">cos_mean </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">cos_samp</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">hypotenuse </span><span class="s2">= (</span><span class="s1">sin_mean</span><span class="s2">**</span><span class="s5">2. </span><span class="s2">+ </span><span class="s1">cos_mean</span><span class="s2">**</span><span class="s5">2.</span><span class="s2">)**</span><span class="s5">0.5</span>
    <span class="s3"># hypotenuse can go slightly above 1 due to rounding errors</span>
    <span class="s0">with </span><span class="s1">np</span><span class="s2">.</span><span class="s1">errstate</span><span class="s2">(</span><span class="s1">invalid</span><span class="s2">=</span><span class="s4">'ignore'</span><span class="s2">):</span>
        <span class="s1">R </span><span class="s2">= </span><span class="s1">xp_minimum</span><span class="s2">(</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s5">1.</span><span class="s2">), </span><span class="s1">hypotenuse</span><span class="s2">)</span>

    <span class="s1">res </span><span class="s2">= </span><span class="s5">1. </span><span class="s2">- </span><span class="s1">R</span>
    <span class="s0">return </span><span class="s1">res</span>


<span class="s2">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s2">(</span>
    <span class="s0">lambda </span><span class="s1">x</span><span class="s2">: </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n_outputs</span><span class="s2">=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">default_axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">result_to_tuple</span><span class="s2">=</span><span class="s0">lambda </span><span class="s1">x</span><span class="s2">: (</span><span class="s1">x</span><span class="s2">,)</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">circstd</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s5">2</span><span class="s2">*</span><span class="s1">pi</span><span class="s2">, </span><span class="s1">low</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">nan_policy</span><span class="s2">=</span><span class="s4">'propagate'</span><span class="s2">, *,</span>
            <span class="s1">normalize</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s6">r&quot;&quot;&quot; 
    Compute the circular standard deviation of a sample of angle observations. 
 
    Given :math:`n` angle observations :math:`x_1, \cdots, x_n` measured in 
    radians, their `circular standard deviation` is defined by 
    ([2]_, Eq. 2.3.11) 
 
    .. math:: 
 
       \sqrt{ -2 \log \left| \frac{1}{n} \sum_{k=1}^n e^{i x_k} \right| } 
 
    where :math:`i` is the imaginary unit and :math:`|z|` gives the length 
    of the complex number :math:`z`.  :math:`|z|` in the above expression 
    is known as the `mean resultant length`. 
 
    Parameters 
    ---------- 
    samples : array_like 
        Input array of angle observations.  The value of a full angle is 
        equal to ``(high - low)``. 
    high : float, optional 
        Upper boundary of the principal value of an angle.  Default is ``2*pi``. 
    low : float, optional 
        Lower boundary of the principal value of an angle.  Default is ``0``. 
    normalize : boolean, optional 
        If ``False`` (the default), the return value is computed from the 
        above formula with the input scaled by ``(2*pi)/(high-low)`` and 
        the output scaled (back) by ``(high-low)/(2*pi)``.  If ``True``, 
        the output is not scaled and is returned directly. 
 
    Returns 
    ------- 
    circstd : float 
        Circular standard deviation, optionally normalized. 
 
        If the input array is empty, ``np.nan`` is returned. 
 
    See Also 
    -------- 
    circmean : Circular mean. 
    circvar : Circular variance. 
 
    Notes 
    ----- 
    In the limit of small angles, the circular standard deviation is close 
    to the 'linear' standard deviation if ``normalize`` is ``False``. 
 
    References 
    ---------- 
    .. [1] Mardia, K. V. (1972). 2. In *Statistics of Directional Data* 
       (pp. 18-24). Academic Press. :doi:`10.1016/C2013-0-07425-7`. 
    .. [2] Mardia, K. V. and Jupp, P. E. *Directional Statistics*. 
           John Wiley &amp; Sons, 1999. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import circstd 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; samples_1 = np.array([0.072, -0.158, 0.077, 0.108, 0.286, 
    ...                       0.133, -0.473, -0.001, -0.348, 0.131]) 
    &gt;&gt;&gt; samples_2 = np.array([0.111, -0.879, 0.078, 0.733, 0.421, 
    ...                       0.104, -0.136, -0.867,  0.012,  0.105]) 
    &gt;&gt;&gt; circstd_1 = circstd(samples_1) 
    &gt;&gt;&gt; circstd_2 = circstd(samples_2) 
 
    Plot the samples. 
 
    &gt;&gt;&gt; fig, (left, right) = plt.subplots(ncols=2) 
    &gt;&gt;&gt; for image in (left, right): 
    ...     image.plot(np.cos(np.linspace(0, 2*np.pi, 500)), 
    ...                np.sin(np.linspace(0, 2*np.pi, 500)), 
    ...                c='k') 
    ...     image.axis('equal') 
    ...     image.axis('off') 
    &gt;&gt;&gt; left.scatter(np.cos(samples_1), np.sin(samples_1), c='k', s=15) 
    &gt;&gt;&gt; left.set_title(f&quot;circular std: {np.round(circstd_1, 2)!r}&quot;) 
    &gt;&gt;&gt; right.plot(np.cos(np.linspace(0, 2*np.pi, 500)), 
    ...            np.sin(np.linspace(0, 2*np.pi, 500)), 
    ...            c='k') 
    &gt;&gt;&gt; right.scatter(np.cos(samples_2), np.sin(samples_2), c='k', s=15) 
    &gt;&gt;&gt; right.set_title(f&quot;circular std: {np.round(circstd_2, 2)!r}&quot;) 
    &gt;&gt;&gt; plt.show() 
 
    &quot;&quot;&quot;</span>
    <span class="s1">xp </span><span class="s2">= </span><span class="s1">array_namespace</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s1">samples</span><span class="s2">, </span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">cos_samp </span><span class="s2">= </span><span class="s1">_circfuncs_common</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">high</span><span class="s2">, </span><span class="s1">low</span><span class="s2">, </span><span class="s1">xp</span><span class="s2">=</span><span class="s1">xp</span><span class="s2">)</span>
    <span class="s1">sin_mean </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">sin_samp</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)  </span><span class="s3"># [1] (2.2.3)</span>
    <span class="s1">cos_mean </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">cos_samp</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)  </span><span class="s3"># [1] (2.2.3)</span>
    <span class="s1">hypotenuse </span><span class="s2">= (</span><span class="s1">sin_mean</span><span class="s2">**</span><span class="s5">2. </span><span class="s2">+ </span><span class="s1">cos_mean</span><span class="s2">**</span><span class="s5">2.</span><span class="s2">)**</span><span class="s5">0.5</span>
    <span class="s3"># hypotenuse can go slightly above 1 due to rounding errors</span>
    <span class="s0">with </span><span class="s1">np</span><span class="s2">.</span><span class="s1">errstate</span><span class="s2">(</span><span class="s1">invalid</span><span class="s2">=</span><span class="s4">'ignore'</span><span class="s2">):</span>
        <span class="s1">R </span><span class="s2">= </span><span class="s1">xp_minimum</span><span class="s2">(</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s5">1.</span><span class="s2">), </span><span class="s1">hypotenuse</span><span class="s2">)  </span><span class="s3"># [1] (2.2.4)</span>

    <span class="s1">res </span><span class="s2">= </span><span class="s1">xp</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(-</span><span class="s5">2</span><span class="s2">*</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">R</span><span class="s2">))</span>
    <span class="s0">if not </span><span class="s1">normalize</span><span class="s2">:</span>
        <span class="s1">res </span><span class="s2">*= (</span><span class="s1">high</span><span class="s2">-</span><span class="s1">low</span><span class="s2">)/(</span><span class="s5">2.</span><span class="s2">*</span><span class="s1">xp</span><span class="s2">.</span><span class="s1">pi</span><span class="s2">)  </span><span class="s3"># [1] (2.3.14) w/ (2.3.7)</span>
    <span class="s0">return </span><span class="s1">res</span>


<span class="s0">class </span><span class="s1">DirectionalStats</span><span class="s2">:</span>
    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">mean_direction</span><span class="s2">, </span><span class="s1">mean_resultant_length</span><span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">mean_direction </span><span class="s2">= </span><span class="s1">mean_direction</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">mean_resultant_length </span><span class="s2">= </span><span class="s1">mean_resultant_length</span>

    <span class="s0">def </span><span class="s1">__repr__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">(</span><span class="s4">f&quot;DirectionalStats(mean_direction=</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">mean_direction</span><span class="s0">}</span><span class="s4">,&quot;</span>
                <span class="s4">f&quot; mean_resultant_length=</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">mean_resultant_length</span><span class="s0">}</span><span class="s4">)&quot;</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">directional_stats</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, *, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">normalize</span><span class="s2">=</span><span class="s0">True</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot; 
    Computes sample statistics for directional data. 
 
    Computes the directional mean (also called the mean direction vector) and 
    mean resultant length of a sample of vectors. 
 
    The directional mean is a measure of &quot;preferred direction&quot; of vector data. 
    It is analogous to the sample mean, but it is for use when the length of 
    the data is irrelevant (e.g. unit vectors). 
 
    The mean resultant length is a value between 0 and 1 used to quantify the 
    dispersion of directional data: the smaller the mean resultant length, the 
    greater the dispersion. Several definitions of directional variance 
    involving the mean resultant length are given in [1]_ and [2]_. 
 
    Parameters 
    ---------- 
    samples : array_like 
        Input array. Must be at least two-dimensional, and the last axis of the 
        input must correspond with the dimensionality of the vector space. 
        When the input is exactly two dimensional, this means that each row 
        of the data is a vector observation. 
    axis : int, default: 0 
        Axis along which the directional mean is computed. 
    normalize: boolean, default: True 
        If True, normalize the input to ensure that each observation is a 
        unit vector. It the observations are already unit vectors, consider 
        setting this to False to avoid unnecessary computation. 
 
    Returns 
    ------- 
    res : DirectionalStats 
        An object containing attributes: 
 
        mean_direction : ndarray 
            Directional mean. 
        mean_resultant_length : ndarray 
            The mean resultant length [1]_. 
 
    See Also 
    -------- 
    circmean: circular mean; i.e. directional mean for 2D *angles* 
    circvar: circular variance; i.e. directional variance for 2D *angles* 
 
    Notes 
    ----- 
    This uses a definition of directional mean from [1]_. 
    Assuming the observations are unit vectors, the calculation is as follows. 
 
    .. code-block:: python 
 
        mean = samples.mean(axis=0) 
        mean_resultant_length = np.linalg.norm(mean) 
        mean_direction = mean / mean_resultant_length 
 
    This definition is appropriate for *directional* data (i.e. vector data 
    for which the magnitude of each observation is irrelevant) but not 
    for *axial* data (i.e. vector data for which the magnitude and *sign* of 
    each observation is irrelevant). 
 
    Several definitions of directional variance involving the mean resultant 
    length ``R`` have been proposed, including ``1 - R`` [1]_, ``1 - R**2`` 
    [2]_, and ``2 * (1 - R)`` [2]_. Rather than choosing one, this function 
    returns ``R`` as attribute `mean_resultant_length` so the user can compute 
    their preferred measure of dispersion. 
 
    References 
    ---------- 
    .. [1] Mardia, Jupp. (2000). *Directional Statistics* 
       (p. 163). Wiley. 
 
    .. [2] https://en.wikipedia.org/wiki/Directional_statistics 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import directional_stats 
    &gt;&gt;&gt; data = np.array([[3, 4],    # first observation, 2D vector space 
    ...                  [6, -8]])  # second observation 
    &gt;&gt;&gt; dirstats = directional_stats(data) 
    &gt;&gt;&gt; dirstats.mean_direction 
    array([1., 0.]) 
 
    In contrast, the regular sample mean of the vectors would be influenced 
    by the magnitude of each observation. Furthermore, the result would not be 
    a unit vector. 
 
    &gt;&gt;&gt; data.mean(axis=0) 
    array([4.5, -2.]) 
 
    An exemplary use case for `directional_stats` is to find a *meaningful* 
    center for a set of observations on a sphere, e.g. geographical locations. 
 
    &gt;&gt;&gt; data = np.array([[0.8660254, 0.5, 0.], 
    ...                  [0.8660254, -0.5, 0.]]) 
    &gt;&gt;&gt; dirstats = directional_stats(data) 
    &gt;&gt;&gt; dirstats.mean_direction 
    array([1., 0., 0.]) 
 
    The regular sample mean on the other hand yields a result which does not 
    lie on the surface of the sphere. 
 
    &gt;&gt;&gt; data.mean(axis=0) 
    array([0.8660254, 0., 0.]) 
 
    The function also returns the mean resultant length, which 
    can be used to calculate a directional variance. For example, using the 
    definition ``Var(z) = 1 - R`` from [2]_ where ``R`` is the 
    mean resultant length, we can calculate the directional variance of the 
    vectors in the above example as: 
 
    &gt;&gt;&gt; 1 - dirstats.mean_resultant_length 
    0.13397459716167093 
    &quot;&quot;&quot;</span>
    <span class="s1">samples </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">samples</span><span class="s2">.</span><span class="s1">ndim </span><span class="s2">&lt; </span><span class="s5">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;samples must at least be two-dimensional. &quot;</span>
                         <span class="s4">f&quot;Instead samples has shape: </span><span class="s0">{</span><span class="s1">samples</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">!r}</span><span class="s4">&quot;</span><span class="s2">)</span>
    <span class="s1">samples </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">moveaxis</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s5">0</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">normalize</span><span class="s2">:</span>
        <span class="s1">vectornorms </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">linalg</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
        <span class="s1">samples </span><span class="s2">= </span><span class="s1">samples</span><span class="s2">/</span><span class="s1">vectornorms</span>
    <span class="s1">mean </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">samples</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
    <span class="s1">mean_resultant_length </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">linalg</span><span class="s2">.</span><span class="s1">norm</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
    <span class="s1">mean_direction </span><span class="s2">= </span><span class="s1">mean </span><span class="s2">/ </span><span class="s1">mean_resultant_length</span>
    <span class="s0">return </span><span class="s1">DirectionalStats</span><span class="s2">(</span><span class="s1">mean_direction</span><span class="s2">,</span>
                            <span class="s1">mean_resultant_length</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(-</span><span class="s5">1</span><span class="s2">)[()])</span>


<span class="s0">def </span><span class="s1">false_discovery_control</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, *, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">method</span><span class="s2">=</span><span class="s4">'bh'</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Adjust p-values to control the false discovery rate. 
 
    The false discovery rate (FDR) is the expected proportion of rejected null 
    hypotheses that are actually true. 
    If the null hypothesis is rejected when the *adjusted* p-value falls below 
    a specified level, the false discovery rate is controlled at that level. 
 
    Parameters 
    ---------- 
    ps : 1D array_like 
        The p-values to adjust. Elements must be real numbers between 0 and 1. 
    axis : int 
        The axis along which to perform the adjustment. The adjustment is 
        performed independently along each axis-slice. If `axis` is None, `ps` 
        is raveled before performing the adjustment. 
    method : {'bh', 'by'} 
        The false discovery rate control procedure to apply: ``'bh'`` is for 
        Benjamini-Hochberg [1]_ (Eq. 1), ``'by'`` is for Benjaminini-Yekutieli 
        [2]_ (Theorem 1.3). The latter is more conservative, but it is 
        guaranteed to control the FDR even when the p-values are not from 
        independent tests. 
 
    Returns 
    ------- 
    ps_adusted : array_like 
        The adjusted p-values. If the null hypothesis is rejected where these 
        fall below a specified level, the false discovery rate is controlled 
        at that level. 
 
    See Also 
    -------- 
    combine_pvalues 
    statsmodels.stats.multitest.multipletests 
 
    Notes 
    ----- 
    In multiple hypothesis testing, false discovery control procedures tend to 
    offer higher power than familywise error rate control procedures (e.g. 
    Bonferroni correction [1]_). 
 
    If the p-values correspond with independent tests (or tests with 
    &quot;positive regression dependencies&quot; [2]_), rejecting null hypotheses 
    corresponding with Benjamini-Hochberg-adjusted p-values below :math:`q` 
    controls the false discovery rate at a level less than or equal to 
    :math:`q m_0 / m`, where :math:`m_0` is the number of true null hypotheses 
    and :math:`m` is the total number of null hypotheses tested. The same is 
    true even for dependent tests when the p-values are adjusted accorded to 
    the more conservative Benjaminini-Yekutieli procedure. 
 
    The adjusted p-values produced by this function are comparable to those 
    produced by the R function ``p.adjust`` and the statsmodels function 
    `statsmodels.stats.multitest.multipletests`. Please consider the latter 
    for more advanced methods of multiple comparison correction. 
 
    References 
    ---------- 
    .. [1] Benjamini, Yoav, and Yosef Hochberg. &quot;Controlling the false 
           discovery rate: a practical and powerful approach to multiple 
           testing.&quot; Journal of the Royal statistical society: series B 
           (Methodological) 57.1 (1995): 289-300. 
 
    .. [2] Benjamini, Yoav, and Daniel Yekutieli. &quot;The control of the false 
           discovery rate in multiple testing under dependency.&quot; Annals of 
           statistics (2001): 1165-1188. 
 
    .. [3] TileStats. FDR - Benjamini-Hochberg explained - Youtube. 
           https://www.youtube.com/watch?v=rZKa4tW2NKs. 
 
    .. [4] Neuhaus, Karl-Ludwig, et al. &quot;Improved thrombolysis in acute 
           myocardial infarction with front-loaded administration of alteplase: 
           results of the rt-PA-APSAC patency study (TAPS).&quot; Journal of the 
           American College of Cardiology 19.5 (1992): 885-891. 
 
    Examples 
    -------- 
    We follow the example from [1]_. 
 
        Thrombolysis with recombinant tissue-type plasminogen activator (rt-PA) 
        and anisoylated plasminogen streptokinase activator (APSAC) in 
        myocardial infarction has been proved to reduce mortality. [4]_ 
        investigated the effects of a new front-loaded administration of rt-PA 
        versus those obtained with a standard regimen of APSAC, in a randomized 
        multicentre trial in 421 patients with acute myocardial infarction. 
 
    There were four families of hypotheses tested in the study, the last of 
    which was &quot;cardiac and other events after the start of thrombolitic 
    treatment&quot;. FDR control may be desired in this family of hypotheses 
    because it would not be appropriate to conclude that the front-loaded 
    treatment is better if it is merely equivalent to the previous treatment. 
 
    The p-values corresponding with the 15 hypotheses in this family were 
 
    &gt;&gt;&gt; ps = [0.0001, 0.0004, 0.0019, 0.0095, 0.0201, 0.0278, 0.0298, 0.0344, 
    ...       0.0459, 0.3240, 0.4262, 0.5719, 0.6528, 0.7590, 1.000] 
 
    If the chosen significance level is 0.05, we may be tempted to reject the 
    null hypotheses for the tests corresponding with the first nine p-values, 
    as the first nine p-values fall below the chosen significance level. 
    However, this would ignore the problem of &quot;multiplicity&quot;: if we fail to 
    correct for the fact that multiple comparisons are being performed, we 
    are more likely to incorrectly reject true null hypotheses. 
 
    One approach to the multiplicity problem is to control the family-wise 
    error rate (FWER), that is, the rate at which the null hypothesis is 
    rejected when it is actually true. A common procedure of this kind is the 
    Bonferroni correction [1]_.  We begin by multiplying the p-values by the 
    number of hypotheses tested. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; np.array(ps) * len(ps) 
    array([1.5000e-03, 6.0000e-03, 2.8500e-02, 1.4250e-01, 3.0150e-01, 
           4.1700e-01, 4.4700e-01, 5.1600e-01, 6.8850e-01, 4.8600e+00, 
           6.3930e+00, 8.5785e+00, 9.7920e+00, 1.1385e+01, 1.5000e+01]) 
 
    To control the FWER at 5%, we reject only the hypotheses corresponding 
    with adjusted p-values less than 0.05. In this case, only the hypotheses 
    corresponding with the first three p-values can be rejected. According to 
    [1]_, these three hypotheses concerned &quot;allergic reaction&quot; and &quot;two 
    different aspects of bleeding.&quot; 
 
    An alternative approach is to control the false discovery rate: the 
    expected fraction of rejected null hypotheses that are actually true. The 
    advantage of this approach is that it typically affords greater power: an 
    increased rate of rejecting the null hypothesis when it is indeed false. To 
    control the false discovery rate at 5%, we apply the Benjamini-Hochberg 
    p-value adjustment. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; stats.false_discovery_control(ps) 
    array([0.0015    , 0.003     , 0.0095    , 0.035625  , 0.0603    , 
           0.06385714, 0.06385714, 0.0645    , 0.0765    , 0.486     , 
           0.58118182, 0.714875  , 0.75323077, 0.81321429, 1.        ]) 
 
    Now, the first *four* adjusted p-values fall below 0.05, so we would reject 
    the null hypotheses corresponding with these *four* p-values. Rejection 
    of the fourth null hypothesis was particularly important to the original 
    study as it led to the conclusion that the new treatment had a 
    &quot;substantially lower in-hospital mortality rate.&quot; 
 
    &quot;&quot;&quot;</span>
    <span class="s3"># Input Validation and Special Cases</span>
    <span class="s1">ps </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">)</span>

    <span class="s1">ps_in_range </span><span class="s2">= (</span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">number</span><span class="s2">)</span>
                   <span class="s0">and </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span><span class="s1">ps </span><span class="s2">== </span><span class="s1">np</span><span class="s2">.</span><span class="s1">clip</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">)))</span>
    <span class="s0">if not </span><span class="s1">ps_in_range</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`ps` must include only numbers between 0 and 1.&quot;</span><span class="s2">)</span>

    <span class="s1">methods </span><span class="s2">= {</span><span class="s4">'bh'</span><span class="s2">, </span><span class="s4">'by'</span><span class="s2">}</span>
    <span class="s0">if </span><span class="s1">method</span><span class="s2">.</span><span class="s1">lower</span><span class="s2">() </span><span class="s0">not in </span><span class="s1">methods</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;Unrecognized `method` '</span><span class="s0">{</span><span class="s1">method</span><span class="s0">}</span><span class="s4">'.&quot;</span>
                         <span class="s4">f&quot;Method must be one of </span><span class="s0">{</span><span class="s1">methods</span><span class="s0">}</span><span class="s4">.&quot;</span><span class="s2">)</span>
    <span class="s1">method </span><span class="s2">= </span><span class="s1">method</span><span class="s2">.</span><span class="s1">lower</span><span class="s2">()</span>

    <span class="s0">if </span><span class="s1">axis </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">axis </span><span class="s2">= </span><span class="s5">0</span>
        <span class="s1">ps </span><span class="s2">= </span><span class="s1">ps</span><span class="s2">.</span><span class="s1">ravel</span><span class="s2">()</span>

    <span class="s1">axis </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">)[()]</span>
    <span class="s0">if not </span><span class="s1">np</span><span class="s2">.</span><span class="s1">issubdtype</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">integer</span><span class="s2">) </span><span class="s0">or </span><span class="s1">axis</span><span class="s2">.</span><span class="s1">size </span><span class="s2">!= </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;`axis` must be an integer or `None`&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">ps</span><span class="s2">.</span><span class="s1">size </span><span class="s2">&lt;= </span><span class="s5">1 </span><span class="s0">or </span><span class="s1">ps</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">] &lt;= </span><span class="s5">1</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">ps</span><span class="s2">[()]</span>

    <span class="s1">ps </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">moveaxis</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, -</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">m </span><span class="s2">= </span><span class="s1">ps</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span>

    <span class="s3"># Main Algorithm</span>
    <span class="s3"># Equivalent to the ideas of [1] and [2], except that this adjusts the</span>
    <span class="s3"># p-values as described in [3]. The results are similar to those produced</span>
    <span class="s3"># by R's p.adjust.</span>

    <span class="s3"># &quot;Let [ps] be the ordered observed p-values...&quot;</span>
    <span class="s1">order </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">argsort</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">ps </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">take_along_axis</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, </span><span class="s1">order</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)  </span><span class="s3"># this copies ps</span>

    <span class="s3"># Equation 1 of [1] rearranged to reject when p is less than specified q</span>
    <span class="s1">i </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">m</span><span class="s2">+</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">ps </span><span class="s2">*= </span><span class="s1">m </span><span class="s2">/ </span><span class="s1">i</span>

    <span class="s3"># Theorem 1.3 of [2]</span>
    <span class="s0">if </span><span class="s1">method </span><span class="s2">== </span><span class="s4">'by'</span><span class="s2">:</span>
        <span class="s1">ps </span><span class="s2">*= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s5">1 </span><span class="s2">/ </span><span class="s1">i</span><span class="s2">)</span>

    <span class="s3"># accounts for rejecting all null hypotheses i for i &lt; k, where k is</span>
    <span class="s3"># defined in Eq. 1 of either [1] or [2]. See [3]. Starting with the index j</span>
    <span class="s3"># of the second to last element, we replace element j with element j+1 if</span>
    <span class="s3"># the latter is smaller.</span>
    <span class="s1">np</span><span class="s2">.</span><span class="s1">minimum</span><span class="s2">.</span><span class="s1">accumulate</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">[..., ::-</span><span class="s5">1</span><span class="s2">], </span><span class="s1">out</span><span class="s2">=</span><span class="s1">ps</span><span class="s2">[..., ::-</span><span class="s5">1</span><span class="s2">], </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>

    <span class="s3"># Restore original order of axes and data</span>
    <span class="s1">np</span><span class="s2">.</span><span class="s1">put_along_axis</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, </span><span class="s1">order</span><span class="s2">, </span><span class="s1">values</span><span class="s2">=</span><span class="s1">ps</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">(), </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
    <span class="s1">ps </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">moveaxis</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, -</span><span class="s5">1</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">clip</span><span class="s2">(</span><span class="s1">ps</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">)</span>
</pre>
</body>
</html>