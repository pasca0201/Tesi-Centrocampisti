<html>
<head>
<title>linalg.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #7a7e85;}
.s5 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
linalg.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">annotations</span>

<span class="s0">from </span><span class="s1">typing </span><span class="s0">import </span><span class="s1">TYPE_CHECKING</span>
<span class="s0">if </span><span class="s1">TYPE_CHECKING</span><span class="s2">:</span>
    <span class="s0">import </span><span class="s1">torch</span>
    <span class="s1">array </span><span class="s2">= </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">Tensor</span>
    <span class="s0">from </span><span class="s1">torch </span><span class="s0">import </span><span class="s1">dtype </span><span class="s0">as </span><span class="s1">Dtype</span>
    <span class="s0">from </span><span class="s1">typing </span><span class="s0">import </span><span class="s1">Optional</span><span class="s2">, </span><span class="s1">Union</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">, </span><span class="s1">Literal</span>
    <span class="s1">inf </span><span class="s2">= </span><span class="s1">float</span><span class="s2">(</span><span class="s3">'inf'</span><span class="s2">)</span>

<span class="s0">from </span><span class="s2">.</span><span class="s1">_aliases </span><span class="s0">import </span><span class="s1">_fix_promotion</span><span class="s2">, </span><span class="s1">sum</span>

<span class="s0">from </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">linalg </span><span class="s0">import </span><span class="s2">* </span><span class="s4"># noqa: F403</span>

<span class="s4"># torch.linalg doesn't define __all__</span>
<span class="s4"># from torch.linalg import __all__ as linalg_all</span>
<span class="s0">from </span><span class="s1">torch </span><span class="s0">import </span><span class="s1">linalg </span><span class="s0">as </span><span class="s1">torch_linalg</span>
<span class="s1">linalg_all </span><span class="s2">= [</span><span class="s1">i </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">dir</span><span class="s2">(</span><span class="s1">torch_linalg</span><span class="s2">) </span><span class="s0">if not </span><span class="s1">i</span><span class="s2">.</span><span class="s1">startswith</span><span class="s2">(</span><span class="s3">'_'</span><span class="s2">)]</span>

<span class="s4"># outer is implemented in torch but aren't in the linalg namespace</span>
<span class="s0">from </span><span class="s1">torch </span><span class="s0">import </span><span class="s1">outer</span>
<span class="s4"># These functions are in both the main and linalg namespaces</span>
<span class="s0">from </span><span class="s2">.</span><span class="s1">_aliases </span><span class="s0">import </span><span class="s1">matmul</span><span class="s2">, </span><span class="s1">matrix_transpose</span><span class="s2">, </span><span class="s1">tensordot</span>

<span class="s4"># Note: torch.linalg.cross does not default to axis=-1 (it defaults to the</span>
<span class="s4"># first axis with size 3), see https://github.com/pytorch/pytorch/issues/58743</span>

<span class="s4"># torch.cross also does not support broadcasting when it would add new</span>
<span class="s4"># dimensions https://github.com/pytorch/pytorch/issues/39656</span>
<span class="s0">def </span><span class="s1">cross</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, /, *, </span><span class="s1">axis</span><span class="s2">: </span><span class="s1">int </span><span class="s2">= -</span><span class="s5">1</span><span class="s2">) </span><span class="s1">-&gt; array</span><span class="s2">:</span>
    <span class="s1">x1</span><span class="s2">, </span><span class="s1">x2 </span><span class="s2">= </span><span class="s1">_fix_promotion</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">only_scalar</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>
    <span class="s0">if not </span><span class="s2">(-</span><span class="s1">min</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">ndim</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">.</span><span class="s1">ndim</span><span class="s2">) &lt;= </span><span class="s1">axis </span><span class="s2">&lt; </span><span class="s1">max</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">ndim</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">.</span><span class="s1">ndim</span><span class="s2">)):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s3">f&quot;axis </span><span class="s0">{</span><span class="s1">axis</span><span class="s0">} </span><span class="s3">out of bounds for cross product of arrays with shapes </span><span class="s0">{</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">} </span><span class="s3">and </span><span class="s0">{</span><span class="s1">x2</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s3">&quot;</span><span class="s2">)</span>
    <span class="s0">if not </span><span class="s2">(</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">] == </span><span class="s1">x2</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">] == </span><span class="s5">3</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s3">f&quot;cross product axis must have size 3, got </span><span class="s0">{</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span><span class="s0">} </span><span class="s3">and </span><span class="s0">{</span><span class="s1">x2</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]</span><span class="s0">}</span><span class="s3">&quot;</span><span class="s2">)</span>
    <span class="s1">x1</span><span class="s2">, </span><span class="s1">x2 </span><span class="s2">= </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">broadcast_tensors</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">torch_linalg</span><span class="s2">.</span><span class="s1">cross</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>

<span class="s0">def </span><span class="s1">vecdot</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, /, *, </span><span class="s1">axis</span><span class="s2">: </span><span class="s1">int </span><span class="s2">= -</span><span class="s5">1</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">) </span><span class="s1">-&gt; array</span><span class="s2">:</span>
    <span class="s0">from </span><span class="s2">.</span><span class="s1">_aliases </span><span class="s0">import </span><span class="s1">isdtype</span>

    <span class="s1">x1</span><span class="s2">, </span><span class="s1">x2 </span><span class="s2">= </span><span class="s1">_fix_promotion</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">only_scalar</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>

    <span class="s4"># torch.linalg.vecdot incorrectly allows broadcasting along the contracted dimension</span>
    <span class="s0">if </span><span class="s1">x1</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">] != </span><span class="s1">x2</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">]:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s3">&quot;x1 and x2 must have the same size along the given axis&quot;</span><span class="s2">)</span>

    <span class="s4"># torch.linalg.vecdot doesn't support integer dtypes</span>
    <span class="s0">if </span><span class="s1">isdtype</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s3">'integral'</span><span class="s2">) </span><span class="s0">or </span><span class="s1">isdtype</span><span class="s2">(</span><span class="s1">x2</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s3">'integral'</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">kwargs</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">RuntimeError</span><span class="s2">(</span><span class="s3">&quot;vecdot kwargs not supported for integral dtypes&quot;</span><span class="s2">)</span>

        <span class="s1">x1_ </span><span class="s2">= </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">moveaxis</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, -</span><span class="s5">1</span><span class="s2">)</span>
        <span class="s1">x2_ </span><span class="s2">= </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">moveaxis</span><span class="s2">(</span><span class="s1">x2</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, -</span><span class="s5">1</span><span class="s2">)</span>
        <span class="s1">x1_</span><span class="s2">, </span><span class="s1">x2_ </span><span class="s2">= </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">broadcast_tensors</span><span class="s2">(</span><span class="s1">x1_</span><span class="s2">, </span><span class="s1">x2_</span><span class="s2">)</span>

        <span class="s1">res </span><span class="s2">= </span><span class="s1">x1_</span><span class="s2">[..., </span><span class="s0">None</span><span class="s2">, :] @ </span><span class="s1">x2_</span><span class="s2">[..., </span><span class="s0">None</span><span class="s2">]</span>
        <span class="s0">return </span><span class="s1">res</span><span class="s2">[..., </span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">]</span>
    <span class="s0">return </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">linalg</span><span class="s2">.</span><span class="s1">vecdot</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">)</span>

<span class="s0">def </span><span class="s1">solve</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, /, **</span><span class="s1">kwargs</span><span class="s2">) </span><span class="s1">-&gt; array</span><span class="s2">:</span>
    <span class="s1">x1</span><span class="s2">, </span><span class="s1">x2 </span><span class="s2">= </span><span class="s1">_fix_promotion</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">only_scalar</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">linalg</span><span class="s2">.</span><span class="s1">solve</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">)</span>

<span class="s4"># torch.trace doesn't support the offset argument and doesn't support stacking</span>
<span class="s0">def </span><span class="s1">trace</span><span class="s2">(</span><span class="s1">x</span><span class="s2">: </span><span class="s1">array</span><span class="s2">, /, *, </span><span class="s1">offset</span><span class="s2">: </span><span class="s1">int </span><span class="s2">= </span><span class="s5">0</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">Optional</span><span class="s2">[</span><span class="s1">Dtype</span><span class="s2">] = </span><span class="s0">None</span><span class="s2">) </span><span class="s1">-&gt; array</span><span class="s2">:</span>
    <span class="s4"># Use our wrapped sum to make sure it does upcasting correctly</span>
    <span class="s0">return </span><span class="s1">sum</span><span class="s2">(</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">diagonal</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">offset</span><span class="s2">=</span><span class="s1">offset</span><span class="s2">, </span><span class="s1">dim1</span><span class="s2">=-</span><span class="s5">2</span><span class="s2">, </span><span class="s1">dim2</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>

<span class="s0">def </span><span class="s1">vector_norm</span><span class="s2">(</span>
    <span class="s1">x</span><span class="s2">: </span><span class="s1">array</span><span class="s2">,</span>
    <span class="s2">/,</span>
    <span class="s2">*,</span>
    <span class="s1">axis</span><span class="s2">: </span><span class="s1">Optional</span><span class="s2">[</span><span class="s1">Union</span><span class="s2">[</span><span class="s1">int</span><span class="s2">, </span><span class="s1">Tuple</span><span class="s2">[</span><span class="s1">int</span><span class="s2">, ...]]] = </span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">keepdims</span><span class="s2">: </span><span class="s1">bool </span><span class="s2">= </span><span class="s0">False</span><span class="s2">,</span>
    <span class="s1">ord</span><span class="s2">: </span><span class="s1">Union</span><span class="s2">[</span><span class="s1">int</span><span class="s2">, </span><span class="s1">float</span><span class="s2">, </span><span class="s1">Literal</span><span class="s2">[</span><span class="s1">inf</span><span class="s2">, -</span><span class="s1">inf</span><span class="s2">]] = </span><span class="s5">2</span><span class="s2">,</span>
    <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
<span class="s2">) </span><span class="s1">-&gt; array</span><span class="s2">:</span>
    <span class="s4"># torch.vector_norm incorrectly treats axis=() the same as axis=None</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s2">== ():</span>
        <span class="s1">keepdims </span><span class="s2">= </span><span class="s0">True</span>
    <span class="s0">return </span><span class="s1">torch</span><span class="s2">.</span><span class="s1">linalg</span><span class="s2">.</span><span class="s1">vector_norm</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">ord</span><span class="s2">=</span><span class="s1">ord</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">keepdim</span><span class="s2">=</span><span class="s1">keepdims</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">)</span>

<span class="s1">__all__ </span><span class="s2">= </span><span class="s1">linalg_all </span><span class="s2">+ [</span><span class="s3">'outer'</span><span class="s2">, </span><span class="s3">'matmul'</span><span class="s2">, </span><span class="s3">'matrix_transpose'</span><span class="s2">, </span><span class="s3">'tensordot'</span><span class="s2">,</span>
                        <span class="s3">'cross'</span><span class="s2">, </span><span class="s3">'vecdot'</span><span class="s2">, </span><span class="s3">'solve'</span><span class="s2">, </span><span class="s3">'trace'</span><span class="s2">, </span><span class="s3">'vector_norm'</span><span class="s2">]</span>

<span class="s1">_all_ignore </span><span class="s2">= [</span><span class="s3">'torch_linalg'</span><span class="s2">, </span><span class="s3">'sum'</span><span class="s2">]</span>

<span class="s0">del </span><span class="s1">linalg_all</span>
</pre>
</body>
</html>