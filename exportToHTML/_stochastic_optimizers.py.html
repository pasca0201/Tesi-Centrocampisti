<html>
<head>
<title>_stochastic_optimizers.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_stochastic_optimizers.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Stochastic optimization methods for MLP&quot;&quot;&quot;</span>

<span class="s2"># Authors: Jiyuan Qian &lt;jq401@nyu.edu&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>


<span class="s3">class </span><span class="s1">BaseOptimizer</span><span class="s4">:</span>
    <span class="s0">&quot;&quot;&quot;Base (Stochastic) gradient descent optimizer 
 
    Parameters 
    ---------- 
    learning_rate_init : float, default=0.1 
        The initial learning rate used. It controls the step-size in updating 
        the weights 
 
    Attributes 
    ---------- 
    learning_rate : float 
        the current learning rate 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">learning_rate_init</span><span class="s4">=</span><span class="s5">0.1</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate_init </span><span class="s4">= </span><span class="s1">learning_rate_init</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">= </span><span class="s1">float</span><span class="s4">(</span><span class="s1">learning_rate_init</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">update_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">params</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Update parameters with given gradients 
 
        Parameters 
        ---------- 
        params : list of length = len(coefs_) + len(intercepts_) 
            The concatenated list containing coefs_ and intercepts_ in MLP 
            model. Used for initializing velocities and updating params 
 
        grads : list of length = len(params) 
            Containing gradients with respect to coefs_ and intercepts_ in MLP 
            model. So length should be aligned with params 
        &quot;&quot;&quot;</span>
        <span class="s1">updates </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_get_updates</span><span class="s4">(</span><span class="s1">grads</span><span class="s4">)</span>
        <span class="s3">for </span><span class="s1">param</span><span class="s4">, </span><span class="s1">update </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">((</span><span class="s1">p </span><span class="s3">for </span><span class="s1">p </span><span class="s3">in </span><span class="s1">params</span><span class="s4">), </span><span class="s1">updates</span><span class="s4">):</span>
            <span class="s1">param </span><span class="s4">+= </span><span class="s1">update</span>

    <span class="s3">def </span><span class="s1">iteration_ends</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">time_step</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Perform update to learning rate and potentially other states at the 
        end of an iteration 
        &quot;&quot;&quot;</span>
        <span class="s3">pass</span>

    <span class="s3">def </span><span class="s1">trigger_stopping</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">msg</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Decides whether it is time to stop training 
 
        Parameters 
        ---------- 
        msg : str 
            Message passed in for verbose output 
 
        verbose : bool 
            Print message to stdin if True 
 
        Returns 
        ------- 
        is_stopping : bool 
            True if training needs to stop 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s1">msg </span><span class="s4">+ </span><span class="s6">&quot; Stopping.&quot;</span><span class="s4">)</span>
        <span class="s3">return True</span>


<span class="s3">class </span><span class="s1">SGDOptimizer</span><span class="s4">(</span><span class="s1">BaseOptimizer</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Stochastic gradient descent optimizer with momentum 
 
    Parameters 
    ---------- 
    params : list, length = len(coefs_) + len(intercepts_) 
        The concatenated list containing coefs_ and intercepts_ in MLP model. 
        Used for initializing velocities and updating params 
 
    learning_rate_init : float, default=0.1 
        The initial learning rate used. It controls the step-size in updating 
        the weights 
 
    lr_schedule : {'constant', 'adaptive', 'invscaling'}, default='constant' 
        Learning rate schedule for weight updates. 
 
        -'constant', is a constant learning rate given by 
         'learning_rate_init'. 
 
        -'invscaling' gradually decreases the learning rate 'learning_rate_' at 
          each time step 't' using an inverse scaling exponent of 'power_t'. 
          learning_rate_ = learning_rate_init / pow(t, power_t) 
 
        -'adaptive', keeps the learning rate constant to 
         'learning_rate_init' as long as the training keeps decreasing. 
         Each time 2 consecutive epochs fail to decrease the training loss by 
         tol, or fail to increase validation score by tol if 'early_stopping' 
         is on, the current learning rate is divided by 5. 
 
    momentum : float, default=0.9 
        Value of momentum used, must be larger than or equal to 0 
 
    nesterov : bool, default=True 
        Whether to use nesterov's momentum or not. Use nesterov's if True 
 
    power_t : float, default=0.5 
        Power of time step 't' in inverse scaling. See `lr_schedule` for 
        more details. 
 
    Attributes 
    ---------- 
    learning_rate : float 
        the current learning rate 
 
    velocities : list, length = len(params) 
        velocities that are used to update params 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">params</span><span class="s4">,</span>
        <span class="s1">learning_rate_init</span><span class="s4">=</span><span class="s5">0.1</span><span class="s4">,</span>
        <span class="s1">lr_schedule</span><span class="s4">=</span><span class="s6">&quot;constant&quot;</span><span class="s4">,</span>
        <span class="s1">momentum</span><span class="s4">=</span><span class="s5">0.9</span><span class="s4">,</span>
        <span class="s1">nesterov</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">power_t</span><span class="s4">=</span><span class="s5">0.5</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">learning_rate_init</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">lr_schedule </span><span class="s4">= </span><span class="s1">lr_schedule</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">momentum </span><span class="s4">= </span><span class="s1">momentum</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">nesterov </span><span class="s4">= </span><span class="s1">nesterov</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">power_t </span><span class="s4">= </span><span class="s1">power_t</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">velocities </span><span class="s4">= [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">param</span><span class="s4">) </span><span class="s3">for </span><span class="s1">param </span><span class="s3">in </span><span class="s1">params</span><span class="s4">]</span>

    <span class="s3">def </span><span class="s1">iteration_ends</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">time_step</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Perform updates to learning rate and potential other states at the 
        end of an iteration 
 
        Parameters 
        ---------- 
        time_step : int 
            number of training samples trained on so far, used to update 
            learning rate for 'invscaling' 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">lr_schedule </span><span class="s4">== </span><span class="s6">&quot;invscaling&quot;</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">= (</span>
                <span class="s1">float</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate_init</span><span class="s4">) / (</span><span class="s1">time_step </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">) ** </span><span class="s1">self</span><span class="s4">.</span><span class="s1">power_t</span>
            <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">trigger_stopping</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">msg</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">lr_schedule </span><span class="s4">!= </span><span class="s6">&quot;adaptive&quot;</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s1">msg </span><span class="s4">+ </span><span class="s6">&quot; Stopping.&quot;</span><span class="s4">)</span>
            <span class="s3">return True</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">&lt;= </span><span class="s5">1e-6</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s1">msg </span><span class="s4">+ </span><span class="s6">&quot; Learning rate too small. Stopping.&quot;</span><span class="s4">)</span>
            <span class="s3">return True</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">/= </span><span class="s5">5.0</span>
        <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s1">msg </span><span class="s4">+ </span><span class="s6">&quot; Setting learning rate to %f&quot; </span><span class="s4">% </span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate</span><span class="s4">)</span>
        <span class="s3">return False</span>

    <span class="s3">def </span><span class="s1">_get_updates</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get the values used to update params with given gradients 
 
        Parameters 
        ---------- 
        grads : list, length = len(coefs_) + len(intercepts_) 
            Containing gradients with respect to coefs_ and intercepts_ in MLP 
            model. So length should be aligned with params 
 
        Returns 
        ------- 
        updates : list, length = len(grads) 
            The values to add to params 
        &quot;&quot;&quot;</span>
        <span class="s1">updates </span><span class="s4">= [</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">momentum </span><span class="s4">* </span><span class="s1">velocity </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">* </span><span class="s1">grad</span>
            <span class="s3">for </span><span class="s1">velocity</span><span class="s4">, </span><span class="s1">grad </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">velocities</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">)</span>
        <span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">velocities </span><span class="s4">= </span><span class="s1">updates</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nesterov</span><span class="s4">:</span>
            <span class="s1">updates </span><span class="s4">= [</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">momentum </span><span class="s4">* </span><span class="s1">velocity </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">* </span><span class="s1">grad</span>
                <span class="s3">for </span><span class="s1">velocity</span><span class="s4">, </span><span class="s1">grad </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">velocities</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">)</span>
            <span class="s4">]</span>

        <span class="s3">return </span><span class="s1">updates</span>


<span class="s3">class </span><span class="s1">AdamOptimizer</span><span class="s4">(</span><span class="s1">BaseOptimizer</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Stochastic gradient descent optimizer with Adam 
 
    Note: All default values are from the original Adam paper 
 
    Parameters 
    ---------- 
    params : list, length = len(coefs_) + len(intercepts_) 
        The concatenated list containing coefs_ and intercepts_ in MLP model. 
        Used for initializing velocities and updating params 
 
    learning_rate_init : float, default=0.001 
        The initial learning rate used. It controls the step-size in updating 
        the weights 
 
    beta_1 : float, default=0.9 
        Exponential decay rate for estimates of first moment vector, should be 
        in [0, 1) 
 
    beta_2 : float, default=0.999 
        Exponential decay rate for estimates of second moment vector, should be 
        in [0, 1) 
 
    epsilon : float, default=1e-8 
        Value for numerical stability 
 
    Attributes 
    ---------- 
    learning_rate : float 
        The current learning rate 
 
    t : int 
        Timestep 
 
    ms : list, length = len(params) 
        First moment vectors 
 
    vs : list, length = len(params) 
        Second moment vectors 
 
    References 
    ---------- 
    :arxiv:`Kingma, Diederik, and Jimmy Ba (2014) &quot;Adam: A method for 
        stochastic optimization.&quot; &lt;1412.6980&gt; 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">, </span><span class="s1">params</span><span class="s4">, </span><span class="s1">learning_rate_init</span><span class="s4">=</span><span class="s5">0.001</span><span class="s4">, </span><span class="s1">beta_1</span><span class="s4">=</span><span class="s5">0.9</span><span class="s4">, </span><span class="s1">beta_2</span><span class="s4">=</span><span class="s5">0.999</span><span class="s4">, </span><span class="s1">epsilon</span><span class="s4">=</span><span class="s5">1e-8</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">learning_rate_init</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">beta_1 </span><span class="s4">= </span><span class="s1">beta_1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">beta_2 </span><span class="s4">= </span><span class="s1">beta_2</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">epsilon </span><span class="s4">= </span><span class="s1">epsilon</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">t </span><span class="s4">= </span><span class="s5">0</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ms </span><span class="s4">= [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">param</span><span class="s4">) </span><span class="s3">for </span><span class="s1">param </span><span class="s3">in </span><span class="s1">params</span><span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">vs </span><span class="s4">= [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">param</span><span class="s4">) </span><span class="s3">for </span><span class="s1">param </span><span class="s3">in </span><span class="s1">params</span><span class="s4">]</span>

    <span class="s3">def </span><span class="s1">_get_updates</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get the values used to update params with given gradients 
 
        Parameters 
        ---------- 
        grads : list, length = len(coefs_) + len(intercepts_) 
            Containing gradients with respect to coefs_ and intercepts_ in MLP 
            model. So length should be aligned with params 
 
        Returns 
        ------- 
        updates : list, length = len(grads) 
            The values to add to params 
        &quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">t </span><span class="s4">+= </span><span class="s5">1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ms </span><span class="s4">= [</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">beta_1 </span><span class="s4">* </span><span class="s1">m </span><span class="s4">+ (</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_1</span><span class="s4">) * </span><span class="s1">grad</span>
            <span class="s3">for </span><span class="s1">m</span><span class="s4">, </span><span class="s1">grad </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">ms</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">)</span>
        <span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">vs </span><span class="s4">= [</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">beta_2 </span><span class="s4">* </span><span class="s1">v </span><span class="s4">+ (</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_2</span><span class="s4">) * (</span><span class="s1">grad</span><span class="s4">**</span><span class="s5">2</span><span class="s4">)</span>
            <span class="s3">for </span><span class="s1">v</span><span class="s4">, </span><span class="s1">grad </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">vs</span><span class="s4">, </span><span class="s1">grads</span><span class="s4">)</span>
        <span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">= (</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate_init</span>
            <span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_2</span><span class="s4">**</span><span class="s1">self</span><span class="s4">.</span><span class="s1">t</span><span class="s4">)</span>
            <span class="s4">/ (</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_1</span><span class="s4">**</span><span class="s1">self</span><span class="s4">.</span><span class="s1">t</span><span class="s4">)</span>
        <span class="s4">)</span>
        <span class="s1">updates </span><span class="s4">= [</span>
            <span class="s4">-</span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_rate </span><span class="s4">* </span><span class="s1">m </span><span class="s4">/ (</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">v</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">epsilon</span><span class="s4">)</span>
            <span class="s3">for </span><span class="s1">m</span><span class="s4">, </span><span class="s1">v </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">ms</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">vs</span><span class="s4">)</span>
        <span class="s4">]</span>
        <span class="s3">return </span><span class="s1">updates</span>
</pre>
</body>
</html>