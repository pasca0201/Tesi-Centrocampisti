<html>
<head>
<title>_nmf.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_nmf.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Non-negative matrix factorization.&quot;&quot;&quot;</span>

<span class="s2"># Author: Vlad Niculae</span>
<span class="s2">#         Lars Buitinck</span>
<span class="s2">#         Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#         Tom Dupre la Tour</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">import </span><span class="s1">time</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABC</span>
<span class="s3">from </span><span class="s1">math </span><span class="s3">import </span><span class="s1">sqrt</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">sparse </span><span class="s3">as </span><span class="s1">sp</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">_config </span><span class="s3">import </span><span class="s1">config_context</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">base </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">BaseEstimator</span><span class="s4">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s4">,</span>
    <span class="s1">TransformerMixin</span><span class="s4">,</span>
    <span class="s1">_fit_context</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils </span><span class="s3">import </span><span class="s1">check_array</span><span class="s4">, </span><span class="s1">check_random_state</span><span class="s4">, </span><span class="s1">gen_batches</span><span class="s4">, </span><span class="s1">metadata_routing</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">Hidden</span><span class="s4">,</span>
    <span class="s1">Interval</span><span class="s4">,</span>
    <span class="s1">StrOptions</span><span class="s4">,</span>
    <span class="s1">validate_params</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">deprecation </span><span class="s3">import </span><span class="s1">_deprecate_Xt_in_inverse_transform</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">extmath </span><span class="s3">import </span><span class="s1">randomized_svd</span><span class="s4">, </span><span class="s1">safe_sparse_dot</span><span class="s4">, </span><span class="s1">squared_norm</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">check_is_fitted</span><span class="s4">,</span>
    <span class="s1">check_non_negative</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_cdnmf_fast </span><span class="s3">import </span><span class="s1">_update_cdnmf_fast</span>

<span class="s1">EPSILON </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">).</span><span class="s1">eps</span>


<span class="s3">def </span><span class="s1">norm</span><span class="s4">(</span><span class="s1">x</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Dot product-based Euclidean norm implementation. 
 
    See: http://fa.bianp.net/blog/2011/computing-the-vector-norm/ 
 
    Parameters 
    ---------- 
    x : array-like 
        Vector for which to compute the norm. 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">squared_norm</span><span class="s4">(</span><span class="s1">x</span><span class="s4">))</span>


<span class="s3">def </span><span class="s1">trace_dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Trace of np.dot(X, Y.T). 
 
    Parameters 
    ---------- 
    X : array-like 
        First matrix. 
    Y : array-like 
        Second matrix. 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">(), </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">())</span>


<span class="s3">def </span><span class="s1">_check_init</span><span class="s4">(</span><span class="s1">A</span><span class="s4">, </span><span class="s1">shape</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">):</span>
    <span class="s1">A </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">A</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] != </span><span class="s6">&quot;auto&quot; </span><span class="s3">and </span><span class="s1">A</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] != </span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s6">f&quot;Array with wrong first dimension passed to </span><span class="s3">{</span><span class="s1">whom</span><span class="s3">}</span><span class="s6">. Expected </span><span class="s3">{</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span><span class="s3">}</span><span class="s6">, &quot;</span>
            <span class="s6">f&quot;but got </span><span class="s3">{</span><span class="s1">A</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span><span class="s3">}</span><span class="s6">.&quot;</span>
        <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] != </span><span class="s6">&quot;auto&quot; </span><span class="s3">and </span><span class="s1">A</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] != </span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s6">f&quot;Array with wrong second dimension passed to </span><span class="s3">{</span><span class="s1">whom</span><span class="s3">}</span><span class="s6">. Expected </span><span class="s3">{</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span><span class="s3">}</span><span class="s6">, &quot;</span>
            <span class="s6">f&quot;but got </span><span class="s3">{</span><span class="s1">A</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span><span class="s3">}</span><span class="s6">.&quot;</span>
        <span class="s4">)</span>
    <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">A</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">A</span><span class="s4">) == </span><span class="s5">0</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">f&quot;Array passed to </span><span class="s3">{</span><span class="s1">whom</span><span class="s3">} </span><span class="s6">is full of zeros.&quot;</span><span class="s4">)</span>


<span class="s3">def </span><span class="s1">_beta_divergence</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">beta</span><span class="s4">, </span><span class="s1">square_root</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the beta-divergence of X and dot(W, H). 
 
    Parameters 
    ---------- 
    X : float or array-like of shape (n_samples, n_features) 
 
    W : float or array-like of shape (n_samples, n_components) 
 
    H : float or array-like of shape (n_components, n_features) 
 
    beta : float or {'frobenius', 'kullback-leibler', 'itakura-saito'} 
        Parameter of the beta-divergence. 
        If beta == 2, this is half the Frobenius *squared* norm. 
        If beta == 1, this is the generalized Kullback-Leibler divergence. 
        If beta == 0, this is the Itakura-Saito divergence. 
        Else, this is the general beta-divergence. 
 
    square_root : bool, default=False 
        If True, return np.sqrt(2 * res) 
        For beta == 2, it corresponds to the Frobenius norm. 
 
    Returns 
    ------- 
        res : float 
            Beta divergence of X and np.dot(X, H). 
    &quot;&quot;&quot;</span>
    <span class="s1">beta </span><span class="s4">= </span><span class="s1">_beta_loss_to_float</span><span class="s4">(</span><span class="s1">beta</span><span class="s4">)</span>

    <span class="s2"># The method can be called with scalars</span>
    <span class="s3">if not </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
    <span class="s1">W </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">W</span><span class="s4">)</span>
    <span class="s1">H </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">H</span><span class="s4">)</span>

    <span class="s2"># Frobenius norm</span>
    <span class="s3">if </span><span class="s1">beta </span><span class="s4">== </span><span class="s5">2</span><span class="s4">:</span>
        <span class="s2"># Avoid the creation of the dense np.dot(W, H) if X is sparse.</span>
        <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
            <span class="s1">norm_X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span><span class="s4">, </span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span><span class="s4">)</span>
            <span class="s1">norm_WH </span><span class="s4">= </span><span class="s1">trace_dot</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">multi_dot</span><span class="s4">([</span><span class="s1">W</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">]), </span><span class="s1">H</span><span class="s4">)</span>
            <span class="s1">cross_prod </span><span class="s4">= </span><span class="s1">trace_dot</span><span class="s4">((</span><span class="s1">X </span><span class="s4">@ </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">), </span><span class="s1">W</span><span class="s4">)</span>
            <span class="s1">res </span><span class="s4">= (</span><span class="s1">norm_X </span><span class="s4">+ </span><span class="s1">norm_WH </span><span class="s4">- </span><span class="s5">2.0 </span><span class="s4">* </span><span class="s1">cross_prod</span><span class="s4">) / </span><span class="s5">2.0</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">res </span><span class="s4">= </span><span class="s1">squared_norm</span><span class="s4">(</span><span class="s1">X </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">)) / </span><span class="s5">2.0</span>

        <span class="s3">if </span><span class="s1">square_root</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">res </span><span class="s4">* </span><span class="s5">2</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">res</span>

    <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
        <span class="s2"># compute np.dot(W, H) only where X is nonzero</span>
        <span class="s1">WH_data </span><span class="s4">= </span><span class="s1">_special_sparse_dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">X</span><span class="s4">).</span><span class="s1">data</span>
        <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">WH </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">)</span>
        <span class="s1">WH_data </span><span class="s4">= </span><span class="s1">WH</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">()</span>
        <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">()</span>

    <span class="s2"># do not affect the zeros: here 0 ** (-1) = 0 and not infinity</span>
    <span class="s1">indices </span><span class="s4">= </span><span class="s1">X_data </span><span class="s4">&gt; </span><span class="s1">EPSILON</span>
    <span class="s1">WH_data </span><span class="s4">= </span><span class="s1">WH_data</span><span class="s4">[</span><span class="s1">indices</span><span class="s4">]</span>
    <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X_data</span><span class="s4">[</span><span class="s1">indices</span><span class="s4">]</span>

    <span class="s2"># used to avoid division by zero</span>
    <span class="s1">WH_data</span><span class="s4">[</span><span class="s1">WH_data </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>

    <span class="s2"># generalized Kullback-Leibler divergence</span>
    <span class="s3">if </span><span class="s1">beta </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
        <span class="s2"># fast and memory efficient computation of np.sum(np.dot(W, H))</span>
        <span class="s1">sum_WH </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">H</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">1</span><span class="s4">))</span>
        <span class="s2"># computes np.sum(X * log(X / WH)) only where X is nonzero</span>
        <span class="s1">div </span><span class="s4">= </span><span class="s1">X_data </span><span class="s4">/ </span><span class="s1">WH_data</span>
        <span class="s1">res </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">X_data</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">div</span><span class="s4">))</span>
        <span class="s2"># add full np.sum(np.dot(W, H)) - np.sum(X)</span>
        <span class="s1">res </span><span class="s4">+= </span><span class="s1">sum_WH </span><span class="s4">- </span><span class="s1">X_data</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>

    <span class="s2"># Itakura-Saito divergence</span>
    <span class="s3">elif </span><span class="s1">beta </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
        <span class="s1">div </span><span class="s4">= </span><span class="s1">X_data </span><span class="s4">/ </span><span class="s1">WH_data</span>
        <span class="s1">res </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">div</span><span class="s4">) - </span><span class="s1">np</span><span class="s4">.</span><span class="s1">prod</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">) - </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">div</span><span class="s4">))</span>

    <span class="s2"># beta-divergence, beta not in (0, 1, 2)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
            <span class="s2"># slow loop, but memory efficient computation of :</span>
            <span class="s2"># np.sum(np.dot(W, H) ** beta)</span>
            <span class="s1">sum_WH_beta </span><span class="s4">= </span><span class="s5">0</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]):</span>
                <span class="s1">sum_WH_beta </span><span class="s4">+= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">[:, </span><span class="s1">i</span><span class="s4">]) ** </span><span class="s1">beta</span><span class="s4">)</span>

        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">sum_WH_beta </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">WH</span><span class="s4">**</span><span class="s1">beta</span><span class="s4">)</span>

        <span class="s1">sum_X_WH </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">X_data</span><span class="s4">, </span><span class="s1">WH_data </span><span class="s4">** (</span><span class="s1">beta </span><span class="s4">- </span><span class="s5">1</span><span class="s4">))</span>
        <span class="s1">res </span><span class="s4">= (</span><span class="s1">X_data</span><span class="s4">**</span><span class="s1">beta</span><span class="s4">).</span><span class="s1">sum</span><span class="s4">() - </span><span class="s1">beta </span><span class="s4">* </span><span class="s1">sum_X_WH</span>
        <span class="s1">res </span><span class="s4">+= </span><span class="s1">sum_WH_beta </span><span class="s4">* (</span><span class="s1">beta </span><span class="s4">- </span><span class="s5">1</span><span class="s4">)</span>
        <span class="s1">res </span><span class="s4">/= </span><span class="s1">beta </span><span class="s4">* (</span><span class="s1">beta </span><span class="s4">- </span><span class="s5">1</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">square_root</span><span class="s4">:</span>
        <span class="s1">res </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s1">res</span><span class="s4">, </span><span class="s5">0</span><span class="s4">)  </span><span class="s2"># avoid negative number due to rounding errors</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">res</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">res</span>


<span class="s3">def </span><span class="s1">_special_sparse_dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Computes np.dot(W, H), only where X is non zero.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
        <span class="s1">ii</span><span class="s4">, </span><span class="s1">jj </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">nonzero</span><span class="s4">()</span>
        <span class="s1">n_vals </span><span class="s4">= </span><span class="s1">ii</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
        <span class="s1">dot_vals </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">(</span><span class="s1">n_vals</span><span class="s4">)</span>
        <span class="s1">n_components </span><span class="s4">= </span><span class="s1">W</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span>

        <span class="s1">batch_size </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s1">n_components</span><span class="s4">, </span><span class="s1">n_vals </span><span class="s4">// </span><span class="s1">n_components</span><span class="s4">)</span>
        <span class="s3">for </span><span class="s1">start </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">n_vals</span><span class="s4">, </span><span class="s1">batch_size</span><span class="s4">):</span>
            <span class="s1">batch </span><span class="s4">= </span><span class="s1">slice</span><span class="s4">(</span><span class="s1">start</span><span class="s4">, </span><span class="s1">start </span><span class="s4">+ </span><span class="s1">batch_size</span><span class="s4">)</span>
            <span class="s1">dot_vals</span><span class="s4">[</span><span class="s1">batch</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">multiply</span><span class="s4">(</span><span class="s1">W</span><span class="s4">[</span><span class="s1">ii</span><span class="s4">[</span><span class="s1">batch</span><span class="s4">], :], </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">[</span><span class="s1">jj</span><span class="s4">[</span><span class="s1">batch</span><span class="s4">], :]).</span><span class="s1">sum</span><span class="s4">(</span>
                <span class="s1">axis</span><span class="s4">=</span><span class="s5">1</span>
            <span class="s4">)</span>

        <span class="s1">WH </span><span class="s4">= </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">coo_matrix</span><span class="s4">((</span><span class="s1">dot_vals</span><span class="s4">, (</span><span class="s1">ii</span><span class="s4">, </span><span class="s1">jj</span><span class="s4">)), </span><span class="s1">shape</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">WH</span><span class="s4">.</span><span class="s1">tocsr</span><span class="s4">()</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">)</span>


<span class="s3">def </span><span class="s1">_beta_loss_to_float</span><span class="s4">(</span><span class="s1">beta_loss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Convert string beta_loss to float.&quot;&quot;&quot;</span>
    <span class="s1">beta_loss_map </span><span class="s4">= {</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">: </span><span class="s5">2</span><span class="s4">, </span><span class="s6">&quot;kullback-leibler&quot;</span><span class="s4">: </span><span class="s5">1</span><span class="s4">, </span><span class="s6">&quot;itakura-saito&quot;</span><span class="s4">: </span><span class="s5">0</span><span class="s4">}</span>
    <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">beta_loss</span><span class="s4">, </span><span class="s1">str</span><span class="s4">):</span>
        <span class="s1">beta_loss </span><span class="s4">= </span><span class="s1">beta_loss_map</span><span class="s4">[</span><span class="s1">beta_loss</span><span class="s4">]</span>
    <span class="s3">return </span><span class="s1">beta_loss</span>


<span class="s3">def </span><span class="s1">_initialize_nmf</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">n_components</span><span class="s4">, </span><span class="s1">init</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eps</span><span class="s4">=</span><span class="s5">1e-6</span><span class="s4">, </span><span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Algorithms for NMF initialization. 
 
    Computes an initial guess for the non-negative 
    rank k matrix approximation for X: X = WH. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        The data matrix to be decomposed. 
 
    n_components : int 
        The number of components desired in the approximation. 
 
    init :  {'random', 'nndsvd', 'nndsvda', 'nndsvdar'}, default=None 
        Method used to initialize the procedure. 
        Valid options: 
 
        - None: 'nndsvda' if n_components &lt;= min(n_samples, n_features), 
            otherwise 'random'. 
 
        - 'random': non-negative random matrices, scaled with: 
            sqrt(X.mean() / n_components) 
 
        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD) 
            initialization (better for sparseness) 
 
        - 'nndsvda': NNDSVD with zeros filled with the average of X 
            (better when sparsity is not desired) 
 
        - 'nndsvdar': NNDSVD with zeros filled with small random values 
            (generally faster, less accurate alternative to NNDSVDa 
            for when sparsity is not desired) 
 
        - 'custom': use custom matrices W and H 
 
        .. versionchanged:: 1.1 
            When `init=None` and n_components is less than n_samples and n_features 
            defaults to `nndsvda` instead of `nndsvd`. 
 
    eps : float, default=1e-6 
        Truncate all values less then this in output to zero. 
 
    random_state : int, RandomState instance or None, default=None 
        Used when ``init`` == 'nndsvdar' or 'random'. Pass an int for 
        reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Returns 
    ------- 
    W : array-like of shape (n_samples, n_components) 
        Initial guesses for solving X ~= WH. 
 
    H : array-like of shape (n_components, n_features) 
        Initial guesses for solving X ~= WH. 
 
    References 
    ---------- 
    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for 
    nonnegative matrix factorization - Pattern Recognition, 2008 
    http://tinyurl.com/nndsvd 
    &quot;&quot;&quot;</span>
    <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s6">&quot;NMF initialization&quot;</span><span class="s4">)</span>
    <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>

    <span class="s3">if </span><span class="s4">(</span>
        <span class="s1">init </span><span class="s3">is not None</span>
        <span class="s3">and </span><span class="s1">init </span><span class="s4">!= </span><span class="s6">&quot;random&quot;</span>
        <span class="s3">and </span><span class="s1">n_components </span><span class="s4">&gt; </span><span class="s1">min</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">)</span>
    <span class="s4">):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s6">&quot;init = '{}' can only be used when &quot;</span>
            <span class="s6">&quot;n_components &lt;= min(n_samples, n_features)&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">init</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">init </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">n_components </span><span class="s4">&lt;= </span><span class="s1">min</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">):</span>
            <span class="s1">init </span><span class="s4">= </span><span class="s6">&quot;nndsvda&quot;</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">init </span><span class="s4">= </span><span class="s6">&quot;random&quot;</span>

    <span class="s2"># Random initialization</span>
    <span class="s3">if </span><span class="s1">init </span><span class="s4">== </span><span class="s6">&quot;random&quot;</span><span class="s4">:</span>
        <span class="s1">avg </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">() / </span><span class="s1">n_components</span><span class="s4">)</span>
        <span class="s1">rng </span><span class="s4">= </span><span class="s1">check_random_state</span><span class="s4">(</span><span class="s1">random_state</span><span class="s4">)</span>
        <span class="s1">H </span><span class="s4">= </span><span class="s1">avg </span><span class="s4">* </span><span class="s1">rng</span><span class="s4">.</span><span class="s1">standard_normal</span><span class="s4">(</span><span class="s1">size</span><span class="s4">=(</span><span class="s1">n_components</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">)).</span><span class="s1">astype</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>
        <span class="s1">W </span><span class="s4">= </span><span class="s1">avg </span><span class="s4">* </span><span class="s1">rng</span><span class="s4">.</span><span class="s1">standard_normal</span><span class="s4">(</span><span class="s1">size</span><span class="s4">=(</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_components</span><span class="s4">)).</span><span class="s1">astype</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>
        <span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">H</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">H</span><span class="s4">)</span>
        <span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">W</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span>

    <span class="s2"># NNDSVD initialization</span>
    <span class="s1">U</span><span class="s4">, </span><span class="s1">S</span><span class="s4">, </span><span class="s1">V </span><span class="s4">= </span><span class="s1">randomized_svd</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">n_components</span><span class="s4">, </span><span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">)</span>
    <span class="s1">W </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">U</span><span class="s4">)</span>
    <span class="s1">H </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">V</span><span class="s4">)</span>

    <span class="s2"># The leading singular triplet is non-negative</span>
    <span class="s2"># so it can be used as is for initialization.</span>
    <span class="s1">W</span><span class="s4">[:, </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">S</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">U</span><span class="s4">[:, </span><span class="s5">0</span><span class="s4">])</span>
    <span class="s1">H</span><span class="s4">[</span><span class="s5">0</span><span class="s4">, :] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">S</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">V</span><span class="s4">[</span><span class="s5">0</span><span class="s4">, :])</span>

    <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">1</span><span class="s4">, </span><span class="s1">n_components</span><span class="s4">):</span>
        <span class="s1">x</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">U</span><span class="s4">[:, </span><span class="s1">j</span><span class="s4">], </span><span class="s1">V</span><span class="s4">[</span><span class="s1">j</span><span class="s4">, :]</span>

        <span class="s2"># extract positive and negative parts of column vectors</span>
        <span class="s1">x_p</span><span class="s4">, </span><span class="s1">y_p </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">maximum</span><span class="s4">(</span><span class="s1">x</span><span class="s4">, </span><span class="s5">0</span><span class="s4">), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">maximum</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s5">0</span><span class="s4">)</span>
        <span class="s1">x_n</span><span class="s4">, </span><span class="s1">y_n </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">minimum</span><span class="s4">(</span><span class="s1">x</span><span class="s4">, </span><span class="s5">0</span><span class="s4">)), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">minimum</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s5">0</span><span class="s4">))</span>

        <span class="s2"># and their norms</span>
        <span class="s1">x_p_nrm</span><span class="s4">, </span><span class="s1">y_p_nrm </span><span class="s4">= </span><span class="s1">norm</span><span class="s4">(</span><span class="s1">x_p</span><span class="s4">), </span><span class="s1">norm</span><span class="s4">(</span><span class="s1">y_p</span><span class="s4">)</span>
        <span class="s1">x_n_nrm</span><span class="s4">, </span><span class="s1">y_n_nrm </span><span class="s4">= </span><span class="s1">norm</span><span class="s4">(</span><span class="s1">x_n</span><span class="s4">), </span><span class="s1">norm</span><span class="s4">(</span><span class="s1">y_n</span><span class="s4">)</span>

        <span class="s1">m_p</span><span class="s4">, </span><span class="s1">m_n </span><span class="s4">= </span><span class="s1">x_p_nrm </span><span class="s4">* </span><span class="s1">y_p_nrm</span><span class="s4">, </span><span class="s1">x_n_nrm </span><span class="s4">* </span><span class="s1">y_n_nrm</span>

        <span class="s2"># choose update</span>
        <span class="s3">if </span><span class="s1">m_p </span><span class="s4">&gt; </span><span class="s1">m_n</span><span class="s4">:</span>
            <span class="s1">u </span><span class="s4">= </span><span class="s1">x_p </span><span class="s4">/ </span><span class="s1">x_p_nrm</span>
            <span class="s1">v </span><span class="s4">= </span><span class="s1">y_p </span><span class="s4">/ </span><span class="s1">y_p_nrm</span>
            <span class="s1">sigma </span><span class="s4">= </span><span class="s1">m_p</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">u </span><span class="s4">= </span><span class="s1">x_n </span><span class="s4">/ </span><span class="s1">x_n_nrm</span>
            <span class="s1">v </span><span class="s4">= </span><span class="s1">y_n </span><span class="s4">/ </span><span class="s1">y_n_nrm</span>
            <span class="s1">sigma </span><span class="s4">= </span><span class="s1">m_n</span>

        <span class="s1">lbd </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">S</span><span class="s4">[</span><span class="s1">j</span><span class="s4">] * </span><span class="s1">sigma</span><span class="s4">)</span>
        <span class="s1">W</span><span class="s4">[:, </span><span class="s1">j</span><span class="s4">] = </span><span class="s1">lbd </span><span class="s4">* </span><span class="s1">u</span>
        <span class="s1">H</span><span class="s4">[</span><span class="s1">j</span><span class="s4">, :] = </span><span class="s1">lbd </span><span class="s4">* </span><span class="s1">v</span>

    <span class="s1">W</span><span class="s4">[</span><span class="s1">W </span><span class="s4">&lt; </span><span class="s1">eps</span><span class="s4">] = </span><span class="s5">0</span>
    <span class="s1">H</span><span class="s4">[</span><span class="s1">H </span><span class="s4">&lt; </span><span class="s1">eps</span><span class="s4">] = </span><span class="s5">0</span>

    <span class="s3">if </span><span class="s1">init </span><span class="s4">== </span><span class="s6">&quot;nndsvd&quot;</span><span class="s4">:</span>
        <span class="s3">pass</span>
    <span class="s3">elif </span><span class="s1">init </span><span class="s4">== </span><span class="s6">&quot;nndsvda&quot;</span><span class="s4">:</span>
        <span class="s1">avg </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">()</span>
        <span class="s1">W</span><span class="s4">[</span><span class="s1">W </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">avg</span>
        <span class="s1">H</span><span class="s4">[</span><span class="s1">H </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">avg</span>
    <span class="s3">elif </span><span class="s1">init </span><span class="s4">== </span><span class="s6">&quot;nndsvdar&quot;</span><span class="s4">:</span>
        <span class="s1">rng </span><span class="s4">= </span><span class="s1">check_random_state</span><span class="s4">(</span><span class="s1">random_state</span><span class="s4">)</span>
        <span class="s1">avg </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">()</span>
        <span class="s1">W</span><span class="s4">[</span><span class="s1">W </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">abs</span><span class="s4">(</span><span class="s1">avg </span><span class="s4">* </span><span class="s1">rng</span><span class="s4">.</span><span class="s1">standard_normal</span><span class="s4">(</span><span class="s1">size</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">W</span><span class="s4">[</span><span class="s1">W </span><span class="s4">== </span><span class="s5">0</span><span class="s4">])) / </span><span class="s5">100</span><span class="s4">)</span>
        <span class="s1">H</span><span class="s4">[</span><span class="s1">H </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">abs</span><span class="s4">(</span><span class="s1">avg </span><span class="s4">* </span><span class="s1">rng</span><span class="s4">.</span><span class="s1">standard_normal</span><span class="s4">(</span><span class="s1">size</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">H</span><span class="s4">[</span><span class="s1">H </span><span class="s4">== </span><span class="s5">0</span><span class="s4">])) / </span><span class="s5">100</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s6">&quot;Invalid init parameter: got %r instead of one of %r&quot;</span>
            <span class="s4">% (</span><span class="s1">init</span><span class="s4">, (</span><span class="s3">None</span><span class="s4">, </span><span class="s6">&quot;random&quot;</span><span class="s4">, </span><span class="s6">&quot;nndsvd&quot;</span><span class="s4">, </span><span class="s6">&quot;nndsvda&quot;</span><span class="s4">, </span><span class="s6">&quot;nndsvdar&quot;</span><span class="s4">))</span>
        <span class="s4">)</span>

    <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span>


<span class="s3">def </span><span class="s1">_update_coordinate_descent</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">Ht</span><span class="s4">, </span><span class="s1">l1_reg</span><span class="s4">, </span><span class="s1">l2_reg</span><span class="s4">, </span><span class="s1">shuffle</span><span class="s4">, </span><span class="s1">random_state</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Helper function for _fit_coordinate_descent. 
 
    Update W to minimize the objective function, iterating once over all 
    coordinates. By symmetry, to update H, one can call 
    _update_coordinate_descent(X.T, Ht, W, ...). 
 
    &quot;&quot;&quot;</span>
    <span class="s1">n_components </span><span class="s4">= </span><span class="s1">Ht</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span>

    <span class="s1">HHt </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">Ht</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">Ht</span><span class="s4">)</span>
    <span class="s1">XHt </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Ht</span><span class="s4">)</span>

    <span class="s2"># L2 regularization corresponds to increase of the diagonal of HHt</span>
    <span class="s3">if </span><span class="s1">l2_reg </span><span class="s4">!= </span><span class="s5">0.0</span><span class="s4">:</span>
        <span class="s2"># adds l2_reg only on the diagonal</span>
        <span class="s1">HHt</span><span class="s4">.</span><span class="s1">flat</span><span class="s4">[:: </span><span class="s1">n_components </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">] += </span><span class="s1">l2_reg</span>
    <span class="s2"># L1 regularization corresponds to decrease of each element of XHt</span>
    <span class="s3">if </span><span class="s1">l1_reg </span><span class="s4">!= </span><span class="s5">0.0</span><span class="s4">:</span>
        <span class="s1">XHt </span><span class="s4">-= </span><span class="s1">l1_reg</span>

    <span class="s3">if </span><span class="s1">shuffle</span><span class="s4">:</span>
        <span class="s1">permutation </span><span class="s4">= </span><span class="s1">random_state</span><span class="s4">.</span><span class="s1">permutation</span><span class="s4">(</span><span class="s1">n_components</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">permutation </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">arange</span><span class="s4">(</span><span class="s1">n_components</span><span class="s4">)</span>
    <span class="s2"># The following seems to be required on 64-bit Windows w/ Python 3.5.</span>
    <span class="s1">permutation </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">permutation</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">intp</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">_update_cdnmf_fast</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">HHt</span><span class="s4">, </span><span class="s1">XHt</span><span class="s4">, </span><span class="s1">permutation</span><span class="s4">)</span>


<span class="s3">def </span><span class="s1">_fit_coordinate_descent</span><span class="s4">(</span>
    <span class="s1">X</span><span class="s4">,</span>
    <span class="s1">W</span><span class="s4">,</span>
    <span class="s1">H</span><span class="s4">,</span>
    <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
    <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
    <span class="s1">l1_reg_W</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">l1_reg_H</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">l2_reg_W</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">l2_reg_H</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
    <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">shuffle</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
    <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Non-negative Matrix Factorization (NMF) with Coordinate Descent 
 
    The objective function is minimized with an alternating minimization of W 
    and H. Each minimization is done with a cyclic (up to a permutation of the 
    features) Coordinate Descent. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Constant matrix. 
 
    W : array-like of shape (n_samples, n_components) 
        Initial guess for the solution. 
 
    H : array-like of shape (n_components, n_features) 
        Initial guess for the solution. 
 
    tol : float, default=1e-4 
        Tolerance of the stopping condition. 
 
    max_iter : int, default=200 
        Maximum number of iterations before timing out. 
 
    l1_reg_W : float, default=0. 
        L1 regularization parameter for W. 
 
    l1_reg_H : float, default=0. 
        L1 regularization parameter for H. 
 
    l2_reg_W : float, default=0. 
        L2 regularization parameter for W. 
 
    l2_reg_H : float, default=0. 
        L2 regularization parameter for H. 
 
    update_H : bool, default=True 
        Set to True, both W and H will be estimated from initial guesses. 
        Set to False, only W will be estimated. 
 
    verbose : int, default=0 
        The verbosity level. 
 
    shuffle : bool, default=False 
        If true, randomize the order of coordinates in the CD solver. 
 
    random_state : int, RandomState instance or None, default=None 
        Used to randomize the coordinates in the CD solver, when 
        ``shuffle`` is set to ``True``. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Returns 
    ------- 
    W : ndarray of shape (n_samples, n_components) 
        Solution to the non-negative least squares problem. 
 
    H : ndarray of shape (n_components, n_features) 
        Solution to the non-negative least squares problem. 
 
    n_iter : int 
        The number of iterations done by the algorithm. 
 
    References 
    ---------- 
    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor 
       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;` 
       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals 
       of electronics, communications and computer sciences 92.3: 708-721, 2009. 
    &quot;&quot;&quot;</span>
    <span class="s2"># so W and Ht are both in C order in memory</span>
    <span class="s1">Ht </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s6">&quot;C&quot;</span><span class="s4">)</span>
    <span class="s1">X </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s6">&quot;csr&quot;</span><span class="s4">)</span>

    <span class="s1">rng </span><span class="s4">= </span><span class="s1">check_random_state</span><span class="s4">(</span><span class="s1">random_state</span><span class="s4">)</span>

    <span class="s3">for </span><span class="s1">n_iter </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">1</span><span class="s4">, </span><span class="s1">max_iter </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">):</span>
        <span class="s1">violation </span><span class="s4">= </span><span class="s5">0.0</span>

        <span class="s2"># Update W</span>
        <span class="s1">violation </span><span class="s4">+= </span><span class="s1">_update_coordinate_descent</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">Ht</span><span class="s4">, </span><span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">shuffle</span><span class="s4">, </span><span class="s1">rng</span>
        <span class="s4">)</span>
        <span class="s2"># Update H</span>
        <span class="s3">if </span><span class="s1">update_H</span><span class="s4">:</span>
            <span class="s1">violation </span><span class="s4">+= </span><span class="s1">_update_coordinate_descent</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">Ht</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">l1_reg_H</span><span class="s4">, </span><span class="s1">l2_reg_H</span><span class="s4">, </span><span class="s1">shuffle</span><span class="s4">, </span><span class="s1">rng</span>
            <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_iter </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">violation_init </span><span class="s4">= </span><span class="s1">violation</span>

        <span class="s3">if </span><span class="s1">violation_init </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">break</span>

        <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;violation:&quot;</span><span class="s4">, </span><span class="s1">violation </span><span class="s4">/ </span><span class="s1">violation_init</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">violation </span><span class="s4">/ </span><span class="s1">violation_init </span><span class="s4">&lt;= </span><span class="s1">tol</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;Converged at iteration&quot;</span><span class="s4">, </span><span class="s1">n_iter </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">)</span>
            <span class="s3">break</span>

    <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">Ht</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">n_iter</span>


<span class="s3">def </span><span class="s1">_multiplicative_update_w</span><span class="s4">(</span>
    <span class="s1">X</span><span class="s4">,</span>
    <span class="s1">W</span><span class="s4">,</span>
    <span class="s1">H</span><span class="s4">,</span>
    <span class="s1">beta_loss</span><span class="s4">,</span>
    <span class="s1">l1_reg_W</span><span class="s4">,</span>
    <span class="s1">l2_reg_W</span><span class="s4">,</span>
    <span class="s1">gamma</span><span class="s4">,</span>
    <span class="s1">H_sum</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">HHt</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">XHt</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Update W in Multiplicative Update NMF.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">2</span><span class="s4">:</span>
        <span class="s2"># Numerator</span>
        <span class="s3">if </span><span class="s1">XHt </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">XHt </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">update_H</span><span class="s4">:</span>
            <span class="s2"># avoid a copy of XHt, which will be re-computed (update_H=True)</span>
            <span class="s1">numerator </span><span class="s4">= </span><span class="s1">XHt</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># preserve the XHt, which is not re-computed (update_H=False)</span>
            <span class="s1">numerator </span><span class="s4">= </span><span class="s1">XHt</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>

        <span class="s2"># Denominator</span>
        <span class="s3">if </span><span class="s1">HHt </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">HHt </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">H</span><span class="s4">, </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s1">denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">HHt</span><span class="s4">)</span>

    <span class="s3">else</span><span class="s4">:</span>
        <span class="s2"># Numerator</span>
        <span class="s2"># if X is sparse, compute WH only where X is non zero</span>
        <span class="s1">WH_safe_X </span><span class="s4">= </span><span class="s1">_special_sparse_dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">= </span><span class="s1">WH_safe_X</span><span class="s4">.</span><span class="s1">data</span>
            <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">= </span><span class="s1">WH_safe_X</span>
            <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span>
            <span class="s2"># copy used in the Denominator</span>
            <span class="s1">WH </span><span class="s4">= </span><span class="s1">WH_safe_X</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>
            <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1.0 </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
                <span class="s1">WH</span><span class="s4">[</span><span class="s1">WH </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>

        <span class="s2"># to avoid taking a negative power of zero</span>
        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">2.0 </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">WH_safe_X_data</span><span class="s4">[</span><span class="s1">WH_safe_X_data </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>

        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">divide</span><span class="s4">(</span><span class="s1">X_data</span><span class="s4">, </span><span class="s1">WH_safe_X_data</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">WH_safe_X_data</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s2"># speeds up computation time</span>
            <span class="s2"># refer to /numpy/numpy/issues/9363</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">**= -</span><span class="s5">1</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">**= </span><span class="s5">2</span>
            <span class="s2"># element-wise multiplication</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">*= </span><span class="s1">X_data</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">**= </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">2</span>
            <span class="s2"># element-wise multiplication</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">*= </span><span class="s1">X_data</span>

        <span class="s2"># here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)</span>
        <span class="s1">numerator </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">WH_safe_X</span><span class="s4">, </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>

        <span class="s2"># Denominator</span>
        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">H_sum </span><span class="s3">is None</span><span class="s4">:</span>
                <span class="s1">H_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">H</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">1</span><span class="s4">)  </span><span class="s2"># shape(n_components, )</span>
            <span class="s1">denominator </span><span class="s4">= </span><span class="s1">H_sum</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">, :]</span>

        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)</span>
            <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
                <span class="s2"># memory efficient computation</span>
                <span class="s2"># (compute row by row, avoiding the dense matrix WH)</span>
                <span class="s1">WHHt </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">(</span><span class="s1">W</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">)</span>
                <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]):</span>
                    <span class="s1">WHi </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :], </span><span class="s1">H</span><span class="s4">)</span>
                    <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1 </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
                        <span class="s1">WHi</span><span class="s4">[</span><span class="s1">WHi </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>
                    <span class="s1">WHi </span><span class="s4">**= </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1</span>
                    <span class="s1">WHHt</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">WHi</span><span class="s4">, </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">WH </span><span class="s4">**= </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1</span>
                <span class="s1">WHHt </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">WH</span><span class="s4">, </span><span class="s1">H</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
            <span class="s1">denominator </span><span class="s4">= </span><span class="s1">WHHt</span>

    <span class="s2"># Add L1 and L2 regularization</span>
    <span class="s3">if </span><span class="s1">l1_reg_W </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
        <span class="s1">denominator </span><span class="s4">+= </span><span class="s1">l1_reg_W</span>
    <span class="s3">if </span><span class="s1">l2_reg_W </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
        <span class="s1">denominator </span><span class="s4">= </span><span class="s1">denominator </span><span class="s4">+ </span><span class="s1">l2_reg_W </span><span class="s4">* </span><span class="s1">W</span>
    <span class="s1">denominator</span><span class="s4">[</span><span class="s1">denominator </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">EPSILON</span>

    <span class="s1">numerator </span><span class="s4">/= </span><span class="s1">denominator</span>
    <span class="s1">delta_W </span><span class="s4">= </span><span class="s1">numerator</span>

    <span class="s2"># gamma is in ]0, 1]</span>
    <span class="s3">if </span><span class="s1">gamma </span><span class="s4">!= </span><span class="s5">1</span><span class="s4">:</span>
        <span class="s1">delta_W </span><span class="s4">**= </span><span class="s1">gamma</span>

    <span class="s1">W </span><span class="s4">*= </span><span class="s1">delta_W</span>

    <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H_sum</span><span class="s4">, </span><span class="s1">HHt</span><span class="s4">, </span><span class="s1">XHt</span>


<span class="s3">def </span><span class="s1">_multiplicative_update_h</span><span class="s4">(</span>
    <span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">beta_loss</span><span class="s4">, </span><span class="s1">l1_reg_H</span><span class="s4">, </span><span class="s1">l2_reg_H</span><span class="s4">, </span><span class="s1">gamma</span><span class="s4">, </span><span class="s1">A</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">B</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">rho</span><span class="s4">=</span><span class="s3">None</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;update H in Multiplicative Update NMF.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">2</span><span class="s4">:</span>
        <span class="s1">numerator </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">multi_dot</span><span class="s4">([</span><span class="s1">W</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">])</span>

    <span class="s3">else</span><span class="s4">:</span>
        <span class="s2"># Numerator</span>
        <span class="s1">WH_safe_X </span><span class="s4">= </span><span class="s1">_special_sparse_dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">= </span><span class="s1">WH_safe_X</span><span class="s4">.</span><span class="s1">data</span>
            <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">= </span><span class="s1">WH_safe_X</span>
            <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span>
            <span class="s2"># copy used in the Denominator</span>
            <span class="s1">WH </span><span class="s4">= </span><span class="s1">WH_safe_X</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>
            <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1.0 </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
                <span class="s1">WH</span><span class="s4">[</span><span class="s1">WH </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>

        <span class="s2"># to avoid division by zero</span>
        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">2.0 </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">WH_safe_X_data</span><span class="s4">[</span><span class="s1">WH_safe_X_data </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>

        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">divide</span><span class="s4">(</span><span class="s1">X_data</span><span class="s4">, </span><span class="s1">WH_safe_X_data</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">WH_safe_X_data</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s2"># speeds up computation time</span>
            <span class="s2"># refer to /numpy/numpy/issues/9363</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">**= -</span><span class="s5">1</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">**= </span><span class="s5">2</span>
            <span class="s2"># element-wise multiplication</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">*= </span><span class="s1">X_data</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">**= </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">2</span>
            <span class="s2"># element-wise multiplication</span>
            <span class="s1">WH_safe_X_data </span><span class="s4">*= </span><span class="s1">X_data</span>

        <span class="s2"># here numerator = dot(W.T, (dot(W, H) ** (beta_loss - 2)) * X)</span>
        <span class="s1">numerator </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">WH_safe_X</span><span class="s4">)</span>

        <span class="s2"># Denominator</span>
        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">W_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)  </span><span class="s2"># shape(n_components, )</span>
            <span class="s1">W_sum</span><span class="s4">[</span><span class="s1">W_sum </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s5">1.0</span>
            <span class="s1">denominator </span><span class="s4">= </span><span class="s1">W_sum</span><span class="s4">[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>

        <span class="s2"># beta_loss not in (1, 2)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># computation of WtWH = dot(W.T, dot(W, H) ** beta_loss - 1)</span>
            <span class="s3">if </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
                <span class="s2"># memory efficient computation</span>
                <span class="s2"># (compute column by column, avoiding the dense matrix WH)</span>
                <span class="s1">WtWH </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">(</span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">)</span>
                <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]):</span>
                    <span class="s1">WHi </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">[:, </span><span class="s1">i</span><span class="s4">])</span>
                    <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1 </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
                        <span class="s1">WHi</span><span class="s4">[</span><span class="s1">WHi </span><span class="s4">&lt; </span><span class="s1">EPSILON</span><span class="s4">] = </span><span class="s1">EPSILON</span>
                    <span class="s1">WHi </span><span class="s4">**= </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1</span>
                    <span class="s1">WtWH</span><span class="s4">[:, </span><span class="s1">i</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">WHi</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">WH </span><span class="s4">**= </span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1</span>
                <span class="s1">WtWH </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">W</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">WH</span><span class="s4">)</span>
            <span class="s1">denominator </span><span class="s4">= </span><span class="s1">WtWH</span>

    <span class="s2"># Add L1 and L2 regularization</span>
    <span class="s3">if </span><span class="s1">l1_reg_H </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
        <span class="s1">denominator </span><span class="s4">+= </span><span class="s1">l1_reg_H</span>
    <span class="s3">if </span><span class="s1">l2_reg_H </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
        <span class="s1">denominator </span><span class="s4">= </span><span class="s1">denominator </span><span class="s4">+ </span><span class="s1">l2_reg_H </span><span class="s4">* </span><span class="s1">H</span>
    <span class="s1">denominator</span><span class="s4">[</span><span class="s1">denominator </span><span class="s4">== </span><span class="s5">0</span><span class="s4">] = </span><span class="s1">EPSILON</span>

    <span class="s3">if </span><span class="s1">A </span><span class="s3">is not None and </span><span class="s1">B </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s2"># Updates for the online nmf</span>
        <span class="s3">if </span><span class="s1">gamma </span><span class="s4">!= </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">H </span><span class="s4">**= </span><span class="s5">1 </span><span class="s4">/ </span><span class="s1">gamma</span>
        <span class="s1">numerator </span><span class="s4">*= </span><span class="s1">H</span>
        <span class="s1">A </span><span class="s4">*= </span><span class="s1">rho</span>
        <span class="s1">B </span><span class="s4">*= </span><span class="s1">rho</span>
        <span class="s1">A </span><span class="s4">+= </span><span class="s1">numerator</span>
        <span class="s1">B </span><span class="s4">+= </span><span class="s1">denominator</span>
        <span class="s1">H </span><span class="s4">= </span><span class="s1">A </span><span class="s4">/ </span><span class="s1">B</span>

        <span class="s3">if </span><span class="s1">gamma </span><span class="s4">!= </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">H </span><span class="s4">**= </span><span class="s1">gamma</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">delta_H </span><span class="s4">= </span><span class="s1">numerator</span>
        <span class="s1">delta_H </span><span class="s4">/= </span><span class="s1">denominator</span>
        <span class="s3">if </span><span class="s1">gamma </span><span class="s4">!= </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">delta_H </span><span class="s4">**= </span><span class="s1">gamma</span>
        <span class="s1">H </span><span class="s4">*= </span><span class="s1">delta_H</span>

    <span class="s3">return </span><span class="s1">H</span>


<span class="s3">def </span><span class="s1">_fit_multiplicative_update</span><span class="s4">(</span>
    <span class="s1">X</span><span class="s4">,</span>
    <span class="s1">W</span><span class="s4">,</span>
    <span class="s1">H</span><span class="s4">,</span>
    <span class="s1">beta_loss</span><span class="s4">=</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">,</span>
    <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
    <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
    <span class="s1">l1_reg_W</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">l1_reg_H</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">l2_reg_W</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">l2_reg_H</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
    <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Non-negative Matrix Factorization with Multiplicative Update. 
 
    The objective function is _beta_divergence(X, WH) and is minimized with an 
    alternating minimization of W and H. Each minimization is done with a 
    Multiplicative Update. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Constant input matrix. 
 
    W : array-like of shape (n_samples, n_components) 
        Initial guess for the solution. 
 
    H : array-like of shape (n_components, n_features) 
        Initial guess for the solution. 
 
    beta_loss : float or {'frobenius', 'kullback-leibler', \ 
            'itakura-saito'}, default='frobenius' 
        String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}. 
        Beta divergence to be minimized, measuring the distance between X 
        and the dot product WH. Note that values different from 'frobenius' 
        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower 
        fits. Note that for beta_loss &lt;= 0 (or 'itakura-saito'), the input 
        matrix X cannot contain zeros. 
 
    max_iter : int, default=200 
        Number of iterations. 
 
    tol : float, default=1e-4 
        Tolerance of the stopping condition. 
 
    l1_reg_W : float, default=0. 
        L1 regularization parameter for W. 
 
    l1_reg_H : float, default=0. 
        L1 regularization parameter for H. 
 
    l2_reg_W : float, default=0. 
        L2 regularization parameter for W. 
 
    l2_reg_H : float, default=0. 
        L2 regularization parameter for H. 
 
    update_H : bool, default=True 
        Set to True, both W and H will be estimated from initial guesses. 
        Set to False, only W will be estimated. 
 
    verbose : int, default=0 
        The verbosity level. 
 
    Returns 
    ------- 
    W : ndarray of shape (n_samples, n_components) 
        Solution to the non-negative least squares problem. 
 
    H : ndarray of shape (n_components, n_features) 
        Solution to the non-negative least squares problem. 
 
    n_iter : int 
        The number of iterations done by the algorithm. 
 
    References 
    ---------- 
    Lee, D. D., &amp; Seung, H., S. (2001). Algorithms for Non-negative Matrix 
    Factorization. Adv. Neural Inform. Process. Syst.. 13. 
    Fevotte, C., &amp; Idier, J. (2011). Algorithms for nonnegative matrix 
    factorization with the beta-divergence. Neural Computation, 23(9). 
    &quot;&quot;&quot;</span>
    <span class="s1">start_time </span><span class="s4">= </span><span class="s1">time</span><span class="s4">.</span><span class="s1">time</span><span class="s4">()</span>

    <span class="s1">beta_loss </span><span class="s4">= </span><span class="s1">_beta_loss_to_float</span><span class="s4">(</span><span class="s1">beta_loss</span><span class="s4">)</span>

    <span class="s2"># gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]</span>
    <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">&lt; </span><span class="s5">1</span><span class="s4">:</span>
        <span class="s1">gamma </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ (</span><span class="s5">2.0 </span><span class="s4">- </span><span class="s1">beta_loss</span><span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">beta_loss </span><span class="s4">&gt; </span><span class="s5">2</span><span class="s4">:</span>
        <span class="s1">gamma </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ (</span><span class="s1">beta_loss </span><span class="s4">- </span><span class="s5">1.0</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">gamma </span><span class="s4">= </span><span class="s5">1.0</span>

    <span class="s2"># used for the convergence criterion</span>
    <span class="s1">error_at_init </span><span class="s4">= </span><span class="s1">_beta_divergence</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">beta_loss</span><span class="s4">, </span><span class="s1">square_root</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s1">previous_error </span><span class="s4">= </span><span class="s1">error_at_init</span>

    <span class="s1">H_sum</span><span class="s4">, </span><span class="s1">HHt</span><span class="s4">, </span><span class="s1">XHt </span><span class="s4">= </span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span>
    <span class="s3">for </span><span class="s1">n_iter </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">1</span><span class="s4">, </span><span class="s1">max_iter </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">):</span>
        <span class="s2"># update W</span>
        <span class="s2"># H_sum, HHt and XHt are saved and reused if not update_H</span>
        <span class="s1">W</span><span class="s4">, </span><span class="s1">H_sum</span><span class="s4">, </span><span class="s1">HHt</span><span class="s4">, </span><span class="s1">XHt </span><span class="s4">= </span><span class="s1">_multiplicative_update_w</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">W</span><span class="s4">,</span>
            <span class="s1">H</span><span class="s4">,</span>
            <span class="s1">beta_loss</span><span class="s4">=</span><span class="s1">beta_loss</span><span class="s4">,</span>
            <span class="s1">l1_reg_W</span><span class="s4">=</span><span class="s1">l1_reg_W</span><span class="s4">,</span>
            <span class="s1">l2_reg_W</span><span class="s4">=</span><span class="s1">l2_reg_W</span><span class="s4">,</span>
            <span class="s1">gamma</span><span class="s4">=</span><span class="s1">gamma</span><span class="s4">,</span>
            <span class="s1">H_sum</span><span class="s4">=</span><span class="s1">H_sum</span><span class="s4">,</span>
            <span class="s1">HHt</span><span class="s4">=</span><span class="s1">HHt</span><span class="s4">,</span>
            <span class="s1">XHt</span><span class="s4">=</span><span class="s1">XHt</span><span class="s4">,</span>
            <span class="s1">update_H</span><span class="s4">=</span><span class="s1">update_H</span><span class="s4">,</span>
        <span class="s4">)</span>

        <span class="s2"># necessary for stability with beta_loss &lt; 1</span>
        <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">&lt; </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">W</span><span class="s4">[</span><span class="s1">W </span><span class="s4">&lt; </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">).</span><span class="s1">eps</span><span class="s4">] = </span><span class="s5">0.0</span>

        <span class="s2"># update H (only at fit or fit_transform)</span>
        <span class="s3">if </span><span class="s1">update_H</span><span class="s4">:</span>
            <span class="s1">H </span><span class="s4">= </span><span class="s1">_multiplicative_update_h</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">,</span>
                <span class="s1">W</span><span class="s4">,</span>
                <span class="s1">H</span><span class="s4">,</span>
                <span class="s1">beta_loss</span><span class="s4">=</span><span class="s1">beta_loss</span><span class="s4">,</span>
                <span class="s1">l1_reg_H</span><span class="s4">=</span><span class="s1">l1_reg_H</span><span class="s4">,</span>
                <span class="s1">l2_reg_H</span><span class="s4">=</span><span class="s1">l2_reg_H</span><span class="s4">,</span>
                <span class="s1">gamma</span><span class="s4">=</span><span class="s1">gamma</span><span class="s4">,</span>
            <span class="s4">)</span>

            <span class="s2"># These values will be recomputed since H changed</span>
            <span class="s1">H_sum</span><span class="s4">, </span><span class="s1">HHt</span><span class="s4">, </span><span class="s1">XHt </span><span class="s4">= </span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span>

            <span class="s2"># necessary for stability with beta_loss &lt; 1</span>
            <span class="s3">if </span><span class="s1">beta_loss </span><span class="s4">&lt;= </span><span class="s5">1</span><span class="s4">:</span>
                <span class="s1">H</span><span class="s4">[</span><span class="s1">H </span><span class="s4">&lt; </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">).</span><span class="s1">eps</span><span class="s4">] = </span><span class="s5">0.0</span>

        <span class="s2"># test convergence criterion every 10 iterations</span>
        <span class="s3">if </span><span class="s1">tol </span><span class="s4">&gt; </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">n_iter </span><span class="s4">% </span><span class="s5">10 </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">error </span><span class="s4">= </span><span class="s1">_beta_divergence</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">beta_loss</span><span class="s4">, </span><span class="s1">square_root</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>

            <span class="s3">if </span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">iter_time </span><span class="s4">= </span><span class="s1">time</span><span class="s4">.</span><span class="s1">time</span><span class="s4">()</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">&quot;Epoch %02d reached after %.3f seconds, error: %f&quot;</span>
                    <span class="s4">% (</span><span class="s1">n_iter</span><span class="s4">, </span><span class="s1">iter_time </span><span class="s4">- </span><span class="s1">start_time</span><span class="s4">, </span><span class="s1">error</span><span class="s4">)</span>
                <span class="s4">)</span>

            <span class="s3">if </span><span class="s4">(</span><span class="s1">previous_error </span><span class="s4">- </span><span class="s1">error</span><span class="s4">) / </span><span class="s1">error_at_init </span><span class="s4">&lt; </span><span class="s1">tol</span><span class="s4">:</span>
                <span class="s3">break</span>
            <span class="s1">previous_error </span><span class="s4">= </span><span class="s1">error</span>

    <span class="s2"># do not print if we have already printed in the convergence test</span>
    <span class="s3">if </span><span class="s1">verbose </span><span class="s3">and </span><span class="s4">(</span><span class="s1">tol </span><span class="s4">== </span><span class="s5">0 </span><span class="s3">or </span><span class="s1">n_iter </span><span class="s4">% </span><span class="s5">10 </span><span class="s4">!= </span><span class="s5">0</span><span class="s4">):</span>
        <span class="s1">end_time </span><span class="s4">= </span><span class="s1">time</span><span class="s4">.</span><span class="s1">time</span><span class="s4">()</span>
        <span class="s1">print</span><span class="s4">(</span>
            <span class="s6">&quot;Epoch %02d reached after %.3f seconds.&quot; </span><span class="s4">% (</span><span class="s1">n_iter</span><span class="s4">, </span><span class="s1">end_time </span><span class="s4">- </span><span class="s1">start_time</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s6">&quot;X&quot;</span><span class="s4">: [</span><span class="s6">&quot;array-like&quot;</span><span class="s4">, </span><span class="s6">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s6">&quot;W&quot;</span><span class="s4">: [</span><span class="s6">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s6">&quot;H&quot;</span><span class="s4">: [</span><span class="s6">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s6">&quot;update_H&quot;</span><span class="s4">: [</span><span class="s6">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">non_negative_factorization</span><span class="s4">(</span>
    <span class="s1">X</span><span class="s4">,</span>
    <span class="s1">W</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">H</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">n_components</span><span class="s4">=</span><span class="s6">&quot;warn&quot;</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">init</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
    <span class="s1">solver</span><span class="s4">=</span><span class="s6">&quot;cd&quot;</span><span class="s4">,</span>
    <span class="s1">beta_loss</span><span class="s4">=</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">,</span>
    <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
    <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
    <span class="s1">alpha_W</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
    <span class="s1">alpha_H</span><span class="s4">=</span><span class="s6">&quot;same&quot;</span><span class="s4">,</span>
    <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
    <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s1">shuffle</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Non-negative Matrix Factorization (NMF). 
 
    Find two non-negative matrices (W, H) whose product approximates the non- 
    negative matrix X. This factorization can be used for example for 
    dimensionality reduction, source separation or topic extraction. 
 
    The objective function is: 
 
        .. math:: 
 
            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2 
 
            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1 
 
            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1 
 
            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2 
 
            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2 
 
    Where: 
 
    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) 
 
    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm) 
 
    The generic norm :math:`||X - WH||_{loss}^2` may represent 
    the Frobenius norm or another supported beta-divergence loss. 
    The choice between options is controlled by the `beta_loss` parameter. 
 
    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for 
    `H` to keep their impact balanced with respect to one another and to the data fit 
    term as independent as possible of the size `n_samples` of the training set. 
 
    The objective function is minimized with an alternating minimization of W 
    and H. If H is given and update_H=False, it solves for W only. 
 
    Note that the transformed data is named W and the components matrix is named H. In 
    the NMF literature, the naming convention is usually the opposite since the data 
    matrix X is transposed. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Constant matrix. 
 
    W : array-like of shape (n_samples, n_components), default=None 
        If `init='custom'`, it is used as initial guess for the solution. 
        If `update_H=False`, it is initialised as an array of zeros, unless 
        `solver='mu'`, then it is filled with values calculated by 
        `np.sqrt(X.mean() / self._n_components)`. 
        If `None`, uses the initialisation method specified in `init`. 
 
    H : array-like of shape (n_components, n_features), default=None 
        If `init='custom'`, it is used as initial guess for the solution. 
        If `update_H=False`, it is used as a constant, to solve for W only. 
        If `None`, uses the initialisation method specified in `init`. 
 
    n_components : int or {'auto'} or None, default=None 
        Number of components, if n_components is not set all features 
        are kept. 
        If `n_components='auto'`, the number of components is automatically inferred 
        from `W` or `H` shapes. 
 
        .. versionchanged:: 1.4 
            Added `'auto'` value. 
 
    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None 
        Method used to initialize the procedure. 
 
        Valid options: 
 
        - None: 'nndsvda' if n_components &lt; n_features, otherwise 'random'. 
        - 'random': non-negative random matrices, scaled with: 
          `sqrt(X.mean() / n_components)` 
        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD) 
          initialization (better for sparseness) 
        - 'nndsvda': NNDSVD with zeros filled with the average of X 
          (better when sparsity is not desired) 
        - 'nndsvdar': NNDSVD with zeros filled with small random values 
          (generally faster, less accurate alternative to NNDSVDa 
          for when sparsity is not desired) 
        - 'custom': If `update_H=True`, use custom matrices W and H which must both 
          be provided. If `update_H=False`, then only custom matrix H is used. 
 
        .. versionchanged:: 0.23 
            The default value of `init` changed from 'random' to None in 0.23. 
 
        .. versionchanged:: 1.1 
            When `init=None` and n_components is less than n_samples and n_features 
            defaults to `nndsvda` instead of `nndsvd`. 
 
    update_H : bool, default=True 
        Set to True, both W and H will be estimated from initial guesses. 
        Set to False, only W will be estimated. 
 
    solver : {'cd', 'mu'}, default='cd' 
        Numerical solver to use: 
 
        - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical 
          Alternating Least Squares (Fast HALS). 
        - 'mu' is a Multiplicative Update solver. 
 
        .. versionadded:: 0.17 
           Coordinate Descent solver. 
 
        .. versionadded:: 0.19 
           Multiplicative Update solver. 
 
    beta_loss : float or {'frobenius', 'kullback-leibler', \ 
            'itakura-saito'}, default='frobenius' 
        Beta divergence to be minimized, measuring the distance between X 
        and the dot product WH. Note that values different from 'frobenius' 
        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower 
        fits. Note that for beta_loss &lt;= 0 (or 'itakura-saito'), the input 
        matrix X cannot contain zeros. Used only in 'mu' solver. 
 
        .. versionadded:: 0.19 
 
    tol : float, default=1e-4 
        Tolerance of the stopping condition. 
 
    max_iter : int, default=200 
        Maximum number of iterations before timing out. 
 
    alpha_W : float, default=0.0 
        Constant that multiplies the regularization terms of `W`. Set it to zero 
        (default) to have no regularization on `W`. 
 
        .. versionadded:: 1.0 
 
    alpha_H : float or &quot;same&quot;, default=&quot;same&quot; 
        Constant that multiplies the regularization terms of `H`. Set it to zero to 
        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as 
        `alpha_W`. 
 
        .. versionadded:: 1.0 
 
    l1_ratio : float, default=0.0 
        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. 
        For l1_ratio = 0 the penalty is an elementwise L2 penalty 
        (aka Frobenius Norm). 
        For l1_ratio = 1 it is an elementwise L1 penalty. 
        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for NMF initialisation (when ``init`` == 'nndsvdar' or 
        'random'), and in Coordinate Descent. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    verbose : int, default=0 
        The verbosity level. 
 
    shuffle : bool, default=False 
        If true, randomize the order of coordinates in the CD solver. 
 
    Returns 
    ------- 
    W : ndarray of shape (n_samples, n_components) 
        Solution to the non-negative least squares problem. 
 
    H : ndarray of shape (n_components, n_features) 
        Solution to the non-negative least squares problem. 
 
    n_iter : int 
        Actual number of iterations. 
 
    References 
    ---------- 
    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor 
       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;` 
       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals 
       of electronics, communications and computer sciences 92.3: 708-721, 2009. 
 
    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the 
       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;` 
       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]) 
    &gt;&gt;&gt; from sklearn.decomposition import non_negative_factorization 
    &gt;&gt;&gt; W, H, n_iter = non_negative_factorization( 
    ...     X, n_components=2, init='random', random_state=0) 
    &quot;&quot;&quot;</span>
    <span class="s1">est </span><span class="s4">= </span><span class="s1">NMF</span><span class="s4">(</span>
        <span class="s1">n_components</span><span class="s4">=</span><span class="s1">n_components</span><span class="s4">,</span>
        <span class="s1">init</span><span class="s4">=</span><span class="s1">init</span><span class="s4">,</span>
        <span class="s1">solver</span><span class="s4">=</span><span class="s1">solver</span><span class="s4">,</span>
        <span class="s1">beta_loss</span><span class="s4">=</span><span class="s1">beta_loss</span><span class="s4">,</span>
        <span class="s1">tol</span><span class="s4">=</span><span class="s1">tol</span><span class="s4">,</span>
        <span class="s1">max_iter</span><span class="s4">=</span><span class="s1">max_iter</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
        <span class="s1">alpha_W</span><span class="s4">=</span><span class="s1">alpha_W</span><span class="s4">,</span>
        <span class="s1">alpha_H</span><span class="s4">=</span><span class="s1">alpha_H</span><span class="s4">,</span>
        <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s1">l1_ratio</span><span class="s4">,</span>
        <span class="s1">verbose</span><span class="s4">=</span><span class="s1">verbose</span><span class="s4">,</span>
        <span class="s1">shuffle</span><span class="s4">=</span><span class="s1">shuffle</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s1">est</span><span class="s4">.</span><span class="s1">_validate_params</span><span class="s4">()</span>

    <span class="s1">X </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s6">&quot;csr&quot;</span><span class="s4">, </span><span class="s6">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">])</span>

    <span class="s3">with </span><span class="s1">config_context</span><span class="s4">(</span><span class="s1">assume_finite</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter </span><span class="s4">= </span><span class="s1">est</span><span class="s4">.</span><span class="s1">_fit_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">=</span><span class="s1">update_H</span><span class="s4">)</span>

    <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter</span>


<span class="s3">class </span><span class="s1">_BaseNMF</span><span class="s4">(</span><span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s4">, </span><span class="s1">TransformerMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span><span class="s4">, </span><span class="s1">ABC</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Base class for NMF and MiniBatchNMF.&quot;&quot;&quot;</span>

    <span class="s2"># This prevents ``set_split_inverse_transform`` to be generated for the</span>
    <span class="s2"># non-standard ``Xt`` arg on ``inverse_transform``.</span>
    <span class="s2"># TODO(1.7): remove when Xt is removed in v1.7 for inverse_transform</span>
    <span class="s1">__metadata_request__inverse_transform </span><span class="s4">= {</span><span class="s6">&quot;Xt&quot;</span><span class="s4">: </span><span class="s1">metadata_routing</span><span class="s4">.</span><span class="s1">UNUSED</span><span class="s4">}</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s6">&quot;n_components&quot;</span><span class="s4">: [</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">),</span>
            <span class="s3">None</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;auto&quot;</span><span class="s4">}),</span>
            <span class="s1">Hidden</span><span class="s4">(</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;warn&quot;</span><span class="s4">})),</span>
        <span class="s4">],</span>
        <span class="s6">&quot;init&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;random&quot;</span><span class="s4">, </span><span class="s6">&quot;nndsvd&quot;</span><span class="s4">, </span><span class="s6">&quot;nndsvda&quot;</span><span class="s4">, </span><span class="s6">&quot;nndsvdar&quot;</span><span class="s4">, </span><span class="s6">&quot;custom&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s6">&quot;beta_loss&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">, </span><span class="s6">&quot;kullback-leibler&quot;</span><span class="s4">, </span><span class="s6">&quot;itakura-saito&quot;</span><span class="s4">}),</span>
            <span class="s1">Real</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s6">&quot;tol&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;max_iter&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;random_state&quot;</span><span class="s4">: [</span><span class="s6">&quot;random_state&quot;</span><span class="s4">],</span>
        <span class="s6">&quot;alpha_W&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;alpha_H&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">), </span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;same&quot;</span><span class="s4">})],</span>
        <span class="s6">&quot;l1_ratio&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;verbose&quot;</span><span class="s4">: [</span><span class="s6">&quot;verbose&quot;</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">n_components</span><span class="s4">=</span><span class="s6">&quot;warn&quot;</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">init</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">beta_loss</span><span class="s4">=</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">,</span>
        <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
        <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">alpha_W</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">alpha_H</span><span class="s4">=</span><span class="s6">&quot;same&quot;</span><span class="s4">,</span>
        <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_components </span><span class="s4">= </span><span class="s1">n_components</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">init </span><span class="s4">= </span><span class="s1">init</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">beta_loss </span><span class="s4">= </span><span class="s1">beta_loss</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">tol </span><span class="s4">= </span><span class="s1">tol</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s4">= </span><span class="s1">max_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">random_state </span><span class="s4">= </span><span class="s1">random_state</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_W </span><span class="s4">= </span><span class="s1">alpha_W</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_H </span><span class="s4">= </span><span class="s1">alpha_H</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">l1_ratio </span><span class="s4">= </span><span class="s1">l1_ratio</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">= </span><span class="s1">verbose</span>

    <span class="s3">def </span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s2"># n_components</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components </span><span class="s4">== </span><span class="s6">&quot;warn&quot;</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s4">(</span>
                    <span class="s6">&quot;The default value of `n_components` will change from `None` to&quot;</span>
                    <span class="s6">&quot; `'auto'` in 1.6. Set the value of `n_components` to `None`&quot;</span>
                    <span class="s6">&quot; explicitly to suppress the warning.&quot;</span>
                <span class="s4">),</span>
                <span class="s1">FutureWarning</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">= </span><span class="s3">None  </span><span class="s2"># Keeping the old default value</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span>

        <span class="s2"># beta_loss</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">= </span><span class="s1">_beta_loss_to_float</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_loss</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_check_w_h</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Check W and H, or initialize them.&quot;&quot;&quot;</span>
        <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">init </span><span class="s4">== </span><span class="s6">&quot;custom&quot; </span><span class="s3">and </span><span class="s1">update_H</span><span class="s4">:</span>
            <span class="s1">_check_init</span><span class="s4">(</span><span class="s1">H</span><span class="s4">, (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">), </span><span class="s6">&quot;NMF (input H)&quot;</span><span class="s4">)</span>
            <span class="s1">_check_init</span><span class="s4">(</span><span class="s1">W</span><span class="s4">, (</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">), </span><span class="s6">&quot;NMF (input W)&quot;</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">== </span><span class="s6">&quot;auto&quot;</span><span class="s4">:</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

            <span class="s3">if </span><span class="s1">H</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype </span><span class="s3">or </span><span class="s1">W</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">TypeError</span><span class="s4">(</span>
                    <span class="s6">&quot;H and W should have the same dtype as X. Got &quot;</span>
                    <span class="s6">&quot;H.dtype = {} and W.dtype = {}.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">H</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">W</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
                <span class="s4">)</span>

        <span class="s3">elif not </span><span class="s1">update_H</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">W </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                    <span class="s6">&quot;When update_H=False, the provided initial W is not used.&quot;</span><span class="s4">,</span>
                    <span class="s1">RuntimeWarning</span><span class="s4">,</span>
                <span class="s4">)</span>

            <span class="s1">_check_init</span><span class="s4">(</span><span class="s1">H</span><span class="s4">, (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">), </span><span class="s6">&quot;NMF (input H)&quot;</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">== </span><span class="s6">&quot;auto&quot;</span><span class="s4">:</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

            <span class="s3">if </span><span class="s1">H</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">TypeError</span><span class="s4">(</span>
                    <span class="s6">&quot;H should have the same dtype as X. Got H.dtype = {}.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                        <span class="s1">H</span><span class="s4">.</span><span class="s1">dtype</span>
                    <span class="s4">)</span>
                <span class="s4">)</span>

            <span class="s2"># 'mu' solver should not be initialized by zeros</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver </span><span class="s4">== </span><span class="s6">&quot;mu&quot;</span><span class="s4">:</span>
                <span class="s1">avg </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">() / </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">)</span>
                <span class="s1">W </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">((</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">), </span><span class="s1">avg</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">W </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>

        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">W </span><span class="s3">is not None or </span><span class="s1">H </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                    <span class="s4">(</span>
                        <span class="s6">&quot;When init!='custom', provided W or H are ignored. Set &quot;</span>
                        <span class="s6">&quot; init='custom' to use them as initialization.&quot;</span>
                    <span class="s4">),</span>
                    <span class="s1">RuntimeWarning</span><span class="s4">,</span>
                <span class="s4">)</span>

            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">== </span><span class="s6">&quot;auto&quot;</span><span class="s4">:</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span>

            <span class="s1">W</span><span class="s4">, </span><span class="s1">H </span><span class="s4">= </span><span class="s1">_initialize_nmf</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">, </span><span class="s1">init</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">init</span><span class="s4">, </span><span class="s1">random_state</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">random_state</span>
            <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span>

    <span class="s3">def </span><span class="s1">_compute_regularization</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute scaled regularization terms.&quot;&quot;&quot;</span>
        <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>
        <span class="s1">alpha_W </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_W</span>
        <span class="s1">alpha_H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_W </span><span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_H </span><span class="s4">== </span><span class="s6">&quot;same&quot; </span><span class="s3">else </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_H</span>

        <span class="s1">l1_reg_W </span><span class="s4">= </span><span class="s1">n_features </span><span class="s4">* </span><span class="s1">alpha_W </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">l1_ratio</span>
        <span class="s1">l1_reg_H </span><span class="s4">= </span><span class="s1">n_samples </span><span class="s4">* </span><span class="s1">alpha_H </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">l1_ratio</span>
        <span class="s1">l2_reg_W </span><span class="s4">= </span><span class="s1">n_features </span><span class="s4">* </span><span class="s1">alpha_W </span><span class="s4">* (</span><span class="s5">1.0 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">l1_ratio</span><span class="s4">)</span>
        <span class="s1">l2_reg_H </span><span class="s4">= </span><span class="s1">n_samples </span><span class="s4">* </span><span class="s1">alpha_H </span><span class="s4">* (</span><span class="s5">1.0 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">l1_ratio</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">l1_reg_H</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">l2_reg_H</span>

    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, **</span><span class="s1">params</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Learn a NMF model for the data X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        **params : kwargs 
            Parameters (keyword arguments) and values passed to 
            the fit_transform instance. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s2"># param validation is done in fit_transform</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">fit_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, **</span><span class="s1">params</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">inverse_transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, *, </span><span class="s1">Xt</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Transform data back to its original space. 
 
        .. versionadded:: 0.18 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_components) 
            Transformed data matrix. 
 
        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components) 
            Transformed data matrix. 
 
            .. deprecated:: 1.5 
                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead. 
 
        Returns 
        ------- 
        X : ndarray of shape (n_samples, n_features) 
            Returns a data matrix of the original shape. 
        &quot;&quot;&quot;</span>

        <span class="s1">X </span><span class="s4">= </span><span class="s1">_deprecate_Xt_in_inverse_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Xt</span><span class="s4">)</span>

        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">X </span><span class="s4">@ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">_n_features_out</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span>
            <span class="s6">&quot;requires_positive_X&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">,</span>
            <span class="s6">&quot;preserves_dtype&quot;</span><span class="s4">: [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">],</span>
        <span class="s4">}</span>


<span class="s3">class </span><span class="s1">NMF</span><span class="s4">(</span><span class="s1">_BaseNMF</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Non-Negative Matrix Factorization (NMF). 
 
    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) 
    whose product approximates the non-negative matrix X. This factorization can be used 
    for example for dimensionality reduction, source separation or topic extraction. 
 
    The objective function is: 
 
        .. math:: 
 
            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2 
 
            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1 
 
            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1 
 
            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2 
 
            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2 
 
    Where: 
 
    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) 
 
    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm) 
 
    The generic norm :math:`||X - WH||_{loss}` may represent 
    the Frobenius norm or another supported beta-divergence loss. 
    The choice between options is controlled by the `beta_loss` parameter. 
 
    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for 
    `H` to keep their impact balanced with respect to one another and to the data fit 
    term as independent as possible of the size `n_samples` of the training set. 
 
    The objective function is minimized with an alternating minimization of W 
    and H. 
 
    Note that the transformed data is named W and the components matrix is named H. In 
    the NMF literature, the naming convention is usually the opposite since the data 
    matrix X is transposed. 
 
    Read more in the :ref:`User Guide &lt;NMF&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int or {'auto'} or None, default=None 
        Number of components, if n_components is not set all features 
        are kept. 
        If `n_components='auto'`, the number of components is automatically inferred 
        from W or H shapes. 
 
        .. versionchanged:: 1.4 
            Added `'auto'` value. 
 
    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None 
        Method used to initialize the procedure. 
        Valid options: 
 
        - `None`: 'nndsvda' if n_components &lt;= min(n_samples, n_features), 
          otherwise random. 
 
        - `'random'`: non-negative random matrices, scaled with: 
          `sqrt(X.mean() / n_components)` 
 
        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD) 
          initialization (better for sparseness) 
 
        - `'nndsvda'`: NNDSVD with zeros filled with the average of X 
          (better when sparsity is not desired) 
 
        - `'nndsvdar'` NNDSVD with zeros filled with small random values 
          (generally faster, less accurate alternative to NNDSVDa 
          for when sparsity is not desired) 
 
        - `'custom'`: Use custom matrices `W` and `H` which must both be provided. 
 
        .. versionchanged:: 1.1 
            When `init=None` and n_components is less than n_samples and n_features 
            defaults to `nndsvda` instead of `nndsvd`. 
 
    solver : {'cd', 'mu'}, default='cd' 
        Numerical solver to use: 
 
        - 'cd' is a Coordinate Descent solver. 
        - 'mu' is a Multiplicative Update solver. 
 
        .. versionadded:: 0.17 
           Coordinate Descent solver. 
 
        .. versionadded:: 0.19 
           Multiplicative Update solver. 
 
    beta_loss : float or {'frobenius', 'kullback-leibler', \ 
            'itakura-saito'}, default='frobenius' 
        Beta divergence to be minimized, measuring the distance between X 
        and the dot product WH. Note that values different from 'frobenius' 
        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower 
        fits. Note that for beta_loss &lt;= 0 (or 'itakura-saito'), the input 
        matrix X cannot contain zeros. Used only in 'mu' solver. 
 
        .. versionadded:: 0.19 
 
    tol : float, default=1e-4 
        Tolerance of the stopping condition. 
 
    max_iter : int, default=200 
        Maximum number of iterations before timing out. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for initialisation (when ``init`` == 'nndsvdar' or 
        'random'), and in Coordinate Descent. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    alpha_W : float, default=0.0 
        Constant that multiplies the regularization terms of `W`. Set it to zero 
        (default) to have no regularization on `W`. 
 
        .. versionadded:: 1.0 
 
    alpha_H : float or &quot;same&quot;, default=&quot;same&quot; 
        Constant that multiplies the regularization terms of `H`. Set it to zero to 
        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as 
        `alpha_W`. 
 
        .. versionadded:: 1.0 
 
    l1_ratio : float, default=0.0 
        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. 
        For l1_ratio = 0 the penalty is an elementwise L2 penalty 
        (aka Frobenius Norm). 
        For l1_ratio = 1 it is an elementwise L1 penalty. 
        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2. 
 
        .. versionadded:: 0.17 
           Regularization parameter *l1_ratio* used in the Coordinate Descent 
           solver. 
 
    verbose : int, default=0 
        Whether to be verbose. 
 
    shuffle : bool, default=False 
        If true, randomize the order of coordinates in the CD solver. 
 
        .. versionadded:: 0.17 
           *shuffle* parameter used in the Coordinate Descent solver. 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        Factorization matrix, sometimes called 'dictionary'. 
 
    n_components_ : int 
        The number of components. It is same as the `n_components` parameter 
        if it was given. Otherwise, it will be same as the number of 
        features. 
 
    reconstruction_err_ : float 
        Frobenius norm of the matrix difference, or beta-divergence, between 
        the training data ``X`` and the reconstructed data ``WH`` from 
        the fitted model. 
 
    n_iter_ : int 
        Actual number of iterations. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    DictionaryLearning : Find a dictionary that sparsely encodes data. 
    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis. 
    PCA : Principal component analysis. 
    SparseCoder : Find a sparse representation of data from a fixed, 
        precomputed dictionary. 
    SparsePCA : Sparse Principal Components Analysis. 
    TruncatedSVD : Dimensionality reduction using truncated SVD. 
 
    References 
    ---------- 
    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor 
       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;` 
       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals 
       of electronics, communications and computer sciences 92.3: 708-721, 2009. 
 
    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the 
       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;` 
       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]) 
    &gt;&gt;&gt; from sklearn.decomposition import NMF 
    &gt;&gt;&gt; model = NMF(n_components=2, init='random', random_state=0) 
    &gt;&gt;&gt; W = model.fit_transform(X) 
    &gt;&gt;&gt; H = model.components_ 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">_BaseNMF</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s6">&quot;solver&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;mu&quot;</span><span class="s4">, </span><span class="s6">&quot;cd&quot;</span><span class="s4">})],</span>
        <span class="s6">&quot;shuffle&quot;</span><span class="s4">: [</span><span class="s6">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">n_components</span><span class="s4">=</span><span class="s6">&quot;warn&quot;</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">init</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">solver</span><span class="s4">=</span><span class="s6">&quot;cd&quot;</span><span class="s4">,</span>
        <span class="s1">beta_loss</span><span class="s4">=</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">,</span>
        <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
        <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">alpha_W</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">alpha_H</span><span class="s4">=</span><span class="s6">&quot;same&quot;</span><span class="s4">,</span>
        <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
        <span class="s1">shuffle</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">n_components</span><span class="s4">=</span><span class="s1">n_components</span><span class="s4">,</span>
            <span class="s1">init</span><span class="s4">=</span><span class="s1">init</span><span class="s4">,</span>
            <span class="s1">beta_loss</span><span class="s4">=</span><span class="s1">beta_loss</span><span class="s4">,</span>
            <span class="s1">tol</span><span class="s4">=</span><span class="s1">tol</span><span class="s4">,</span>
            <span class="s1">max_iter</span><span class="s4">=</span><span class="s1">max_iter</span><span class="s4">,</span>
            <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s1">alpha_W</span><span class="s4">=</span><span class="s1">alpha_W</span><span class="s4">,</span>
            <span class="s1">alpha_H</span><span class="s4">=</span><span class="s1">alpha_H</span><span class="s4">,</span>
            <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s1">l1_ratio</span><span class="s4">,</span>
            <span class="s1">verbose</span><span class="s4">=</span><span class="s1">verbose</span><span class="s4">,</span>
        <span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">solver </span><span class="s4">= </span><span class="s1">solver</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">shuffle </span><span class="s4">= </span><span class="s1">shuffle</span>

    <span class="s3">def </span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s2"># solver</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver </span><span class="s4">!= </span><span class="s6">&quot;mu&quot; </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_loss </span><span class="s3">not in </span><span class="s4">(</span><span class="s5">2</span><span class="s4">, </span><span class="s6">&quot;frobenius&quot;</span><span class="s4">):</span>
            <span class="s2"># 'mu' is the only solver that handles other beta losses than 'frobenius'</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s6">f&quot;Invalid beta_loss parameter: solver </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver</span><span class="s3">!r} </span><span class="s6">does not handle &quot;</span>
                <span class="s6">f&quot;beta_loss = </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">beta_loss</span><span class="s3">!r}</span><span class="s6">&quot;</span>
            <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver </span><span class="s4">== </span><span class="s6">&quot;mu&quot; </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">init </span><span class="s4">== </span><span class="s6">&quot;nndsvd&quot;</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s4">(</span>
                    <span class="s6">&quot;The multiplicative update ('mu') solver cannot update &quot;</span>
                    <span class="s6">&quot;zeros present in the initialization, and so leads to &quot;</span>
                    <span class="s6">&quot;poorer results when used jointly with init='nndsvd'. &quot;</span>
                    <span class="s6">&quot;You may try init='nndsvda' or init='nndsvdar' instead.&quot;</span>
                <span class="s4">),</span>
                <span class="s1">UserWarning</span><span class="s4">,</span>
            <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit_transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data. 
 
        This is more efficient than calling fit followed by transform. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : Ignored 
            Not used, present for API consistency by convention. 
 
        W : array-like of shape (n_samples, n_components), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `None`, uses the initialisation method specified in `init`. 
 
        H : array-like of shape (n_components, n_features), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `None`, uses the initialisation method specified in `init`. 
 
        Returns 
        ------- 
        W : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s6">&quot;csr&quot;</span><span class="s4">, </span><span class="s6">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">]</span>
        <span class="s4">)</span>

        <span class="s3">with </span><span class="s1">config_context</span><span class="s4">(</span><span class="s1">assume_finite</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
            <span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_fit_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s1">H</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">reconstruction_err_ </span><span class="s4">= </span><span class="s1">_beta_divergence</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">, </span><span class="s1">square_root</span><span class="s4">=</span><span class="s3">True</span>
        <span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_components_ </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">= </span><span class="s1">H</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_iter_ </span><span class="s4">= </span><span class="s1">n_iter</span>

        <span class="s3">return </span><span class="s1">W</span>

    <span class="s3">def </span><span class="s1">_fit_transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Data matrix to be decomposed 
 
        y : Ignored 
 
        W : array-like of shape (n_samples, n_components), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `update_H=False`, it is initialised as an array of zeros, unless 
            `solver='mu'`, then it is filled with values calculated by 
            `np.sqrt(X.mean() / self._n_components)`. 
            If `None`, uses the initialisation method specified in `init`. 
 
        H : array-like of shape (n_components, n_features), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `update_H=False`, it is used as a constant, to solve for W only. 
            If `None`, uses the initialisation method specified in `init`. 
 
        update_H : bool, default=True 
            If True, both W and H will be estimated from initial guesses, 
            this corresponds to a call to the 'fit_transform' method. 
            If False, only W will be estimated, this corresponds to a call 
            to the 'transform' method. 
 
        Returns 
        ------- 
        W : ndarray of shape (n_samples, n_components) 
            Transformed data. 
 
        H : ndarray of shape (n_components, n_features) 
            Factorization matrix, sometimes called 'dictionary'. 
 
        n_iter_ : int 
            Actual number of iterations. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s6">&quot;NMF (input X)&quot;</span><span class="s4">)</span>

        <span class="s2"># check parameters</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() == </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">&lt;= </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s6">&quot;When beta_loss &lt;= 0 and X contains zeros, &quot;</span>
                <span class="s6">&quot;the solver may diverge. Please add small values &quot;</span>
                <span class="s6">&quot;to X, or use a positive beta_loss.&quot;</span>
            <span class="s4">)</span>

        <span class="s2"># initialize or check W and H</span>
        <span class="s1">W</span><span class="s4">, </span><span class="s1">H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_w_h</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">)</span>

        <span class="s2"># scale the regularization terms</span>
        <span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">l1_reg_H</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">l2_reg_H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_compute_regularization</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver </span><span class="s4">== </span><span class="s6">&quot;cd&quot;</span><span class="s4">:</span>
            <span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter </span><span class="s4">= </span><span class="s1">_fit_coordinate_descent</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">,</span>
                <span class="s1">W</span><span class="s4">,</span>
                <span class="s1">H</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span><span class="s4">,</span>
                <span class="s1">l1_reg_W</span><span class="s4">,</span>
                <span class="s1">l1_reg_H</span><span class="s4">,</span>
                <span class="s1">l2_reg_W</span><span class="s4">,</span>
                <span class="s1">l2_reg_H</span><span class="s4">,</span>
                <span class="s1">update_H</span><span class="s4">=</span><span class="s1">update_H</span><span class="s4">,</span>
                <span class="s1">verbose</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">,</span>
                <span class="s1">shuffle</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">shuffle</span><span class="s4">,</span>
                <span class="s1">random_state</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver </span><span class="s4">== </span><span class="s6">&quot;mu&quot;</span><span class="s4">:</span>
            <span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter</span><span class="s4">, *</span><span class="s1">_ </span><span class="s4">= </span><span class="s1">_fit_multiplicative_update</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">,</span>
                <span class="s1">W</span><span class="s4">,</span>
                <span class="s1">H</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">,</span>
                <span class="s1">l1_reg_W</span><span class="s4">,</span>
                <span class="s1">l1_reg_H</span><span class="s4">,</span>
                <span class="s1">l2_reg_W</span><span class="s4">,</span>
                <span class="s1">l2_reg_H</span><span class="s4">,</span>
                <span class="s1">update_H</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">,</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Invalid solver parameter '%s'.&quot; </span><span class="s4">% </span><span class="s1">self</span><span class="s4">.</span><span class="s1">solver</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_iter </span><span class="s4">== </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s6">&quot;Maximum number of iterations %d reached. Increase &quot;</span>
                <span class="s6">&quot;it to improve convergence.&quot; </span><span class="s4">% </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span><span class="s4">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
            <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter</span>

    <span class="s3">def </span><span class="s1">transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Transform the data X according to the fitted NMF model. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        Returns 
        ------- 
        W : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s6">&quot;csr&quot;</span><span class="s4">, </span><span class="s6">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">], </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>

        <span class="s3">with </span><span class="s1">config_context</span><span class="s4">(</span><span class="s1">assume_finite</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
            <span class="s1">W</span><span class="s4">, *</span><span class="s1">_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_fit_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">W</span>


<span class="s3">class </span><span class="s1">MiniBatchNMF</span><span class="s4">(</span><span class="s1">_BaseNMF</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Mini-Batch Non-Negative Matrix Factorization (NMF). 
 
    .. versionadded:: 1.1 
 
    Find two non-negative matrices, i.e. matrices with all non-negative elements, 
    (`W`, `H`) whose product approximates the non-negative matrix `X`. This 
    factorization can be used for example for dimensionality reduction, source 
    separation or topic extraction. 
 
    The objective function is: 
 
        .. math:: 
 
            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2 
 
            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1 
 
            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1 
 
            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2 
 
            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2 
 
    Where: 
 
    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) 
 
    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm) 
 
    The generic norm :math:`||X - WH||_{loss}^2` may represent 
    the Frobenius norm or another supported beta-divergence loss. 
    The choice between options is controlled by the `beta_loss` parameter. 
 
    The objective function is minimized with an alternating minimization of `W` 
    and `H`. 
 
    Note that the transformed data is named `W` and the components matrix is 
    named `H`. In the NMF literature, the naming convention is usually the opposite 
    since the data matrix `X` is transposed. 
 
    Read more in the :ref:`User Guide &lt;MiniBatchNMF&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int or {'auto'} or None, default=None 
        Number of components, if `n_components` is not set all features 
        are kept. 
        If `n_components='auto'`, the number of components is automatically inferred 
        from W or H shapes. 
 
        .. versionchanged:: 1.4 
            Added `'auto'` value. 
 
    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None 
        Method used to initialize the procedure. 
        Valid options: 
 
        - `None`: 'nndsvda' if `n_components &lt;= min(n_samples, n_features)`, 
          otherwise random. 
 
        - `'random'`: non-negative random matrices, scaled with: 
          `sqrt(X.mean() / n_components)` 
 
        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD) 
          initialization (better for sparseness). 
 
        - `'nndsvda'`: NNDSVD with zeros filled with the average of X 
          (better when sparsity is not desired). 
 
        - `'nndsvdar'` NNDSVD with zeros filled with small random values 
          (generally faster, less accurate alternative to NNDSVDa 
          for when sparsity is not desired). 
 
        - `'custom'`: Use custom matrices `W` and `H` which must both be provided. 
 
    batch_size : int, default=1024 
        Number of samples in each mini-batch. Large batch sizes 
        give better long-term convergence at the cost of a slower start. 
 
    beta_loss : float or {'frobenius', 'kullback-leibler', \ 
            'itakura-saito'}, default='frobenius' 
        Beta divergence to be minimized, measuring the distance between `X` 
        and the dot product `WH`. Note that values different from 'frobenius' 
        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower 
        fits. Note that for `beta_loss &lt;= 0` (or 'itakura-saito'), the input 
        matrix `X` cannot contain zeros. 
 
    tol : float, default=1e-4 
        Control early stopping based on the norm of the differences in `H` 
        between 2 steps. To disable early stopping based on changes in `H`, set 
        `tol` to 0.0. 
 
    max_no_improvement : int, default=10 
        Control early stopping based on the consecutive number of mini batches 
        that does not yield an improvement on the smoothed cost function. 
        To disable convergence detection based on cost function, set 
        `max_no_improvement` to None. 
 
    max_iter : int, default=200 
        Maximum number of iterations over the complete dataset before 
        timing out. 
 
    alpha_W : float, default=0.0 
        Constant that multiplies the regularization terms of `W`. Set it to zero 
        (default) to have no regularization on `W`. 
 
    alpha_H : float or &quot;same&quot;, default=&quot;same&quot; 
        Constant that multiplies the regularization terms of `H`. Set it to zero to 
        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as 
        `alpha_W`. 
 
    l1_ratio : float, default=0.0 
        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1. 
        For l1_ratio = 0 the penalty is an elementwise L2 penalty 
        (aka Frobenius Norm). 
        For l1_ratio = 1 it is an elementwise L1 penalty. 
        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2. 
 
    forget_factor : float, default=0.7 
        Amount of rescaling of past information. Its value could be 1 with 
        finite datasets. Choosing values &lt; 1 is recommended with online 
        learning as more recent batches will weight more than past batches. 
 
    fresh_restarts : bool, default=False 
        Whether to completely solve for W at each step. Doing fresh restarts will likely 
        lead to a better solution for a same number of iterations but it is much slower. 
 
    fresh_restarts_max_iter : int, default=30 
        Maximum number of iterations when solving for W at each step. Only used when 
        doing fresh restarts. These iterations may be stopped early based on a small 
        change of W controlled by `tol`. 
 
    transform_max_iter : int, default=None 
        Maximum number of iterations when solving for W at transform time. 
        If None, it defaults to `max_iter`. 
 
    random_state : int, RandomState instance or None, default=None 
        Used for initialisation (when ``init`` == 'nndsvdar' or 
        'random'), and in Coordinate Descent. Pass an int for reproducible 
        results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    verbose : bool, default=False 
        Whether to be verbose. 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        Factorization matrix, sometimes called 'dictionary'. 
 
    n_components_ : int 
        The number of components. It is same as the `n_components` parameter 
        if it was given. Otherwise, it will be same as the number of 
        features. 
 
    reconstruction_err_ : float 
        Frobenius norm of the matrix difference, or beta-divergence, between 
        the training data `X` and the reconstructed data `WH` from 
        the fitted model. 
 
    n_iter_ : int 
        Actual number of started iterations over the whole dataset. 
 
    n_steps_ : int 
        Number of mini-batches processed. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
    See Also 
    -------- 
    NMF : Non-negative matrix factorization. 
    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent 
        data using a sparse code. 
 
    References 
    ---------- 
    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor 
       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;` 
       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals 
       of electronics, communications and computer sciences 92.3: 708-721, 2009. 
 
    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the 
       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;` 
       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9). 
 
    .. [3] :doi:`&quot;Online algorithms for nonnegative matrix factorization with the 
       Itakura-Saito divergence&quot; &lt;10.1109/ASPAA.2011.6082314&gt;` 
       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]) 
    &gt;&gt;&gt; from sklearn.decomposition import MiniBatchNMF 
    &gt;&gt;&gt; model = MiniBatchNMF(n_components=2, init='random', random_state=0) 
    &gt;&gt;&gt; W = model.fit_transform(X) 
    &gt;&gt;&gt; H = model.components_ 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">_BaseNMF</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s6">&quot;max_no_improvement&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s6">&quot;batch_size&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;forget_factor&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;fresh_restarts&quot;</span><span class="s4">: [</span><span class="s6">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s6">&quot;fresh_restarts_max_iter&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;transform_max_iter&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">n_components</span><span class="s4">=</span><span class="s6">&quot;warn&quot;</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">init</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">batch_size</span><span class="s4">=</span><span class="s5">1024</span><span class="s4">,</span>
        <span class="s1">beta_loss</span><span class="s4">=</span><span class="s6">&quot;frobenius&quot;</span><span class="s4">,</span>
        <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
        <span class="s1">max_no_improvement</span><span class="s4">=</span><span class="s5">10</span><span class="s4">,</span>
        <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
        <span class="s1">alpha_W</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">alpha_H</span><span class="s4">=</span><span class="s6">&quot;same&quot;</span><span class="s4">,</span>
        <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">forget_factor</span><span class="s4">=</span><span class="s5">0.7</span><span class="s4">,</span>
        <span class="s1">fresh_restarts</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
        <span class="s1">fresh_restarts_max_iter</span><span class="s4">=</span><span class="s5">30</span><span class="s4">,</span>
        <span class="s1">transform_max_iter</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">n_components</span><span class="s4">=</span><span class="s1">n_components</span><span class="s4">,</span>
            <span class="s1">init</span><span class="s4">=</span><span class="s1">init</span><span class="s4">,</span>
            <span class="s1">beta_loss</span><span class="s4">=</span><span class="s1">beta_loss</span><span class="s4">,</span>
            <span class="s1">tol</span><span class="s4">=</span><span class="s1">tol</span><span class="s4">,</span>
            <span class="s1">max_iter</span><span class="s4">=</span><span class="s1">max_iter</span><span class="s4">,</span>
            <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s1">alpha_W</span><span class="s4">=</span><span class="s1">alpha_W</span><span class="s4">,</span>
            <span class="s1">alpha_H</span><span class="s4">=</span><span class="s1">alpha_H</span><span class="s4">,</span>
            <span class="s1">l1_ratio</span><span class="s4">=</span><span class="s1">l1_ratio</span><span class="s4">,</span>
            <span class="s1">verbose</span><span class="s4">=</span><span class="s1">verbose</span><span class="s4">,</span>
        <span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_no_improvement </span><span class="s4">= </span><span class="s1">max_no_improvement</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">batch_size </span><span class="s4">= </span><span class="s1">batch_size</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">forget_factor </span><span class="s4">= </span><span class="s1">forget_factor</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">fresh_restarts </span><span class="s4">= </span><span class="s1">fresh_restarts</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">fresh_restarts_max_iter </span><span class="s4">= </span><span class="s1">fresh_restarts_max_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">transform_max_iter </span><span class="s4">= </span><span class="s1">transform_max_iter</span>

    <span class="s3">def </span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s2"># batch_size</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_batch_size </span><span class="s4">= </span><span class="s1">min</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">batch_size</span><span class="s4">, </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">])</span>

        <span class="s2"># forget_factor</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_rho </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">forget_factor </span><span class="s4">** (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_batch_size </span><span class="s4">/ </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">])</span>

        <span class="s2"># gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">&lt; </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_gamma </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ (</span><span class="s5">2.0 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">&gt; </span><span class="s5">2</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_gamma </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">- </span><span class="s5">1.0</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_gamma </span><span class="s4">= </span><span class="s5">1.0</span>

        <span class="s2"># transform_max_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_transform_max_iter </span><span class="s4">= (</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">transform_max_iter </span><span class="s3">is None</span>
            <span class="s3">else </span><span class="s1">self</span><span class="s4">.</span><span class="s1">transform_max_iter</span>
        <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_solve_W</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">max_iter</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Minimize the objective function w.r.t W. 
 
        Update W with H being fixed, until convergence. This is the heart 
        of `transform` but it's also used during `fit` when doing fresh restarts. 
        &quot;&quot;&quot;</span>
        <span class="s1">avg </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">() / </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">)</span>
        <span class="s1">W </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">((</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_n_components</span><span class="s4">), </span><span class="s1">avg</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s1">W_buffer </span><span class="s4">= </span><span class="s1">W</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>

        <span class="s2"># Get scaled regularization terms. Done for each minibatch to take into account</span>
        <span class="s2"># variable sizes of minibatches.</span>
        <span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_compute_regularization</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">max_iter</span><span class="s4">):</span>
            <span class="s1">W</span><span class="s4">, *</span><span class="s1">_ </span><span class="s4">= </span><span class="s1">_multiplicative_update_w</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">, </span><span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_gamma</span>
            <span class="s4">)</span>

            <span class="s1">W_diff </span><span class="s4">= </span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">W </span><span class="s4">- </span><span class="s1">W_buffer</span><span class="s4">) / </span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">W</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol </span><span class="s4">&gt; </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">W_diff </span><span class="s4">&lt;= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">:</span>
                <span class="s3">break</span>

            <span class="s1">W_buffer</span><span class="s4">[:] = </span><span class="s1">W</span>

        <span class="s3">return </span><span class="s1">W</span>

    <span class="s3">def </span><span class="s1">_minibatch_step</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Perform the update of W and H for one minibatch.&quot;&quot;&quot;</span>
        <span class="s1">batch_size </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

        <span class="s2"># get scaled regularization terms. Done for each minibatch to take into account</span>
        <span class="s2"># variable sizes of minibatches.</span>
        <span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">l1_reg_H</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">l2_reg_H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_compute_regularization</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s2"># update W</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">fresh_restarts </span><span class="s3">or </span><span class="s1">W </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">W </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_solve_W</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">fresh_restarts_max_iter</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">W</span><span class="s4">, *</span><span class="s1">_ </span><span class="s4">= </span><span class="s1">_multiplicative_update_w</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">, </span><span class="s1">l1_reg_W</span><span class="s4">, </span><span class="s1">l2_reg_W</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_gamma</span>
            <span class="s4">)</span>

        <span class="s2"># necessary for stability with beta_loss &lt; 1</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">&lt; </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">W</span><span class="s4">[</span><span class="s1">W </span><span class="s4">&lt; </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">).</span><span class="s1">eps</span><span class="s4">] = </span><span class="s5">0.0</span>

        <span class="s1">batch_cost </span><span class="s4">= (</span>
            <span class="s1">_beta_divergence</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">)</span>
            <span class="s4">+ </span><span class="s1">l1_reg_W </span><span class="s4">* </span><span class="s1">W</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
            <span class="s4">+ </span><span class="s1">l1_reg_H </span><span class="s4">* </span><span class="s1">H</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
            <span class="s4">+ </span><span class="s1">l2_reg_W </span><span class="s4">* (</span><span class="s1">W</span><span class="s4">**</span><span class="s5">2</span><span class="s4">).</span><span class="s1">sum</span><span class="s4">()</span>
            <span class="s4">+ </span><span class="s1">l2_reg_H </span><span class="s4">* (</span><span class="s1">H</span><span class="s4">**</span><span class="s5">2</span><span class="s4">).</span><span class="s1">sum</span><span class="s4">()</span>
        <span class="s4">) / </span><span class="s1">batch_size</span>

        <span class="s2"># update H (only at fit or fit_transform)</span>
        <span class="s3">if </span><span class="s1">update_H</span><span class="s4">:</span>
            <span class="s1">H</span><span class="s4">[:] = </span><span class="s1">_multiplicative_update_h</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">,</span>
                <span class="s1">W</span><span class="s4">,</span>
                <span class="s1">H</span><span class="s4">,</span>
                <span class="s1">beta_loss</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">,</span>
                <span class="s1">l1_reg_H</span><span class="s4">=</span><span class="s1">l1_reg_H</span><span class="s4">,</span>
                <span class="s1">l2_reg_H</span><span class="s4">=</span><span class="s1">l2_reg_H</span><span class="s4">,</span>
                <span class="s1">gamma</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_gamma</span><span class="s4">,</span>
                <span class="s1">A</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_components_numerator</span><span class="s4">,</span>
                <span class="s1">B</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_components_denominator</span><span class="s4">,</span>
                <span class="s1">rho</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_rho</span><span class="s4">,</span>
            <span class="s4">)</span>

            <span class="s2"># necessary for stability with beta_loss &lt; 1</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">&lt;= </span><span class="s5">1</span><span class="s4">:</span>
                <span class="s1">H</span><span class="s4">[</span><span class="s1">H </span><span class="s4">&lt; </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">).</span><span class="s1">eps</span><span class="s4">] = </span><span class="s5">0.0</span>

        <span class="s3">return </span><span class="s1">batch_cost</span>

    <span class="s3">def </span><span class="s1">_minibatch_convergence</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">batch_cost</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">H_buffer</span><span class="s4">, </span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">step</span><span class="s4">, </span><span class="s1">n_steps</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Helper function to encapsulate the early stopping logic&quot;&quot;&quot;</span>
        <span class="s1">batch_size </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

        <span class="s2"># counts steps starting from 1 for user friendly verbose mode.</span>
        <span class="s1">step </span><span class="s4">= </span><span class="s1">step </span><span class="s4">+ </span><span class="s5">1</span>

        <span class="s2"># Ignore first iteration because H is not updated yet.</span>
        <span class="s3">if </span><span class="s1">step </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;Minibatch step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s6">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s6">: mean batch cost: </span><span class="s3">{</span><span class="s1">batch_cost</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
            <span class="s3">return False</span>

        <span class="s2"># Compute an Exponentially Weighted Average of the cost function to</span>
        <span class="s2"># monitor the convergence while discarding minibatch-local stochastic</span>
        <span class="s2"># variability: https://en.wikipedia.org/wiki/Moving_average</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost </span><span class="s4">= </span><span class="s1">batch_cost</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">alpha </span><span class="s4">= </span><span class="s1">batch_size </span><span class="s4">/ (</span><span class="s1">n_samples </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">)</span>
            <span class="s1">alpha </span><span class="s4">= </span><span class="s1">min</span><span class="s4">(</span><span class="s1">alpha</span><span class="s4">, </span><span class="s5">1</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost </span><span class="s4">* (</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">alpha</span><span class="s4">) + </span><span class="s1">batch_cost </span><span class="s4">* </span><span class="s1">alpha</span>

        <span class="s2"># Log progress to be able to monitor convergence</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span>
                <span class="s6">f&quot;Minibatch step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s6">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s6">: mean batch cost: &quot;</span>
                <span class="s6">f&quot;</span><span class="s3">{</span><span class="s1">batch_cost</span><span class="s3">}</span><span class="s6">, ewa cost: </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost</span><span class="s3">}</span><span class="s6">&quot;</span>
            <span class="s4">)</span>

        <span class="s2"># Early stopping based on change of H</span>
        <span class="s1">H_diff </span><span class="s4">= </span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">H </span><span class="s4">- </span><span class="s1">H_buffer</span><span class="s4">) / </span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">H</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol </span><span class="s4">&gt; </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">H_diff </span><span class="s4">&lt;= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;Converged (small H change) at step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s6">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
            <span class="s3">return True</span>

        <span class="s2"># Early stopping heuristic due to lack of improvement on smoothed</span>
        <span class="s2"># cost function</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost_min </span><span class="s3">is None or </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost </span><span class="s4">&lt; </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost_min</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_no_improvement </span><span class="s4">= </span><span class="s5">0</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost_min </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_no_improvement </span><span class="s4">+= </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">max_no_improvement </span><span class="s3">is not None</span>
            <span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_no_improvement </span><span class="s4">&gt;= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_no_improvement</span>
        <span class="s4">):</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">&quot;Converged (lack of improvement in objective function) &quot;</span>
                    <span class="s6">f&quot;at step </span><span class="s3">{</span><span class="s1">step</span><span class="s3">}</span><span class="s6">/</span><span class="s3">{</span><span class="s1">n_steps</span><span class="s3">}</span><span class="s6">&quot;</span>
                <span class="s4">)</span>
            <span class="s3">return True</span>

        <span class="s3">return False</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit_transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data. 
 
        This is more efficient than calling fit followed by transform. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Data matrix to be decomposed. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        W : array-like of shape (n_samples, n_components), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `None`, uses the initialisation method specified in `init`. 
 
        H : array-like of shape (n_components, n_features), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `None`, uses the initialisation method specified in `init`. 
 
        Returns 
        ------- 
        W : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s6">&quot;csr&quot;</span><span class="s4">, </span><span class="s6">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">]</span>
        <span class="s4">)</span>

        <span class="s3">with </span><span class="s1">config_context</span><span class="s4">(</span><span class="s1">assume_finite</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
            <span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter</span><span class="s4">, </span><span class="s1">n_steps </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_fit_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s1">H</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">reconstruction_err_ </span><span class="s4">= </span><span class="s1">_beta_divergence</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss</span><span class="s4">, </span><span class="s1">square_root</span><span class="s4">=</span><span class="s3">True</span>
        <span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_components_ </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">= </span><span class="s1">H</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_iter_ </span><span class="s4">= </span><span class="s1">n_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_steps_ </span><span class="s4">= </span><span class="s1">n_steps</span>

        <span class="s3">return </span><span class="s1">W</span>

    <span class="s3">def </span><span class="s1">_fit_transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Learn a NMF model for the data X and returns the transformed data. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            Data matrix to be decomposed. 
 
        W : array-like of shape (n_samples, n_components), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `update_H=False`, it is initialised as an array of zeros, unless 
            `solver='mu'`, then it is filled with values calculated by 
            `np.sqrt(X.mean() / self._n_components)`. 
            If `None`, uses the initialisation method specified in `init`. 
 
        H : array-like of shape (n_components, n_features), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            If `update_H=False`, it is used as a constant, to solve for W only. 
            If `None`, uses the initialisation method specified in `init`. 
 
        update_H : bool, default=True 
            If True, both W and H will be estimated from initial guesses, 
            this corresponds to a call to the `fit_transform` method. 
            If False, only W will be estimated, this corresponds to a call 
            to the `transform` method. 
 
        Returns 
        ------- 
        W : ndarray of shape (n_samples, n_components) 
            Transformed data. 
 
        H : ndarray of shape (n_components, n_features) 
            Factorization matrix, sometimes called 'dictionary'. 
 
        n_iter : int 
            Actual number of started iterations over the whole dataset. 
 
        n_steps : int 
            Number of mini-batches processed. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s6">&quot;MiniBatchNMF (input X)&quot;</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() == </span><span class="s5">0 </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_beta_loss </span><span class="s4">&lt;= </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s6">&quot;When beta_loss &lt;= 0 and X contains zeros, &quot;</span>
                <span class="s6">&quot;the solver may diverge. Please add small values &quot;</span>
                <span class="s6">&quot;to X, or use a positive beta_loss.&quot;</span>
            <span class="s4">)</span>

        <span class="s1">n_samples </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

        <span class="s2"># initialize or check W and H</span>
        <span class="s1">W</span><span class="s4">, </span><span class="s1">H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_w_h</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">)</span>
        <span class="s1">H_buffer </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>

        <span class="s2"># Initialize auxiliary matrices</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_components_numerator </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_components_denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">(</span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">H</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>

        <span class="s2"># Attributes to monitor the convergence</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost </span><span class="s4">= </span><span class="s3">None</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_ewa_cost_min </span><span class="s4">= </span><span class="s3">None</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_no_improvement </span><span class="s4">= </span><span class="s5">0</span>

        <span class="s1">batches </span><span class="s4">= </span><span class="s1">gen_batches</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_batch_size</span><span class="s4">)</span>
        <span class="s1">batches </span><span class="s4">= </span><span class="s1">itertools</span><span class="s4">.</span><span class="s1">cycle</span><span class="s4">(</span><span class="s1">batches</span><span class="s4">)</span>
        <span class="s1">n_steps_per_iter </span><span class="s4">= </span><span class="s1">int</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">ceil</span><span class="s4">(</span><span class="s1">n_samples </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_batch_size</span><span class="s4">))</span>
        <span class="s1">n_steps </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s4">* </span><span class="s1">n_steps_per_iter</span>

        <span class="s3">for </span><span class="s1">i</span><span class="s4">, </span><span class="s1">batch </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">range</span><span class="s4">(</span><span class="s1">n_steps</span><span class="s4">), </span><span class="s1">batches</span><span class="s4">):</span>
            <span class="s1">batch_cost </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_minibatch_step</span><span class="s4">(</span><span class="s1">X</span><span class="s4">[</span><span class="s1">batch</span><span class="s4">], </span><span class="s1">W</span><span class="s4">[</span><span class="s1">batch</span><span class="s4">], </span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">)</span>

            <span class="s3">if </span><span class="s1">update_H </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_minibatch_convergence</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">[</span><span class="s1">batch</span><span class="s4">], </span><span class="s1">batch_cost</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">H_buffer</span><span class="s4">, </span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">i</span><span class="s4">, </span><span class="s1">n_steps</span>
            <span class="s4">):</span>
                <span class="s3">break</span>

            <span class="s1">H_buffer</span><span class="s4">[:] = </span><span class="s1">H</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">fresh_restarts</span><span class="s4">:</span>
            <span class="s1">W </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_solve_W</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_transform_max_iter</span><span class="s4">)</span>

        <span class="s1">n_steps </span><span class="s4">= </span><span class="s1">i </span><span class="s4">+ </span><span class="s5">1</span>
        <span class="s1">n_iter </span><span class="s4">= </span><span class="s1">int</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">ceil</span><span class="s4">(</span><span class="s1">n_steps </span><span class="s4">/ </span><span class="s1">n_steps_per_iter</span><span class="s4">))</span>

        <span class="s3">if </span><span class="s1">n_iter </span><span class="s4">== </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s4">(</span>
                    <span class="s6">f&quot;Maximum number of iterations </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span><span class="s3">} </span><span class="s6">reached. &quot;</span>
                    <span class="s6">&quot;Increase it to improve convergence.&quot;</span>
                <span class="s4">),</span>
                <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
            <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">n_iter</span><span class="s4">, </span><span class="s1">n_steps</span>

    <span class="s3">def </span><span class="s1">transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Transform the data X according to the fitted MiniBatchNMF model. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Data matrix to be transformed by the model. 
 
        Returns 
        ------- 
        W : ndarray of shape (n_samples, n_components) 
            Transformed data. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s6">&quot;csr&quot;</span><span class="s4">, </span><span class="s6">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">], </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>

        <span class="s1">W </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_solve_W</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_transform_max_iter</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">W</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">partial_fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Update the model using the data in `X` as a mini-batch. 
 
        This method is expected to be called several times consecutively 
        on different chunks of a dataset so as to implement out-of-core 
        or online learning. 
 
        This is especially useful when the whole dataset is too big to fit in 
        memory at once (see :ref:`scaling_strategies`). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Data matrix to be decomposed. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        W : array-like of shape (n_samples, n_components), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            Only used for the first call to `partial_fit`. 
 
        H : array-like of shape (n_components, n_features), default=None 
            If `init='custom'`, it is used as initial guess for the solution. 
            Only used for the first call to `partial_fit`. 
 
        Returns 
        ------- 
        self 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">has_components </span><span class="s4">= </span><span class="s1">hasattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s6">&quot;components_&quot;</span><span class="s4">)</span>

        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s6">&quot;csr&quot;</span><span class="s4">, </span><span class="s6">&quot;csc&quot;</span><span class="s4">),</span>
            <span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">],</span>
            <span class="s1">reset</span><span class="s4">=</span><span class="s3">not </span><span class="s1">has_components</span><span class="s4">,</span>
        <span class="s4">)</span>

        <span class="s3">if not </span><span class="s1">has_components</span><span class="s4">:</span>
            <span class="s2"># This instance has not been fitted yet (fit or partial_fit)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_check_params</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
            <span class="s1">_</span><span class="s4">, </span><span class="s1">H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_w_h</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">W</span><span class="s4">=</span><span class="s1">W</span><span class="s4">, </span><span class="s1">H</span><span class="s4">=</span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>

            <span class="s1">self</span><span class="s4">.</span><span class="s1">_components_numerator </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_components_denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">(</span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">H</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">n_steps_ </span><span class="s4">= </span><span class="s5">0</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">H </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">_minibatch_step</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">H</span><span class="s4">, </span><span class="s1">update_H</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_components_ </span><span class="s4">= </span><span class="s1">H</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">= </span><span class="s1">H</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_steps_ </span><span class="s4">+= </span><span class="s5">1</span>

        <span class="s3">return </span><span class="s1">self</span>
</pre>
</body>
</html>