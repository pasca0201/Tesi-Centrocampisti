<html>
<head>
<title>test_gradient_boosting.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #2aacb8;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #6aab73;}
.s6 { color: #7a7e85;}
.s7 { color: #a5c261;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gradient_boosting.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">copyreg</span>
<span class="s0">import </span><span class="s1">io</span>
<span class="s0">import </span><span class="s1">pickle</span>
<span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">unittest</span><span class="s2">.</span><span class="s1">mock </span><span class="s0">import </span><span class="s1">Mock</span>

<span class="s0">import </span><span class="s1">joblib</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">joblib</span><span class="s2">.</span><span class="s1">numpy_pickle </span><span class="s0">import </span><span class="s1">NumpyPickler</span>
<span class="s0">from </span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">assert_array_equal</span>

<span class="s0">import </span><span class="s1">sklearn</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">_loss</span><span class="s2">.</span><span class="s1">loss </span><span class="s0">import </span><span class="s2">(</span>
    <span class="s1">AbsoluteError</span><span class="s2">,</span>
    <span class="s1">HalfBinomialLoss</span><span class="s2">,</span>
    <span class="s1">HalfSquaredError</span><span class="s2">,</span>
    <span class="s1">PinballLoss</span><span class="s2">,</span>
<span class="s2">)</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">base </span><span class="s0">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">clone</span><span class="s2">, </span><span class="s1">is_regressor</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">compose </span><span class="s0">import </span><span class="s1">make_column_transformer</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">datasets </span><span class="s0">import </span><span class="s1">make_classification</span><span class="s2">, </span><span class="s1">make_low_rank_matrix</span><span class="s2">, </span><span class="s1">make_regression</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">dummy </span><span class="s0">import </span><span class="s1">DummyRegressor</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">ensemble </span><span class="s0">import </span><span class="s2">(</span>
    <span class="s1">HistGradientBoostingClassifier</span><span class="s2">,</span>
    <span class="s1">HistGradientBoostingRegressor</span><span class="s2">,</span>
<span class="s2">)</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">ensemble</span><span class="s2">.</span><span class="s1">_hist_gradient_boosting</span><span class="s2">.</span><span class="s1">binning </span><span class="s0">import </span><span class="s1">_BinMapper</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">ensemble</span><span class="s2">.</span><span class="s1">_hist_gradient_boosting</span><span class="s2">.</span><span class="s1">common </span><span class="s0">import </span><span class="s1">G_H_DTYPE</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">ensemble</span><span class="s2">.</span><span class="s1">_hist_gradient_boosting</span><span class="s2">.</span><span class="s1">grower </span><span class="s0">import </span><span class="s1">TreeGrower</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">ensemble</span><span class="s2">.</span><span class="s1">_hist_gradient_boosting</span><span class="s2">.</span><span class="s1">predictor </span><span class="s0">import </span><span class="s1">TreePredictor</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">metrics </span><span class="s0">import </span><span class="s1">get_scorer</span><span class="s2">, </span><span class="s1">mean_gamma_deviance</span><span class="s2">, </span><span class="s1">mean_poisson_deviance</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">model_selection </span><span class="s0">import </span><span class="s1">cross_val_score</span><span class="s2">, </span><span class="s1">train_test_split</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">preprocessing </span><span class="s0">import </span><span class="s1">KBinsDiscretizer</span><span class="s2">, </span><span class="s1">MinMaxScaler</span><span class="s2">, </span><span class="s1">OneHotEncoder</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">utils </span><span class="s0">import </span><span class="s1">shuffle</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">utils</span><span class="s2">.</span><span class="s1">_openmp_helpers </span><span class="s0">import </span><span class="s1">_openmp_effective_n_threads</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">utils</span><span class="s2">.</span><span class="s1">_testing </span><span class="s0">import </span><span class="s1">_convert_container</span>
<span class="s0">from </span><span class="s1">sklearn</span><span class="s2">.</span><span class="s1">utils</span><span class="s2">.</span><span class="s1">fixes </span><span class="s0">import </span><span class="s1">_IS_32BIT</span>

<span class="s1">n_threads </span><span class="s2">= </span><span class="s1">_openmp_effective_n_threads</span><span class="s2">()</span>

<span class="s1">X_classification</span><span class="s2">, </span><span class="s1">y_classification </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
<span class="s1">X_regression</span><span class="s2">, </span><span class="s1">y_regression </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
<span class="s1">X_multi_classification</span><span class="s2">, </span><span class="s1">y_multi_classification </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span>
    <span class="s1">n_classes</span><span class="s2">=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">n_informative</span><span class="s2">=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span>
<span class="s2">)</span>


<span class="s0">def </span><span class="s1">_make_dumb_dataset</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Make a dumb dataset to test early stopping.&quot;&quot;&quot;</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">X_dumb </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randn</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">y_dumb </span><span class="s2">= (</span><span class="s1">X_dumb</span><span class="s2">[:, </span><span class="s3">0</span><span class="s2">] &gt; </span><span class="s3">0</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s5">&quot;int64&quot;</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">X_dumb</span><span class="s2">, </span><span class="s1">y_dumb</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">X_classification</span><span class="s2">, </span><span class="s1">y_classification</span><span class="s2">),</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">, </span><span class="s1">X_regression</span><span class="s2">, </span><span class="s1">y_regression</span><span class="s2">),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;params, err_msg&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span>
            <span class="s2">{</span><span class="s5">&quot;interaction_cst&quot;</span><span class="s2">: [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]},</span>
            <span class="s5">&quot;Interaction constraints must be a sequence of tuples or lists&quot;</span><span class="s2">,</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s2">{</span><span class="s5">&quot;interaction_cst&quot;</span><span class="s2">: [{</span><span class="s3">0</span><span class="s2">, </span><span class="s3">9999</span><span class="s2">}]},</span>
            <span class="s5">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span>
            <span class="s5">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span><span class="s2">,</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s2">{</span><span class="s5">&quot;interaction_cst&quot;</span><span class="s2">: [{-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">}]},</span>
            <span class="s5">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span>
            <span class="s5">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span><span class="s2">,</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s2">{</span><span class="s5">&quot;interaction_cst&quot;</span><span class="s2">: [{</span><span class="s3">0.5</span><span class="s2">}]},</span>
            <span class="s5">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span>
            <span class="s5">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span><span class="s2">,</span>
        <span class="s2">),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_init_parameters_validation</span><span class="s2">(</span><span class="s1">GradientBoosting</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">err_msg</span><span class="s2">):</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">err_msg</span><span class="s2">):</span>
        <span class="s1">GradientBoosting</span><span class="s2">(**</span><span class="s1">params</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;scoring, validation_fraction, early_stopping, n_iter_no_change, tol&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span><span class="s5">&quot;neg_mean_squared_error&quot;</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-7</span><span class="s2">),  </span><span class="s6"># use scorer</span>
        <span class="s2">(</span><span class="s5">&quot;neg_mean_squared_error&quot;</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-1</span><span class="s2">),  </span><span class="s6"># use scorer on train</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-7</span><span class="s2">),  </span><span class="s6"># same with default scorer</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-1</span><span class="s2">),</span>
        <span class="s2">(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-7</span><span class="s2">),  </span><span class="s6"># use loss</span>
        <span class="s2">(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-1</span><span class="s2">),  </span><span class="s6"># use loss on training data</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">False</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">),  </span><span class="s6"># no early stopping</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping_regression</span><span class="s2">(</span>
    <span class="s1">scoring</span><span class="s2">, </span><span class="s1">validation_fraction</span><span class="s2">, </span><span class="s1">early_stopping</span><span class="s2">, </span><span class="s1">n_iter_no_change</span><span class="s2">, </span><span class="s1">tol</span>
<span class="s2">):</span>
    <span class="s1">max_iter </span><span class="s2">= </span><span class="s3">200</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>

    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">verbose</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,  </span><span class="s6"># just for coverage</span>
        <span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">5</span><span class="s2">,  </span><span class="s6"># easier to overfit fast</span>
        <span class="s1">scoring</span><span class="s2">=</span><span class="s1">scoring</span><span class="s2">,</span>
        <span class="s1">tol</span><span class="s2">=</span><span class="s1">tol</span><span class="s2">,</span>
        <span class="s1">early_stopping</span><span class="s2">=</span><span class="s1">early_stopping</span><span class="s2">,</span>
        <span class="s1">validation_fraction</span><span class="s2">=</span><span class="s1">validation_fraction</span><span class="s2">,</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s1">max_iter</span><span class="s2">,</span>
        <span class="s1">n_iter_no_change</span><span class="s2">=</span><span class="s1">n_iter_no_change</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">early_stopping</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s1">n_iter_no_change </span><span class="s2">&lt;= </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_ </span><span class="s2">&lt; </span><span class="s1">max_iter</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_ </span><span class="s2">== </span><span class="s1">max_iter</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;data&quot;</span><span class="s2">,</span>
    <span class="s2">(</span>
        <span class="s1">make_classification</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s3">30</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">),</span>
        <span class="s1">make_classification</span><span class="s2">(</span>
            <span class="s1">n_samples</span><span class="s2">=</span><span class="s3">30</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">n_clusters_per_class</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span>
        <span class="s2">),</span>
    <span class="s2">),</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;scoring, validation_fraction, early_stopping, n_iter_no_change, tol&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-7</span><span class="s2">),  </span><span class="s6"># use scorer</span>
        <span class="s2">(</span><span class="s5">&quot;accuracy&quot;</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-1</span><span class="s2">),  </span><span class="s6"># use scorer on training data</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-7</span><span class="s2">),  </span><span class="s6"># same with default scorer</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-1</span><span class="s2">),</span>
        <span class="s2">(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-7</span><span class="s2">),  </span><span class="s6"># use loss</span>
        <span class="s2">(</span><span class="s5">&quot;loss&quot;</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">1e-1</span><span class="s2">),  </span><span class="s6"># use loss on training data</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">False</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">),  </span><span class="s6"># no early stopping</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping_classification</span><span class="s2">(</span>
    <span class="s1">data</span><span class="s2">, </span><span class="s1">scoring</span><span class="s2">, </span><span class="s1">validation_fraction</span><span class="s2">, </span><span class="s1">early_stopping</span><span class="s2">, </span><span class="s1">n_iter_no_change</span><span class="s2">, </span><span class="s1">tol</span>
<span class="s2">):</span>
    <span class="s1">max_iter </span><span class="s2">= </span><span class="s3">50</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">data</span>

    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">verbose</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,  </span><span class="s6"># just for coverage</span>
        <span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">5</span><span class="s2">,  </span><span class="s6"># easier to overfit fast</span>
        <span class="s1">scoring</span><span class="s2">=</span><span class="s1">scoring</span><span class="s2">,</span>
        <span class="s1">tol</span><span class="s2">=</span><span class="s1">tol</span><span class="s2">,</span>
        <span class="s1">early_stopping</span><span class="s2">=</span><span class="s1">early_stopping</span><span class="s2">,</span>
        <span class="s1">validation_fraction</span><span class="s2">=</span><span class="s1">validation_fraction</span><span class="s2">,</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s1">max_iter</span><span class="s2">,</span>
        <span class="s1">n_iter_no_change</span><span class="s2">=</span><span class="s1">n_iter_no_change</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">early_stopping </span><span class="s0">is True</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s1">n_iter_no_change </span><span class="s2">&lt;= </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_ </span><span class="s2">&lt; </span><span class="s1">max_iter</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_ </span><span class="s2">== </span><span class="s1">max_iter</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;GradientBoosting, X, y&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, *</span><span class="s1">_make_dumb_dataset</span><span class="s2">(</span><span class="s3">10000</span><span class="s2">)),</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, *</span><span class="s1">_make_dumb_dataset</span><span class="s2">(</span><span class="s3">10001</span><span class="s2">)),</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">, *</span><span class="s1">_make_dumb_dataset</span><span class="s2">(</span><span class="s3">10000</span><span class="s2">)),</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">, *</span><span class="s1">_make_dumb_dataset</span><span class="s2">(</span><span class="s3">10001</span><span class="s2">)),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping_default</span><span class="s2">(</span><span class="s1">GradientBoosting</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">):</span>
    <span class="s6"># Test that early stopping is enabled by default if and only if there</span>
    <span class="s6"># are more than 10000 samples</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">GradientBoosting</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">n_iter_no_change</span><span class="s2">=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">=</span><span class="s3">1e-1</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">] &gt; </span><span class="s3">10000</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_ </span><span class="s2">&lt; </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">max_iter</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_ </span><span class="s2">== </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">max_iter</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;scores, n_iter_no_change, tol, stopping&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">([], </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0.001</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># not enough iterations</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">], </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.001</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># not enough iterations</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">], </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.001</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># not enough iterations</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">], </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.001</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># significant improvement</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">], </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># significant improvement</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">], </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.999</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># significant improvement</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">], </span><span class="s3">5</span><span class="s2">, </span><span class="s3">5 </span><span class="s2">- </span><span class="s3">1e-5</span><span class="s2">, </span><span class="s0">False</span><span class="s2">),  </span><span class="s6"># significant improvement</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">] * </span><span class="s3">6</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">, </span><span class="s0">True</span><span class="s2">),  </span><span class="s6"># no significant improvement</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">] * </span><span class="s3">6</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">0.001</span><span class="s2">, </span><span class="s0">True</span><span class="s2">),  </span><span class="s6"># no significant improvement</span>
        <span class="s2">([</span><span class="s3">1</span><span class="s2">] * </span><span class="s3">6</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s0">True</span><span class="s2">),  </span><span class="s6"># no significant improvement</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_should_stop</span><span class="s2">(</span><span class="s1">scores</span><span class="s2">, </span><span class="s1">n_iter_no_change</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">stopping</span><span class="s2">):</span>
    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">n_iter_no_change</span><span class="s2">=</span><span class="s1">n_iter_no_change</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">=</span><span class="s1">tol</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">gbdt</span><span class="s2">.</span><span class="s1">_should_stop</span><span class="s2">(</span><span class="s1">scores</span><span class="s2">) == </span><span class="s1">stopping</span>


<span class="s0">def </span><span class="s1">test_absolute_error</span><span class="s2">():</span>
    <span class="s6"># For coverage only.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s3">500</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;absolute_error&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">gbdt</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">) &gt; </span><span class="s3">0.9</span>


<span class="s0">def </span><span class="s1">test_absolute_error_sample_weight</span><span class="s2">():</span>
    <span class="s6"># non regression test for issue #19400</span>
    <span class="s6"># make sure no error is thrown during fit of</span>
    <span class="s6"># HistGradientBoostingRegressor with absolute_error loss function</span>
    <span class="s6"># and passing sample_weight</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">100</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">uniform</span><span class="s2">(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s3">2</span><span class="s2">))</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">uniform</span><span class="s2">(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">uniform</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;absolute_error&quot;</span><span class="s2">)</span>
    <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;y&quot;</span><span class="s2">, [([</span><span class="s3">1.0</span><span class="s2">, -</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">]), ([</span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">2.0</span><span class="s2">])])</span>
<span class="s0">def </span><span class="s1">test_gamma_y_positive</span><span class="s2">(</span><span class="s1">y</span><span class="s2">):</span>
    <span class="s6"># Test that ValueError is raised if any y_i &lt;= 0.</span>
    <span class="s1">err_msg </span><span class="s2">= </span><span class="s5">r&quot;loss='gamma' requires strictly positive y.&quot;</span>
    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;gamma&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">err_msg</span><span class="s2">):</span>
        <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">y</span><span class="s2">), </span><span class="s3">1</span><span class="s2">)), </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_gamma</span><span class="s2">():</span>
    <span class="s6"># For a Gamma distributed target, we expect an HGBT trained with the Gamma deviance</span>
    <span class="s6"># (loss) to give better results than an HGBT with any other loss function, measured</span>
    <span class="s6"># in out-of-sample Gamma deviance as metric/score.</span>
    <span class="s6"># Note that squared error could potentially predict negative values which is</span>
    <span class="s6"># invalid (np.inf) for the Gamma deviance. A Poisson HGBT (having a log link)</span>
    <span class="s6"># does not have that defect.</span>
    <span class="s6"># Important note: It seems that a Poisson HGBT almost always has better</span>
    <span class="s6"># out-of-sample performance than the Gamma HGBT, measured in Gamma deviance.</span>
    <span class="s6"># LightGBM shows the same behaviour. Hence, we only compare to a squared error</span>
    <span class="s6"># HGBT, but not to a Poisson deviance HGBT.</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">n_train</span><span class="s2">, </span><span class="s1">n_test</span><span class="s2">, </span><span class="s1">n_features </span><span class="s2">= </span><span class="s3">500</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s3">20</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">make_low_rank_matrix</span><span class="s2">(</span>
        <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_train </span><span class="s2">+ </span><span class="s1">n_test</span><span class="s2">,</span>
        <span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s6"># We create a log-linear Gamma model. This gives y.min ~ 1e-2, y.max ~ 1e2</span>
    <span class="s1">coef </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">uniform</span><span class="s2">(</span><span class="s1">low</span><span class="s2">=-</span><span class="s3">10</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">)</span>
    <span class="s6"># Numpy parametrizes gamma(shape=k, scale=theta) with mean = k * theta and</span>
    <span class="s6"># variance = k * theta^2. We parametrize it instead with mean = exp(X @ coef)</span>
    <span class="s6"># and variance = dispersion * mean^2 by setting k = 1 / dispersion,</span>
    <span class="s6"># theta =  dispersion * mean.</span>
    <span class="s1">dispersion </span><span class="s2">= </span><span class="s3">0.5</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">gamma</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=</span><span class="s3">1 </span><span class="s2">/ </span><span class="s1">dispersion</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s1">dispersion </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">exp</span><span class="s2">(</span><span class="s1">X </span><span class="s2">@ </span><span class="s1">coef</span><span class="s2">))</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test </span><span class="s2">= </span><span class="s1">train_test_split</span><span class="s2">(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size</span><span class="s2">=</span><span class="s1">n_test</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span>
    <span class="s2">)</span>
    <span class="s1">gbdt_gamma </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;gamma&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">123</span><span class="s2">)</span>
    <span class="s1">gbdt_mse </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">123</span><span class="s2">)</span>
    <span class="s1">dummy </span><span class="s2">= </span><span class="s1">DummyRegressor</span><span class="s2">(</span><span class="s1">strategy</span><span class="s2">=</span><span class="s5">&quot;mean&quot;</span><span class="s2">)</span>
    <span class="s0">for </span><span class="s1">model </span><span class="s0">in </span><span class="s2">(</span><span class="s1">gbdt_gamma</span><span class="s2">, </span><span class="s1">gbdt_mse</span><span class="s2">, </span><span class="s1">dummy</span><span class="s2">):</span>
        <span class="s1">model</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s0">for </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s0">in </span><span class="s2">[(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">), (</span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">)]:</span>
        <span class="s1">loss_gbdt_gamma </span><span class="s2">= </span><span class="s1">mean_gamma_deviance</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">gbdt_gamma</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>
        <span class="s6"># We restrict the squared error HGBT to predict at least the minimum seen y at</span>
        <span class="s6"># train time to make it strictly positive.</span>
        <span class="s1">loss_gbdt_mse </span><span class="s2">= </span><span class="s1">mean_gamma_deviance</span><span class="s2">(</span>
            <span class="s1">y</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">min</span><span class="s2">(</span><span class="s1">y_train</span><span class="s2">), </span><span class="s1">gbdt_mse</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>
        <span class="s2">)</span>
        <span class="s1">loss_dummy </span><span class="s2">= </span><span class="s1">mean_gamma_deviance</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">dummy</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>
        <span class="s0">assert </span><span class="s1">loss_gbdt_gamma </span><span class="s2">&lt; </span><span class="s1">loss_dummy</span>
        <span class="s0">assert </span><span class="s1">loss_gbdt_gamma </span><span class="s2">&lt; </span><span class="s1">loss_gbdt_mse</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;quantile&quot;</span><span class="s2">, [</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.8</span><span class="s2">])</span>
<span class="s0">def </span><span class="s1">test_quantile_asymmetric_error</span><span class="s2">(</span><span class="s1">quantile</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Test quantile regression for asymmetric distributed targets.&quot;&quot;&quot;</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">10_000</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s6"># take care that X @ coef + intercept &gt; 0</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(</span>
        <span class="s2">(</span>
            <span class="s1">np</span><span class="s2">.</span><span class="s1">abs</span><span class="s2">(</span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randn</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">)[:, </span><span class="s0">None</span><span class="s2">]),</span>
            <span class="s2">-</span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)),</span>
        <span class="s2">),</span>
        <span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s1">intercept </span><span class="s2">= </span><span class="s3">1.23</span>
    <span class="s1">coef </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s3">0.5</span><span class="s2">, -</span><span class="s3">2</span><span class="s2">])</span>
    <span class="s6"># For an exponential distribution with rate lambda, e.g. exp(-lambda * x),</span>
    <span class="s6"># the quantile at level q is:</span>
    <span class="s6">#   quantile(q) = - log(1 - q) / lambda</span>
    <span class="s6">#   scale = 1/lambda = -quantile(q) / log(1-q)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">exponential</span><span class="s2">(</span>
        <span class="s1">scale</span><span class="s2">=-(</span><span class="s1">X </span><span class="s2">@ </span><span class="s1">coef </span><span class="s2">+ </span><span class="s1">intercept</span><span class="s2">) / </span><span class="s1">np</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s3">1 </span><span class="s2">- </span><span class="s1">quantile</span><span class="s2">), </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span>
    <span class="s2">)</span>
    <span class="s1">model </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;quantile&quot;</span><span class="s2">,</span>
        <span class="s1">quantile</span><span class="s2">=</span><span class="s1">quantile</span><span class="s2">,</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">25</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s2">=</span><span class="s3">10</span><span class="s2">,</span>
    <span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">model</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">) &gt; </span><span class="s1">y</span><span class="s2">), </span><span class="s1">quantile</span><span class="s2">, </span><span class="s1">rtol</span><span class="s2">=</span><span class="s3">1e-2</span><span class="s2">)</span>

    <span class="s1">pinball_loss </span><span class="s2">= </span><span class="s1">PinballLoss</span><span class="s2">(</span><span class="s1">quantile</span><span class="s2">=</span><span class="s1">quantile</span><span class="s2">)</span>
    <span class="s1">loss_true_quantile </span><span class="s2">= </span><span class="s1">pinball_loss</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">X </span><span class="s2">@ </span><span class="s1">coef </span><span class="s2">+ </span><span class="s1">intercept</span><span class="s2">)</span>
    <span class="s1">loss_pred_quantile </span><span class="s2">= </span><span class="s1">pinball_loss</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">model</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>
    <span class="s6"># we are overfitting</span>
    <span class="s0">assert </span><span class="s1">loss_pred_quantile </span><span class="s2">&lt;= </span><span class="s1">loss_true_quantile</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;y&quot;</span><span class="s2">, [([</span><span class="s3">1.0</span><span class="s2">, -</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">]), ([</span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">, </span><span class="s3">0.0</span><span class="s2">])])</span>
<span class="s0">def </span><span class="s1">test_poisson_y_positive</span><span class="s2">(</span><span class="s1">y</span><span class="s2">):</span>
    <span class="s6"># Test that ValueError is raised if either one y_i &lt; 0 or sum(y_i) &lt;= 0.</span>
    <span class="s1">err_msg </span><span class="s2">= </span><span class="s5">r&quot;loss='poisson' requires non-negative y and sum\(y\) &gt; 0.&quot;</span>
    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">err_msg</span><span class="s2">):</span>
        <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">y</span><span class="s2">), </span><span class="s3">1</span><span class="s2">)), </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_poisson</span><span class="s2">():</span>
    <span class="s6"># For Poisson distributed target, Poisson loss should give better results</span>
    <span class="s6"># than least squares measured in Poisson deviance as metric.</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">n_train</span><span class="s2">, </span><span class="s1">n_test</span><span class="s2">, </span><span class="s1">n_features </span><span class="s2">= </span><span class="s3">500</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s3">100</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">make_low_rank_matrix</span><span class="s2">(</span>
        <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_train </span><span class="s2">+ </span><span class="s1">n_test</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span>
    <span class="s2">)</span>
    <span class="s6"># We create a log-linear Poisson model and downscale coef as it will get</span>
    <span class="s6"># exponentiated.</span>
    <span class="s1">coef </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">uniform</span><span class="s2">(</span><span class="s1">low</span><span class="s2">=-</span><span class="s3">2</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">) / </span><span class="s1">np</span><span class="s2">.</span><span class="s1">max</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">poisson</span><span class="s2">(</span><span class="s1">lam</span><span class="s2">=</span><span class="s1">np</span><span class="s2">.</span><span class="s1">exp</span><span class="s2">(</span><span class="s1">X </span><span class="s2">@ </span><span class="s1">coef</span><span class="s2">))</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test </span><span class="s2">= </span><span class="s1">train_test_split</span><span class="s2">(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size</span><span class="s2">=</span><span class="s1">n_test</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span>
    <span class="s2">)</span>
    <span class="s1">gbdt_pois </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">)</span>
    <span class="s1">gbdt_ls </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;squared_error&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">)</span>
    <span class="s1">gbdt_pois</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>
    <span class="s1">gbdt_ls</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>
    <span class="s1">dummy </span><span class="s2">= </span><span class="s1">DummyRegressor</span><span class="s2">(</span><span class="s1">strategy</span><span class="s2">=</span><span class="s5">&quot;mean&quot;</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s0">for </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s0">in </span><span class="s2">[(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">), (</span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">)]:</span>
        <span class="s1">metric_pois </span><span class="s2">= </span><span class="s1">mean_poisson_deviance</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">gbdt_pois</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>
        <span class="s6"># squared_error might produce non-positive predictions =&gt; clip</span>
        <span class="s1">metric_ls </span><span class="s2">= </span><span class="s1">mean_poisson_deviance</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">clip</span><span class="s2">(</span><span class="s1">gbdt_ls</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">), </span><span class="s3">1e-15</span><span class="s2">, </span><span class="s0">None</span><span class="s2">))</span>
        <span class="s1">metric_dummy </span><span class="s2">= </span><span class="s1">mean_poisson_deviance</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s1">dummy</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>
        <span class="s0">assert </span><span class="s1">metric_pois </span><span class="s2">&lt; </span><span class="s1">metric_ls</span>
        <span class="s0">assert </span><span class="s1">metric_pois </span><span class="s2">&lt; </span><span class="s1">metric_dummy</span>


<span class="s0">def </span><span class="s1">test_binning_train_validation_are_separated</span><span class="s2">():</span>
    <span class="s6"># Make sure training and validation data are binned separately.</span>
    <span class="s6"># See issue 13926</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">validation_fraction </span><span class="s2">= </span><span class="s3">0.2</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">early_stopping</span><span class="s2">=</span><span class="s0">True</span><span class="s2">, </span><span class="s1">validation_fraction</span><span class="s2">=</span><span class="s1">validation_fraction</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span>
    <span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_classification</span><span class="s2">, </span><span class="s1">y_classification</span><span class="s2">)</span>
    <span class="s1">mapper_training_data </span><span class="s2">= </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">_bin_mapper</span>

    <span class="s6"># Note that since the data is small there is no subsampling and the</span>
    <span class="s6"># random_state doesn't matter</span>
    <span class="s1">mapper_whole_data </span><span class="s2">= </span><span class="s1">_BinMapper</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">mapper_whole_data</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_classification</span><span class="s2">)</span>

    <span class="s1">n_samples </span><span class="s2">= </span><span class="s1">X_classification</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
    <span class="s0">assert </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span>
        <span class="s1">mapper_training_data</span><span class="s2">.</span><span class="s1">n_bins_non_missing_</span>
        <span class="s2">== </span><span class="s1">int</span><span class="s2">((</span><span class="s3">1 </span><span class="s2">- </span><span class="s1">validation_fraction</span><span class="s2">) * </span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span>
        <span class="s1">mapper_training_data</span><span class="s2">.</span><span class="s1">n_bins_non_missing_</span>
        <span class="s2">!= </span><span class="s1">mapper_whole_data</span><span class="s2">.</span><span class="s1">n_bins_non_missing_</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_missing_values_trivial</span><span class="s2">():</span>
    <span class="s6"># sanity check for missing values support. With only one feature and</span>
    <span class="s6"># y == isnan(X), the gbdt is supposed to reach perfect accuracy on the</span>
    <span class="s6"># training set.</span>

    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">100</span>
    <span class="s1">n_features </span><span class="s2">= </span><span class="s3">1</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>

    <span class="s1">X </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">normal</span><span class="s2">(</span><span class="s1">size</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">))</span>
    <span class="s1">mask </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">binomial</span><span class="s2">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">bool</span><span class="s2">)</span>
    <span class="s1">X</span><span class="s2">[</span><span class="s1">mask</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">mask</span><span class="s2">.</span><span class="s1">ravel</span><span class="s2">()</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">()</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">) == </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s3">1</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;problem&quot;</span><span class="s2">, (</span><span class="s5">&quot;classification&quot;</span><span class="s2">, </span><span class="s5">&quot;regression&quot;</span><span class="s2">))</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s2">(</span>
        <span class="s5">&quot;missing_proportion, expected_min_score_classification, &quot;</span>
        <span class="s5">&quot;expected_min_score_regression&quot;</span>
    <span class="s2">),</span>
    <span class="s2">[(</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.97</span><span class="s2">, </span><span class="s3">0.89</span><span class="s2">), (</span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">0.93</span><span class="s2">, </span><span class="s3">0.81</span><span class="s2">), (</span><span class="s3">0.5</span><span class="s2">, </span><span class="s3">0.79</span><span class="s2">, </span><span class="s3">0.52</span><span class="s2">)],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_missing_values_resilience</span><span class="s2">(</span>
    <span class="s1">problem</span><span class="s2">,</span>
    <span class="s1">missing_proportion</span><span class="s2">,</span>
    <span class="s1">expected_min_score_classification</span><span class="s2">,</span>
    <span class="s1">expected_min_score_regression</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s6"># Make sure the estimators can deal with missing values and still yield</span>
    <span class="s6"># decent predictions</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">1000</span>
    <span class="s1">n_features </span><span class="s2">= </span><span class="s3">2</span>
    <span class="s0">if </span><span class="s1">problem </span><span class="s2">== </span><span class="s5">&quot;regression&quot;</span><span class="s2">:</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span>
            <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">,</span>
            <span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">n_informative</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">()</span>
        <span class="s1">expected_min_score </span><span class="s2">= </span><span class="s1">expected_min_score_regression</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span>
            <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">,</span>
            <span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">n_informative</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">n_redundant</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
            <span class="s1">n_repeated</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
            <span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">()</span>
        <span class="s1">expected_min_score </span><span class="s2">= </span><span class="s1">expected_min_score_classification</span>

    <span class="s1">mask </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">binomial</span><span class="s2">(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">missing_proportion</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">bool</span><span class="s2">)</span>
    <span class="s1">X</span><span class="s2">[</span><span class="s1">mask</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">) &gt; </span><span class="s1">expected_min_score</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;data&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">=</span><span class="s3">2</span><span class="s2">),</span>
        <span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_classes</span><span class="s2">=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">n_informative</span><span class="s2">=</span><span class="s3">3</span><span class="s2">),</span>
    <span class="s2">],</span>
    <span class="s1">ids</span><span class="s2">=[</span><span class="s5">&quot;binary_log_loss&quot;</span><span class="s2">, </span><span class="s5">&quot;multiclass_log_loss&quot;</span><span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_zero_division_hessians</span><span class="s2">(</span><span class="s1">data</span><span class="s2">):</span>
    <span class="s6"># non regression test for issue #14018</span>
    <span class="s6"># make sure we avoid zero division errors when computing the leaves values.</span>

    <span class="s6"># If the learning rate is too high, the raw predictions are bad and will</span>
    <span class="s6"># saturate the softmax (or sigmoid in binary classif). This leads to</span>
    <span class="s6"># probabilities being exactly 0 or 1, gradients being constant, and</span>
    <span class="s6"># hessians being zero.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">data</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">10</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_small_trainset</span><span class="s2">():</span>
    <span class="s6"># Make sure that the small trainset is stratified and has the expected</span>
    <span class="s6"># length (10k samples)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">20000</span>
    <span class="s1">original_distrib </span><span class="s2">= {</span><span class="s3">0</span><span class="s2">: </span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">: </span><span class="s3">0.2</span><span class="s2">, </span><span class="s3">2</span><span class="s2">: </span><span class="s3">0.3</span><span class="s2">, </span><span class="s3">3</span><span class="s2">: </span><span class="s3">0.4</span><span class="s2">}</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randn</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">).</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= [</span>
        <span class="s2">[</span><span class="s1">class_</span><span class="s2">] * </span><span class="s1">int</span><span class="s2">(</span><span class="s1">prop </span><span class="s2">* </span><span class="s1">n_samples</span><span class="s2">) </span><span class="s0">for </span><span class="s2">(</span><span class="s1">class_</span><span class="s2">, </span><span class="s1">prop</span><span class="s2">) </span><span class="s0">in </span><span class="s1">original_distrib</span><span class="s2">.</span><span class="s1">items</span><span class="s2">()</span>
    <span class="s2">]</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">shuffle</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">(</span><span class="s1">y</span><span class="s2">))</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">()</span>

    <span class="s6"># Compute the small training set</span>
    <span class="s1">X_small</span><span class="s2">, </span><span class="s1">y_small</span><span class="s2">, *</span><span class="s1">_ </span><span class="s2">= </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">_get_small_trainset</span><span class="s2">(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">seed</span><span class="s2">=</span><span class="s3">42</span><span class="s2">, </span><span class="s1">sample_weight_train</span><span class="s2">=</span><span class="s0">None</span>
    <span class="s2">)</span>

    <span class="s6"># Compute the class distribution in the small training set</span>
    <span class="s1">unique</span><span class="s2">, </span><span class="s1">counts </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">unique</span><span class="s2">(</span><span class="s1">y_small</span><span class="s2">, </span><span class="s1">return_counts</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
    <span class="s1">small_distrib </span><span class="s2">= {</span><span class="s1">class_</span><span class="s2">: </span><span class="s1">count </span><span class="s2">/ </span><span class="s3">10000 </span><span class="s0">for </span><span class="s2">(</span><span class="s1">class_</span><span class="s2">, </span><span class="s1">count</span><span class="s2">) </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">unique</span><span class="s2">, </span><span class="s1">counts</span><span class="s2">)}</span>

    <span class="s6"># Test that the small training set has the expected length</span>
    <span class="s0">assert </span><span class="s1">X_small</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">] == </span><span class="s3">10000</span>
    <span class="s0">assert </span><span class="s1">y_small</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">] == </span><span class="s3">10000</span>

    <span class="s6"># Test that the class distributions in the whole dataset and in the small</span>
    <span class="s6"># training set are identical</span>
    <span class="s0">assert </span><span class="s1">small_distrib </span><span class="s2">== </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">original_distrib</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_missing_values_minmax_imputation</span><span class="s2">():</span>
    <span class="s6"># Compare the buit-in missing value handling of Histogram GBC with an</span>
    <span class="s6"># a-priori missing value imputation strategy that should yield the same</span>
    <span class="s6"># results in terms of decision function.</span>
    <span class="s6">#</span>
    <span class="s6"># Each feature (containing NaNs) is replaced by 2 features:</span>
    <span class="s6"># - one where the nans are replaced by min(feature) - 1</span>
    <span class="s6"># - one where the nans are replaced by max(feature) + 1</span>
    <span class="s6"># A split where nans go to the left has an equivalent split in the</span>
    <span class="s6"># first (min) feature, and a split where nans go to the right has an</span>
    <span class="s6"># equivalent split in the second (max) feature.</span>
    <span class="s6">#</span>
    <span class="s6"># Assuming the data is such that there is never a tie to select the best</span>
    <span class="s6"># feature to split on during training, the learned decision trees should be</span>
    <span class="s6"># strictly equivalent (learn a sequence of splits that encode the same</span>
    <span class="s6"># decision function).</span>
    <span class="s6">#</span>
    <span class="s6"># The MinMaxImputer transformer is meant to be a toy implementation of the</span>
    <span class="s6"># &quot;Missing In Attributes&quot; (MIA) missing value handling for decision trees</span>
    <span class="s6"># https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305</span>
    <span class="s6"># The implementation of MIA as an imputation transformer was suggested by</span>
    <span class="s6"># &quot;Remark 3&quot; in :arxiv:'&lt;1902.06931&gt;`</span>

    <span class="s0">class </span><span class="s1">MinMaxImputer</span><span class="s2">(</span><span class="s1">TransformerMixin</span><span class="s2">, </span><span class="s1">BaseEstimator</span><span class="s2">):</span>
        <span class="s0">def </span><span class="s1">fit</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
            <span class="s1">mm </span><span class="s2">= </span><span class="s1">MinMaxScaler</span><span class="s2">().</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">data_min_ </span><span class="s2">= </span><span class="s1">mm</span><span class="s2">.</span><span class="s1">data_min_</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">data_max_ </span><span class="s2">= </span><span class="s1">mm</span><span class="s2">.</span><span class="s1">data_max_</span>
            <span class="s0">return </span><span class="s1">self</span>

        <span class="s0">def </span><span class="s1">transform</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">):</span>
            <span class="s1">X_min</span><span class="s2">, </span><span class="s1">X_max </span><span class="s2">= </span><span class="s1">X</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">(), </span><span class="s1">X</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">()</span>

            <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]):</span>
                <span class="s1">nan_mask </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">isnan</span><span class="s2">(</span><span class="s1">X</span><span class="s2">[:, </span><span class="s1">feature_idx</span><span class="s2">])</span>
                <span class="s1">X_min</span><span class="s2">[</span><span class="s1">nan_mask</span><span class="s2">, </span><span class="s1">feature_idx</span><span class="s2">] = </span><span class="s1">self</span><span class="s2">.</span><span class="s1">data_min_</span><span class="s2">[</span><span class="s1">feature_idx</span><span class="s2">] - </span><span class="s3">1</span>
                <span class="s1">X_max</span><span class="s2">[</span><span class="s1">nan_mask</span><span class="s2">, </span><span class="s1">feature_idx</span><span class="s2">] = </span><span class="s1">self</span><span class="s2">.</span><span class="s1">data_max_</span><span class="s2">[</span><span class="s1">feature_idx</span><span class="s2">] + </span><span class="s3">1</span>

            <span class="s0">return </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">([</span><span class="s1">X_min</span><span class="s2">, </span><span class="s1">X_max</span><span class="s2">], </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">make_missing_value_data</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s1">int</span><span class="s2">(</span><span class="s3">1e4</span><span class="s2">), </span><span class="s1">seed</span><span class="s2">=</span><span class="s3">0</span><span class="s2">):</span>
        <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s1">seed</span><span class="s2">)</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">)</span>

        <span class="s6"># Pre-bin the data to ensure a deterministic handling by the 2</span>
        <span class="s6"># strategies and also make it easier to insert np.nan in a structured</span>
        <span class="s6"># way:</span>
        <span class="s1">X </span><span class="s2">= </span><span class="s1">KBinsDiscretizer</span><span class="s2">(</span><span class="s1">n_bins</span><span class="s2">=</span><span class="s3">42</span><span class="s2">, </span><span class="s1">encode</span><span class="s2">=</span><span class="s5">&quot;ordinal&quot;</span><span class="s2">).</span><span class="s1">fit_transform</span><span class="s2">(</span><span class="s1">X</span><span class="s2">)</span>

        <span class="s6"># First feature has missing values completely at random:</span>
        <span class="s1">rnd_mask </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">rand</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]) &gt; </span><span class="s3">0.9</span>
        <span class="s1">X</span><span class="s2">[</span><span class="s1">rnd_mask</span><span class="s2">, </span><span class="s3">0</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

        <span class="s6"># Second and third features have missing values for extreme values</span>
        <span class="s6"># (censoring missingness):</span>
        <span class="s1">low_mask </span><span class="s2">= </span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">] == </span><span class="s3">0</span>
        <span class="s1">X</span><span class="s2">[</span><span class="s1">low_mask</span><span class="s2">, </span><span class="s3">1</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

        <span class="s1">high_mask </span><span class="s2">= </span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">2</span><span class="s2">] == </span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">2</span><span class="s2">].</span><span class="s1">max</span><span class="s2">()</span>
        <span class="s1">X</span><span class="s2">[</span><span class="s1">high_mask</span><span class="s2">, </span><span class="s3">2</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

        <span class="s6"># Make the last feature nan pattern very informative:</span>
        <span class="s1">y_max </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">percentile</span><span class="s2">(</span><span class="s1">y</span><span class="s2">, </span><span class="s3">70</span><span class="s2">)</span>
        <span class="s1">y_max_mask </span><span class="s2">= </span><span class="s1">y </span><span class="s2">&gt;= </span><span class="s1">y_max</span>
        <span class="s1">y</span><span class="s2">[</span><span class="s1">y_max_mask</span><span class="s2">] = </span><span class="s1">y_max</span>
        <span class="s1">X</span><span class="s2">[</span><span class="s1">y_max_mask</span><span class="s2">, </span><span class="s3">3</span><span class="s2">] = </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span>

        <span class="s6"># Check that there is at least one missing value in each feature:</span>
        <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]):</span>
            <span class="s0">assert </span><span class="s1">any</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">isnan</span><span class="s2">(</span><span class="s1">X</span><span class="s2">[:, </span><span class="s1">feature_idx</span><span class="s2">]))</span>

        <span class="s6"># Let's use a test set to check that the learned decision function is</span>
        <span class="s6"># the same as evaluated on unseen data. Otherwise it could just be the</span>
        <span class="s6"># case that we find two independent ways to overfit the training set.</span>
        <span class="s0">return </span><span class="s1">train_test_split</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">)</span>

    <span class="s6"># n_samples need to be large enough to minimize the likelihood of having</span>
    <span class="s6"># several candidate splits with the same gain value in a given tree.</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test </span><span class="s2">= </span><span class="s1">make_missing_value_data</span><span class="s2">(</span>
        <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">int</span><span class="s2">(</span><span class="s3">1e4</span><span class="s2">), </span><span class="s1">seed</span><span class="s2">=</span><span class="s3">0</span>
    <span class="s2">)</span>

    <span class="s6"># Use a small number of leaf nodes and iterations so as to keep</span>
    <span class="s6"># under-fitting models to minimize the likelihood of ties when training the</span>
    <span class="s6"># model.</span>
    <span class="s1">gbm1 </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">max_leaf_nodes</span><span class="s2">=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">gbm1</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s1">gbm2 </span><span class="s2">= </span><span class="s1">make_pipeline</span><span class="s2">(</span><span class="s1">MinMaxImputer</span><span class="s2">(), </span><span class="s1">clone</span><span class="s2">(</span><span class="s1">gbm1</span><span class="s2">))</span>
    <span class="s1">gbm2</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s6"># Check that the model reach the same score:</span>
    <span class="s0">assert </span><span class="s1">gbm1</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">) == </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">gbm2</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">))</span>

    <span class="s0">assert </span><span class="s1">gbm1</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">) == </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">gbm2</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">))</span>

    <span class="s6"># Check the individual prediction match as a finer grained</span>
    <span class="s6"># decision function check.</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">gbm1</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">), </span><span class="s1">gbm2</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">))</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">gbm1</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">), </span><span class="s1">gbm2</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">))</span>


<span class="s0">def </span><span class="s1">test_infinite_values</span><span class="s2">():</span>
    <span class="s6"># Basic test for infinite values</span>

    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([-</span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span><span class="s2">]).</span><span class="s1">reshape</span><span class="s2">(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">])</span>

    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">np</span><span class="s2">.</span><span class="s1">testing</span><span class="s2">.</span><span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">gbdt</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">), </span><span class="s1">y</span><span class="s2">, </span><span class="s1">atol</span><span class="s2">=</span><span class="s3">1e-4</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_consistent_lengths</span><span class="s2">():</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([-</span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span><span class="s2">]).</span><span class="s1">reshape</span><span class="s2">(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">])</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.3</span><span class="s2">, </span><span class="s3">0.1</span><span class="s2">])</span>
    <span class="s1">gbdt </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">()</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s5">r&quot;sample_weight.shape == \(3,\), expected&quot;</span><span class="s2">):</span>
        <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">)</span>

    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span>
        <span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s5">&quot;Found input variables with inconsistent number&quot;</span>
    <span class="s2">):</span>
        <span class="s1">gbdt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">[</span><span class="s3">1</span><span class="s2">:])</span>


<span class="s0">def </span><span class="s1">test_infinite_values_missing_values</span><span class="s2">():</span>
    <span class="s6"># High level test making sure that inf and nan values are properly handled</span>
    <span class="s6"># when both are present. This is similar to</span>
    <span class="s6"># test_split_on_nan_with_infinite_values() in test_grower.py, though we</span>
    <span class="s6"># cannot check the predictions for binned values here.</span>

    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">([-</span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">]).</span><span class="s1">reshape</span><span class="s2">(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">y_isnan </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">isnan</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">ravel</span><span class="s2">())</span>
    <span class="s1">y_isinf </span><span class="s2">= </span><span class="s1">X</span><span class="s2">.</span><span class="s1">ravel</span><span class="s2">() == </span><span class="s1">np</span><span class="s2">.</span><span class="s1">inf</span>

    <span class="s1">stump_clf </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">2</span>
    <span class="s2">)</span>

    <span class="s0">assert </span><span class="s1">stump_clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y_isinf</span><span class="s2">).</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y_isinf</span><span class="s2">) == </span><span class="s3">1</span>
    <span class="s0">assert </span><span class="s1">stump_clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y_isnan</span><span class="s2">).</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y_isnan</span><span class="s2">) == </span><span class="s3">1</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;scoring&quot;</span><span class="s2">, [</span><span class="s0">None</span><span class="s2">, </span><span class="s5">&quot;loss&quot;</span><span class="s2">])</span>
<span class="s0">def </span><span class="s1">test_string_target_early_stopping</span><span class="s2">(</span><span class="s1">scoring</span><span class="s2">):</span>
    <span class="s6"># Regression tests for #14709 where the targets need to be encoded before</span>
    <span class="s6"># to compute the score</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randn</span><span class="s2">(</span><span class="s3">100</span><span class="s2">, </span><span class="s3">10</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s5">&quot;x&quot;</span><span class="s2">] * </span><span class="s3">50 </span><span class="s2">+ [</span><span class="s5">&quot;y&quot;</span><span class="s2">] * </span><span class="s3">50</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">object</span><span class="s2">)</span>
    <span class="s1">gbrt </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">n_iter_no_change</span><span class="s2">=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">scoring</span><span class="s2">=</span><span class="s1">scoring</span><span class="s2">)</span>
    <span class="s1">gbrt</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_zero_sample_weights_regression</span><span class="s2">():</span>
    <span class="s6"># Make sure setting a SW to zero amounts to ignoring the corresponding</span>
    <span class="s6"># sample</span>

    <span class="s1">X </span><span class="s2">= [[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]]</span>
    <span class="s1">y </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]</span>
    <span class="s6"># ignore the first 2 training samples by setting their weight to 0</span>
    <span class="s1">sample_weight </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]])[</span><span class="s3">0</span><span class="s2">] &gt; </span><span class="s3">0.5</span>


<span class="s0">def </span><span class="s1">test_zero_sample_weights_classification</span><span class="s2">():</span>
    <span class="s6"># Make sure setting a SW to zero amounts to ignoring the corresponding</span>
    <span class="s6"># sample</span>

    <span class="s1">X </span><span class="s2">= [[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]]</span>
    <span class="s1">y </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]</span>
    <span class="s6"># ignore the first 2 training samples by setting their weight to 0</span>
    <span class="s1">sample_weight </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;log_loss&quot;</span><span class="s2">, </span><span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>
    <span class="s1">assert_array_equal</span><span class="s2">(</span><span class="s1">gb</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]]), [</span><span class="s3">1</span><span class="s2">])</span>

    <span class="s1">X </span><span class="s2">= [[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">], [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">], [</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]]</span>
    <span class="s1">y </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">]</span>
    <span class="s6"># ignore the first 2 training samples by setting their weight to 0</span>
    <span class="s1">sample_weight </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s5">&quot;log_loss&quot;</span><span class="s2">, </span><span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>
    <span class="s1">assert_array_equal</span><span class="s2">(</span><span class="s1">gb</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]]), [</span><span class="s3">1</span><span class="s2">])</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;problem&quot;</span><span class="s2">, (</span><span class="s5">&quot;regression&quot;</span><span class="s2">, </span><span class="s5">&quot;binary_classification&quot;</span><span class="s2">, </span><span class="s5">&quot;multiclass_classification&quot;</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;duplication&quot;</span><span class="s2">, (</span><span class="s5">&quot;half&quot;</span><span class="s2">, </span><span class="s5">&quot;all&quot;</span><span class="s2">))</span>
<span class="s0">def </span><span class="s1">test_sample_weight_effect</span><span class="s2">(</span><span class="s1">problem</span><span class="s2">, </span><span class="s1">duplication</span><span class="s2">):</span>
    <span class="s6"># High level test to make sure that duplicating a sample is equivalent to</span>
    <span class="s6"># giving it weight of 2.</span>

    <span class="s6"># fails for n_samples &gt; 255 because binning does not take sample weights</span>
    <span class="s6"># into account. Keeping n_samples &lt;= 255 makes</span>
    <span class="s6"># sure only unique values are used so SW have no effect on binning.</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">255</span>
    <span class="s1">n_features </span><span class="s2">= </span><span class="s3">2</span>
    <span class="s0">if </span><span class="s1">problem </span><span class="s2">== </span><span class="s5">&quot;regression&quot;</span><span class="s2">:</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span>
            <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">,</span>
            <span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">n_informative</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">Klass </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">n_classes </span><span class="s2">= </span><span class="s3">2 </span><span class="s0">if </span><span class="s1">problem </span><span class="s2">== </span><span class="s5">&quot;binary_classification&quot; </span><span class="s0">else </span><span class="s3">3</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span>
            <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">,</span>
            <span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">n_informative</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
            <span class="s1">n_redundant</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
            <span class="s1">n_clusters_per_class</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
            <span class="s1">n_classes</span><span class="s2">=</span><span class="s1">n_classes</span><span class="s2">,</span>
            <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">Klass </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span>

    <span class="s6"># This test can't pass if min_samples_leaf &gt; 1 because that would force 2</span>
    <span class="s6"># samples to be in the same node in est_sw, while these samples would be</span>
    <span class="s6"># free to be separate in est_dup: est_dup would just group together the</span>
    <span class="s6"># duplicated samples.</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">Klass</span><span class="s2">(</span><span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>

    <span class="s6"># Create dataset with duplicate and corresponding sample weights</span>
    <span class="s0">if </span><span class="s1">duplication </span><span class="s2">== </span><span class="s5">&quot;half&quot;</span><span class="s2">:</span>
        <span class="s1">lim </span><span class="s2">= </span><span class="s1">n_samples </span><span class="s2">// </span><span class="s3">2</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">lim </span><span class="s2">= </span><span class="s1">n_samples</span>
    <span class="s1">X_dup </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">r_</span><span class="s2">[</span><span class="s1">X</span><span class="s2">, </span><span class="s1">X</span><span class="s2">[:</span><span class="s1">lim</span><span class="s2">]]</span>
    <span class="s1">y_dup </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">r_</span><span class="s2">[</span><span class="s1">y</span><span class="s2">, </span><span class="s1">y</span><span class="s2">[:</span><span class="s1">lim</span><span class="s2">]]</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ones</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">))</span>
    <span class="s1">sample_weight</span><span class="s2">[:</span><span class="s1">lim</span><span class="s2">] = </span><span class="s3">2</span>

    <span class="s1">est_sw </span><span class="s2">= </span><span class="s1">clone</span><span class="s2">(</span><span class="s1">est</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>
    <span class="s1">est_dup </span><span class="s2">= </span><span class="s1">clone</span><span class="s2">(</span><span class="s1">est</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_dup</span><span class="s2">, </span><span class="s1">y_dup</span><span class="s2">)</span>

    <span class="s6"># checking raw_predict is stricter than just predict for classification</span>
    <span class="s0">assert </span><span class="s1">np</span><span class="s2">.</span><span class="s1">allclose</span><span class="s2">(</span><span class="s1">est_sw</span><span class="s2">.</span><span class="s1">_raw_predict</span><span class="s2">(</span><span class="s1">X_dup</span><span class="s2">), </span><span class="s1">est_dup</span><span class="s2">.</span><span class="s1">_raw_predict</span><span class="s2">(</span><span class="s1">X_dup</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;Loss&quot;</span><span class="s2">, (</span><span class="s1">HalfSquaredError</span><span class="s2">, </span><span class="s1">AbsoluteError</span><span class="s2">))</span>
<span class="s0">def </span><span class="s1">test_sum_hessians_are_sample_weight</span><span class="s2">(</span><span class="s1">Loss</span><span class="s2">):</span>
    <span class="s6"># For losses with constant hessians, the sum_hessians field of the</span>
    <span class="s6"># histograms must be equal to the sum of the sample weight of samples at</span>
    <span class="s6"># the corresponding bin.</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">1000</span>
    <span class="s1">n_features </span><span class="s2">= </span><span class="s3">2</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s1">rng</span><span class="s2">)</span>
    <span class="s1">bin_mapper </span><span class="s2">= </span><span class="s1">_BinMapper</span><span class="s2">()</span>
    <span class="s1">X_binned </span><span class="s2">= </span><span class="s1">bin_mapper</span><span class="s2">.</span><span class="s1">fit_transform</span><span class="s2">(</span><span class="s1">X</span><span class="s2">)</span>

    <span class="s6"># While sample weights are supposed to be positive, this still works.</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">normal</span><span class="s2">(</span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>

    <span class="s1">loss </span><span class="s2">= </span><span class="s1">Loss</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>
    <span class="s1">gradients</span><span class="s2">, </span><span class="s1">hessians </span><span class="s2">= </span><span class="s1">loss</span><span class="s2">.</span><span class="s1">init_gradient_and_hessian</span><span class="s2">(</span>
        <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">G_H_DTYPE</span>
    <span class="s2">)</span>
    <span class="s1">gradients</span><span class="s2">, </span><span class="s1">hessians </span><span class="s2">= </span><span class="s1">gradients</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">((-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)), </span><span class="s1">hessians</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">((-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">))</span>
    <span class="s1">raw_predictions </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">normal</span><span class="s2">(</span><span class="s1">size</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s3">1</span><span class="s2">))</span>
    <span class="s1">loss</span><span class="s2">.</span><span class="s1">gradient_hessian</span><span class="s2">(</span>
        <span class="s1">y_true</span><span class="s2">=</span><span class="s1">y</span><span class="s2">,</span>
        <span class="s1">raw_prediction</span><span class="s2">=</span><span class="s1">raw_predictions</span><span class="s2">,</span>
        <span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">,</span>
        <span class="s1">gradient_out</span><span class="s2">=</span><span class="s1">gradients</span><span class="s2">,</span>
        <span class="s1">hessian_out</span><span class="s2">=</span><span class="s1">hessians</span><span class="s2">,</span>
        <span class="s1">n_threads</span><span class="s2">=</span><span class="s1">n_threads</span><span class="s2">,</span>
    <span class="s2">)</span>

    <span class="s6"># build sum_sample_weight which contains the sum of the sample weights at</span>
    <span class="s6"># each bin (for each feature). This must be equal to the sum_hessians</span>
    <span class="s6"># field of the corresponding histogram</span>
    <span class="s1">sum_sw </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=(</span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">bin_mapper</span><span class="s2">.</span><span class="s1">n_bins</span><span class="s2">))</span>
    <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">n_features</span><span class="s2">):</span>
        <span class="s0">for </span><span class="s1">sample_idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">):</span>
            <span class="s1">sum_sw</span><span class="s2">[</span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">X_binned</span><span class="s2">[</span><span class="s1">sample_idx</span><span class="s2">, </span><span class="s1">feature_idx</span><span class="s2">]] += </span><span class="s1">sample_weight</span><span class="s2">[</span>
                <span class="s1">sample_idx</span>
            <span class="s2">]</span>

    <span class="s6"># Build histogram</span>
    <span class="s1">grower </span><span class="s2">= </span><span class="s1">TreeGrower</span><span class="s2">(</span>
        <span class="s1">X_binned</span><span class="s2">, </span><span class="s1">gradients</span><span class="s2">[:, </span><span class="s3">0</span><span class="s2">], </span><span class="s1">hessians</span><span class="s2">[:, </span><span class="s3">0</span><span class="s2">], </span><span class="s1">n_bins</span><span class="s2">=</span><span class="s1">bin_mapper</span><span class="s2">.</span><span class="s1">n_bins</span>
    <span class="s2">)</span>
    <span class="s1">histograms </span><span class="s2">= </span><span class="s1">grower</span><span class="s2">.</span><span class="s1">histogram_builder</span><span class="s2">.</span><span class="s1">compute_histograms_brute</span><span class="s2">(</span>
        <span class="s1">grower</span><span class="s2">.</span><span class="s1">root</span><span class="s2">.</span><span class="s1">sample_indices</span>
    <span class="s2">)</span>

    <span class="s0">for </span><span class="s1">feature_idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">n_features</span><span class="s2">):</span>
        <span class="s0">for </span><span class="s1">bin_idx </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">bin_mapper</span><span class="s2">.</span><span class="s1">n_bins</span><span class="s2">):</span>
            <span class="s0">assert </span><span class="s1">histograms</span><span class="s2">[</span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">bin_idx</span><span class="s2">][</span><span class="s5">&quot;sum_hessians&quot;</span><span class="s2">] == (</span>
                <span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">sum_sw</span><span class="s2">[</span><span class="s1">feature_idx</span><span class="s2">, </span><span class="s1">bin_idx</span><span class="s2">], </span><span class="s1">rel</span><span class="s2">=</span><span class="s3">1e-5</span><span class="s2">)</span>
            <span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_max_depth_max_leaf_nodes</span><span class="s2">():</span>
    <span class="s6"># Non regression test for</span>
    <span class="s6"># https://github.com/scikit-learn/scikit-learn/issues/16179</span>
    <span class="s6"># there was a bug when the max_depth and the max_leaf_nodes criteria were</span>
    <span class="s6"># met at the same time, which would lead to max_leaf_nodes not being</span>
    <span class="s6"># respected.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">max_leaf_nodes</span><span class="s2">=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">1</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span>
    <span class="s2">)</span>
    <span class="s1">tree </span><span class="s2">= </span><span class="s1">est</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">[</span><span class="s3">0</span><span class="s2">][</span><span class="s3">0</span><span class="s2">]</span>
    <span class="s0">assert </span><span class="s1">tree</span><span class="s2">.</span><span class="s1">get_max_depth</span><span class="s2">() == </span><span class="s3">2</span>
    <span class="s0">assert </span><span class="s1">tree</span><span class="s2">.</span><span class="s1">get_n_leaf_nodes</span><span class="s2">() == </span><span class="s3">3  </span><span class="s6"># would be 4 prior to bug fix</span>


<span class="s0">def </span><span class="s1">test_early_stopping_on_test_set_with_warm_start</span><span class="s2">():</span>
    <span class="s6"># Non regression test for #16661 where second fit fails with</span>
    <span class="s6"># warm_start=True, early_stopping is on, and no validation set</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
        <span class="s1">scoring</span><span class="s2">=</span><span class="s5">&quot;loss&quot;</span><span class="s2">,</span>
        <span class="s1">warm_start</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">early_stopping</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">n_iter_no_change</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
        <span class="s1">validation_fraction</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s2">)</span>

    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s6"># does not raise on second call</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">2</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_early_stopping_with_sample_weights</span><span class="s2">(</span><span class="s1">monkeypatch</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Check that sample weights is passed in to the scorer and _raw_predict is not 
    called.&quot;&quot;&quot;</span>

    <span class="s1">mock_scorer </span><span class="s2">= </span><span class="s1">Mock</span><span class="s2">(</span><span class="s1">side_effect</span><span class="s2">=</span><span class="s1">get_scorer</span><span class="s2">(</span><span class="s5">&quot;neg_median_absolute_error&quot;</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">mock_check_scoring</span><span class="s2">(</span><span class="s1">estimator</span><span class="s2">, </span><span class="s1">scoring</span><span class="s2">):</span>
        <span class="s0">assert </span><span class="s1">scoring </span><span class="s2">== </span><span class="s5">&quot;neg_median_absolute_error&quot;</span>
        <span class="s0">return </span><span class="s1">mock_scorer</span>

    <span class="s1">monkeypatch</span><span class="s2">.</span><span class="s1">setattr</span><span class="s2">(</span>
        <span class="s1">sklearn</span><span class="s2">.</span><span class="s1">ensemble</span><span class="s2">.</span><span class="s1">_hist_gradient_boosting</span><span class="s2">.</span><span class="s1">gradient_boosting</span><span class="s2">,</span>
        <span class="s5">&quot;check_scoring&quot;</span><span class="s2">,</span>
        <span class="s1">mock_check_scoring</span><span class="s2">,</span>
    <span class="s2">)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ones_like</span><span class="s2">(</span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">hist </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">2</span><span class="s2">,</span>
        <span class="s1">early_stopping</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">scoring</span><span class="s2">=</span><span class="s5">&quot;neg_median_absolute_error&quot;</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s1">mock_raw_predict </span><span class="s2">= </span><span class="s1">Mock</span><span class="s2">(</span><span class="s1">side_effect</span><span class="s2">=</span><span class="s1">hist</span><span class="s2">.</span><span class="s1">_raw_predict</span><span class="s2">)</span>
    <span class="s1">hist</span><span class="s2">.</span><span class="s1">_raw_predict </span><span class="s2">= </span><span class="s1">mock_raw_predict</span>
    <span class="s1">hist</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>

    <span class="s6"># _raw_predict should never be called with scoring as a string</span>
    <span class="s0">assert </span><span class="s1">mock_raw_predict</span><span class="s2">.</span><span class="s1">call_count </span><span class="s2">== </span><span class="s3">0</span>

    <span class="s6"># For scorer is called twice (train and val) for the baseline score, and twice</span>
    <span class="s6"># per iteration (train and val) after that. So 6 times in total for `max_iter=2`.</span>
    <span class="s0">assert </span><span class="s1">mock_scorer</span><span class="s2">.</span><span class="s1">call_count </span><span class="s2">== </span><span class="s3">6</span>
    <span class="s0">for </span><span class="s1">arg_list </span><span class="s0">in </span><span class="s1">mock_scorer</span><span class="s2">.</span><span class="s1">call_args_list</span><span class="s2">:</span>
        <span class="s0">assert </span><span class="s5">&quot;sample_weight&quot; </span><span class="s0">in </span><span class="s1">arg_list</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]</span>


<span class="s0">def </span><span class="s1">test_raw_predict_is_called_with_custom_scorer</span><span class="s2">():</span>
    <span class="s4">&quot;&quot;&quot;Custom scorer will still call _raw_predict.&quot;&quot;&quot;</span>

    <span class="s1">mock_scorer </span><span class="s2">= </span><span class="s1">Mock</span><span class="s2">(</span><span class="s1">side_effect</span><span class="s2">=</span><span class="s1">get_scorer</span><span class="s2">(</span><span class="s5">&quot;neg_median_absolute_error&quot;</span><span class="s2">))</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">hist </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">2</span><span class="s2">,</span>
        <span class="s1">early_stopping</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">scoring</span><span class="s2">=</span><span class="s1">mock_scorer</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s1">mock_raw_predict </span><span class="s2">= </span><span class="s1">Mock</span><span class="s2">(</span><span class="s1">side_effect</span><span class="s2">=</span><span class="s1">hist</span><span class="s2">.</span><span class="s1">_raw_predict</span><span class="s2">)</span>
    <span class="s1">hist</span><span class="s2">.</span><span class="s1">_raw_predict </span><span class="s2">= </span><span class="s1">mock_raw_predict</span>
    <span class="s1">hist</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s6"># `_raw_predict` and scorer is called twice (train and val) for the baseline score,</span>
    <span class="s6"># and twice per iteration (train and val) after that. So 6 times in total for</span>
    <span class="s6"># `max_iter=2`.</span>
    <span class="s0">assert </span><span class="s1">mock_raw_predict</span><span class="s2">.</span><span class="s1">call_count </span><span class="s2">== </span><span class="s3">6</span>
    <span class="s0">assert </span><span class="s1">mock_scorer</span><span class="s2">.</span><span class="s1">call_count </span><span class="s2">== </span><span class="s3">6</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_single_node_trees</span><span class="s2">(</span><span class="s1">Est</span><span class="s2">):</span>
    <span class="s6"># Make sure it's still possible to build single-node trees. In that case</span>
    <span class="s6"># the value of the root is set to 0. That's a correct value: if the tree is</span>
    <span class="s6"># single-node that's because min_gain_to_split is not respected right from</span>
    <span class="s6"># the root, so we don't want the tree to have any impact on the</span>
    <span class="s6"># predictions.</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">y</span><span class="s2">[:] = </span><span class="s3">1  </span><span class="s6"># constant target will lead to a single root node</span>

    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">20</span><span class="s2">)</span>
    <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">assert </span><span class="s1">all</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">predictor</span><span class="s2">[</span><span class="s3">0</span><span class="s2">].</span><span class="s1">nodes</span><span class="s2">) == </span><span class="s3">1 </span><span class="s0">for </span><span class="s1">predictor </span><span class="s0">in </span><span class="s1">est</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">all</span><span class="s2">(</span><span class="s1">predictor</span><span class="s2">[</span><span class="s3">0</span><span class="s2">].</span><span class="s1">nodes</span><span class="s2">[</span><span class="s3">0</span><span class="s2">][</span><span class="s5">&quot;value&quot;</span><span class="s2">] == </span><span class="s3">0 </span><span class="s0">for </span><span class="s1">predictor </span><span class="s0">in </span><span class="s1">est</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">)</span>
    <span class="s6"># Still gives correct predictions thanks to the baseline prediction</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">), </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est, loss, X, y&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span>
            <span class="s1">HistGradientBoostingClassifier</span><span class="s2">,</span>
            <span class="s1">HalfBinomialLoss</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s0">None</span><span class="s2">),</span>
            <span class="s1">X_classification</span><span class="s2">,</span>
            <span class="s1">y_classification</span><span class="s2">,</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s1">HistGradientBoostingRegressor</span><span class="s2">,</span>
            <span class="s1">HalfSquaredError</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s0">None</span><span class="s2">),</span>
            <span class="s1">X_regression</span><span class="s2">,</span>
            <span class="s1">y_regression</span><span class="s2">,</span>
        <span class="s2">),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_custom_loss</span><span class="s2">(</span><span class="s1">Est</span><span class="s2">, </span><span class="s1">loss</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">):</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">=</span><span class="s1">loss</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">20</span><span class="s2">)</span>
    <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;HistGradientBoosting, X, y&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">X_classification</span><span class="s2">, </span><span class="s1">y_classification</span><span class="s2">),</span>
        <span class="s2">(</span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">, </span><span class="s1">X_regression</span><span class="s2">, </span><span class="s1">y_regression</span><span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s1">HistGradientBoostingClassifier</span><span class="s2">,</span>
            <span class="s1">X_multi_classification</span><span class="s2">,</span>
            <span class="s1">y_multi_classification</span><span class="s2">,</span>
        <span class="s2">),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_staged_predict</span><span class="s2">(</span><span class="s1">HistGradientBoosting</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">):</span>
    <span class="s6"># Test whether staged predictor eventually gives</span>
    <span class="s6"># the same prediction.</span>
    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test </span><span class="s2">= </span><span class="s1">train_test_split</span><span class="s2">(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">test_size</span><span class="s2">=</span><span class="s3">0.5</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span>
    <span class="s2">)</span>
    <span class="s1">gb </span><span class="s2">= </span><span class="s1">HistGradientBoosting</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">10</span><span class="s2">)</span>

    <span class="s6"># test raise NotFittedError if not fitted</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">NotFittedError</span><span class="s2">):</span>
        <span class="s1">next</span><span class="s2">(</span><span class="s1">gb</span><span class="s2">.</span><span class="s1">staged_predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">))</span>

    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s6"># test if the staged predictions of each iteration</span>
    <span class="s6"># are equal to the corresponding predictions of the same estimator</span>
    <span class="s6"># trained from scratch.</span>
    <span class="s6"># this also test limit case when max_iter = 1</span>
    <span class="s1">method_names </span><span class="s2">= (</span>
        <span class="s2">[</span><span class="s5">&quot;predict&quot;</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">is_regressor</span><span class="s2">(</span><span class="s1">gb</span><span class="s2">)</span>
        <span class="s0">else </span><span class="s2">[</span><span class="s5">&quot;predict&quot;</span><span class="s2">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s2">]</span>
    <span class="s2">)</span>
    <span class="s0">for </span><span class="s1">method_name </span><span class="s0">in </span><span class="s1">method_names</span><span class="s2">:</span>
        <span class="s1">staged_method </span><span class="s2">= </span><span class="s1">getattr</span><span class="s2">(</span><span class="s1">gb</span><span class="s2">, </span><span class="s5">&quot;staged_&quot; </span><span class="s2">+ </span><span class="s1">method_name</span><span class="s2">)</span>
        <span class="s1">staged_predictions </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">staged_method</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">))</span>
        <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">staged_predictions</span><span class="s2">) == </span><span class="s1">gb</span><span class="s2">.</span><span class="s1">n_iter_</span>
        <span class="s0">for </span><span class="s1">n_iter</span><span class="s2">, </span><span class="s1">staged_predictions </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">staged_method</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">), </span><span class="s3">1</span><span class="s2">):</span>
            <span class="s1">aux </span><span class="s2">= </span><span class="s1">HistGradientBoosting</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s1">n_iter</span><span class="s2">)</span>
            <span class="s1">aux</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>
            <span class="s1">pred_aux </span><span class="s2">= </span><span class="s1">getattr</span><span class="s2">(</span><span class="s1">aux</span><span class="s2">, </span><span class="s1">method_name</span><span class="s2">)(</span><span class="s1">X_test</span><span class="s2">)</span>

            <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">staged_predictions</span><span class="s2">, </span><span class="s1">pred_aux</span><span class="s2">)</span>
            <span class="s0">assert </span><span class="s1">staged_predictions</span><span class="s2">.</span><span class="s1">shape </span><span class="s2">== </span><span class="s1">pred_aux</span><span class="s2">.</span><span class="s1">shape</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;insert_missing&quot;</span><span class="s2">, [</span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">])</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">, </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;bool_categorical_parameter&quot;</span><span class="s2">, [</span><span class="s0">True</span><span class="s2">, </span><span class="s0">False</span><span class="s2">])</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;missing_value&quot;</span><span class="s2">, [</span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">])</span>
<span class="s0">def </span><span class="s1">test_unknown_categories_nan</span><span class="s2">(</span>
    <span class="s1">insert_missing</span><span class="s2">, </span><span class="s1">Est</span><span class="s2">, </span><span class="s1">bool_categorical_parameter</span><span class="s2">, </span><span class="s1">missing_value</span>
<span class="s2">):</span>
    <span class="s6"># Make sure no error is raised at predict if a category wasn't seen during</span>
    <span class="s6"># fit. We also make sure they're treated as nans.</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">1000</span>
    <span class="s1">f1 </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">rand</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">f2 </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">4</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">c_</span><span class="s2">[</span><span class="s1">f1</span><span class="s2">, </span><span class="s1">f2</span><span class="s2">]</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">y</span><span class="s2">[</span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">] % </span><span class="s3">2 </span><span class="s2">== </span><span class="s3">0</span><span class="s2">] = </span><span class="s3">1</span>

    <span class="s0">if </span><span class="s1">bool_categorical_parameter</span><span class="s2">:</span>
        <span class="s1">categorical_features </span><span class="s2">= [</span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">]</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">categorical_features </span><span class="s2">= [</span><span class="s3">1</span><span class="s2">]</span>

    <span class="s0">if </span><span class="s1">insert_missing</span><span class="s2">:</span>
        <span class="s1">mask </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">binomial</span><span class="s2">(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0.01</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">bool</span><span class="s2">)</span>
        <span class="s0">assert </span><span class="s1">mask</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">() &gt; </span><span class="s3">0</span>
        <span class="s1">X</span><span class="s2">[</span><span class="s1">mask</span><span class="s2">] = </span><span class="s1">missing_value</span>

    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s1">categorical_features</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">assert_array_equal</span><span class="s2">(</span><span class="s1">est</span><span class="s2">.</span><span class="s1">is_categorical_</span><span class="s2">, [</span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">])</span>

    <span class="s6"># Make sure no error is raised on unknown categories and nans</span>
    <span class="s6"># unknown categories will be treated as nans</span>
    <span class="s1">X_test </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">((</span><span class="s3">10</span><span class="s2">, </span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]), </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">float</span><span class="s2">)</span>
    <span class="s1">X_test</span><span class="s2">[:</span><span class="s3">5</span><span class="s2">, </span><span class="s3">1</span><span class="s2">] = </span><span class="s3">30</span>
    <span class="s1">X_test</span><span class="s2">[</span><span class="s3">5</span><span class="s2">:, </span><span class="s3">1</span><span class="s2">] = </span><span class="s1">missing_value</span>
    <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">unique</span><span class="s2">(</span><span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">))) == </span><span class="s3">1</span>


<span class="s0">def </span><span class="s1">test_categorical_encoding_strategies</span><span class="s2">():</span>
    <span class="s6"># Check native categorical handling vs different encoding strategies. We</span>
    <span class="s6"># make sure that native encoding needs only 1 split to achieve a perfect</span>
    <span class="s6"># prediction on a simple dataset. In contrast, OneHotEncoded data needs</span>
    <span class="s6"># more depth / splits, and treating categories as ordered (just using</span>
    <span class="s6"># OrdinalEncoder) requires even more depth.</span>

    <span class="s6"># dataset with one random continuous feature, and one categorical feature</span>
    <span class="s6"># with values in [0, 5], e.g. from an OrdinalEncoder.</span>
    <span class="s6"># class == 1 iff categorical value in {0, 2, 4}</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">10_000</span>
    <span class="s1">f1 </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">rand</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">f2 </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">6</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">c_</span><span class="s2">[</span><span class="s1">f1</span><span class="s2">, </span><span class="s1">f2</span><span class="s2">]</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">y</span><span class="s2">[</span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">] % </span><span class="s3">2 </span><span class="s2">== </span><span class="s3">0</span><span class="s2">] = </span><span class="s3">1</span>

    <span class="s6"># make sure dataset is balanced so that the baseline_prediction doesn't</span>
    <span class="s6"># influence predictions too much with max_iter = 1</span>
    <span class="s0">assert </span><span class="s3">0.49 </span><span class="s2">&lt; </span><span class="s1">y</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">() &lt; </span><span class="s3">0.51</span>

    <span class="s1">native_cat_specs </span><span class="s2">= [</span>
        <span class="s2">[</span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">],</span>
        <span class="s2">[</span><span class="s3">1</span><span class="s2">],</span>
    <span class="s2">]</span>
    <span class="s0">try</span><span class="s2">:</span>
        <span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>

        <span class="s1">X </span><span class="s2">= </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">DataFrame</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">columns</span><span class="s2">=[</span><span class="s5">&quot;f_0&quot;</span><span class="s2">, </span><span class="s5">&quot;f_1&quot;</span><span class="s2">])</span>
        <span class="s1">native_cat_specs</span><span class="s2">.</span><span class="s1">append</span><span class="s2">([</span><span class="s5">&quot;f_1&quot;</span><span class="s2">])</span>
    <span class="s0">except </span><span class="s1">ImportError</span><span class="s2">:</span>
        <span class="s0">pass</span>

    <span class="s0">for </span><span class="s1">native_cat_spec </span><span class="s0">in </span><span class="s1">native_cat_specs</span><span class="s2">:</span>
        <span class="s1">clf_cat </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
            <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s1">native_cat_spec</span>
        <span class="s2">)</span>
        <span class="s1">clf_cat</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

        <span class="s6"># Using native categorical encoding, we get perfect predictions with just</span>
        <span class="s6"># one split</span>
        <span class="s0">assert </span><span class="s1">cross_val_score</span><span class="s2">(</span><span class="s1">clf_cat</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">).</span><span class="s1">mean</span><span class="s2">() == </span><span class="s3">1</span>

    <span class="s6"># quick sanity check for the bitset: 0, 2, 4 = 2**0 + 2**2 + 2**4 = 21</span>
    <span class="s1">expected_left_bitset </span><span class="s2">= [</span><span class="s3">21</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">left_bitset </span><span class="s2">= </span><span class="s1">clf_cat</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">).</span><span class="s1">_predictors</span><span class="s2">[</span><span class="s3">0</span><span class="s2">][</span><span class="s3">0</span><span class="s2">].</span><span class="s1">raw_left_cat_bitsets</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">assert_array_equal</span><span class="s2">(</span><span class="s1">left_bitset</span><span class="s2">, </span><span class="s1">expected_left_bitset</span><span class="s2">)</span>

    <span class="s6"># Treating categories as ordered, we need more depth / more splits to get</span>
    <span class="s6"># the same predictions</span>
    <span class="s1">clf_no_cat </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s0">None</span>
    <span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score</span><span class="s2">(</span><span class="s1">clf_no_cat</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">).</span><span class="s1">mean</span><span class="s2">() &lt; </span><span class="s3">0.9</span>

    <span class="s1">clf_no_cat</span><span class="s2">.</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">5</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score</span><span class="s2">(</span><span class="s1">clf_no_cat</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">).</span><span class="s1">mean</span><span class="s2">() == </span><span class="s3">1</span>

    <span class="s6"># Using OHEd data, we need less splits than with pure OEd data, but we</span>
    <span class="s6"># still need more splits than with the native categorical splits</span>
    <span class="s1">ct </span><span class="s2">= </span><span class="s1">make_column_transformer</span><span class="s2">(</span>
        <span class="s2">(</span><span class="s1">OneHotEncoder</span><span class="s2">(</span><span class="s1">sparse_output</span><span class="s2">=</span><span class="s0">False</span><span class="s2">), [</span><span class="s3">1</span><span class="s2">]), </span><span class="s1">remainder</span><span class="s2">=</span><span class="s5">&quot;passthrough&quot;</span>
    <span class="s2">)</span>
    <span class="s1">X_ohe </span><span class="s2">= </span><span class="s1">ct</span><span class="s2">.</span><span class="s1">fit_transform</span><span class="s2">(</span><span class="s1">X</span><span class="s2">)</span>
    <span class="s1">clf_no_cat</span><span class="s2">.</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">2</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score</span><span class="s2">(</span><span class="s1">clf_no_cat</span><span class="s2">, </span><span class="s1">X_ohe</span><span class="s2">, </span><span class="s1">y</span><span class="s2">).</span><span class="s1">mean</span><span class="s2">() &lt; </span><span class="s3">0.9</span>

    <span class="s1">clf_no_cat</span><span class="s2">.</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">3</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">cross_val_score</span><span class="s2">(</span><span class="s1">clf_no_cat</span><span class="s2">, </span><span class="s1">X_ohe</span><span class="s2">, </span><span class="s1">y</span><span class="s2">).</span><span class="s1">mean</span><span class="s2">() == </span><span class="s3">1</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;categorical_features, monotonic_cst, expected_msg&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span>
            <span class="s2">[</span><span class="s7">b&quot;hello&quot;</span><span class="s2">, </span><span class="s7">b&quot;world&quot;</span><span class="s2">],</span>
            <span class="s0">None</span><span class="s2">,</span>
            <span class="s1">re</span><span class="s2">.</span><span class="s1">escape</span><span class="s2">(</span>
                <span class="s5">&quot;categorical_features must be an array-like of bool, int or str, &quot;</span>
                <span class="s5">&quot;got: bytes40.&quot;</span>
            <span class="s2">),</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([</span><span class="s7">b&quot;hello&quot;</span><span class="s2">, </span><span class="s3">1.3</span><span class="s2">], </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">object</span><span class="s2">),</span>
            <span class="s0">None</span><span class="s2">,</span>
            <span class="s1">re</span><span class="s2">.</span><span class="s1">escape</span><span class="s2">(</span>
                <span class="s5">&quot;categorical_features must be an array-like of bool, int or str, &quot;</span>
                <span class="s5">&quot;got: bytes, float.&quot;</span>
            <span class="s2">),</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s2">[</span><span class="s3">0</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">],</span>
            <span class="s0">None</span><span class="s2">,</span>
            <span class="s1">re</span><span class="s2">.</span><span class="s1">escape</span><span class="s2">(</span>
                <span class="s5">&quot;categorical_features set as integer indices must be in &quot;</span>
                <span class="s5">&quot;[0, n_features - 1]&quot;</span>
            <span class="s2">),</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s2">[</span><span class="s0">True</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s0">False</span><span class="s2">, </span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">],</span>
            <span class="s0">None</span><span class="s2">,</span>
            <span class="s1">re</span><span class="s2">.</span><span class="s1">escape</span><span class="s2">(</span>
                <span class="s5">&quot;categorical_features set as a boolean mask must have shape &quot;</span>
                <span class="s5">&quot;(n_features,)&quot;</span>
            <span class="s2">),</span>
        <span class="s2">),</span>
        <span class="s2">(</span>
            <span class="s2">[</span><span class="s0">True</span><span class="s2">, </span><span class="s0">True</span><span class="s2">, </span><span class="s0">False</span><span class="s2">, </span><span class="s0">False</span><span class="s2">],</span>
            <span class="s2">[</span><span class="s3">0</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">],</span>
            <span class="s5">&quot;Categorical features cannot have monotonic constraints&quot;</span><span class="s2">,</span>
        <span class="s2">),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_categorical_spec_errors</span><span class="s2">(</span>
    <span class="s1">Est</span><span class="s2">, </span><span class="s1">categorical_features</span><span class="s2">, </span><span class="s1">monotonic_cst</span><span class="s2">, </span><span class="s1">expected_msg</span>
<span class="s2">):</span>
    <span class="s6"># Test errors when categories are specified incorrectly</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">100</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">X</span><span class="s2">[:, </span><span class="s3">0</span><span class="s2">] = </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">] = </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">10</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s1">categorical_features</span><span class="s2">, </span><span class="s1">monotonic_cst</span><span class="s2">=</span><span class="s1">monotonic_cst</span><span class="s2">)</span>

    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">expected_msg</span><span class="s2">):</span>
        <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_categorical_spec_errors_with_feature_names</span><span class="s2">(</span><span class="s1">Est</span><span class="s2">):</span>
    <span class="s1">pd </span><span class="s2">= </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s5">&quot;pandas&quot;</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">10</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">DataFrame</span><span class="s2">(</span>
        <span class="s2">{</span>
            <span class="s5">&quot;f0&quot;</span><span class="s2">: </span><span class="s1">range</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">),</span>
            <span class="s5">&quot;f1&quot;</span><span class="s2">: </span><span class="s1">range</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">),</span>
            <span class="s5">&quot;f2&quot;</span><span class="s2">: [</span><span class="s3">1.0</span><span class="s2">] * </span><span class="s1">n_samples</span><span class="s2">,</span>
        <span class="s2">}</span>
    <span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">] * (</span><span class="s1">n_samples </span><span class="s2">// </span><span class="s3">2</span><span class="s2">)</span>

    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=[</span><span class="s5">&quot;f0&quot;</span><span class="s2">, </span><span class="s5">&quot;f1&quot;</span><span class="s2">, </span><span class="s5">&quot;f3&quot;</span><span class="s2">])</span>
    <span class="s1">expected_msg </span><span class="s2">= </span><span class="s1">re</span><span class="s2">.</span><span class="s1">escape</span><span class="s2">(</span>
        <span class="s5">&quot;categorical_features has a item value 'f3' which is not a valid &quot;</span>
        <span class="s5">&quot;feature name of the training data.&quot;</span>
    <span class="s2">)</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">expected_msg</span><span class="s2">):</span>
        <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=[</span><span class="s5">&quot;f0&quot;</span><span class="s2">, </span><span class="s5">&quot;f1&quot;</span><span class="s2">])</span>
    <span class="s1">expected_msg </span><span class="s2">= </span><span class="s1">re</span><span class="s2">.</span><span class="s1">escape</span><span class="s2">(</span>
        <span class="s5">&quot;categorical_features should be passed as an array of integers or &quot;</span>
        <span class="s5">&quot;as a boolean mask when the model is fitted on data without feature &quot;</span>
        <span class="s5">&quot;names.&quot;</span>
    <span class="s2">)</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">expected_msg</span><span class="s2">):</span>
        <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">to_numpy</span><span class="s2">(), </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;categorical_features&quot;</span><span class="s2">, ([</span><span class="s0">False</span><span class="s2">, </span><span class="s0">False</span><span class="s2">], []))</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;as_array&quot;</span><span class="s2">, (</span><span class="s0">True</span><span class="s2">, </span><span class="s0">False</span><span class="s2">))</span>
<span class="s0">def </span><span class="s1">test_categorical_spec_no_categories</span><span class="s2">(</span><span class="s1">Est</span><span class="s2">, </span><span class="s1">categorical_features</span><span class="s2">, </span><span class="s1">as_array</span><span class="s2">):</span>
    <span class="s6"># Make sure we can properly detect that no categorical features are present</span>
    <span class="s6"># even if the categorical_features parameter is not None</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s3">10</span><span class="s2">).</span><span class="s1">reshape</span><span class="s2">(</span><span class="s3">5</span><span class="s2">, </span><span class="s3">2</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s3">5</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">as_array</span><span class="s2">:</span>
        <span class="s1">categorical_features </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">)</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s1">categorical_features</span><span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">est</span><span class="s2">.</span><span class="s1">is_categorical_ </span><span class="s0">is None</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;use_pandas, feature_name&quot;</span><span class="s2">, [(</span><span class="s0">False</span><span class="s2">, </span><span class="s5">&quot;at index 0&quot;</span><span class="s2">), (</span><span class="s0">True</span><span class="s2">, </span><span class="s5">&quot;'f0'&quot;</span><span class="s2">)]</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_categorical_bad_encoding_errors</span><span class="s2">(</span><span class="s1">Est</span><span class="s2">, </span><span class="s1">use_pandas</span><span class="s2">, </span><span class="s1">feature_name</span><span class="s2">):</span>
    <span class="s6"># Test errors when categories are encoded incorrectly</span>

    <span class="s1">gb </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=[</span><span class="s0">True</span><span class="s2">], </span><span class="s1">max_bins</span><span class="s2">=</span><span class="s3">2</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">use_pandas</span><span class="s2">:</span>
        <span class="s1">pd </span><span class="s2">= </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s5">&quot;pandas&quot;</span><span class="s2">)</span>
        <span class="s1">X </span><span class="s2">= </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">DataFrame</span><span class="s2">({</span><span class="s5">&quot;f0&quot;</span><span class="s2">: [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">]})</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">]]).</span><span class="s1">T</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s3">3</span><span class="s2">)</span>
    <span class="s1">msg </span><span class="s2">= (</span>
        <span class="s5">f&quot;Categorical feature </span><span class="s0">{</span><span class="s1">feature_name</span><span class="s0">} </span><span class="s5">is expected to have a &quot;</span>
        <span class="s5">&quot;cardinality &lt;= 2 but actually has a cardinality of 3.&quot;</span>
    <span class="s2">)</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">msg</span><span class="s2">):</span>
        <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s6"># nans are ignored in the counts</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">([[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">]]).</span><span class="s1">T</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">arange</span><span class="s2">(</span><span class="s3">3</span><span class="s2">)</span>
    <span class="s1">gb</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;Est&quot;</span><span class="s2">, (</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">)</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_uint8_predict</span><span class="s2">(</span><span class="s1">Est</span><span class="s2">):</span>
    <span class="s6"># Non regression test for</span>
    <span class="s6"># https://github.com/scikit-learn/scikit-learn/issues/18408</span>
    <span class="s6"># Make sure X can be of dtype uint8 (i.e. X_BINNED_DTYPE) in predict. It</span>
    <span class="s6"># will be converted to X_DTYPE.</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>

    <span class="s1">X </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">100</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">2</span><span class="s2">)).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">uint8</span><span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s3">10</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">np</span><span class="s2">.</span><span class="s1">uint8</span><span class="s2">)</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">Est</span><span class="s2">()</span>
    <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;interaction_cst, n_features, result&quot;</span><span class="s2">,</span>
    <span class="s2">[</span>
        <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s3">931</span><span class="s2">, </span><span class="s0">None</span><span class="s2">),</span>
        <span class="s2">([{</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">}], </span><span class="s3">2</span><span class="s2">, [{</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">}]),</span>
        <span class="s2">(</span><span class="s5">&quot;pairwise&quot;</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, [{</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">}]),</span>
        <span class="s2">(</span><span class="s5">&quot;pairwise&quot;</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, [{</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">}, {</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">}, {</span><span class="s3">0</span><span class="s2">, </span><span class="s3">3</span><span class="s2">}, {</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">}, {</span><span class="s3">1</span><span class="s2">, </span><span class="s3">3</span><span class="s2">}, {</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">}]),</span>
        <span class="s2">(</span><span class="s5">&quot;no_interactions&quot;</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, [{</span><span class="s3">0</span><span class="s2">}, {</span><span class="s3">1</span><span class="s2">}]),</span>
        <span class="s2">(</span><span class="s5">&quot;no_interactions&quot;</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, [{</span><span class="s3">0</span><span class="s2">}, {</span><span class="s3">1</span><span class="s2">}, {</span><span class="s3">2</span><span class="s2">}, {</span><span class="s3">3</span><span class="s2">}]),</span>
        <span class="s2">([(</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">), [</span><span class="s3">5</span><span class="s2">, </span><span class="s3">1</span><span class="s2">]], </span><span class="s3">6</span><span class="s2">, [{</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">}, {</span><span class="s3">1</span><span class="s2">, </span><span class="s3">5</span><span class="s2">}, {</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">}]),</span>
    <span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_check_interaction_cst</span><span class="s2">(</span><span class="s1">interaction_cst</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">result</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Check that _check_interaction_cst returns the expected list of sets&quot;&quot;&quot;</span>
    <span class="s1">est </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">()</span>
    <span class="s1">est</span><span class="s2">.</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">interaction_cst</span><span class="s2">=</span><span class="s1">interaction_cst</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">est</span><span class="s2">.</span><span class="s1">_check_interaction_cst</span><span class="s2">(</span><span class="s1">n_features</span><span class="s2">) == </span><span class="s1">result</span>


<span class="s0">def </span><span class="s1">test_interaction_cst_numerically</span><span class="s2">():</span>
    <span class="s4">&quot;&quot;&quot;Check that interaction constraints have no forbidden interactions.&quot;&quot;&quot;</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">1000</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">uniform</span><span class="s2">(</span><span class="s1">size</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s3">2</span><span class="s2">))</span>
    <span class="s6"># Construct y with a strong interaction term</span>
    <span class="s6"># y = x0 + x1 + 5 * x0 * x1</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">hstack</span><span class="s2">((</span><span class="s1">X</span><span class="s2">, </span><span class="s3">5 </span><span class="s2">* </span><span class="s1">X</span><span class="s2">[:, [</span><span class="s3">0</span><span class="s2">]] * </span><span class="s1">X</span><span class="s2">[:, [</span><span class="s3">1</span><span class="s2">]])).</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>

    <span class="s1">est </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">est_no_interactions </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">interaction_cst</span><span class="s2">=[{</span><span class="s3">0</span><span class="s2">}, {</span><span class="s3">1</span><span class="s2">}], </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">42</span>
    <span class="s2">)</span>
    <span class="s1">est_no_interactions</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s1">delta </span><span class="s2">= </span><span class="s3">0.25</span>
    <span class="s6"># Make sure we do not extrapolate out of the training set as tree-based estimators</span>
    <span class="s6"># are very bad in doing so.</span>
    <span class="s1">X_test </span><span class="s2">= </span><span class="s1">X</span><span class="s2">[(</span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">0</span><span class="s2">] &lt; </span><span class="s3">1 </span><span class="s2">- </span><span class="s1">delta</span><span class="s2">) &amp; (</span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">] &lt; </span><span class="s3">1 </span><span class="s2">- </span><span class="s1">delta</span><span class="s2">)]</span>
    <span class="s1">X_delta_d_0 </span><span class="s2">= </span><span class="s1">X_test </span><span class="s2">+ [</span><span class="s1">delta</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">X_delta_0_d </span><span class="s2">= </span><span class="s1">X_test </span><span class="s2">+ [</span><span class="s3">0</span><span class="s2">, </span><span class="s1">delta</span><span class="s2">]</span>
    <span class="s1">X_delta_d_d </span><span class="s2">= </span><span class="s1">X_test </span><span class="s2">+ [</span><span class="s1">delta</span><span class="s2">, </span><span class="s1">delta</span><span class="s2">]</span>

    <span class="s6"># Note: For the y from above as a function of x0 and x1, we have</span>
    <span class="s6"># y(x0+d, x1+d) = y(x0, x1) + 5 * d * (2/5 + x0 + x1) + 5 * d**2</span>
    <span class="s6"># y(x0+d, x1)   = y(x0, x1) + 5 * d * (1/5 + x1)</span>
    <span class="s6"># y(x0,   x1+d) = y(x0, x1) + 5 * d * (1/5 + x0)</span>
    <span class="s6"># Without interaction constraints, we would expect a result of 5 * d**2 for the</span>
    <span class="s6"># following expression, but zero with constraints in place.</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span>
        <span class="s1">est_no_interactions</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_delta_d_d</span><span class="s2">)</span>
        <span class="s2">+ </span><span class="s1">est_no_interactions</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">)</span>
        <span class="s2">- </span><span class="s1">est_no_interactions</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_delta_d_0</span><span class="s2">)</span>
        <span class="s2">- </span><span class="s1">est_no_interactions</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_delta_0_d</span><span class="s2">),</span>
        <span class="s3">0</span><span class="s2">,</span>
        <span class="s1">atol</span><span class="s2">=</span><span class="s3">1e-12</span><span class="s2">,</span>
    <span class="s2">)</span>

    <span class="s6"># Correct result of the expressions is 5 * delta**2. But this is hard to achieve by</span>
    <span class="s6"># a fitted tree-based model. However, with 100 iterations the expression should</span>
    <span class="s6"># at least be positive!</span>
    <span class="s0">assert </span><span class="s1">np</span><span class="s2">.</span><span class="s1">all</span><span class="s2">(</span>
        <span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_delta_d_d</span><span class="s2">)</span>
        <span class="s2">+ </span><span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">)</span>
        <span class="s2">- </span><span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_delta_d_0</span><span class="s2">)</span>
        <span class="s2">- </span><span class="s1">est</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_delta_0_d</span><span class="s2">)</span>
        <span class="s2">&gt; </span><span class="s3">0.01</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_no_user_warning_with_scoring</span><span class="s2">():</span>
    <span class="s4">&quot;&quot;&quot;Check that no UserWarning is raised when scoring is set. 
 
    Non-regression test for #22907. 
    &quot;&quot;&quot;</span>
    <span class="s1">pd </span><span class="s2">= </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s5">&quot;pandas&quot;</span><span class="s2">)</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_regression</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">X_df </span><span class="s2">= </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">DataFrame</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">columns</span><span class="s2">=[</span><span class="s5">f&quot;col</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s5">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">X</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">])])</span>

    <span class="s1">est </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">scoring</span><span class="s2">=</span><span class="s5">&quot;neg_mean_absolute_error&quot;</span><span class="s2">, </span><span class="s1">early_stopping</span><span class="s2">=</span><span class="s0">True</span>
    <span class="s2">)</span>
    <span class="s0">with </span><span class="s1">warnings</span><span class="s2">.</span><span class="s1">catch_warnings</span><span class="s2">():</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">simplefilter</span><span class="s2">(</span><span class="s5">&quot;error&quot;</span><span class="s2">, </span><span class="s1">UserWarning</span><span class="s2">)</span>
        <span class="s1">est</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_df</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_class_weights</span><span class="s2">():</span>
    <span class="s4">&quot;&quot;&quot;High level test to check class_weights.&quot;&quot;&quot;</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">255</span>
    <span class="s1">n_features </span><span class="s2">= </span><span class="s3">2</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span>
        <span class="s1">n_samples</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">,</span>
        <span class="s1">n_features</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
        <span class="s1">n_informative</span><span class="s2">=</span><span class="s1">n_features</span><span class="s2">,</span>
        <span class="s1">n_redundant</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">n_clusters_per_class</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
        <span class="s1">n_classes</span><span class="s2">=</span><span class="s3">2</span><span class="s2">,</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s1">y_is_1 </span><span class="s2">= </span><span class="s1">y </span><span class="s2">== </span><span class="s3">1</span>

    <span class="s6"># class_weight is the same as sample weights with the corresponding class</span>
    <span class="s1">clf </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">min_samples_leaf</span><span class="s2">=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">2</span>
    <span class="s2">)</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ones</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=(</span><span class="s1">n_samples</span><span class="s2">))</span>
    <span class="s1">sample_weight</span><span class="s2">[</span><span class="s1">y_is_1</span><span class="s2">] = </span><span class="s3">3.0</span>
    <span class="s1">clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>

    <span class="s1">class_weight </span><span class="s2">= {</span><span class="s3">0</span><span class="s2">: </span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">: </span><span class="s3">3.0</span><span class="s2">}</span>
    <span class="s1">clf_class_weighted </span><span class="s2">= </span><span class="s1">clone</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">).</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">class_weight</span><span class="s2">=</span><span class="s1">class_weight</span><span class="s2">)</span>
    <span class="s1">clf_class_weighted</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">.</span><span class="s1">decision_function</span><span class="s2">(</span><span class="s1">X</span><span class="s2">), </span><span class="s1">clf_class_weighted</span><span class="s2">.</span><span class="s1">decision_function</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>

    <span class="s6"># Check that sample_weight and class_weight are multiplicative</span>
    <span class="s1">clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">**</span><span class="s3">2</span><span class="s2">)</span>
    <span class="s1">clf_class_weighted</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">.</span><span class="s1">decision_function</span><span class="s2">(</span><span class="s1">X</span><span class="s2">), </span><span class="s1">clf_class_weighted</span><span class="s2">.</span><span class="s1">decision_function</span><span class="s2">(</span><span class="s1">X</span><span class="s2">))</span>

    <span class="s6"># Make imbalanced dataset</span>
    <span class="s1">X_imb </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">((</span><span class="s1">X</span><span class="s2">[~</span><span class="s1">y_is_1</span><span class="s2">], </span><span class="s1">X</span><span class="s2">[</span><span class="s1">y_is_1</span><span class="s2">][:</span><span class="s3">10</span><span class="s2">]))</span>
    <span class="s1">y_imb </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">concatenate</span><span class="s2">((</span><span class="s1">y</span><span class="s2">[~</span><span class="s1">y_is_1</span><span class="s2">], </span><span class="s1">y</span><span class="s2">[</span><span class="s1">y_is_1</span><span class="s2">][:</span><span class="s3">10</span><span class="s2">]))</span>

    <span class="s6"># class_weight=&quot;balanced&quot; is the same as sample_weights to be</span>
    <span class="s6"># inversely proportional to n_samples / (n_classes * np.bincount(y))</span>
    <span class="s1">clf_balanced </span><span class="s2">= </span><span class="s1">clone</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">).</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">class_weight</span><span class="s2">=</span><span class="s5">&quot;balanced&quot;</span><span class="s2">)</span>
    <span class="s1">clf_balanced</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_imb</span><span class="s2">, </span><span class="s1">y_imb</span><span class="s2">)</span>

    <span class="s1">class_weight </span><span class="s2">= </span><span class="s1">y_imb</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">] / (</span><span class="s3">2 </span><span class="s2">* </span><span class="s1">np</span><span class="s2">.</span><span class="s1">bincount</span><span class="s2">(</span><span class="s1">y_imb</span><span class="s2">))</span>
    <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">class_weight</span><span class="s2">[</span><span class="s1">y_imb</span><span class="s2">]</span>
    <span class="s1">clf_sample_weight </span><span class="s2">= </span><span class="s1">clone</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">).</span><span class="s1">set_params</span><span class="s2">(</span><span class="s1">class_weight</span><span class="s2">=</span><span class="s0">None</span><span class="s2">)</span>
    <span class="s1">clf_sample_weight</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_imb</span><span class="s2">, </span><span class="s1">y_imb</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>

    <span class="s1">assert_allclose</span><span class="s2">(</span>
        <span class="s1">clf_balanced</span><span class="s2">.</span><span class="s1">decision_function</span><span class="s2">(</span><span class="s1">X_imb</span><span class="s2">),</span>
        <span class="s1">clf_sample_weight</span><span class="s2">.</span><span class="s1">decision_function</span><span class="s2">(</span><span class="s1">X_imb</span><span class="s2">),</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_unknown_category_that_are_negative</span><span class="s2">():</span>
    <span class="s4">&quot;&quot;&quot;Check that unknown categories that are negative does not error. 
 
    Non-regression test for #24274. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">1000</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">c_</span><span class="s2">[</span><span class="s1">rng</span><span class="s2">.</span><span class="s1">rand</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">), </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">4</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)]</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">shape</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">y</span><span class="s2">[</span><span class="s1">X</span><span class="s2">[:, </span><span class="s3">1</span><span class="s2">] % </span><span class="s3">2 </span><span class="s2">== </span><span class="s3">0</span><span class="s2">] = </span><span class="s3">1</span>

    <span class="s1">hist </span><span class="s2">= </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">(</span>
        <span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
        <span class="s1">categorical_features</span><span class="s2">=[</span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">],</span>
        <span class="s1">max_iter</span><span class="s2">=</span><span class="s3">10</span><span class="s2">,</span>
    <span class="s2">).</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s6"># Check that negative values from the second column are treated like a</span>
    <span class="s6"># missing category</span>
    <span class="s1">X_test_neg </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, -</span><span class="s3">2</span><span class="s2">], [</span><span class="s3">3</span><span class="s2">, -</span><span class="s3">4</span><span class="s2">]])</span>
    <span class="s1">X_test_nan </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">([[</span><span class="s3">1</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">], [</span><span class="s3">3</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">nan</span><span class="s2">]])</span>

    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">hist</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test_neg</span><span class="s2">), </span><span class="s1">hist</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test_nan</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;dataframe_lib&quot;</span><span class="s2">, [</span><span class="s5">&quot;pandas&quot;</span><span class="s2">, </span><span class="s5">&quot;polars&quot;</span><span class="s2">])</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;HistGradientBoosting&quot;</span><span class="s2">,</span>
    <span class="s2">[</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_dataframe_categorical_results_same_as_ndarray</span><span class="s2">(</span>
    <span class="s1">dataframe_lib</span><span class="s2">, </span><span class="s1">HistGradientBoosting</span>
<span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Check that pandas categorical give the same results as ndarray.&quot;&quot;&quot;</span>
    <span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s1">dataframe_lib</span><span class="s2">)</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">5_000</span>
    <span class="s1">n_cardinality </span><span class="s2">= </span><span class="s3">50</span>
    <span class="s1">max_bins </span><span class="s2">= </span><span class="s3">100</span>
    <span class="s1">f_num </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">rand</span><span class="s2">(</span><span class="s1">n_samples</span><span class="s2">)</span>
    <span class="s1">f_cat </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s1">n_cardinality</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>

    <span class="s6"># Make f_cat an informative feature</span>
    <span class="s1">y </span><span class="s2">= (</span><span class="s1">f_cat </span><span class="s2">% </span><span class="s3">3 </span><span class="s2">== </span><span class="s3">0</span><span class="s2">) &amp; (</span><span class="s1">f_num </span><span class="s2">&gt; </span><span class="s3">0.2</span><span class="s2">)</span>

    <span class="s1">X </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">c_</span><span class="s2">[</span><span class="s1">f_num</span><span class="s2">, </span><span class="s1">f_cat</span><span class="s2">]</span>
    <span class="s1">f_cat </span><span class="s2">= [</span><span class="s5">f&quot;cat</span><span class="s0">{</span><span class="s1">c</span><span class="s0">:</span><span class="s5">0&gt;3</span><span class="s0">}</span><span class="s5">&quot; </span><span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">f_cat</span><span class="s2">]</span>
    <span class="s1">X_df </span><span class="s2">= </span><span class="s1">_convert_container</span><span class="s2">(</span>
        <span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">([</span><span class="s1">f_num</span><span class="s2">, </span><span class="s1">f_cat</span><span class="s2">]).</span><span class="s1">T</span><span class="s2">,</span>
        <span class="s1">dataframe_lib</span><span class="s2">,</span>
        <span class="s2">[</span><span class="s5">&quot;f_num&quot;</span><span class="s2">, </span><span class="s5">&quot;f_cat&quot;</span><span class="s2">],</span>
        <span class="s1">categorical_feature_names</span><span class="s2">=[</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">],</span>
    <span class="s2">)</span>

    <span class="s1">X_train</span><span class="s2">, </span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">X_train_df</span><span class="s2">, </span><span class="s1">X_test_df</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">, </span><span class="s1">y_test </span><span class="s2">= </span><span class="s1">train_test_split</span><span class="s2">(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">X_df</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span>
    <span class="s2">)</span>

    <span class="s1">hist_kwargs </span><span class="s2">= </span><span class="s1">dict</span><span class="s2">(</span><span class="s1">max_iter</span><span class="s2">=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">max_bins</span><span class="s2">=</span><span class="s1">max_bins</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">hist_np </span><span class="s2">= </span><span class="s1">HistGradientBoosting</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=[</span><span class="s0">False</span><span class="s2">, </span><span class="s0">True</span><span class="s2">], **</span><span class="s1">hist_kwargs</span><span class="s2">)</span>
    <span class="s1">hist_np</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s1">hist_pd </span><span class="s2">= </span><span class="s1">HistGradientBoosting</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s5">&quot;from_dtype&quot;</span><span class="s2">, **</span><span class="s1">hist_kwargs</span><span class="s2">)</span>
    <span class="s1">hist_pd</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_train_df</span><span class="s2">, </span><span class="s1">y_train</span><span class="s2">)</span>

    <span class="s6"># Check categories are correct and sorted</span>
    <span class="s1">categories </span><span class="s2">= </span><span class="s1">hist_pd</span><span class="s2">.</span><span class="s1">_preprocessor</span><span class="s2">.</span><span class="s1">named_transformers_</span><span class="s2">[</span><span class="s5">&quot;encoder&quot;</span><span class="s2">].</span><span class="s1">categories_</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">assert_array_equal</span><span class="s2">(</span><span class="s1">categories</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">unique</span><span class="s2">(</span><span class="s1">f_cat</span><span class="s2">))</span>

    <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">hist_np</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">) == </span><span class="s1">len</span><span class="s2">(</span><span class="s1">hist_pd</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">)</span>
    <span class="s0">for </span><span class="s1">predictor_1</span><span class="s2">, </span><span class="s1">predictor_2 </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">hist_np</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">, </span><span class="s1">hist_pd</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">):</span>
        <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">predictor_1</span><span class="s2">[</span><span class="s3">0</span><span class="s2">].</span><span class="s1">nodes</span><span class="s2">) == </span><span class="s1">len</span><span class="s2">(</span><span class="s1">predictor_2</span><span class="s2">[</span><span class="s3">0</span><span class="s2">].</span><span class="s1">nodes</span><span class="s2">)</span>

    <span class="s1">score_np </span><span class="s2">= </span><span class="s1">hist_np</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">)</span>
    <span class="s1">score_pd </span><span class="s2">= </span><span class="s1">hist_pd</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X_test_df</span><span class="s2">, </span><span class="s1">y_test</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">score_np </span><span class="s2">== </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">score_pd</span><span class="s2">)</span>
    <span class="s1">assert_allclose</span><span class="s2">(</span><span class="s1">hist_np</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test</span><span class="s2">), </span><span class="s1">hist_pd</span><span class="s2">.</span><span class="s1">predict</span><span class="s2">(</span><span class="s1">X_test_df</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;dataframe_lib&quot;</span><span class="s2">, [</span><span class="s5">&quot;pandas&quot;</span><span class="s2">, </span><span class="s5">&quot;polars&quot;</span><span class="s2">])</span>
<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span>
    <span class="s5">&quot;HistGradientBoosting&quot;</span><span class="s2">,</span>
    <span class="s2">[</span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">, </span><span class="s1">HistGradientBoostingRegressor</span><span class="s2">],</span>
<span class="s2">)</span>
<span class="s0">def </span><span class="s1">test_dataframe_categorical_errors</span><span class="s2">(</span><span class="s1">dataframe_lib</span><span class="s2">, </span><span class="s1">HistGradientBoosting</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Check error cases for pandas categorical feature.&quot;&quot;&quot;</span>
    <span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s1">dataframe_lib</span><span class="s2">)</span>
    <span class="s1">msg </span><span class="s2">= </span><span class="s5">&quot;Categorical feature 'f_cat' is expected to have a cardinality &lt;= 16&quot;</span>
    <span class="s1">hist </span><span class="s2">= </span><span class="s1">HistGradientBoosting</span><span class="s2">(</span><span class="s1">categorical_features</span><span class="s2">=</span><span class="s5">&quot;from_dtype&quot;</span><span class="s2">, </span><span class="s1">max_bins</span><span class="s2">=</span><span class="s3">16</span><span class="s2">)</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">f_cat </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s3">100</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s3">100</span><span class="s2">).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">str</span><span class="s2">)</span>
    <span class="s1">X_df </span><span class="s2">= </span><span class="s1">_convert_container</span><span class="s2">(</span>
        <span class="s1">f_cat</span><span class="s2">[:, </span><span class="s0">None</span><span class="s2">], </span><span class="s1">dataframe_lib</span><span class="s2">, [</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">], </span><span class="s1">categorical_feature_names</span><span class="s2">=[</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">]</span>
    <span class="s2">)</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>

    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">raises</span><span class="s2">(</span><span class="s1">ValueError</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">msg</span><span class="s2">):</span>
        <span class="s1">hist</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X_df</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">mark</span><span class="s2">.</span><span class="s1">parametrize</span><span class="s2">(</span><span class="s5">&quot;dataframe_lib&quot;</span><span class="s2">, [</span><span class="s5">&quot;pandas&quot;</span><span class="s2">, </span><span class="s5">&quot;polars&quot;</span><span class="s2">])</span>
<span class="s0">def </span><span class="s1">test_categorical_different_order_same_model</span><span class="s2">(</span><span class="s1">dataframe_lib</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Check that the order of the categorical gives same model.&quot;&quot;&quot;</span>
    <span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s1">dataframe_lib</span><span class="s2">)</span>
    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">RandomState</span><span class="s2">(</span><span class="s3">42</span><span class="s2">)</span>
    <span class="s1">n_samples </span><span class="s2">= </span><span class="s3">1_000</span>
    <span class="s1">f_ints </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">randint</span><span class="s2">(</span><span class="s1">low</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">high</span><span class="s2">=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">)</span>

    <span class="s6"># Construct a target with some noise</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">f_ints</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">()</span>
    <span class="s1">flipped </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">choice</span><span class="s2">([</span><span class="s0">True</span><span class="s2">, </span><span class="s0">False</span><span class="s2">], </span><span class="s1">size</span><span class="s2">=</span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">p</span><span class="s2">=[</span><span class="s3">0.1</span><span class="s2">, </span><span class="s3">0.9</span><span class="s2">])</span>
    <span class="s1">y</span><span class="s2">[</span><span class="s1">flipped</span><span class="s2">] = </span><span class="s3">1 </span><span class="s2">- </span><span class="s1">y</span><span class="s2">[</span><span class="s1">flipped</span><span class="s2">]</span>

    <span class="s6"># Construct categorical where 0 -&gt; A and 1 -&gt; B and 1 -&gt; A and 0 -&gt; B</span>
    <span class="s1">f_cat_a_b </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">([</span><span class="s5">&quot;A&quot;</span><span class="s2">, </span><span class="s5">&quot;B&quot;</span><span class="s2">])[</span><span class="s1">f_ints</span><span class="s2">]</span>
    <span class="s1">f_cat_b_a </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">asarray</span><span class="s2">([</span><span class="s5">&quot;B&quot;</span><span class="s2">, </span><span class="s5">&quot;A&quot;</span><span class="s2">])[</span><span class="s1">f_ints</span><span class="s2">]</span>
    <span class="s1">df_a_b </span><span class="s2">= </span><span class="s1">_convert_container</span><span class="s2">(</span>
        <span class="s1">f_cat_a_b</span><span class="s2">[:, </span><span class="s0">None</span><span class="s2">],</span>
        <span class="s1">dataframe_lib</span><span class="s2">,</span>
        <span class="s2">[</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">],</span>
        <span class="s1">categorical_feature_names</span><span class="s2">=[</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">],</span>
    <span class="s2">)</span>
    <span class="s1">df_b_a </span><span class="s2">= </span><span class="s1">_convert_container</span><span class="s2">(</span>
        <span class="s1">f_cat_b_a</span><span class="s2">[:, </span><span class="s0">None</span><span class="s2">],</span>
        <span class="s1">dataframe_lib</span><span class="s2">,</span>
        <span class="s2">[</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">],</span>
        <span class="s1">categorical_feature_names</span><span class="s2">=[</span><span class="s5">&quot;f_cat&quot;</span><span class="s2">],</span>
    <span class="s2">)</span>

    <span class="s1">hist_a_b </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">categorical_features</span><span class="s2">=</span><span class="s5">&quot;from_dtype&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span>
    <span class="s2">)</span>
    <span class="s1">hist_b_a </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span>
        <span class="s1">categorical_features</span><span class="s2">=</span><span class="s5">&quot;from_dtype&quot;</span><span class="s2">, </span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span>
    <span class="s2">)</span>

    <span class="s1">hist_a_b</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">df_a_b</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">hist_b_a</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">df_b_a</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">hist_a_b</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">) == </span><span class="s1">len</span><span class="s2">(</span><span class="s1">hist_b_a</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">)</span>
    <span class="s0">for </span><span class="s1">predictor_1</span><span class="s2">, </span><span class="s1">predictor_2 </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">hist_a_b</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">, </span><span class="s1">hist_b_a</span><span class="s2">.</span><span class="s1">_predictors</span><span class="s2">):</span>
        <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">predictor_1</span><span class="s2">[</span><span class="s3">0</span><span class="s2">].</span><span class="s1">nodes</span><span class="s2">) == </span><span class="s1">len</span><span class="s2">(</span><span class="s1">predictor_2</span><span class="s2">[</span><span class="s3">0</span><span class="s2">].</span><span class="s1">nodes</span><span class="s2">)</span>


<span class="s6"># TODO(1.6): Remove warning and change default in 1.6</span>
<span class="s0">def </span><span class="s1">test_categorical_features_warn</span><span class="s2">():</span>
    <span class="s4">&quot;&quot;&quot;Raise warning when there are categorical features in the input DataFrame. 
 
    This is not tested for polars because polars categories must always be 
    strings and strings can only be handled as categories. Therefore the 
    situation in which a categorical column is currently being treated as 
    numbers and in the future will be treated as categories cannot occur with 
    polars. 
    &quot;&quot;&quot;</span>
    <span class="s1">pd </span><span class="s2">= </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s5">&quot;pandas&quot;</span><span class="s2">)</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">DataFrame</span><span class="s2">({</span><span class="s5">&quot;a&quot;</span><span class="s2">: </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">Series</span><span class="s2">([</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">], </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;category&quot;</span><span class="s2">), </span><span class="s5">&quot;b&quot;</span><span class="s2">: [</span><span class="s3">4</span><span class="s2">, </span><span class="s3">5</span><span class="s2">, </span><span class="s3">6</span><span class="s2">]})</span>
    <span class="s1">y </span><span class="s2">= [</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">]</span>
    <span class="s1">hist </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>

    <span class="s1">msg </span><span class="s2">= </span><span class="s5">&quot;The categorical_features parameter will change to 'from_dtype' in v1.6&quot;</span>
    <span class="s0">with </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">warns</span><span class="s2">(</span><span class="s1">FutureWarning</span><span class="s2">, </span><span class="s1">match</span><span class="s2">=</span><span class="s1">msg</span><span class="s2">):</span>
        <span class="s1">hist</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">get_different_bitness_node_ndarray</span><span class="s2">(</span><span class="s1">node_ndarray</span><span class="s2">):</span>
    <span class="s1">new_dtype_for_indexing_fields </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">int64 </span><span class="s0">if </span><span class="s1">_IS_32BIT </span><span class="s0">else </span><span class="s1">np</span><span class="s2">.</span><span class="s1">int32</span>

    <span class="s6"># field names in Node struct with np.intp types (see</span>
    <span class="s6"># sklearn/ensemble/_hist_gradient_boosting/common.pyx)</span>
    <span class="s1">indexing_field_names </span><span class="s2">= [</span><span class="s5">&quot;feature_idx&quot;</span><span class="s2">]</span>

    <span class="s1">new_dtype_dict </span><span class="s2">= {</span>
        <span class="s1">name</span><span class="s2">: </span><span class="s1">dtype </span><span class="s0">for </span><span class="s1">name</span><span class="s2">, (</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">_</span><span class="s2">) </span><span class="s0">in </span><span class="s1">node_ndarray</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">.</span><span class="s1">fields</span><span class="s2">.</span><span class="s1">items</span><span class="s2">()</span>
    <span class="s2">}</span>
    <span class="s0">for </span><span class="s1">name </span><span class="s0">in </span><span class="s1">indexing_field_names</span><span class="s2">:</span>
        <span class="s1">new_dtype_dict</span><span class="s2">[</span><span class="s1">name</span><span class="s2">] = </span><span class="s1">new_dtype_for_indexing_fields</span>

    <span class="s1">new_dtype </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">(</span>
        <span class="s2">{</span><span class="s5">&quot;names&quot;</span><span class="s2">: </span><span class="s1">list</span><span class="s2">(</span><span class="s1">new_dtype_dict</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">()), </span><span class="s5">&quot;formats&quot;</span><span class="s2">: </span><span class="s1">list</span><span class="s2">(</span><span class="s1">new_dtype_dict</span><span class="s2">.</span><span class="s1">values</span><span class="s2">())}</span>
    <span class="s2">)</span>
    <span class="s0">return </span><span class="s1">node_ndarray</span><span class="s2">.</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">new_dtype</span><span class="s2">, </span><span class="s1">casting</span><span class="s2">=</span><span class="s5">&quot;same_kind&quot;</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">reduce_predictor_with_different_bitness</span><span class="s2">(</span><span class="s1">predictor</span><span class="s2">):</span>
    <span class="s1">cls</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">state </span><span class="s2">= </span><span class="s1">predictor</span><span class="s2">.</span><span class="s1">__reduce__</span><span class="s2">()</span>

    <span class="s1">new_state </span><span class="s2">= </span><span class="s1">state</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">()</span>
    <span class="s1">new_state</span><span class="s2">[</span><span class="s5">&quot;nodes&quot;</span><span class="s2">] = </span><span class="s1">get_different_bitness_node_ndarray</span><span class="s2">(</span><span class="s1">new_state</span><span class="s2">[</span><span class="s5">&quot;nodes&quot;</span><span class="s2">])</span>

    <span class="s0">return </span><span class="s2">(</span><span class="s1">cls</span><span class="s2">, </span><span class="s1">args</span><span class="s2">, </span><span class="s1">new_state</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_different_bitness_pickle</span><span class="s2">():</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>

    <span class="s1">clf </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">3</span><span class="s2">)</span>
    <span class="s1">clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">score </span><span class="s2">= </span><span class="s1">clf</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">pickle_dump_with_different_bitness</span><span class="s2">():</span>
        <span class="s1">f </span><span class="s2">= </span><span class="s1">io</span><span class="s2">.</span><span class="s1">BytesIO</span><span class="s2">()</span>
        <span class="s1">p </span><span class="s2">= </span><span class="s1">pickle</span><span class="s2">.</span><span class="s1">Pickler</span><span class="s2">(</span><span class="s1">f</span><span class="s2">)</span>
        <span class="s1">p</span><span class="s2">.</span><span class="s1">dispatch_table </span><span class="s2">= </span><span class="s1">copyreg</span><span class="s2">.</span><span class="s1">dispatch_table</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">()</span>
        <span class="s1">p</span><span class="s2">.</span><span class="s1">dispatch_table</span><span class="s2">[</span><span class="s1">TreePredictor</span><span class="s2">] = </span><span class="s1">reduce_predictor_with_different_bitness</span>

        <span class="s1">p</span><span class="s2">.</span><span class="s1">dump</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">)</span>
        <span class="s1">f</span><span class="s2">.</span><span class="s1">seek</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">f</span>

    <span class="s6"># Simulate loading a pickle of the same model trained on a platform with different</span>
    <span class="s6"># bitness that than the platform it will be used to make predictions on:</span>
    <span class="s1">new_clf </span><span class="s2">= </span><span class="s1">pickle</span><span class="s2">.</span><span class="s1">load</span><span class="s2">(</span><span class="s1">pickle_dump_with_different_bitness</span><span class="s2">())</span>
    <span class="s1">new_score </span><span class="s2">= </span><span class="s1">new_clf</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">score </span><span class="s2">== </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">new_score</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_different_bitness_joblib_pickle</span><span class="s2">():</span>
    <span class="s6"># Make sure that a platform specific pickle generated on a 64 bit</span>
    <span class="s6"># platform can be converted at pickle load time into an estimator</span>
    <span class="s6"># with Cython code that works with the host's native integer precision</span>
    <span class="s6"># to index nodes in the tree data structure when the host is a 32 bit</span>
    <span class="s6"># platform (and vice versa).</span>
    <span class="s6">#</span>
    <span class="s6"># This is in particular useful to be able to train a model on a 64 bit Linux</span>
    <span class="s6"># server and deploy the model as part of a (32 bit) WASM in-browser</span>
    <span class="s6"># application using pyodide.</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y </span><span class="s2">= </span><span class="s1">make_classification</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>

    <span class="s1">clf </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">(</span><span class="s1">random_state</span><span class="s2">=</span><span class="s3">0</span><span class="s2">, </span><span class="s1">max_depth</span><span class="s2">=</span><span class="s3">3</span><span class="s2">)</span>
    <span class="s1">clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s1">score </span><span class="s2">= </span><span class="s1">clf</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">joblib_dump_with_different_bitness</span><span class="s2">():</span>
        <span class="s1">f </span><span class="s2">= </span><span class="s1">io</span><span class="s2">.</span><span class="s1">BytesIO</span><span class="s2">()</span>
        <span class="s1">p </span><span class="s2">= </span><span class="s1">NumpyPickler</span><span class="s2">(</span><span class="s1">f</span><span class="s2">)</span>
        <span class="s1">p</span><span class="s2">.</span><span class="s1">dispatch_table </span><span class="s2">= </span><span class="s1">copyreg</span><span class="s2">.</span><span class="s1">dispatch_table</span><span class="s2">.</span><span class="s1">copy</span><span class="s2">()</span>
        <span class="s1">p</span><span class="s2">.</span><span class="s1">dispatch_table</span><span class="s2">[</span><span class="s1">TreePredictor</span><span class="s2">] = </span><span class="s1">reduce_predictor_with_different_bitness</span>

        <span class="s1">p</span><span class="s2">.</span><span class="s1">dump</span><span class="s2">(</span><span class="s1">clf</span><span class="s2">)</span>
        <span class="s1">f</span><span class="s2">.</span><span class="s1">seek</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">f</span>

    <span class="s1">new_clf </span><span class="s2">= </span><span class="s1">joblib</span><span class="s2">.</span><span class="s1">load</span><span class="s2">(</span><span class="s1">joblib_dump_with_different_bitness</span><span class="s2">())</span>
    <span class="s1">new_score </span><span class="s2">= </span><span class="s1">new_clf</span><span class="s2">.</span><span class="s1">score</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
    <span class="s0">assert </span><span class="s1">score </span><span class="s2">== </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">approx</span><span class="s2">(</span><span class="s1">new_score</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">test_pandas_nullable_dtype</span><span class="s2">():</span>
    <span class="s6"># Non regression test for https://github.com/scikit-learn/scikit-learn/issues/28317</span>
    <span class="s1">pd </span><span class="s2">= </span><span class="s1">pytest</span><span class="s2">.</span><span class="s1">importorskip</span><span class="s2">(</span><span class="s5">&quot;pandas&quot;</span><span class="s2">)</span>

    <span class="s1">rng </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">random</span><span class="s2">.</span><span class="s1">default_rng</span><span class="s2">(</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">X </span><span class="s2">= </span><span class="s1">pd</span><span class="s2">.</span><span class="s1">DataFrame</span><span class="s2">({</span><span class="s5">&quot;a&quot;</span><span class="s2">: </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">integers</span><span class="s2">(</span><span class="s3">10</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)}).</span><span class="s1">astype</span><span class="s2">(</span><span class="s1">pd</span><span class="s2">.</span><span class="s1">Int64Dtype</span><span class="s2">())</span>
    <span class="s1">y </span><span class="s2">= </span><span class="s1">rng</span><span class="s2">.</span><span class="s1">integers</span><span class="s2">(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size</span><span class="s2">=</span><span class="s3">100</span><span class="s2">)</span>

    <span class="s1">clf </span><span class="s2">= </span><span class="s1">HistGradientBoostingClassifier</span><span class="s2">()</span>
    <span class="s1">clf</span><span class="s2">.</span><span class="s1">fit</span><span class="s2">(</span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">)</span>
</pre>
</body>
</html>