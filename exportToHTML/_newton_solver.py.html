<html>
<head>
<title>_newton_solver.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #5f826b; font-style: italic;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_newton_solver.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: The scikit-learn developers</span>
<span class="s0"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="s2">&quot;&quot;&quot; 
Newton solver for Generalized Linear Models 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABC</span><span class="s4">, </span><span class="s1">abstractmethod</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span>
<span class="s3">import </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">optimize</span>

<span class="s3">from </span><span class="s4">...</span><span class="s1">_loss</span><span class="s4">.</span><span class="s1">loss </span><span class="s3">import </span><span class="s1">HalfSquaredError</span>
<span class="s3">from </span><span class="s4">...</span><span class="s1">exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s4">...</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">optimize </span><span class="s3">import </span><span class="s1">_check_optimize_result</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">_linear_loss </span><span class="s3">import </span><span class="s1">LinearModelLoss</span>


<span class="s3">class </span><span class="s1">NewtonSolver</span><span class="s4">(</span><span class="s1">ABC</span><span class="s4">):</span>
    <span class="s2">&quot;&quot;&quot;Newton solver for GLMs. 
 
    This class implements Newton/2nd-order optimization routines for GLMs. Each Newton 
    iteration aims at finding the Newton step which is done by the inner solver. With 
    Hessian H, gradient g and coefficients coef, one step solves: 
 
        H @ coef_newton = -g 
 
    For our GLM / LinearModelLoss, we have gradient g and Hessian H: 
 
        g = X.T @ loss.gradient + l2_reg_strength * coef 
        H = X.T @ diag(loss.hessian) @ X + l2_reg_strength * identity 
 
    Backtracking line search updates coef = coef_old + t * coef_newton for some t in 
    (0, 1]. 
 
    This is a base class, actual implementations (child classes) may deviate from the 
    above pattern and use structure specific tricks. 
 
    Usage pattern: 
        - initialize solver: sol = NewtonSolver(...) 
        - solve the problem: sol.solve(X, y, sample_weight) 
 
    References 
    ---------- 
    - Jorge Nocedal, Stephen J. Wright. (2006) &quot;Numerical Optimization&quot; 
      2nd edition 
      https://doi.org/10.1007/978-0-387-40065-5 
 
    - Stephen P. Boyd, Lieven Vandenberghe. (2004) &quot;Convex Optimization.&quot; 
      Cambridge University Press, 2004. 
      https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf 
 
    Parameters 
    ---------- 
    coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,) 
        Initial coefficients of a linear model. 
        If shape (n_classes * n_dof,), the classes of one feature are contiguous, 
        i.e. one reconstructs the 2d-array via 
        coef.reshape((n_classes, -1), order=&quot;F&quot;). 
 
    linear_loss : LinearModelLoss 
        The loss to be minimized. 
 
    l2_reg_strength : float, default=0.0 
        L2 regularization strength. 
 
    tol : float, default=1e-4 
        The optimization problem is solved when each of the following condition is 
        fulfilled: 
        1. maximum |gradient| &lt;= tol 
        2. Newton decrement d: 1/2 * d^2 &lt;= tol 
 
    max_iter : int, default=100 
        Maximum number of Newton steps allowed. 
 
    n_threads : int, default=1 
        Number of OpenMP threads to use for the computation of the Hessian and gradient 
        of the loss function. 
 
    Attributes 
    ---------- 
    coef_old : ndarray of shape coef.shape 
        Coefficient of previous iteration. 
 
    coef_newton : ndarray of shape coef.shape 
        Newton step. 
 
    gradient : ndarray of shape coef.shape 
        Gradient of the loss w.r.t. the coefficients. 
 
    gradient_old : ndarray of shape coef.shape 
        Gradient of previous iteration. 
 
    loss_value : float 
        Value of objective function = loss + penalty. 
 
    loss_value_old : float 
        Value of objective function of previous itertion. 
 
    raw_prediction : ndarray of shape (n_samples,) or (n_samples, n_classes) 
 
    converged : bool 
        Indicator for convergence of the solver. 
 
    iteration : int 
        Number of Newton steps, i.e. calls to inner_solve 
 
    use_fallback_lbfgs_solve : bool 
        If set to True, the solver will resort to call LBFGS to finish the optimisation 
        procedure in case of convergence issues. 
 
    gradient_times_newton : float 
        gradient @ coef_newton, set in inner_solve and used by line_search. If the 
        Newton step is a descent direction, this is negative. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">coef</span><span class="s4">,</span>
        <span class="s1">linear_loss</span><span class="s4">=</span><span class="s1">LinearModelLoss</span><span class="s4">(</span><span class="s1">base_loss</span><span class="s4">=</span><span class="s1">HalfSquaredError</span><span class="s4">(), </span><span class="s1">fit_intercept</span><span class="s4">=</span><span class="s3">True</span><span class="s4">),</span>
        <span class="s1">l2_reg_strength</span><span class="s4">=</span><span class="s5">0.0</span><span class="s4">,</span>
        <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
        <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">100</span><span class="s4">,</span>
        <span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
        <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">coef </span><span class="s4">= </span><span class="s1">coef</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss </span><span class="s4">= </span><span class="s1">linear_loss</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">l2_reg_strength </span><span class="s4">= </span><span class="s1">l2_reg_strength</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">tol </span><span class="s4">= </span><span class="s1">tol</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s4">= </span><span class="s1">max_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_threads </span><span class="s4">= </span><span class="s1">n_threads</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">= </span><span class="s1">verbose</span>

    <span class="s3">def </span><span class="s1">setup</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Precomputations 
 
        If None, initializes: 
            - self.coef 
        Sets: 
            - self.raw_prediction 
            - self.loss_value 
        &quot;&quot;&quot;</span>
        <span class="s1">_</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">weight_intercept_raw</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">loss</span><span class="s4">(</span>
            <span class="s1">coef</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">,</span>
            <span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">,</span>
            <span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">l2_reg_strength</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">l2_reg_strength</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_threads</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">raw_prediction</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">update_gradient_hessian</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Update gradient and Hessian.&quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">inner_solve</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Compute Newton step. 
 
        Sets: 
            - self.coef_newton 
            - self.gradient_times_newton 
        &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">fallback_lbfgs_solve</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Fallback solver in case of emergency. 
 
        If a solver detects convergence problems, it may fall back to this methods in 
        the hope to exit with success instead of raising an error. 
 
        Sets: 
            - self.coef 
            - self.converged 
        &quot;&quot;&quot;</span>
        <span class="s1">opt_res </span><span class="s4">= </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">optimize</span><span class="s4">.</span><span class="s1">minimize</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">loss_gradient</span><span class="s4">,</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">,</span>
            <span class="s1">method</span><span class="s4">=</span><span class="s6">&quot;L-BFGS-B&quot;</span><span class="s4">,</span>
            <span class="s1">jac</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
            <span class="s1">options</span><span class="s4">={</span>
                <span class="s6">&quot;maxiter&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span><span class="s4">,</span>
                <span class="s6">&quot;maxls&quot;</span><span class="s4">: </span><span class="s5">50</span><span class="s4">,  </span><span class="s0"># default is 20</span>
                <span class="s6">&quot;iprint&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">- </span><span class="s5">1</span><span class="s4">,</span>
                <span class="s6">&quot;gtol&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">,</span>
                <span class="s6">&quot;ftol&quot;</span><span class="s4">: </span><span class="s5">64 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">).</span><span class="s1">eps</span><span class="s4">,</span>
            <span class="s4">},</span>
            <span class="s1">args</span><span class="s4">=(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">l2_reg_strength</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_threads</span><span class="s4">),</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_iter_ </span><span class="s4">= </span><span class="s1">_check_optimize_result</span><span class="s4">(</span><span class="s6">&quot;lbfgs&quot;</span><span class="s4">, </span><span class="s1">opt_res</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">coef </span><span class="s4">= </span><span class="s1">opt_res</span><span class="s4">.</span><span class="s1">x</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">converged </span><span class="s4">= </span><span class="s1">opt_res</span><span class="s4">.</span><span class="s1">status </span><span class="s4">== </span><span class="s5">0</span>

    <span class="s3">def </span><span class="s1">line_search</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Backtracking line search. 
 
        Sets: 
            - self.coef_old 
            - self.coef 
            - self.loss_value_old 
            - self.loss_value 
            - self.gradient_old 
            - self.gradient 
            - self.raw_prediction 
        &quot;&quot;&quot;</span>
        <span class="s0"># line search parameters</span>
        <span class="s1">beta</span><span class="s4">, </span><span class="s1">sigma </span><span class="s4">= </span><span class="s5">0.5</span><span class="s4">, </span><span class="s5">0.00048828125  </span><span class="s0"># 1/2, 1/2**11</span>
        <span class="s1">eps </span><span class="s4">= </span><span class="s5">16 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">).</span><span class="s1">eps</span>
        <span class="s1">t </span><span class="s4">= </span><span class="s5">1  </span><span class="s0"># step size</span>

        <span class="s0"># gradient_times_newton = self.gradient @ self.coef_newton</span>
        <span class="s0"># was computed in inner_solve.</span>
        <span class="s1">armijo_term </span><span class="s4">= </span><span class="s1">sigma </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient_times_newton</span>
        <span class="s1">_</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">raw_prediction_newton </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">weight_intercept_raw</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">coef_newton</span><span class="s4">, </span><span class="s1">X</span>
        <span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">coef_old </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value_old </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">gradient_old </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient</span>

        <span class="s0"># np.sum(np.abs(self.gradient_old))</span>
        <span class="s1">sum_abs_grad_old </span><span class="s4">= -</span><span class="s5">1</span>

        <span class="s1">is_verbose </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">&gt;= </span><span class="s5">2</span>
        <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;  Backtracking Line Search&quot;</span><span class="s4">)</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;    eps=16 * finfo.eps=</span><span class="s3">{</span><span class="s1">eps</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>

        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">21</span><span class="s4">):  </span><span class="s0"># until and including t = beta**20 ~ 1e-6</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">coef </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef_old </span><span class="s4">+ </span><span class="s1">t </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef_newton</span>
            <span class="s1">raw </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">raw_prediction </span><span class="s4">+ </span><span class="s1">t </span><span class="s4">* </span><span class="s1">raw_prediction_newton</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">loss_gradient</span><span class="s4">(</span>
                <span class="s1">coef</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">,</span>
                <span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">,</span>
                <span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">,</span>
                <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
                <span class="s1">l2_reg_strength</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">l2_reg_strength</span><span class="s4">,</span>
                <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_threads</span><span class="s4">,</span>
                <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s0"># Note: If coef_newton is too large, loss_gradient may produce inf values,</span>
            <span class="s0"># potentially accompanied by a RuntimeWarning.</span>
            <span class="s0"># This case will be captured by the Armijo condition.</span>

            <span class="s0"># 1. Check Armijo / sufficient decrease condition.</span>
            <span class="s0"># The smaller (more negative) the better.</span>
            <span class="s1">loss_improvement </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value_old</span>
            <span class="s1">check </span><span class="s4">= </span><span class="s1">loss_improvement </span><span class="s4">&lt;= </span><span class="s1">t </span><span class="s4">* </span><span class="s1">armijo_term</span>
            <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">f&quot;    line search iteration=</span><span class="s3">{</span><span class="s1">i</span><span class="s4">+</span><span class="s5">1</span><span class="s3">}</span><span class="s6">, step size=</span><span class="s3">{</span><span class="s1">t</span><span class="s3">}\n</span><span class="s6">&quot;</span>
                    <span class="s6">f&quot;      check loss improvement &lt;= armijo term: </span><span class="s3">{</span><span class="s1">loss_improvement</span><span class="s3">} </span><span class="s6">&quot;</span>
                    <span class="s6">f&quot;&lt;= </span><span class="s3">{</span><span class="s1">t </span><span class="s4">* </span><span class="s1">armijo_term</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s6">&quot;</span>
                <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">check</span><span class="s4">:</span>
                <span class="s3">break</span>
            <span class="s0"># 2. Deal with relative loss differences around machine precision.</span>
            <span class="s1">tiny_loss </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">loss_value_old </span><span class="s4">* </span><span class="s1">eps</span><span class="s4">)</span>
            <span class="s1">check </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">loss_improvement</span><span class="s4">) &lt;= </span><span class="s1">tiny_loss</span>
            <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">&quot;      check loss |improvement| &lt;= eps * |loss_old|:&quot;</span>
                    <span class="s6">f&quot; </span><span class="s3">{</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">loss_improvement</span><span class="s4">)</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">tiny_loss</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s6">&quot;</span>
                <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">check</span><span class="s4">:</span>
                <span class="s3">if </span><span class="s1">sum_abs_grad_old </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
                    <span class="s1">sum_abs_grad_old </span><span class="s4">= </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient_old</span><span class="s4">, </span><span class="s1">ord</span><span class="s4">=</span><span class="s5">1</span><span class="s4">)</span>
                <span class="s0"># 2.1 Check sum of absolute gradients as alternative condition.</span>
                <span class="s1">sum_abs_grad </span><span class="s4">= </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient</span><span class="s4">, </span><span class="s1">ord</span><span class="s4">=</span><span class="s5">1</span><span class="s4">)</span>
                <span class="s1">check </span><span class="s4">= </span><span class="s1">sum_abs_grad </span><span class="s4">&lt; </span><span class="s1">sum_abs_grad_old</span>
                <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                    <span class="s1">print</span><span class="s4">(</span>
                        <span class="s6">&quot;      check sum(|gradient|) &lt; sum(|gradient_old|): &quot;</span>
                        <span class="s6">f&quot;</span><span class="s3">{</span><span class="s1">sum_abs_grad</span><span class="s3">} </span><span class="s6">&lt; </span><span class="s3">{</span><span class="s1">sum_abs_grad_old</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s6">&quot;</span>
                    <span class="s4">)</span>
                <span class="s3">if </span><span class="s1">check</span><span class="s4">:</span>
                    <span class="s3">break</span>

            <span class="s1">t </span><span class="s4">*= </span><span class="s1">beta</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s4">(</span>
                    <span class="s6">f&quot;Line search of Newton solver </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s3">} </span><span class="s6">at&quot;</span>
                    <span class="s6">f&quot; iteration #</span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">iteration</span><span class="s3">} </span><span class="s6">did no converge after 21 line search&quot;</span>
                    <span class="s6">&quot; refinement iterations. It will now resort to lbfgs instead.&quot;</span>
                <span class="s4">),</span>
                <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;  Line search did not converge and resorts to lbfgs instead.&quot;</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve </span><span class="s4">= </span><span class="s3">True</span>
            <span class="s3">return</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw</span>

    <span class="s3">def </span><span class="s1">check_convergence</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Check for convergence. 
 
        Sets self.converged. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;  Check Convergence&quot;</span><span class="s4">)</span>
        <span class="s0"># Note: Checking maximum relative change of coefficient &lt;= tol is a bad</span>
        <span class="s0"># convergence criterion because even a large step could have brought us close</span>
        <span class="s0"># to the true minimum.</span>
        <span class="s0"># coef_step = self.coef - self.coef_old</span>
        <span class="s0"># check = np.max(np.abs(coef_step) / np.maximum(1, np.abs(self.coef_old)))</span>

        <span class="s0"># 1. Criterion: maximum |gradient| &lt;= tol</span>
        <span class="s0">#    The gradient was already updated in line_search()</span>
        <span class="s1">check </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient</span><span class="s4">))</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;    1. max |gradient| </span><span class="s3">{</span><span class="s1">check</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">check </span><span class="s4">&gt; </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">:</span>
            <span class="s3">return</span>

        <span class="s0"># 2. Criterion: For Newton decrement d, check 1/2 * d^2 &lt;= tol</span>
        <span class="s0">#       d = sqrt(grad @ hessian^-1 @ grad)</span>
        <span class="s0">#         = sqrt(coef_newton @ hessian @ coef_newton)</span>
        <span class="s0">#    See Boyd, Vanderberghe (2009) &quot;Convex Optimization&quot; Chapter 9.5.1.</span>
        <span class="s1">d2 </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef_newton </span><span class="s4">@ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hessian </span><span class="s4">@ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef_newton</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;    2. Newton decrement </span><span class="s3">{</span><span class="s5">0.5 </span><span class="s4">* </span><span class="s1">d2</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s5">0.5 </span><span class="s4">* </span><span class="s1">d2 </span><span class="s4">&gt; </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tol</span><span class="s4">:</span>
            <span class="s3">return</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
            <span class="s1">loss_value </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">loss</span><span class="s4">(</span>
                <span class="s1">coef</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">,</span>
                <span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">,</span>
                <span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">,</span>
                <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
                <span class="s1">l2_reg_strength</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">l2_reg_strength</span><span class="s4">,</span>
                <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_threads</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;  Solver did converge at loss = </span><span class="s3">{</span><span class="s1">loss_value</span><span class="s3">}</span><span class="s6">.&quot;</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">converged </span><span class="s4">= </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">finalize</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Finalize the solvers results. 
 
        Some solvers may need this, others not. 
        &quot;&quot;&quot;</span>
        <span class="s3">pass</span>

    <span class="s3">def </span><span class="s1">solve</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s2">&quot;&quot;&quot;Solve the optimization problem. 
 
        This is the main routine. 
 
        Order of calls: 
            self.setup() 
            while iteration: 
                self.update_gradient_hessian() 
                self.inner_solve() 
                self.line_search() 
                self.check_convergence() 
            self.finalize() 
 
        Returns 
        ------- 
        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,) 
            Solution of the optimization problem. 
        &quot;&quot;&quot;</span>
        <span class="s0"># setup usually:</span>
        <span class="s0">#   - initializes self.coef if needed</span>
        <span class="s0">#   - initializes and calculates self.raw_predictions, self.loss_value</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">setup</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">iteration </span><span class="s4">= </span><span class="s5">1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">converged </span><span class="s4">= </span><span class="s3">False</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve </span><span class="s4">= </span><span class="s3">False</span>

        <span class="s3">while </span><span class="s1">self</span><span class="s4">.</span><span class="s1">iteration </span><span class="s4">&lt;= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s3">and not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">converged</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;Newton iter=</span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">iteration</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>

            <span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve </span><span class="s4">= </span><span class="s3">False  </span><span class="s0"># Fallback solver.</span>

            <span class="s0"># 1. Update Hessian and gradient</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">update_gradient_hessian</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>

            <span class="s0"># TODO:</span>
            <span class="s0"># if iteration == 1:</span>
            <span class="s0"># We might stop early, e.g. we already are close to the optimum,</span>
            <span class="s0"># usually detected by zero gradients at this stage.</span>

            <span class="s0"># 2. Inner solver</span>
            <span class="s0">#    Calculate Newton step/direction</span>
            <span class="s0">#    This usually sets self.coef_newton and self.gradient_times_newton.</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">inner_solve</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve</span><span class="s4">:</span>
                <span class="s3">break</span>

            <span class="s0"># 3. Backtracking line search</span>
            <span class="s0">#    This usually sets self.coef_old, self.coef, self.loss_value_old</span>
            <span class="s0">#    self.loss_value, self.gradient_old, self.gradient,</span>
            <span class="s0">#    self.raw_prediction.</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">line_search</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve</span><span class="s4">:</span>
                <span class="s3">break</span>

            <span class="s0"># 4. Check convergence</span>
            <span class="s0">#    Sets self.converged.</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">check_convergence</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>

            <span class="s0"># 5. Next iteration</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">iteration </span><span class="s4">+= </span><span class="s5">1</span>

        <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">converged</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve</span><span class="s4">:</span>
                <span class="s0"># Note: The fallback solver circumvents check_convergence and relies on</span>
                <span class="s0"># the convergence checks of lbfgs instead. Enough warnings have been</span>
                <span class="s0"># raised on the way.</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">fallback_lbfgs_solve</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                    <span class="s4">(</span>
                        <span class="s6">f&quot;Newton solver did not converge after </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">iteration </span><span class="s4">- </span><span class="s5">1</span><span class="s3">} </span><span class="s6">&quot;</span>
                        <span class="s6">&quot;iterations.&quot;</span>
                    <span class="s4">),</span>
                    <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
                <span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">iteration </span><span class="s4">-= </span><span class="s5">1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">finalize</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span>


<span class="s3">class </span><span class="s1">NewtonCholeskySolver</span><span class="s4">(</span><span class="s1">NewtonSolver</span><span class="s4">):</span>
    <span class="s2">&quot;&quot;&quot;Cholesky based Newton solver. 
 
    Inner solver for finding the Newton step H w_newton = -g uses Cholesky based linear 
    solver. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">setup</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">setup</span><span class="s4">(</span><span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s1">n_dof </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">fit_intercept</span><span class="s4">:</span>
            <span class="s1">n_dof </span><span class="s4">+= </span><span class="s5">1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">hessian </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">, </span><span class="s1">shape</span><span class="s4">=(</span><span class="s1">n_dof</span><span class="s4">, </span><span class="s1">n_dof</span><span class="s4">))</span>

    <span class="s3">def </span><span class="s1">update_gradient_hessian</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s1">_</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hessian_warning </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">linear_loss</span><span class="s4">.</span><span class="s1">gradient_hessian</span><span class="s4">(</span>
            <span class="s1">coef</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef</span><span class="s4">,</span>
            <span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">,</span>
            <span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">l2_reg_strength</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">l2_reg_strength</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_threads</span><span class="s4">,</span>
            <span class="s1">gradient_out</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient</span><span class="s4">,</span>
            <span class="s1">hessian_out</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">hessian</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">raw_prediction</span><span class="s4">,  </span><span class="s0"># this was updated in line_search</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">inner_solve</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hessian_warning</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s4">(</span>
                    <span class="s6">f&quot;The inner solver of </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s3">} </span><span class="s6">detected a &quot;</span>
                    <span class="s6">&quot;pointwise hessian with many negative values at iteration &quot;</span>
                    <span class="s6">f&quot;#</span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">iteration</span><span class="s3">}</span><span class="s6">. It will now resort to lbfgs instead.&quot;</span>
                <span class="s4">),</span>
                <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">&quot;  The inner solver detected a pointwise Hessian with many &quot;</span>
                    <span class="s6">&quot;negative values and resorts to lbfgs instead.&quot;</span>
                <span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve </span><span class="s4">= </span><span class="s3">True</span>
            <span class="s3">return</span>

        <span class="s3">try</span><span class="s4">:</span>
            <span class="s3">with </span><span class="s1">warnings</span><span class="s4">.</span><span class="s1">catch_warnings</span><span class="s4">():</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">simplefilter</span><span class="s4">(</span><span class="s6">&quot;error&quot;</span><span class="s4">, </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">LinAlgWarning</span><span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">coef_newton </span><span class="s4">= </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">solve</span><span class="s4">(</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">hessian</span><span class="s4">, -</span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient</span><span class="s4">, </span><span class="s1">check_finite</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">assume_a</span><span class="s4">=</span><span class="s6">&quot;sym&quot;</span>
                <span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">gradient_times_newton </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient </span><span class="s4">@ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef_newton</span>
                <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gradient_times_newton </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
                    <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                        <span class="s1">print</span><span class="s4">(</span>
                            <span class="s6">&quot;  The inner solver found a Newton step that is not a &quot;</span>
                            <span class="s6">&quot;descent direction and resorts to LBFGS steps instead.&quot;</span>
                        <span class="s4">)</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve </span><span class="s4">= </span><span class="s3">True</span>
                    <span class="s3">return</span>
        <span class="s3">except </span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">LinAlgError</span><span class="s4">, </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">LinAlgWarning</span><span class="s4">) </span><span class="s3">as </span><span class="s1">e</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s6">f&quot;The inner solver of </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s3">} </span><span class="s6">stumbled upon a &quot;</span>
                <span class="s6">&quot;singular or very ill-conditioned Hessian matrix at iteration &quot;</span>
                <span class="s6">f&quot;#</span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">iteration</span><span class="s3">}</span><span class="s6">. It will now resort to lbfgs instead.</span><span class="s3">\n</span><span class="s6">&quot;</span>
                <span class="s6">&quot;Further options are to use another solver or to avoid such situation &quot;</span>
                <span class="s6">&quot;in the first place. Possible remedies are removing collinear features&quot;</span>
                <span class="s6">&quot; of X or increasing the penalization strengths.</span><span class="s3">\n</span><span class="s6">&quot;</span>
                <span class="s6">&quot;The original Linear Algebra message was:</span><span class="s3">\n</span><span class="s6">&quot; </span><span class="s4">+ </span><span class="s1">str</span><span class="s4">(</span><span class="s1">e</span><span class="s4">),</span>
                <span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">LinAlgWarning</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s0"># Possible causes:</span>
            <span class="s0"># 1. hess_pointwise is negative. But this is already taken care in</span>
            <span class="s0">#    LinearModelLoss.gradient_hessian.</span>
            <span class="s0"># 2. X is singular or ill-conditioned</span>
            <span class="s0">#    This might be the most probable cause.</span>
            <span class="s0">#</span>
            <span class="s0"># There are many possible ways to deal with this situation. Most of them</span>
            <span class="s0"># add, explicitly or implicitly, a matrix to the hessian to make it</span>
            <span class="s0"># positive definite, confer to Chapter 3.4 of Nocedal &amp; Wright 2nd ed.</span>
            <span class="s0"># Instead, we resort to lbfgs.</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">&quot;  The inner solver stumbled upon an singular or ill-conditioned &quot;</span>
                    <span class="s6">&quot;Hessian matrix and resorts to LBFGS instead.&quot;</span>
                <span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">use_fallback_lbfgs_solve </span><span class="s4">= </span><span class="s3">True</span>
            <span class="s3">return</span>
</pre>
</body>
</html>