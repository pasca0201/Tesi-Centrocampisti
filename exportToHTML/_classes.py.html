<html>
<head>
<title>_classes.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #6aab73;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_classes.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
This module gathers tree-based methods, including decision, regression and 
randomized trees. Single and multi-output problems are both handled. 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="s2">#          Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt;</span>
<span class="s2">#          Brian Holt &lt;bdholt1@gmail.com&gt;</span>
<span class="s2">#          Noel Dawe &lt;noel@dawe.me&gt;</span>
<span class="s2">#          Satrajit Gosh &lt;satrajit.ghosh@gmail.com&gt;</span>
<span class="s2">#          Joly Arnaud &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="s2">#          Fares Hedayati &lt;fares.hedayati@gmail.com&gt;</span>
<span class="s2">#          Nelson Liu &lt;nelson@nelsonliu.me&gt;</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">copy</span>
<span class="s3">import </span><span class="s1">numbers</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABCMeta</span><span class="s4">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">math </span><span class="s3">import </span><span class="s1">ceil</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">sparse </span><span class="s3">import </span><span class="s1">issparse</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">base </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">BaseEstimator</span><span class="s4">,</span>
    <span class="s1">ClassifierMixin</span><span class="s4">,</span>
    <span class="s1">MultiOutputMixin</span><span class="s4">,</span>
    <span class="s1">RegressorMixin</span><span class="s4">,</span>
    <span class="s1">_fit_context</span><span class="s4">,</span>
    <span class="s1">clone</span><span class="s4">,</span>
    <span class="s1">is_classifier</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils </span><span class="s3">import </span><span class="s1">Bunch</span><span class="s4">, </span><span class="s1">check_random_state</span><span class="s4">, </span><span class="s1">compute_sample_weight</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s1">Hidden</span><span class="s4">, </span><span class="s1">Interval</span><span class="s4">, </span><span class="s1">RealNotInt</span><span class="s4">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_assert_all_finite_element_wise</span><span class="s4">,</span>
    <span class="s1">_check_sample_weight</span><span class="s4">,</span>
    <span class="s1">assert_all_finite</span><span class="s4">,</span>
    <span class="s1">check_is_fitted</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">. </span><span class="s3">import </span><span class="s1">_criterion</span><span class="s4">, </span><span class="s1">_splitter</span><span class="s4">, </span><span class="s1">_tree</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_criterion </span><span class="s3">import </span><span class="s1">Criterion</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_splitter </span><span class="s3">import </span><span class="s1">Splitter</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_tree </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">BestFirstTreeBuilder</span><span class="s4">,</span>
    <span class="s1">DepthFirstTreeBuilder</span><span class="s4">,</span>
    <span class="s1">Tree</span><span class="s4">,</span>
    <span class="s1">_build_pruned_tree_ccp</span><span class="s4">,</span>
    <span class="s1">ccp_pruning_path</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_utils </span><span class="s3">import </span><span class="s1">_any_isnan_axis0</span>

<span class="s1">__all__ </span><span class="s4">= [</span>
    <span class="s5">&quot;DecisionTreeClassifier&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;DecisionTreeRegressor&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;ExtraTreeClassifier&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;ExtraTreeRegressor&quot;</span><span class="s4">,</span>
<span class="s4">]</span>


<span class="s2"># =============================================================================</span>
<span class="s2"># Types and constants</span>
<span class="s2"># =============================================================================</span>

<span class="s1">DTYPE </span><span class="s4">= </span><span class="s1">_tree</span><span class="s4">.</span><span class="s1">DTYPE</span>
<span class="s1">DOUBLE </span><span class="s4">= </span><span class="s1">_tree</span><span class="s4">.</span><span class="s1">DOUBLE</span>

<span class="s1">CRITERIA_CLF </span><span class="s4">= {</span>
    <span class="s5">&quot;gini&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">Gini</span><span class="s4">,</span>
    <span class="s5">&quot;log_loss&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">Entropy</span><span class="s4">,</span>
    <span class="s5">&quot;entropy&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">Entropy</span><span class="s4">,</span>
<span class="s4">}</span>
<span class="s1">CRITERIA_REG </span><span class="s4">= {</span>
    <span class="s5">&quot;squared_error&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">MSE</span><span class="s4">,</span>
    <span class="s5">&quot;friedman_mse&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">FriedmanMSE</span><span class="s4">,</span>
    <span class="s5">&quot;absolute_error&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">MAE</span><span class="s4">,</span>
    <span class="s5">&quot;poisson&quot;</span><span class="s4">: </span><span class="s1">_criterion</span><span class="s4">.</span><span class="s1">Poisson</span><span class="s4">,</span>
<span class="s4">}</span>

<span class="s1">DENSE_SPLITTERS </span><span class="s4">= {</span><span class="s5">&quot;best&quot;</span><span class="s4">: </span><span class="s1">_splitter</span><span class="s4">.</span><span class="s1">BestSplitter</span><span class="s4">, </span><span class="s5">&quot;random&quot;</span><span class="s4">: </span><span class="s1">_splitter</span><span class="s4">.</span><span class="s1">RandomSplitter</span><span class="s4">}</span>

<span class="s1">SPARSE_SPLITTERS </span><span class="s4">= {</span>
    <span class="s5">&quot;best&quot;</span><span class="s4">: </span><span class="s1">_splitter</span><span class="s4">.</span><span class="s1">BestSparseSplitter</span><span class="s4">,</span>
    <span class="s5">&quot;random&quot;</span><span class="s4">: </span><span class="s1">_splitter</span><span class="s4">.</span><span class="s1">RandomSparseSplitter</span><span class="s4">,</span>
<span class="s4">}</span>

<span class="s2"># =============================================================================</span>
<span class="s2"># Base decision tree</span>
<span class="s2"># =============================================================================</span>


<span class="s3">class </span><span class="s1">BaseDecisionTree</span><span class="s4">(</span><span class="s1">MultiOutputMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span><span class="s4">, </span><span class="s1">metaclass</span><span class="s4">=</span><span class="s1">ABCMeta</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Base class for decision trees. 
 
    Warning: This class should not be used directly. 
    Use derived classes instead. 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s5">&quot;splitter&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;best&quot;</span><span class="s4">, </span><span class="s5">&quot;random&quot;</span><span class="s4">})],</span>
        <span class="s5">&quot;max_depth&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;min_samples_split&quot;</span><span class="s4">: [</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">2</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">),</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">RealNotInt</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;right&quot;</span><span class="s4">),</span>
        <span class="s4">],</span>
        <span class="s5">&quot;min_samples_leaf&quot;</span><span class="s4">: [</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">),</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">RealNotInt</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;neither&quot;</span><span class="s4">),</span>
        <span class="s4">],</span>
        <span class="s5">&quot;min_weight_fraction_leaf&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">0.5</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;max_features&quot;</span><span class="s4">: [</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">),</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">RealNotInt</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;right&quot;</span><span class="s4">),</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;sqrt&quot;</span><span class="s4">, </span><span class="s5">&quot;log2&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s4">: [</span><span class="s5">&quot;random_state&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">2</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;min_impurity_decrease&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;ccp_alpha&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;monotonic_cst&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">criterion</span><span class="s4">,</span>
        <span class="s1">splitter</span><span class="s4">,</span>
        <span class="s1">max_depth</span><span class="s4">,</span>
        <span class="s1">min_samples_split</span><span class="s4">,</span>
        <span class="s1">min_samples_leaf</span><span class="s4">,</span>
        <span class="s1">min_weight_fraction_leaf</span><span class="s4">,</span>
        <span class="s1">max_features</span><span class="s4">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">,</span>
        <span class="s1">min_impurity_decrease</span><span class="s4">,</span>
        <span class="s1">class_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">criterion </span><span class="s4">= </span><span class="s1">criterion</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">splitter </span><span class="s4">= </span><span class="s1">splitter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_depth </span><span class="s4">= </span><span class="s1">max_depth</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_split </span><span class="s4">= </span><span class="s1">min_samples_split</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_leaf </span><span class="s4">= </span><span class="s1">min_samples_leaf</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">min_weight_fraction_leaf </span><span class="s4">= </span><span class="s1">min_weight_fraction_leaf</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_features </span><span class="s4">= </span><span class="s1">max_features</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_leaf_nodes </span><span class="s4">= </span><span class="s1">max_leaf_nodes</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">random_state </span><span class="s4">= </span><span class="s1">random_state</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">min_impurity_decrease </span><span class="s4">= </span><span class="s1">min_impurity_decrease</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_weight </span><span class="s4">= </span><span class="s1">class_weight</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ccp_alpha </span><span class="s4">= </span><span class="s1">ccp_alpha</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">monotonic_cst </span><span class="s4">= </span><span class="s1">monotonic_cst</span>

    <span class="s3">def </span><span class="s1">get_depth</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the depth of the decision tree. 
 
        The depth of a tree is the maximum distance between the root 
        and any leaf. 
 
        Returns 
        ------- 
        self.tree_.max_depth : int 
            The maximum depth of the tree. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">max_depth</span>

    <span class="s3">def </span><span class="s1">get_n_leaves</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the number of leaves of the decision tree. 
 
        Returns 
        ------- 
        self.tree_.n_leaves : int 
            Number of leaves. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">n_leaves</span>

    <span class="s3">def </span><span class="s1">_support_missing_values</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">(</span>
            <span class="s3">not </span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
            <span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_get_tags</span><span class="s4">()[</span><span class="s5">&quot;allow_nan&quot;</span><span class="s4">]</span>
            <span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">monotonic_cst </span><span class="s3">is None</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_compute_missing_values_in_feature_mask</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">estimator_name</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return boolean mask denoting if there are missing values for each feature. 
 
        This method also ensures that X is finite. 
 
        Parameter 
        --------- 
        X : array-like of shape (n_samples, n_features), dtype=DOUBLE 
            Input data. 
 
        estimator_name : str or None, default=None 
            Name to use when raising an error. Defaults to the class name. 
 
        Returns 
        ------- 
        missing_values_in_feature_mask : ndarray of shape (n_features,), or None 
            Missing value mask. If missing values are not supported or there 
            are no missing values, return None. 
        &quot;&quot;&quot;</span>
        <span class="s1">estimator_name </span><span class="s4">= </span><span class="s1">estimator_name </span><span class="s3">or </span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span>
        <span class="s1">common_kwargs </span><span class="s4">= </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">estimator_name</span><span class="s4">=</span><span class="s1">estimator_name</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;X&quot;</span><span class="s4">)</span>

        <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_support_missing_values</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
            <span class="s1">assert_all_finite</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, **</span><span class="s1">common_kwargs</span><span class="s4">)</span>
            <span class="s3">return None</span>

        <span class="s3">with </span><span class="s1">np</span><span class="s4">.</span><span class="s1">errstate</span><span class="s4">(</span><span class="s1">over</span><span class="s4">=</span><span class="s5">&quot;ignore&quot;</span><span class="s4">):</span>
            <span class="s1">overall_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isfinite</span><span class="s4">(</span><span class="s1">overall_sum</span><span class="s4">):</span>
            <span class="s2"># Raise a ValueError in case of the presence of an infinite element.</span>
            <span class="s1">_assert_all_finite_element_wise</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">xp</span><span class="s4">=</span><span class="s1">np</span><span class="s4">, </span><span class="s1">allow_nan</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, **</span><span class="s1">common_kwargs</span><span class="s4">)</span>

        <span class="s2"># If the sum is not nan, then there are no missing values</span>
        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isnan</span><span class="s4">(</span><span class="s1">overall_sum</span><span class="s4">):</span>
            <span class="s3">return None</span>

        <span class="s1">missing_values_in_feature_mask </span><span class="s4">= </span><span class="s1">_any_isnan_axis0</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">missing_values_in_feature_mask</span>

    <span class="s3">def </span><span class="s1">_fit</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">X</span><span class="s4">,</span>
        <span class="s1">y</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">missing_values_in_feature_mask</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">random_state </span><span class="s4">= </span><span class="s1">check_random_state</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">random_state</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">check_input</span><span class="s4">:</span>
            <span class="s2"># Need to validate separately here.</span>
            <span class="s2"># We can't pass multi_output=True because that would allow y to be</span>
            <span class="s2"># csr.</span>

            <span class="s2"># _compute_missing_values_in_feature_mask will check for finite values and</span>
            <span class="s2"># compute the missing mask if the tree supports missing values</span>
            <span class="s1">check_X_params </span><span class="s4">= </span><span class="s1">dict</span><span class="s4">(</span>
                <span class="s1">dtype</span><span class="s4">=</span><span class="s1">DTYPE</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s5">&quot;csc&quot;</span><span class="s4">, </span><span class="s1">force_all_finite</span><span class="s4">=</span><span class="s3">False</span>
            <span class="s4">)</span>
            <span class="s1">check_y_params </span><span class="s4">= </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s3">None</span><span class="s4">)</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">validate_separately</span><span class="s4">=(</span><span class="s1">check_X_params</span><span class="s4">, </span><span class="s1">check_y_params</span><span class="s4">)</span>
            <span class="s4">)</span>

            <span class="s1">missing_values_in_feature_mask </span><span class="s4">= (</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">_compute_missing_values_in_feature_mask</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
            <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
                <span class="s1">X</span><span class="s4">.</span><span class="s1">sort_indices</span><span class="s4">()</span>

                <span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indices</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">intc </span><span class="s3">or </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indptr</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">intc</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s5">&quot;No support for np.int64 index based sparse matrices&quot;</span>
                    <span class="s4">)</span>

            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">criterion </span><span class="s4">== </span><span class="s5">&quot;poisson&quot;</span><span class="s4">:</span>
                <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">y </span><span class="s4">&lt; </span><span class="s6">0</span><span class="s4">):</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s5">&quot;Some value(s) of y are negative which is&quot;</span>
                        <span class="s5">&quot; not allowed for Poisson regression.&quot;</span>
                    <span class="s4">)</span>
                <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">y</span><span class="s4">) &lt;= </span><span class="s6">0</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s5">&quot;Sum of y is not positive which is &quot;</span>
                        <span class="s5">&quot;necessary for Poisson regression.&quot;</span>
                    <span class="s4">)</span>

        <span class="s2"># Determine output settings</span>
        <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_ </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>
        <span class="s1">is_classification </span><span class="s4">= </span><span class="s1">is_classifier</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>

        <span class="s1">y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_1d</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
        <span class="s1">expanded_class_weight </span><span class="s4">= </span><span class="s3">None</span>

        <span class="s3">if </span><span class="s1">y</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s2"># reshape is necessary to preserve the data contiguity against vs</span>
            <span class="s2"># [:, np.newaxis] that does not.</span>
            <span class="s1">y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, (-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">))</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">= </span><span class="s1">y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>

        <span class="s3">if </span><span class="s1">is_classification</span><span class="s4">:</span>
            <span class="s1">check_classification_targets</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
            <span class="s1">y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>

            <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_ </span><span class="s4">= []</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_ </span><span class="s4">= []</span>

            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_weight </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">y_original </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>

            <span class="s1">y_encoded </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">int</span><span class="s4">)</span>
            <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">):</span>
                <span class="s1">classes_k</span><span class="s4">, </span><span class="s1">y_encoded</span><span class="s4">[:, </span><span class="s1">k</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y</span><span class="s4">[:, </span><span class="s1">k</span><span class="s4">], </span><span class="s1">return_inverse</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">classes_k</span><span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">classes_k</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">])</span>
            <span class="s1">y </span><span class="s4">= </span><span class="s1">y_encoded</span>

            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_weight </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">expanded_class_weight </span><span class="s4">= </span><span class="s1">compute_sample_weight</span><span class="s4">(</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">class_weight</span><span class="s4">, </span><span class="s1">y_original</span>
                <span class="s4">)</span>

            <span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">intp</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">getattr</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s5">&quot;dtype&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">) != </span><span class="s1">DOUBLE </span><span class="s3">or not </span><span class="s1">y</span><span class="s4">.</span><span class="s1">flags</span><span class="s4">.</span><span class="s1">contiguous</span><span class="s4">:</span>
            <span class="s1">y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ascontiguousarray</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">DOUBLE</span><span class="s4">)</span>

        <span class="s1">max_depth </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">iinfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">int32</span><span class="s4">).</span><span class="s1">max </span><span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_depth </span><span class="s3">is None else </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_depth</span>

        <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_leaf</span><span class="s4">, </span><span class="s1">numbers</span><span class="s4">.</span><span class="s1">Integral</span><span class="s4">):</span>
            <span class="s1">min_samples_leaf </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_leaf</span>
        <span class="s3">else</span><span class="s4">:  </span><span class="s2"># float</span>
            <span class="s1">min_samples_leaf </span><span class="s4">= </span><span class="s1">int</span><span class="s4">(</span><span class="s1">ceil</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_leaf </span><span class="s4">* </span><span class="s1">n_samples</span><span class="s4">))</span>

        <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_split</span><span class="s4">, </span><span class="s1">numbers</span><span class="s4">.</span><span class="s1">Integral</span><span class="s4">):</span>
            <span class="s1">min_samples_split </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_split</span>
        <span class="s3">else</span><span class="s4">:  </span><span class="s2"># float</span>
            <span class="s1">min_samples_split </span><span class="s4">= </span><span class="s1">int</span><span class="s4">(</span><span class="s1">ceil</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_samples_split </span><span class="s4">* </span><span class="s1">n_samples</span><span class="s4">))</span>
            <span class="s1">min_samples_split </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s6">2</span><span class="s4">, </span><span class="s1">min_samples_split</span><span class="s4">)</span>

        <span class="s1">min_samples_split </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s1">min_samples_split</span><span class="s4">, </span><span class="s6">2 </span><span class="s4">* </span><span class="s1">min_samples_leaf</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features</span><span class="s4">, </span><span class="s1">str</span><span class="s4">):</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features </span><span class="s4">== </span><span class="s5">&quot;sqrt&quot;</span><span class="s4">:</span>
                <span class="s1">max_features </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s6">1</span><span class="s4">, </span><span class="s1">int</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">)))</span>
            <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features </span><span class="s4">== </span><span class="s5">&quot;log2&quot;</span><span class="s4">:</span>
                <span class="s1">max_features </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s6">1</span><span class="s4">, </span><span class="s1">int</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">log2</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">)))</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">max_features </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span>
        <span class="s3">elif </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features</span><span class="s4">, </span><span class="s1">numbers</span><span class="s4">.</span><span class="s1">Integral</span><span class="s4">):</span>
            <span class="s1">max_features </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features</span>
        <span class="s3">else</span><span class="s4">:  </span><span class="s2"># float</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features </span><span class="s4">&gt; </span><span class="s6">0.0</span><span class="s4">:</span>
                <span class="s1">max_features </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s6">1</span><span class="s4">, </span><span class="s1">int</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_features </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">))</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">max_features </span><span class="s4">= </span><span class="s6">0</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_features_ </span><span class="s4">= </span><span class="s1">max_features</span>

        <span class="s1">max_leaf_nodes </span><span class="s4">= -</span><span class="s6">1 </span><span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_leaf_nodes </span><span class="s3">is None else </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_leaf_nodes</span>

        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">y</span><span class="s4">) != </span><span class="s1">n_samples</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Number of labels=%d does not match number of samples=%d&quot;</span>
                <span class="s4">% (</span><span class="s1">len</span><span class="s4">(</span><span class="s1">y</span><span class="s4">), </span><span class="s1">n_samples</span><span class="s4">)</span>
            <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">DOUBLE</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">expanded_class_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">sample_weight </span><span class="s4">* </span><span class="s1">expanded_class_weight</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">expanded_class_weight</span>

        <span class="s2"># Set min_weight_leaf from min_weight_fraction_leaf</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">min_weight_leaf </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_weight_fraction_leaf </span><span class="s4">* </span><span class="s1">n_samples</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">min_weight_leaf </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_weight_fraction_leaf </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>

        <span class="s2"># Build tree</span>
        <span class="s1">criterion </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">criterion</span>
        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">criterion</span><span class="s4">, </span><span class="s1">Criterion</span><span class="s4">):</span>
            <span class="s3">if </span><span class="s1">is_classification</span><span class="s4">:</span>
                <span class="s1">criterion </span><span class="s4">= </span><span class="s1">CRITERIA_CLF</span><span class="s4">[</span><span class="s1">self</span><span class="s4">.</span><span class="s1">criterion</span><span class="s4">](</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span>
                <span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">criterion </span><span class="s4">= </span><span class="s1">CRITERIA_REG</span><span class="s4">[</span><span class="s1">self</span><span class="s4">.</span><span class="s1">criterion</span><span class="s4">](</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">, </span><span class="s1">n_samples</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># Make a deepcopy in case the criterion has mutable attributes that</span>
            <span class="s2"># might be shared and modified concurrently during parallel fitting</span>
            <span class="s1">criterion </span><span class="s4">= </span><span class="s1">copy</span><span class="s4">.</span><span class="s1">deepcopy</span><span class="s4">(</span><span class="s1">criterion</span><span class="s4">)</span>

        <span class="s1">SPLITTERS </span><span class="s4">= </span><span class="s1">SPARSE_SPLITTERS </span><span class="s3">if </span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">) </span><span class="s3">else </span><span class="s1">DENSE_SPLITTERS</span>

        <span class="s1">splitter </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">splitter</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">monotonic_cst </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">monotonic_cst </span><span class="s4">= </span><span class="s3">None</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">&gt; </span><span class="s6">1</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;Monotonicity constraints are not supported with multiple outputs.&quot;</span>
                <span class="s4">)</span>
            <span class="s2"># Check to correct monotonicity constraint' specification,</span>
            <span class="s2"># by applying element-wise logical conjunction</span>
            <span class="s2"># Note: we do not cast `np.asarray(self.monotonic_cst, dtype=np.int8)`</span>
            <span class="s2"># straight away here so as to generate error messages for invalid</span>
            <span class="s2"># values using the original values prior to any dtype related conversion.</span>
            <span class="s1">monotonic_cst </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">monotonic_cst</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">monotonic_cst</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] != </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;monotonic_cst has shape {} but the input data &quot;</span>
                    <span class="s5">&quot;X has {} features.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">monotonic_cst</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">])</span>
                <span class="s4">)</span>
            <span class="s1">valid_constraints </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isin</span><span class="s4">(</span><span class="s1">monotonic_cst</span><span class="s4">, (-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">))</span>
            <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">valid_constraints</span><span class="s4">):</span>
                <span class="s1">unique_constaints_value </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">monotonic_cst</span><span class="s4">)</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;monotonic_cst must be None or an array-like of -1, 0 or 1, but&quot;</span>
                    <span class="s5">f&quot; got </span><span class="s3">{</span><span class="s1">unique_constaints_value</span><span class="s3">}</span><span class="s5">&quot;</span>
                <span class="s4">)</span>
            <span class="s1">monotonic_cst </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">monotonic_cst</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">int8</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">is_classifier</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
                <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] &gt; </span><span class="s6">2</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s5">&quot;Monotonicity constraints are not supported with multiclass &quot;</span>
                        <span class="s5">&quot;classification&quot;</span>
                    <span class="s4">)</span>
                <span class="s2"># Binary classification trees are built by constraining probabilities</span>
                <span class="s2"># of the *negative class* in order to make the implementation similar</span>
                <span class="s2"># to regression trees.</span>
                <span class="s2"># Since self.monotonic_cst encodes constraints on probabilities of the</span>
                <span class="s2"># *positive class*, all signs must be flipped.</span>
                <span class="s1">monotonic_cst </span><span class="s4">*= -</span><span class="s6">1</span>

        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">splitter</span><span class="s4">, </span><span class="s1">Splitter</span><span class="s4">):</span>
            <span class="s1">splitter </span><span class="s4">= </span><span class="s1">SPLITTERS</span><span class="s4">[</span><span class="s1">self</span><span class="s4">.</span><span class="s1">splitter</span><span class="s4">](</span>
                <span class="s1">criterion</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">max_features_</span><span class="s4">,</span>
                <span class="s1">min_samples_leaf</span><span class="s4">,</span>
                <span class="s1">min_weight_leaf</span><span class="s4">,</span>
                <span class="s1">random_state</span><span class="s4">,</span>
                <span class="s1">monotonic_cst</span><span class="s4">,</span>
            <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">is_classifier</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">tree_ </span><span class="s4">= </span><span class="s1">Tree</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">tree_ </span><span class="s4">= </span><span class="s1">Tree</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">,</span>
                <span class="s2"># TODO: tree shouldn't need this in this case</span>
                <span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s6">1</span><span class="s4">] * </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">intp</span><span class="s4">),</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">,</span>
            <span class="s4">)</span>

        <span class="s2"># Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise</span>
        <span class="s3">if </span><span class="s1">max_leaf_nodes </span><span class="s4">&lt; </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s1">builder </span><span class="s4">= </span><span class="s1">DepthFirstTreeBuilder</span><span class="s4">(</span>
                <span class="s1">splitter</span><span class="s4">,</span>
                <span class="s1">min_samples_split</span><span class="s4">,</span>
                <span class="s1">min_samples_leaf</span><span class="s4">,</span>
                <span class="s1">min_weight_leaf</span><span class="s4">,</span>
                <span class="s1">max_depth</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">min_impurity_decrease</span><span class="s4">,</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">builder </span><span class="s4">= </span><span class="s1">BestFirstTreeBuilder</span><span class="s4">(</span>
                <span class="s1">splitter</span><span class="s4">,</span>
                <span class="s1">min_samples_split</span><span class="s4">,</span>
                <span class="s1">min_samples_leaf</span><span class="s4">,</span>
                <span class="s1">min_weight_leaf</span><span class="s4">,</span>
                <span class="s1">max_depth</span><span class="s4">,</span>
                <span class="s1">max_leaf_nodes</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">min_impurity_decrease</span><span class="s4">,</span>
            <span class="s4">)</span>

        <span class="s1">builder</span><span class="s4">.</span><span class="s1">build</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">missing_values_in_feature_mask</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">== </span><span class="s6">1 </span><span class="s3">and </span><span class="s1">is_classifier</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">_prune_tree</span><span class="s4">()</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_validate_X_predict</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Validate the training data on predict (probabilities).&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">check_input</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_support_missing_values</span><span class="s4">(</span><span class="s1">X</span><span class="s4">):</span>
                <span class="s1">force_all_finite </span><span class="s4">= </span><span class="s5">&quot;allow-nan&quot;</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">force_all_finite </span><span class="s4">= </span><span class="s3">True</span>
            <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">,</span>
                <span class="s1">dtype</span><span class="s4">=</span><span class="s1">DTYPE</span><span class="s4">,</span>
                <span class="s1">accept_sparse</span><span class="s4">=</span><span class="s5">&quot;csr&quot;</span><span class="s4">,</span>
                <span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
                <span class="s1">force_all_finite</span><span class="s4">=</span><span class="s1">force_all_finite</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">) </span><span class="s3">and </span><span class="s4">(</span>
                <span class="s1">X</span><span class="s4">.</span><span class="s1">indices</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">intc </span><span class="s3">or </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indptr</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">!= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">intc</span>
            <span class="s4">):</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;No support for np.int64 index based sparse matrices&quot;</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># The number of features is checked regardless of `check_input`</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_check_n_features</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">X</span>

    <span class="s3">def </span><span class="s1">predict</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict class or regression value for X. 
 
        For a classification model, the predicted class for each sample in X is 
        returned. For a regression model, the predicted value based on X is 
        returned. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you're doing. 
 
        Returns 
        ------- 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            The predicted classes, or the predict values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_X_predict</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">)</span>
        <span class="s1">proba </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">predict</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">n_samples </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>

        <span class="s2"># Classification</span>
        <span class="s3">if </span><span class="s1">is_classifier</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">.</span><span class="s1">take</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">argmax</span><span class="s4">(</span><span class="s1">proba</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">), </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>

            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">class_type </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">].</span><span class="s1">dtype</span>
                <span class="s1">predictions </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">class_type</span><span class="s4">)</span>
                <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">):</span>
                    <span class="s1">predictions</span><span class="s4">[:, </span><span class="s1">k</span><span class="s4">] = </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s1">k</span><span class="s4">].</span><span class="s1">take</span><span class="s4">(</span>
                        <span class="s1">np</span><span class="s4">.</span><span class="s1">argmax</span><span class="s4">(</span><span class="s1">proba</span><span class="s4">[:, </span><span class="s1">k</span><span class="s4">], </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">), </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span>
                    <span class="s4">)</span>

                <span class="s3">return </span><span class="s1">predictions</span>

        <span class="s2"># Regression</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">proba</span><span class="s4">[:, </span><span class="s6">0</span><span class="s4">]</span>

            <span class="s3">else</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">proba</span><span class="s4">[:, :, </span><span class="s6">0</span><span class="s4">]</span>

    <span class="s3">def </span><span class="s1">apply</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the index of the leaf that each sample is predicted as. 
 
        .. versionadded:: 0.17 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you're doing. 
 
        Returns 
        ------- 
        X_leaves : array-like of shape (n_samples,) 
            For each datapoint x in X, return the index of the leaf x 
            ends up in. Leaves are numbered within 
            ``[0; self.tree_.node_count)``, possibly with gaps in the 
            numbering. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_X_predict</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">apply</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">decision_path</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the decision path in the tree. 
 
        .. versionadded:: 0.18 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you're doing. 
 
        Returns 
        ------- 
        indicator : sparse matrix of shape (n_samples, n_nodes) 
            Return a node indicator CSR matrix where non zero elements 
            indicates that the samples goes through the nodes. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_X_predict</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">decision_path</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_prune_tree</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Prune tree using Minimal Cost-Complexity Pruning.&quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">ccp_alpha </span><span class="s4">== </span><span class="s6">0.0</span><span class="s4">:</span>
            <span class="s3">return</span>

        <span class="s2"># build pruned tree</span>
        <span class="s3">if </span><span class="s1">is_classifier</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
            <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_1d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">)</span>
            <span class="s1">pruned_tree </span><span class="s4">= </span><span class="s1">Tree</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">pruned_tree </span><span class="s4">= </span><span class="s1">Tree</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">,</span>
                <span class="s2"># TODO: the tree shouldn't need this param</span>
                <span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s6">1</span><span class="s4">] * </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">intp</span><span class="s4">),</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">,</span>
            <span class="s4">)</span>
        <span class="s1">_build_pruned_tree_ccp</span><span class="s4">(</span><span class="s1">pruned_tree</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">ccp_alpha</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">tree_ </span><span class="s4">= </span><span class="s1">pruned_tree</span>

    <span class="s3">def </span><span class="s1">cost_complexity_pruning_path</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the pruning path during Minimal Cost-Complexity Pruning. 
 
        See :ref:`minimal_cost_complexity_pruning` for details on the pruning 
        process. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csc_matrix``. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            The target values (class labels) as integers or strings. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. Splits 
            that would create child nodes with net zero or negative weight are 
            ignored while searching for a split in each node. Splits are also 
            ignored if they would result in any single class carrying a 
            negative weight in either child node. 
 
        Returns 
        ------- 
        ccp_path : :class:`~sklearn.utils.Bunch` 
            Dictionary-like object, with the following attributes. 
 
            ccp_alphas : ndarray 
                Effective alphas of subtree during pruning. 
 
            impurities : ndarray 
                Sum of the impurities of the subtree leaves for the 
                corresponding alpha value in ``ccp_alphas``. 
        &quot;&quot;&quot;</span>
        <span class="s1">est </span><span class="s4">= </span><span class="s1">clone</span><span class="s4">(</span><span class="s1">self</span><span class="s4">).</span><span class="s1">set_params</span><span class="s4">(</span><span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">)</span>
        <span class="s1">est</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">Bunch</span><span class="s4">(**</span><span class="s1">ccp_pruning_path</span><span class="s4">(</span><span class="s1">est</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">))</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">feature_importances_</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the feature importances. 
 
        The importance of a feature is computed as the (normalized) total 
        reduction of the criterion brought by that feature. 
        It is also known as the Gini importance. 
 
        Warning: impurity-based feature importances can be misleading for 
        high cardinality features (many unique values). See 
        :func:`sklearn.inspection.permutation_importance` as an alternative. 
 
        Returns 
        ------- 
        feature_importances_ : ndarray of shape (n_features,) 
            Normalized total reduction of criteria by feature 
            (Gini importance). 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">compute_feature_importances</span><span class="s4">()</span>


<span class="s2"># =============================================================================</span>
<span class="s2"># Public estimators</span>
<span class="s2"># =============================================================================</span>


<span class="s3">class </span><span class="s1">DecisionTreeClassifier</span><span class="s4">(</span><span class="s1">ClassifierMixin</span><span class="s4">, </span><span class="s1">BaseDecisionTree</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;A decision tree classifier. 
 
    Read more in the :ref:`User Guide &lt;tree&gt;`. 
 
    Parameters 
    ---------- 
    criterion : {&quot;gini&quot;, &quot;entropy&quot;, &quot;log_loss&quot;}, default=&quot;gini&quot; 
        The function to measure the quality of a split. Supported criteria are 
        &quot;gini&quot; for the Gini impurity and &quot;log_loss&quot; and &quot;entropy&quot; both for the 
        Shannon information gain, see :ref:`tree_mathematical_formulation`. 
 
    splitter : {&quot;best&quot;, &quot;random&quot;}, default=&quot;best&quot; 
        The strategy used to choose the split at each node. Supported 
        strategies are &quot;best&quot; to choose the best split and &quot;random&quot; to choose 
        the best random split. 
 
    max_depth : int, default=None 
        The maximum depth of the tree. If None, then nodes are expanded until 
        all leaves are pure or until all leaves contain less than 
        min_samples_split samples. 
 
    min_samples_split : int or float, default=2 
        The minimum number of samples required to split an internal node: 
 
        - If int, then consider `min_samples_split` as the minimum number. 
        - If float, then `min_samples_split` is a fraction and 
          `ceil(min_samples_split * n_samples)` are the minimum 
          number of samples for each split. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_samples_leaf : int or float, default=1 
        The minimum number of samples required to be at a leaf node. 
        A split point at any depth will only be considered if it leaves at 
        least ``min_samples_leaf`` training samples in each of the left and 
        right branches.  This may have the effect of smoothing the model, 
        especially in regression. 
 
        - If int, then consider `min_samples_leaf` as the minimum number. 
        - If float, then `min_samples_leaf` is a fraction and 
          `ceil(min_samples_leaf * n_samples)` are the minimum 
          number of samples for each node. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_weight_fraction_leaf : float, default=0.0 
        The minimum weighted fraction of the sum total of weights (of all 
        the input samples) required to be at a leaf node. Samples have 
        equal weight when sample_weight is not provided. 
 
    max_features : int, float or {&quot;sqrt&quot;, &quot;log2&quot;}, default=None 
        The number of features to consider when looking for the best split: 
 
            - If int, then consider `max_features` features at each split. 
            - If float, then `max_features` is a fraction and 
              `max(1, int(max_features * n_features_in_))` features are considered at 
              each split. 
            - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`. 
            - If &quot;log2&quot;, then `max_features=log2(n_features)`. 
            - If None, then `max_features=n_features`. 
 
        Note: the search for a split does not stop until at least one 
        valid partition of the node samples is found, even if it requires to 
        effectively inspect more than ``max_features`` features. 
 
    random_state : int, RandomState instance or None, default=None 
        Controls the randomness of the estimator. The features are always 
        randomly permuted at each split, even if ``splitter`` is set to 
        ``&quot;best&quot;``. When ``max_features &lt; n_features``, the algorithm will 
        select ``max_features`` at random at each split before finding the best 
        split among them. But the best found split may vary across different 
        runs, even if ``max_features=n_features``. That is the case, if the 
        improvement of the criterion is identical for several splits and one 
        split has to be selected at random. To obtain a deterministic behaviour 
        during fitting, ``random_state`` has to be fixed to an integer. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
    max_leaf_nodes : int, default=None 
        Grow a tree with ``max_leaf_nodes`` in best-first fashion. 
        Best nodes are defined as relative reduction in impurity. 
        If None then unlimited number of leaf nodes. 
 
    min_impurity_decrease : float, default=0.0 
        A node will be split if this split induces a decrease of the impurity 
        greater than or equal to this value. 
 
        The weighted impurity decrease equation is the following:: 
 
            N_t / N * (impurity - N_t_R / N_t * right_impurity 
                                - N_t_L / N_t * left_impurity) 
 
        where ``N`` is the total number of samples, ``N_t`` is the number of 
        samples at the current node, ``N_t_L`` is the number of samples in the 
        left child, and ``N_t_R`` is the number of samples in the right child. 
 
        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, 
        if ``sample_weight`` is passed. 
 
        .. versionadded:: 0.19 
 
    class_weight : dict, list of dict or &quot;balanced&quot;, default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If None, all classes are supposed to have weight one. For 
        multi-output problems, a list of dicts can be provided in the same 
        order as the columns of y. 
 
        Note that for multioutput (including multilabel) weights should be 
        defined for each class of every column in its own dict. For example, 
        for four-class multilabel classification weights should be 
        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of 
        [{1:1}, {2:5}, {3:1}, {4:1}]. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))`` 
 
        For multi-output, the weights of each column of y will be multiplied. 
 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if sample_weight is specified. 
 
    ccp_alpha : non-negative float, default=0.0 
        Complexity parameter used for Minimal Cost-Complexity Pruning. The 
        subtree with the largest cost complexity that is smaller than 
        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See 
        :ref:`minimal_cost_complexity_pruning` for details. 
 
        .. versionadded:: 0.22 
 
    monotonic_cst : array-like of int of shape (n_features), default=None 
        Indicates the monotonicity constraint to enforce on each feature. 
          - 1: monotonic increase 
          - 0: no constraint 
          - -1: monotonic decrease 
 
        If monotonic_cst is None, no constraints are applied. 
 
        Monotonicity constraints are not supported for: 
          - multiclass classifications (i.e. when `n_classes &gt; 2`), 
          - multioutput classifications (i.e. when `n_outputs_ &gt; 1`), 
          - classifications trained on data with missing values. 
 
        The constraints hold over the probability of the positive class. 
 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
 
        .. versionadded:: 1.4 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes,) or list of ndarray 
        The classes labels (single output problem), 
        or a list of arrays of class labels (multi-output problem). 
 
    feature_importances_ : ndarray of shape (n_features,) 
        The impurity-based feature importances. 
        The higher, the more important the feature. 
        The importance of a feature is computed as the (normalized) 
        total reduction of the criterion brought by that feature.  It is also 
        known as the Gini importance [4]_. 
 
        Warning: impurity-based feature importances can be misleading for 
        high cardinality features (many unique values). See 
        :func:`sklearn.inspection.permutation_importance` as an alternative. 
 
    max_features_ : int 
        The inferred value of max_features. 
 
    n_classes_ : int or list of int 
        The number of classes (for single output problems), 
        or a list containing the number of classes for each 
        output (for multi-output problems). 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_outputs_ : int 
        The number of outputs when ``fit`` is performed. 
 
    tree_ : Tree instance 
        The underlying Tree object. Please refer to 
        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and 
        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` 
        for basic usage of these attributes. 
 
    See Also 
    -------- 
    DecisionTreeRegressor : A decision tree regressor. 
 
    Notes 
    ----- 
    The default values for the parameters controlling the size of the trees 
    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and 
    unpruned trees which can potentially be very large on some data sets. To 
    reduce memory consumption, the complexity and size of the trees should be 
    controlled by setting those parameter values. 
 
    The :meth:`predict` method operates using the :func:`numpy.argmax` 
    function on the outputs of :meth:`predict_proba`. This means that in 
    case the highest predicted probabilities are tied, the classifier will 
    predict the tied class with the lowest index in :term:`classes_`. 
 
    References 
    ---------- 
 
    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning 
 
    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, &quot;Classification 
           and Regression Trees&quot;, Wadsworth, Belmont, CA, 1984. 
 
    .. [3] T. Hastie, R. Tibshirani and J. Friedman. &quot;Elements of Statistical 
           Learning&quot;, Springer, 2009. 
 
    .. [4] L. Breiman, and A. Cutler, &quot;Random Forests&quot;, 
           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.model_selection import cross_val_score 
    &gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier 
    &gt;&gt;&gt; clf = DecisionTreeClassifier(random_state=0) 
    &gt;&gt;&gt; iris = load_iris() 
    &gt;&gt;&gt; cross_val_score(clf, iris.data, iris.target, cv=10) 
    ...                             # doctest: +SKIP 
    ... 
    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93..., 
            0.93...,  0.93...,  1.     ,  0.93...,  1.      ]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">BaseDecisionTree</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s5">&quot;criterion&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;gini&quot;</span><span class="s4">, </span><span class="s5">&quot;entropy&quot;</span><span class="s4">, </span><span class="s5">&quot;log_loss&quot;</span><span class="s4">}), </span><span class="s1">Hidden</span><span class="s4">(</span><span class="s1">Criterion</span><span class="s4">)],</span>
        <span class="s5">&quot;class_weight&quot;</span><span class="s4">: [</span><span class="s1">dict</span><span class="s4">, </span><span class="s1">list</span><span class="s4">, </span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;balanced&quot;</span><span class="s4">}), </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">criterion</span><span class="s4">=</span><span class="s5">&quot;gini&quot;</span><span class="s4">,</span>
        <span class="s1">splitter</span><span class="s4">=</span><span class="s5">&quot;best&quot;</span><span class="s4">,</span>
        <span class="s1">max_depth</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s6">2</span><span class="s4">,</span>
        <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">max_features</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">class_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">criterion</span><span class="s4">=</span><span class="s1">criterion</span><span class="s4">,</span>
            <span class="s1">splitter</span><span class="s4">=</span><span class="s1">splitter</span><span class="s4">,</span>
            <span class="s1">max_depth</span><span class="s4">=</span><span class="s1">max_depth</span><span class="s4">,</span>
            <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s1">min_samples_split</span><span class="s4">,</span>
            <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s1">min_samples_leaf</span><span class="s4">,</span>
            <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s1">min_weight_fraction_leaf</span><span class="s4">,</span>
            <span class="s1">max_features</span><span class="s4">=</span><span class="s1">max_features</span><span class="s4">,</span>
            <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s1">max_leaf_nodes</span><span class="s4">,</span>
            <span class="s1">class_weight</span><span class="s4">=</span><span class="s1">class_weight</span><span class="s4">,</span>
            <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s1">min_impurity_decrease</span><span class="s4">,</span>
            <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s1">monotonic_cst</span><span class="s4">,</span>
            <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s1">ccp_alpha</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Build a decision tree classifier from the training set (X, y). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csc_matrix``. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            The target values (class labels) as integers or strings. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. Splits 
            that would create child nodes with net zero or negative weight are 
            ignored while searching for a split in each node. Splits are also 
            ignored if they would result in any single class carrying a 
            negative weight in either child node. 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you're doing. 
 
        Returns 
        ------- 
        self : DecisionTreeClassifier 
            Fitted estimator. 
        &quot;&quot;&quot;</span>

        <span class="s1">super</span><span class="s4">().</span><span class="s1">_fit</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">y</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">check_input</span><span class="s4">=</span><span class="s1">check_input</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict class probabilities of the input samples X. 
 
        The predicted class probability is the fraction of samples of the same 
        class in a leaf. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you're doing. 
 
        Returns 
        ------- 
        proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \ 
            such arrays if n_outputs &gt; 1 
            The class probabilities of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_X_predict</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">)</span>
        <span class="s1">proba </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">predict</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">proba</span><span class="s4">[:, : </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">]</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">all_proba </span><span class="s4">= []</span>
            <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">):</span>
                <span class="s1">proba_k </span><span class="s4">= </span><span class="s1">proba</span><span class="s4">[:, </span><span class="s1">k</span><span class="s4">, : </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes_</span><span class="s4">[</span><span class="s1">k</span><span class="s4">]]</span>
                <span class="s1">all_proba</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">proba_k</span><span class="s4">)</span>
            <span class="s3">return </span><span class="s1">all_proba</span>

    <span class="s3">def </span><span class="s1">predict_log_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict class log-probabilities of the input samples X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csr_matrix``. 
 
        Returns 
        ------- 
        proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \ 
            such arrays if n_outputs &gt; 1 
            The class log-probabilities of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">proba </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_ </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">proba</span><span class="s4">)</span>

        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_outputs_</span><span class="s4">):</span>
                <span class="s1">proba</span><span class="s4">[</span><span class="s1">k</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">proba</span><span class="s4">[</span><span class="s1">k</span><span class="s4">])</span>

            <span class="s3">return </span><span class="s1">proba</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s2"># XXX: nan is only support for dense arrays, but we set this for common test to</span>
        <span class="s2"># pass, specifically: check_estimators_nan_inf</span>
        <span class="s1">allow_nan </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">splitter </span><span class="s4">== </span><span class="s5">&quot;best&quot; </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">criterion </span><span class="s3">in </span><span class="s4">{</span>
            <span class="s5">&quot;gini&quot;</span><span class="s4">,</span>
            <span class="s5">&quot;log_loss&quot;</span><span class="s4">,</span>
            <span class="s5">&quot;entropy&quot;</span><span class="s4">,</span>
        <span class="s4">}</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;multilabel&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">, </span><span class="s5">&quot;allow_nan&quot;</span><span class="s4">: </span><span class="s1">allow_nan</span><span class="s4">}</span>


<span class="s3">class </span><span class="s1">DecisionTreeRegressor</span><span class="s4">(</span><span class="s1">RegressorMixin</span><span class="s4">, </span><span class="s1">BaseDecisionTree</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;A decision tree regressor. 
 
    Read more in the :ref:`User Guide &lt;tree&gt;`. 
 
    Parameters 
    ---------- 
    criterion : {&quot;squared_error&quot;, &quot;friedman_mse&quot;, &quot;absolute_error&quot;, \ 
            &quot;poisson&quot;}, default=&quot;squared_error&quot; 
        The function to measure the quality of a split. Supported criteria 
        are &quot;squared_error&quot; for the mean squared error, which is equal to 
        variance reduction as feature selection criterion and minimizes the L2 
        loss using the mean of each terminal node, &quot;friedman_mse&quot;, which uses 
        mean squared error with Friedman's improvement score for potential 
        splits, &quot;absolute_error&quot; for the mean absolute error, which minimizes 
        the L1 loss using the median of each terminal node, and &quot;poisson&quot; which 
        uses reduction in the half mean Poisson deviance to find splits. 
 
        .. versionadded:: 0.18 
           Mean Absolute Error (MAE) criterion. 
 
        .. versionadded:: 0.24 
            Poisson deviance criterion. 
 
    splitter : {&quot;best&quot;, &quot;random&quot;}, default=&quot;best&quot; 
        The strategy used to choose the split at each node. Supported 
        strategies are &quot;best&quot; to choose the best split and &quot;random&quot; to choose 
        the best random split. 
 
    max_depth : int, default=None 
        The maximum depth of the tree. If None, then nodes are expanded until 
        all leaves are pure or until all leaves contain less than 
        min_samples_split samples. 
 
    min_samples_split : int or float, default=2 
        The minimum number of samples required to split an internal node: 
 
        - If int, then consider `min_samples_split` as the minimum number. 
        - If float, then `min_samples_split` is a fraction and 
          `ceil(min_samples_split * n_samples)` are the minimum 
          number of samples for each split. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_samples_leaf : int or float, default=1 
        The minimum number of samples required to be at a leaf node. 
        A split point at any depth will only be considered if it leaves at 
        least ``min_samples_leaf`` training samples in each of the left and 
        right branches.  This may have the effect of smoothing the model, 
        especially in regression. 
 
        - If int, then consider `min_samples_leaf` as the minimum number. 
        - If float, then `min_samples_leaf` is a fraction and 
          `ceil(min_samples_leaf * n_samples)` are the minimum 
          number of samples for each node. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_weight_fraction_leaf : float, default=0.0 
        The minimum weighted fraction of the sum total of weights (of all 
        the input samples) required to be at a leaf node. Samples have 
        equal weight when sample_weight is not provided. 
 
    max_features : int, float or {&quot;sqrt&quot;, &quot;log2&quot;}, default=None 
        The number of features to consider when looking for the best split: 
 
        - If int, then consider `max_features` features at each split. 
        - If float, then `max_features` is a fraction and 
          `max(1, int(max_features * n_features_in_))` features are considered at each 
          split. 
        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`. 
        - If &quot;log2&quot;, then `max_features=log2(n_features)`. 
        - If None, then `max_features=n_features`. 
 
        Note: the search for a split does not stop until at least one 
        valid partition of the node samples is found, even if it requires to 
        effectively inspect more than ``max_features`` features. 
 
    random_state : int, RandomState instance or None, default=None 
        Controls the randomness of the estimator. The features are always 
        randomly permuted at each split, even if ``splitter`` is set to 
        ``&quot;best&quot;``. When ``max_features &lt; n_features``, the algorithm will 
        select ``max_features`` at random at each split before finding the best 
        split among them. But the best found split may vary across different 
        runs, even if ``max_features=n_features``. That is the case, if the 
        improvement of the criterion is identical for several splits and one 
        split has to be selected at random. To obtain a deterministic behaviour 
        during fitting, ``random_state`` has to be fixed to an integer. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
    max_leaf_nodes : int, default=None 
        Grow a tree with ``max_leaf_nodes`` in best-first fashion. 
        Best nodes are defined as relative reduction in impurity. 
        If None then unlimited number of leaf nodes. 
 
    min_impurity_decrease : float, default=0.0 
        A node will be split if this split induces a decrease of the impurity 
        greater than or equal to this value. 
 
        The weighted impurity decrease equation is the following:: 
 
            N_t / N * (impurity - N_t_R / N_t * right_impurity 
                                - N_t_L / N_t * left_impurity) 
 
        where ``N`` is the total number of samples, ``N_t`` is the number of 
        samples at the current node, ``N_t_L`` is the number of samples in the 
        left child, and ``N_t_R`` is the number of samples in the right child. 
 
        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, 
        if ``sample_weight`` is passed. 
 
        .. versionadded:: 0.19 
 
    ccp_alpha : non-negative float, default=0.0 
        Complexity parameter used for Minimal Cost-Complexity Pruning. The 
        subtree with the largest cost complexity that is smaller than 
        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See 
        :ref:`minimal_cost_complexity_pruning` for details. 
 
        .. versionadded:: 0.22 
 
    monotonic_cst : array-like of int of shape (n_features), default=None 
        Indicates the monotonicity constraint to enforce on each feature. 
          - 1: monotonic increase 
          - 0: no constraint 
          - -1: monotonic decrease 
 
        If monotonic_cst is None, no constraints are applied. 
 
        Monotonicity constraints are not supported for: 
          - multioutput regressions (i.e. when `n_outputs_ &gt; 1`), 
          - regressions trained on data with missing values. 
 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
 
        .. versionadded:: 1.4 
 
    Attributes 
    ---------- 
    feature_importances_ : ndarray of shape (n_features,) 
        The feature importances. 
        The higher, the more important the feature. 
        The importance of a feature is computed as the 
        (normalized) total reduction of the criterion brought 
        by that feature. It is also known as the Gini importance [4]_. 
 
        Warning: impurity-based feature importances can be misleading for 
        high cardinality features (many unique values). See 
        :func:`sklearn.inspection.permutation_importance` as an alternative. 
 
    max_features_ : int 
        The inferred value of max_features. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_outputs_ : int 
        The number of outputs when ``fit`` is performed. 
 
    tree_ : Tree instance 
        The underlying Tree object. Please refer to 
        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and 
        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` 
        for basic usage of these attributes. 
 
    See Also 
    -------- 
    DecisionTreeClassifier : A decision tree classifier. 
 
    Notes 
    ----- 
    The default values for the parameters controlling the size of the trees 
    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and 
    unpruned trees which can potentially be very large on some data sets. To 
    reduce memory consumption, the complexity and size of the trees should be 
    controlled by setting those parameter values. 
 
    References 
    ---------- 
 
    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning 
 
    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, &quot;Classification 
           and Regression Trees&quot;, Wadsworth, Belmont, CA, 1984. 
 
    .. [3] T. Hastie, R. Tibshirani and J. Friedman. &quot;Elements of Statistical 
           Learning&quot;, Springer, 2009. 
 
    .. [4] L. Breiman, and A. Cutler, &quot;Random Forests&quot;, 
           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_diabetes 
    &gt;&gt;&gt; from sklearn.model_selection import cross_val_score 
    &gt;&gt;&gt; from sklearn.tree import DecisionTreeRegressor 
    &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True) 
    &gt;&gt;&gt; regressor = DecisionTreeRegressor(random_state=0) 
    &gt;&gt;&gt; cross_val_score(regressor, X, y, cv=10) 
    ...                    # doctest: +SKIP 
    ... 
    array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50..., 
           0.16...,  0.11..., -0.73..., -0.30..., -0.00...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">BaseDecisionTree</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s5">&quot;criterion&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;squared_error&quot;</span><span class="s4">, </span><span class="s5">&quot;friedman_mse&quot;</span><span class="s4">, </span><span class="s5">&quot;absolute_error&quot;</span><span class="s4">, </span><span class="s5">&quot;poisson&quot;</span><span class="s4">}),</span>
            <span class="s1">Hidden</span><span class="s4">(</span><span class="s1">Criterion</span><span class="s4">),</span>
        <span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">criterion</span><span class="s4">=</span><span class="s5">&quot;squared_error&quot;</span><span class="s4">,</span>
        <span class="s1">splitter</span><span class="s4">=</span><span class="s5">&quot;best&quot;</span><span class="s4">,</span>
        <span class="s1">max_depth</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s6">2</span><span class="s4">,</span>
        <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">max_features</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">criterion</span><span class="s4">=</span><span class="s1">criterion</span><span class="s4">,</span>
            <span class="s1">splitter</span><span class="s4">=</span><span class="s1">splitter</span><span class="s4">,</span>
            <span class="s1">max_depth</span><span class="s4">=</span><span class="s1">max_depth</span><span class="s4">,</span>
            <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s1">min_samples_split</span><span class="s4">,</span>
            <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s1">min_samples_leaf</span><span class="s4">,</span>
            <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s1">min_weight_fraction_leaf</span><span class="s4">,</span>
            <span class="s1">max_features</span><span class="s4">=</span><span class="s1">max_features</span><span class="s4">,</span>
            <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s1">max_leaf_nodes</span><span class="s4">,</span>
            <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s1">min_impurity_decrease</span><span class="s4">,</span>
            <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s1">ccp_alpha</span><span class="s4">,</span>
            <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s1">monotonic_cst</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">check_input</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Build a decision tree regressor from the training set (X, y). 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            The training input samples. Internally, it will be converted to 
            ``dtype=np.float32`` and if a sparse matrix is provided 
            to a sparse ``csc_matrix``. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_outputs) 
            The target values (real numbers). Use ``dtype=np.float64`` and 
            ``order='C'`` for maximum efficiency. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. Splits 
            that would create child nodes with net zero or negative weight are 
            ignored while searching for a split in each node. 
 
        check_input : bool, default=True 
            Allow to bypass several input checking. 
            Don't use this parameter unless you know what you're doing. 
 
        Returns 
        ------- 
        self : DecisionTreeRegressor 
            Fitted estimator. 
        &quot;&quot;&quot;</span>

        <span class="s1">super</span><span class="s4">().</span><span class="s1">_fit</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">y</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">check_input</span><span class="s4">=</span><span class="s1">check_input</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_compute_partial_dependence_recursion</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">grid</span><span class="s4">, </span><span class="s1">target_features</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fast partial dependence computation. 
 
        Parameters 
        ---------- 
        grid : ndarray of shape (n_samples, n_target_features), dtype=np.float32 
            The grid points on which the partial dependence should be 
            evaluated. 
        target_features : ndarray of shape (n_target_features), dtype=np.intp 
            The set of target features for which the partial dependence 
            should be evaluated. 
 
        Returns 
        ------- 
        averaged_predictions : ndarray of shape (n_samples,), dtype=np.float64 
            The value of the partial dependence function on each grid point. 
        &quot;&quot;&quot;</span>
        <span class="s1">grid </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">grid</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">DTYPE</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s5">&quot;C&quot;</span><span class="s4">)</span>
        <span class="s1">averaged_predictions </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span>
            <span class="s1">shape</span><span class="s4">=</span><span class="s1">grid</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s5">&quot;C&quot;</span>
        <span class="s4">)</span>
        <span class="s1">target_features </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">target_features</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">intp</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s5">&quot;C&quot;</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">tree_</span><span class="s4">.</span><span class="s1">compute_partial_dependence</span><span class="s4">(</span>
            <span class="s1">grid</span><span class="s4">, </span><span class="s1">target_features</span><span class="s4">, </span><span class="s1">averaged_predictions</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">averaged_predictions</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s2"># XXX: nan is only support for dense arrays, but we set this for common test to</span>
        <span class="s2"># pass, specifically: check_estimators_nan_inf</span>
        <span class="s1">allow_nan </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">splitter </span><span class="s4">== </span><span class="s5">&quot;best&quot; </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">criterion </span><span class="s3">in </span><span class="s4">{</span>
            <span class="s5">&quot;squared_error&quot;</span><span class="s4">,</span>
            <span class="s5">&quot;friedman_mse&quot;</span><span class="s4">,</span>
            <span class="s5">&quot;poisson&quot;</span><span class="s4">,</span>
        <span class="s4">}</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;allow_nan&quot;</span><span class="s4">: </span><span class="s1">allow_nan</span><span class="s4">}</span>


<span class="s3">class </span><span class="s1">ExtraTreeClassifier</span><span class="s4">(</span><span class="s1">DecisionTreeClassifier</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;An extremely randomized tree classifier. 
 
    Extra-trees differ from classic decision trees in the way they are built. 
    When looking for the best split to separate the samples of a node into two 
    groups, random splits are drawn for each of the `max_features` randomly 
    selected features and the best split among those is chosen. When 
    `max_features` is set 1, this amounts to building a totally random 
    decision tree. 
 
    Warning: Extra-trees should only be used within ensemble methods. 
 
    Read more in the :ref:`User Guide &lt;tree&gt;`. 
 
    Parameters 
    ---------- 
    criterion : {&quot;gini&quot;, &quot;entropy&quot;, &quot;log_loss&quot;}, default=&quot;gini&quot; 
        The function to measure the quality of a split. Supported criteria are 
        &quot;gini&quot; for the Gini impurity and &quot;log_loss&quot; and &quot;entropy&quot; both for the 
        Shannon information gain, see :ref:`tree_mathematical_formulation`. 
 
    splitter : {&quot;random&quot;, &quot;best&quot;}, default=&quot;random&quot; 
        The strategy used to choose the split at each node. Supported 
        strategies are &quot;best&quot; to choose the best split and &quot;random&quot; to choose 
        the best random split. 
 
    max_depth : int, default=None 
        The maximum depth of the tree. If None, then nodes are expanded until 
        all leaves are pure or until all leaves contain less than 
        min_samples_split samples. 
 
    min_samples_split : int or float, default=2 
        The minimum number of samples required to split an internal node: 
 
        - If int, then consider `min_samples_split` as the minimum number. 
        - If float, then `min_samples_split` is a fraction and 
          `ceil(min_samples_split * n_samples)` are the minimum 
          number of samples for each split. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_samples_leaf : int or float, default=1 
        The minimum number of samples required to be at a leaf node. 
        A split point at any depth will only be considered if it leaves at 
        least ``min_samples_leaf`` training samples in each of the left and 
        right branches.  This may have the effect of smoothing the model, 
        especially in regression. 
 
        - If int, then consider `min_samples_leaf` as the minimum number. 
        - If float, then `min_samples_leaf` is a fraction and 
          `ceil(min_samples_leaf * n_samples)` are the minimum 
          number of samples for each node. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_weight_fraction_leaf : float, default=0.0 
        The minimum weighted fraction of the sum total of weights (of all 
        the input samples) required to be at a leaf node. Samples have 
        equal weight when sample_weight is not provided. 
 
    max_features : int, float, {&quot;sqrt&quot;, &quot;log2&quot;} or None, default=&quot;sqrt&quot; 
        The number of features to consider when looking for the best split: 
 
        - If int, then consider `max_features` features at each split. 
        - If float, then `max_features` is a fraction and 
          `max(1, int(max_features * n_features_in_))` features are considered at 
          each split. 
        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`. 
        - If &quot;log2&quot;, then `max_features=log2(n_features)`. 
        - If None, then `max_features=n_features`. 
 
        .. versionchanged:: 1.1 
            The default of `max_features` changed from `&quot;auto&quot;` to `&quot;sqrt&quot;`. 
 
        Note: the search for a split does not stop until at least one 
        valid partition of the node samples is found, even if it requires to 
        effectively inspect more than ``max_features`` features. 
 
    random_state : int, RandomState instance or None, default=None 
        Used to pick randomly the `max_features` used at each split. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
    max_leaf_nodes : int, default=None 
        Grow a tree with ``max_leaf_nodes`` in best-first fashion. 
        Best nodes are defined as relative reduction in impurity. 
        If None then unlimited number of leaf nodes. 
 
    min_impurity_decrease : float, default=0.0 
        A node will be split if this split induces a decrease of the impurity 
        greater than or equal to this value. 
 
        The weighted impurity decrease equation is the following:: 
 
            N_t / N * (impurity - N_t_R / N_t * right_impurity 
                                - N_t_L / N_t * left_impurity) 
 
        where ``N`` is the total number of samples, ``N_t`` is the number of 
        samples at the current node, ``N_t_L`` is the number of samples in the 
        left child, and ``N_t_R`` is the number of samples in the right child. 
 
        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, 
        if ``sample_weight`` is passed. 
 
        .. versionadded:: 0.19 
 
    class_weight : dict, list of dict or &quot;balanced&quot;, default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If None, all classes are supposed to have weight one. For 
        multi-output problems, a list of dicts can be provided in the same 
        order as the columns of y. 
 
        Note that for multioutput (including multilabel) weights should be 
        defined for each class of every column in its own dict. For example, 
        for four-class multilabel classification weights should be 
        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of 
        [{1:1}, {2:5}, {3:1}, {4:1}]. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))`` 
 
        For multi-output, the weights of each column of y will be multiplied. 
 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if sample_weight is specified. 
 
    ccp_alpha : non-negative float, default=0.0 
        Complexity parameter used for Minimal Cost-Complexity Pruning. The 
        subtree with the largest cost complexity that is smaller than 
        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See 
        :ref:`minimal_cost_complexity_pruning` for details. 
 
        .. versionadded:: 0.22 
 
    monotonic_cst : array-like of int of shape (n_features), default=None 
        Indicates the monotonicity constraint to enforce on each feature. 
          - 1: monotonic increase 
          - 0: no constraint 
          - -1: monotonic decrease 
 
        If monotonic_cst is None, no constraints are applied. 
 
        Monotonicity constraints are not supported for: 
          - multiclass classifications (i.e. when `n_classes &gt; 2`), 
          - multioutput classifications (i.e. when `n_outputs_ &gt; 1`), 
          - classifications trained on data with missing values. 
 
        The constraints hold over the probability of the positive class. 
 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
 
        .. versionadded:: 1.4 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes,) or list of ndarray 
        The classes labels (single output problem), 
        or a list of arrays of class labels (multi-output problem). 
 
    max_features_ : int 
        The inferred value of max_features. 
 
    n_classes_ : int or list of int 
        The number of classes (for single output problems), 
        or a list containing the number of classes for each 
        output (for multi-output problems). 
 
    feature_importances_ : ndarray of shape (n_features,) 
        The impurity-based feature importances. 
        The higher, the more important the feature. 
        The importance of a feature is computed as the (normalized) 
        total reduction of the criterion brought by that feature.  It is also 
        known as the Gini importance. 
 
        Warning: impurity-based feature importances can be misleading for 
        high cardinality features (many unique values). See 
        :func:`sklearn.inspection.permutation_importance` as an alternative. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_outputs_ : int 
        The number of outputs when ``fit`` is performed. 
 
    tree_ : Tree instance 
        The underlying Tree object. Please refer to 
        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and 
        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` 
        for basic usage of these attributes. 
 
    See Also 
    -------- 
    ExtraTreeRegressor : An extremely randomized tree regressor. 
    sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier. 
    sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor. 
    sklearn.ensemble.RandomForestClassifier : A random forest classifier. 
    sklearn.ensemble.RandomForestRegressor : A random forest regressor. 
    sklearn.ensemble.RandomTreesEmbedding : An ensemble of 
        totally random trees. 
 
    Notes 
    ----- 
    The default values for the parameters controlling the size of the trees 
    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and 
    unpruned trees which can potentially be very large on some data sets. To 
    reduce memory consumption, the complexity and size of the trees should be 
    controlled by setting those parameter values. 
 
    References 
    ---------- 
 
    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized trees&quot;, 
           Machine Learning, 63(1), 3-42, 2006. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.model_selection import train_test_split 
    &gt;&gt;&gt; from sklearn.ensemble import BaggingClassifier 
    &gt;&gt;&gt; from sklearn.tree import ExtraTreeClassifier 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split( 
    ...    X, y, random_state=0) 
    &gt;&gt;&gt; extra_tree = ExtraTreeClassifier(random_state=0) 
    &gt;&gt;&gt; cls = BaggingClassifier(extra_tree, random_state=0).fit( 
    ...    X_train, y_train) 
    &gt;&gt;&gt; cls.score(X_test, y_test) 
    0.8947... 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">criterion</span><span class="s4">=</span><span class="s5">&quot;gini&quot;</span><span class="s4">,</span>
        <span class="s1">splitter</span><span class="s4">=</span><span class="s5">&quot;random&quot;</span><span class="s4">,</span>
        <span class="s1">max_depth</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s6">2</span><span class="s4">,</span>
        <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">max_features</span><span class="s4">=</span><span class="s5">&quot;sqrt&quot;</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">class_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">criterion</span><span class="s4">=</span><span class="s1">criterion</span><span class="s4">,</span>
            <span class="s1">splitter</span><span class="s4">=</span><span class="s1">splitter</span><span class="s4">,</span>
            <span class="s1">max_depth</span><span class="s4">=</span><span class="s1">max_depth</span><span class="s4">,</span>
            <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s1">min_samples_split</span><span class="s4">,</span>
            <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s1">min_samples_leaf</span><span class="s4">,</span>
            <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s1">min_weight_fraction_leaf</span><span class="s4">,</span>
            <span class="s1">max_features</span><span class="s4">=</span><span class="s1">max_features</span><span class="s4">,</span>
            <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s1">max_leaf_nodes</span><span class="s4">,</span>
            <span class="s1">class_weight</span><span class="s4">=</span><span class="s1">class_weight</span><span class="s4">,</span>
            <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s1">min_impurity_decrease</span><span class="s4">,</span>
            <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s1">ccp_alpha</span><span class="s4">,</span>
            <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s1">monotonic_cst</span><span class="s4">,</span>
        <span class="s4">)</span>


<span class="s3">class </span><span class="s1">ExtraTreeRegressor</span><span class="s4">(</span><span class="s1">DecisionTreeRegressor</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;An extremely randomized tree regressor. 
 
    Extra-trees differ from classic decision trees in the way they are built. 
    When looking for the best split to separate the samples of a node into two 
    groups, random splits are drawn for each of the `max_features` randomly 
    selected features and the best split among those is chosen. When 
    `max_features` is set 1, this amounts to building a totally random 
    decision tree. 
 
    Warning: Extra-trees should only be used within ensemble methods. 
 
    Read more in the :ref:`User Guide &lt;tree&gt;`. 
 
    Parameters 
    ---------- 
    criterion : {&quot;squared_error&quot;, &quot;friedman_mse&quot;, &quot;absolute_error&quot;, &quot;poisson&quot;}, \ 
            default=&quot;squared_error&quot; 
        The function to measure the quality of a split. Supported criteria 
        are &quot;squared_error&quot; for the mean squared error, which is equal to 
        variance reduction as feature selection criterion and minimizes the L2 
        loss using the mean of each terminal node, &quot;friedman_mse&quot;, which uses 
        mean squared error with Friedman's improvement score for potential 
        splits, &quot;absolute_error&quot; for the mean absolute error, which minimizes 
        the L1 loss using the median of each terminal node, and &quot;poisson&quot; which 
        uses reduction in Poisson deviance to find splits. 
 
        .. versionadded:: 0.18 
           Mean Absolute Error (MAE) criterion. 
 
        .. versionadded:: 0.24 
            Poisson deviance criterion. 
 
    splitter : {&quot;random&quot;, &quot;best&quot;}, default=&quot;random&quot; 
        The strategy used to choose the split at each node. Supported 
        strategies are &quot;best&quot; to choose the best split and &quot;random&quot; to choose 
        the best random split. 
 
    max_depth : int, default=None 
        The maximum depth of the tree. If None, then nodes are expanded until 
        all leaves are pure or until all leaves contain less than 
        min_samples_split samples. 
 
    min_samples_split : int or float, default=2 
        The minimum number of samples required to split an internal node: 
 
        - If int, then consider `min_samples_split` as the minimum number. 
        - If float, then `min_samples_split` is a fraction and 
          `ceil(min_samples_split * n_samples)` are the minimum 
          number of samples for each split. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_samples_leaf : int or float, default=1 
        The minimum number of samples required to be at a leaf node. 
        A split point at any depth will only be considered if it leaves at 
        least ``min_samples_leaf`` training samples in each of the left and 
        right branches.  This may have the effect of smoothing the model, 
        especially in regression. 
 
        - If int, then consider `min_samples_leaf` as the minimum number. 
        - If float, then `min_samples_leaf` is a fraction and 
          `ceil(min_samples_leaf * n_samples)` are the minimum 
          number of samples for each node. 
 
        .. versionchanged:: 0.18 
           Added float values for fractions. 
 
    min_weight_fraction_leaf : float, default=0.0 
        The minimum weighted fraction of the sum total of weights (of all 
        the input samples) required to be at a leaf node. Samples have 
        equal weight when sample_weight is not provided. 
 
    max_features : int, float, {&quot;sqrt&quot;, &quot;log2&quot;} or None, default=1.0 
        The number of features to consider when looking for the best split: 
 
        - If int, then consider `max_features` features at each split. 
        - If float, then `max_features` is a fraction and 
          `max(1, int(max_features * n_features_in_))` features are considered at each 
          split. 
        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`. 
        - If &quot;log2&quot;, then `max_features=log2(n_features)`. 
        - If None, then `max_features=n_features`. 
 
        .. versionchanged:: 1.1 
            The default of `max_features` changed from `&quot;auto&quot;` to `1.0`. 
 
        Note: the search for a split does not stop until at least one 
        valid partition of the node samples is found, even if it requires to 
        effectively inspect more than ``max_features`` features. 
 
    random_state : int, RandomState instance or None, default=None 
        Used to pick randomly the `max_features` used at each split. 
        See :term:`Glossary &lt;random_state&gt;` for details. 
 
    min_impurity_decrease : float, default=0.0 
        A node will be split if this split induces a decrease of the impurity 
        greater than or equal to this value. 
 
        The weighted impurity decrease equation is the following:: 
 
            N_t / N * (impurity - N_t_R / N_t * right_impurity 
                                - N_t_L / N_t * left_impurity) 
 
        where ``N`` is the total number of samples, ``N_t`` is the number of 
        samples at the current node, ``N_t_L`` is the number of samples in the 
        left child, and ``N_t_R`` is the number of samples in the right child. 
 
        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, 
        if ``sample_weight`` is passed. 
 
        .. versionadded:: 0.19 
 
    max_leaf_nodes : int, default=None 
        Grow a tree with ``max_leaf_nodes`` in best-first fashion. 
        Best nodes are defined as relative reduction in impurity. 
        If None then unlimited number of leaf nodes. 
 
    ccp_alpha : non-negative float, default=0.0 
        Complexity parameter used for Minimal Cost-Complexity Pruning. The 
        subtree with the largest cost complexity that is smaller than 
        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See 
        :ref:`minimal_cost_complexity_pruning` for details. 
 
        .. versionadded:: 0.22 
 
    monotonic_cst : array-like of int of shape (n_features), default=None 
        Indicates the monotonicity constraint to enforce on each feature. 
          - 1: monotonic increase 
          - 0: no constraint 
          - -1: monotonic decrease 
 
        If monotonic_cst is None, no constraints are applied. 
 
        Monotonicity constraints are not supported for: 
          - multioutput regressions (i.e. when `n_outputs_ &gt; 1`), 
          - regressions trained on data with missing values. 
 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
 
        .. versionadded:: 1.4 
 
    Attributes 
    ---------- 
    max_features_ : int 
        The inferred value of max_features. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    feature_importances_ : ndarray of shape (n_features,) 
        Return impurity-based feature importances (the higher, the more 
        important the feature). 
 
        Warning: impurity-based feature importances can be misleading for 
        high cardinality features (many unique values). See 
        :func:`sklearn.inspection.permutation_importance` as an alternative. 
 
    n_outputs_ : int 
        The number of outputs when ``fit`` is performed. 
 
    tree_ : Tree instance 
        The underlying Tree object. Please refer to 
        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and 
        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` 
        for basic usage of these attributes. 
 
    See Also 
    -------- 
    ExtraTreeClassifier : An extremely randomized tree classifier. 
    sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier. 
    sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor. 
 
    Notes 
    ----- 
    The default values for the parameters controlling the size of the trees 
    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and 
    unpruned trees which can potentially be very large on some data sets. To 
    reduce memory consumption, the complexity and size of the trees should be 
    controlled by setting those parameter values. 
 
    References 
    ---------- 
 
    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized trees&quot;, 
           Machine Learning, 63(1), 3-42, 2006. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_diabetes 
    &gt;&gt;&gt; from sklearn.model_selection import train_test_split 
    &gt;&gt;&gt; from sklearn.ensemble import BaggingRegressor 
    &gt;&gt;&gt; from sklearn.tree import ExtraTreeRegressor 
    &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True) 
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split( 
    ...     X, y, random_state=0) 
    &gt;&gt;&gt; extra_tree = ExtraTreeRegressor(random_state=0) 
    &gt;&gt;&gt; reg = BaggingRegressor(extra_tree, random_state=0).fit( 
    ...     X_train, y_train) 
    &gt;&gt;&gt; reg.score(X_test, y_test) 
    0.33... 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">criterion</span><span class="s4">=</span><span class="s5">&quot;squared_error&quot;</span><span class="s4">,</span>
        <span class="s1">splitter</span><span class="s4">=</span><span class="s5">&quot;random&quot;</span><span class="s4">,</span>
        <span class="s1">max_depth</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s6">2</span><span class="s4">,</span>
        <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">max_features</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">criterion</span><span class="s4">=</span><span class="s1">criterion</span><span class="s4">,</span>
            <span class="s1">splitter</span><span class="s4">=</span><span class="s1">splitter</span><span class="s4">,</span>
            <span class="s1">max_depth</span><span class="s4">=</span><span class="s1">max_depth</span><span class="s4">,</span>
            <span class="s1">min_samples_split</span><span class="s4">=</span><span class="s1">min_samples_split</span><span class="s4">,</span>
            <span class="s1">min_samples_leaf</span><span class="s4">=</span><span class="s1">min_samples_leaf</span><span class="s4">,</span>
            <span class="s1">min_weight_fraction_leaf</span><span class="s4">=</span><span class="s1">min_weight_fraction_leaf</span><span class="s4">,</span>
            <span class="s1">max_features</span><span class="s4">=</span><span class="s1">max_features</span><span class="s4">,</span>
            <span class="s1">max_leaf_nodes</span><span class="s4">=</span><span class="s1">max_leaf_nodes</span><span class="s4">,</span>
            <span class="s1">min_impurity_decrease</span><span class="s4">=</span><span class="s1">min_impurity_decrease</span><span class="s4">,</span>
            <span class="s1">random_state</span><span class="s4">=</span><span class="s1">random_state</span><span class="s4">,</span>
            <span class="s1">ccp_alpha</span><span class="s4">=</span><span class="s1">ccp_alpha</span><span class="s4">,</span>
            <span class="s1">monotonic_cst</span><span class="s4">=</span><span class="s1">monotonic_cst</span><span class="s4">,</span>
        <span class="s4">)</span>
</pre>
</body>
</html>