<html>
<head>
<title>_entropy.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #6aab73;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_entropy.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Created on Fri Apr  2 09:06:05 2021 
 
@author: matth 
&quot;&quot;&quot;</span>

<span class="s2">from </span><span class="s1">__future__ </span><span class="s2">import </span><span class="s1">annotations</span>
<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">special</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">_axis_nan_policy </span><span class="s2">import </span><span class="s1">_axis_nan_policy_factory</span><span class="s3">, </span><span class="s1">_broadcast_arrays</span>
<span class="s2">from </span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">_lib</span><span class="s3">.</span><span class="s1">_array_api </span><span class="s2">import </span><span class="s1">array_namespace</span>

<span class="s1">__all__ </span><span class="s3">= [</span><span class="s4">'entropy'</span><span class="s3">, </span><span class="s4">'differential_entropy'</span><span class="s3">]</span>


<span class="s3">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s3">(</span>
    <span class="s2">lambda </span><span class="s1">x</span><span class="s3">: </span><span class="s1">x</span><span class="s3">,</span>
    <span class="s1">n_samples</span><span class="s3">=</span><span class="s2">lambda </span><span class="s1">kwgs</span><span class="s3">: (</span>
        <span class="s5">2 </span><span class="s2">if </span><span class="s3">(</span><span class="s4">&quot;qk&quot; </span><span class="s2">in </span><span class="s1">kwgs </span><span class="s2">and </span><span class="s1">kwgs</span><span class="s3">[</span><span class="s4">&quot;qk&quot;</span><span class="s3">] </span><span class="s2">is not None</span><span class="s3">)</span>
        <span class="s2">else </span><span class="s5">1</span>
    <span class="s3">),</span>
    <span class="s1">n_outputs</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">result_to_tuple</span><span class="s3">=</span><span class="s2">lambda </span><span class="s1">x</span><span class="s3">: (</span><span class="s1">x</span><span class="s3">,), </span><span class="s1">paired</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
    <span class="s1">too_small</span><span class="s3">=-</span><span class="s5">1  </span><span class="s6"># entropy doesn't have too small inputs</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">entropy</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">: </span><span class="s1">np</span><span class="s3">.</span><span class="s1">typing</span><span class="s3">.</span><span class="s1">ArrayLike</span><span class="s3">,</span>
            <span class="s1">qk</span><span class="s3">: </span><span class="s1">np</span><span class="s3">.</span><span class="s1">typing</span><span class="s3">.</span><span class="s1">ArrayLike </span><span class="s3">| </span><span class="s2">None </span><span class="s3">= </span><span class="s2">None</span><span class="s3">,</span>
            <span class="s1">base</span><span class="s3">: </span><span class="s1">float </span><span class="s3">| </span><span class="s2">None </span><span class="s3">= </span><span class="s2">None</span><span class="s3">,</span>
            <span class="s1">axis</span><span class="s3">: </span><span class="s1">int </span><span class="s3">= </span><span class="s5">0</span>
            <span class="s3">) </span><span class="s1">-&gt; np</span><span class="s3">.</span><span class="s1">number </span><span class="s3">| </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ndarray</span><span class="s3">:</span>
    <span class="s0">&quot;&quot;&quot; 
    Calculate the Shannon entropy/relative entropy of given distribution(s). 
 
    If only probabilities `pk` are given, the Shannon entropy is calculated as 
    ``H = -sum(pk * log(pk))``. 
 
    If `qk` is not None, then compute the relative entropy 
    ``D = sum(pk * log(pk / qk))``. This quantity is also known 
    as the Kullback-Leibler divergence. 
 
    This routine will normalize `pk` and `qk` if they don't sum to 1. 
 
    Parameters 
    ---------- 
    pk : array_like 
        Defines the (discrete) distribution. Along each axis-slice of ``pk``, 
        element ``i`` is the  (possibly unnormalized) probability of event 
        ``i``. 
    qk : array_like, optional 
        Sequence against which the relative entropy is computed. Should be in 
        the same format as `pk`. 
    base : float, optional 
        The logarithmic base to use, defaults to ``e`` (natural logarithm). 
    axis : int, optional 
        The axis along which the entropy is calculated. Default is 0. 
 
    Returns 
    ------- 
    S : {float, array_like} 
        The calculated entropy. 
 
    Notes 
    ----- 
    Informally, the Shannon entropy quantifies the expected uncertainty 
    inherent in the possible outcomes of a discrete random variable. 
    For example, 
    if messages consisting of sequences of symbols from a set are to be 
    encoded and transmitted over a noiseless channel, then the Shannon entropy 
    ``H(pk)`` gives a tight lower bound for the average number of units of 
    information needed per symbol if the symbols occur with frequencies 
    governed by the discrete distribution `pk` [1]_. The choice of base 
    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc. 
 
    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average 
    number of units of information needed per symbol if the encoding is 
    optimized for the probability distribution `qk` instead of the true 
    distribution `pk`. Informally, the relative entropy quantifies the expected 
    excess in surprise experienced if one believes the true distribution is 
    `qk` when it is actually `pk`. 
 
    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the 
    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with 
    the formula ``CE = -sum(pk * log(qk))``. It gives the average 
    number of units of information needed per symbol if an encoding is 
    optimized for the probability distribution `qk` when the true distribution 
    is `pk`. It is not computed directly by `entropy`, but it can be computed 
    using two calls to the function (see Examples). 
 
    See [2]_ for more information. 
 
    References 
    ---------- 
    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication. 
           Bell System Technical Journal, 27: 379-423. 
           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x 
    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information 
           Theory (Wiley Series in Telecommunications and Signal Processing). 
           Wiley-Interscience, USA. 
 
 
    Examples 
    -------- 
    The outcome of a fair coin is the most uncertain: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import entropy 
    &gt;&gt;&gt; base = 2  # work in units of bits 
    &gt;&gt;&gt; pk = np.array([1/2, 1/2])  # fair coin 
    &gt;&gt;&gt; H = entropy(pk, base=base) 
    &gt;&gt;&gt; H 
    1.0 
    &gt;&gt;&gt; H == -np.sum(pk * np.log(pk)) / np.log(base) 
    True 
 
    The outcome of a biased coin is less uncertain: 
 
    &gt;&gt;&gt; qk = np.array([9/10, 1/10])  # biased coin 
    &gt;&gt;&gt; entropy(qk, base=base) 
    0.46899559358928117 
 
    The relative entropy between the fair coin and biased coin is calculated 
    as: 
 
    &gt;&gt;&gt; D = entropy(pk, qk, base=base) 
    &gt;&gt;&gt; D 
    0.7369655941662062 
    &gt;&gt;&gt; D == np.sum(pk * np.log(pk/qk)) / np.log(base) 
    True 
 
    The cross entropy can be calculated as the sum of the entropy and 
    relative entropy`: 
 
    &gt;&gt;&gt; CE = entropy(pk, base=base) + entropy(pk, qk, base=base) 
    &gt;&gt;&gt; CE 
    1.736965594166206 
    &gt;&gt;&gt; CE == -np.sum(pk * np.log(qk)) / np.log(base) 
    True 
 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None and </span><span class="s1">base </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
        <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span><span class="s4">&quot;`base` must be a positive number or `None`.&quot;</span><span class="s3">)</span>

    <span class="s1">xp </span><span class="s3">= </span><span class="s1">array_namespace</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">) </span><span class="s2">if </span><span class="s1">qk </span><span class="s2">is None else </span><span class="s1">array_namespace</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">, </span><span class="s1">qk</span><span class="s3">)</span>

    <span class="s1">pk </span><span class="s3">= </span><span class="s1">xp</span><span class="s3">.</span><span class="s1">asarray</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">)</span>
    <span class="s2">with </span><span class="s1">np</span><span class="s3">.</span><span class="s1">errstate</span><span class="s3">(</span><span class="s1">invalid</span><span class="s3">=</span><span class="s4">'ignore'</span><span class="s3">):</span>
        <span class="s1">pk </span><span class="s3">= </span><span class="s5">1.0</span><span class="s3">*</span><span class="s1">pk </span><span class="s3">/ </span><span class="s1">xp</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=</span><span class="s1">axis</span><span class="s3">, </span><span class="s1">keepdims</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)  </span><span class="s6"># type: ignore[operator]</span>
    <span class="s2">if </span><span class="s1">qk </span><span class="s2">is None</span><span class="s3">:</span>
        <span class="s1">vec </span><span class="s3">= </span><span class="s1">special</span><span class="s3">.</span><span class="s1">entr</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">qk </span><span class="s3">= </span><span class="s1">xp</span><span class="s3">.</span><span class="s1">asarray</span><span class="s3">(</span><span class="s1">qk</span><span class="s3">)</span>
        <span class="s1">pk</span><span class="s3">, </span><span class="s1">qk </span><span class="s3">= </span><span class="s1">_broadcast_arrays</span><span class="s3">((</span><span class="s1">pk</span><span class="s3">, </span><span class="s1">qk</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=</span><span class="s2">None</span><span class="s3">, </span><span class="s1">xp</span><span class="s3">=</span><span class="s1">xp</span><span class="s3">)  </span><span class="s6"># don't ignore any axes</span>
        <span class="s1">sum_kwargs </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span><span class="s1">axis</span><span class="s3">=</span><span class="s1">axis</span><span class="s3">, </span><span class="s1">keepdims</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)</span>
        <span class="s1">qk </span><span class="s3">= </span><span class="s5">1.0</span><span class="s3">*</span><span class="s1">qk </span><span class="s3">/ </span><span class="s1">xp</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">qk</span><span class="s3">, **</span><span class="s1">sum_kwargs</span><span class="s3">)  </span><span class="s6"># type: ignore[operator, call-overload]</span>
        <span class="s1">vec </span><span class="s3">= </span><span class="s1">special</span><span class="s3">.</span><span class="s1">rel_entr</span><span class="s3">(</span><span class="s1">pk</span><span class="s3">, </span><span class="s1">qk</span><span class="s3">)</span>
    <span class="s1">S </span><span class="s3">= </span><span class="s1">xp</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">vec</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=</span><span class="s1">axis</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None</span><span class="s3">:</span>
        <span class="s1">S </span><span class="s3">/= </span><span class="s1">math</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">base</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">S</span>


<span class="s2">def </span><span class="s1">_differential_entropy_is_too_small</span><span class="s3">(</span><span class="s1">samples</span><span class="s3">, </span><span class="s1">kwargs</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">):</span>
    <span class="s1">values </span><span class="s3">= </span><span class="s1">samples</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>
    <span class="s1">n </span><span class="s3">= </span><span class="s1">values</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s1">axis</span><span class="s3">]</span>
    <span class="s1">window_length </span><span class="s3">= </span><span class="s1">kwargs</span><span class="s3">.</span><span class="s1">get</span><span class="s3">(</span><span class="s4">&quot;window_length&quot;</span><span class="s3">,</span>
                               <span class="s1">math</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">(</span><span class="s1">math</span><span class="s3">.</span><span class="s1">sqrt</span><span class="s3">(</span><span class="s1">n</span><span class="s3">) + </span><span class="s5">0.5</span><span class="s3">))</span>
    <span class="s2">if not </span><span class="s5">2 </span><span class="s3">&lt;= </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">window_length </span><span class="s3">&lt; </span><span class="s1">n</span><span class="s3">:</span>
        <span class="s2">return True</span>
    <span class="s2">return False</span>


<span class="s3">@</span><span class="s1">_axis_nan_policy_factory</span><span class="s3">(</span>
    <span class="s2">lambda </span><span class="s1">x</span><span class="s3">: </span><span class="s1">x</span><span class="s3">, </span><span class="s1">n_outputs</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">result_to_tuple</span><span class="s3">=</span><span class="s2">lambda </span><span class="s1">x</span><span class="s3">: (</span><span class="s1">x</span><span class="s3">,),</span>
    <span class="s1">too_small</span><span class="s3">=</span><span class="s1">_differential_entropy_is_too_small</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">differential_entropy</span><span class="s3">(</span>
    <span class="s1">values</span><span class="s3">: </span><span class="s1">np</span><span class="s3">.</span><span class="s1">typing</span><span class="s3">.</span><span class="s1">ArrayLike</span><span class="s3">,</span>
    <span class="s3">*,</span>
    <span class="s1">window_length</span><span class="s3">: </span><span class="s1">int </span><span class="s3">| </span><span class="s2">None </span><span class="s3">= </span><span class="s2">None</span><span class="s3">,</span>
    <span class="s1">base</span><span class="s3">: </span><span class="s1">float </span><span class="s3">| </span><span class="s2">None </span><span class="s3">= </span><span class="s2">None</span><span class="s3">,</span>
    <span class="s1">axis</span><span class="s3">: </span><span class="s1">int </span><span class="s3">= </span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">method</span><span class="s3">: </span><span class="s1">str </span><span class="s3">= </span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
<span class="s3">) </span><span class="s1">-&gt; np</span><span class="s3">.</span><span class="s1">number </span><span class="s3">| </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ndarray</span><span class="s3">:</span>
    <span class="s0">r&quot;&quot;&quot;Given a sample of a distribution, estimate the differential entropy. 
 
    Several estimation methods are available using the `method` parameter. By 
    default, a method is selected based the size of the sample. 
 
    Parameters 
    ---------- 
    values : sequence 
        Sample from a continuous distribution. 
    window_length : int, optional 
        Window length for computing Vasicek estimate. Must be an integer 
        between 1 and half of the sample size. If ``None`` (the default), it 
        uses the heuristic value 
 
        .. math:: 
            \left \lfloor \sqrt{n} + 0.5 \right \rfloor 
 
        where :math:`n` is the sample size. This heuristic was originally 
        proposed in [2]_ and has become common in the literature. 
    base : float, optional 
        The logarithmic base to use, defaults to ``e`` (natural logarithm). 
    axis : int, optional 
        The axis along which the differential entropy is calculated. 
        Default is 0. 
    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional 
        The method used to estimate the differential entropy from the sample. 
        Default is ``'auto'``.  See Notes for more information. 
 
    Returns 
    ------- 
    entropy : float 
        The calculated differential entropy. 
 
    Notes 
    ----- 
    This function will converge to the true differential entropy in the limit 
 
    .. math:: 
        n \to \infty, \quad m \to \infty, \quad \frac{m}{n} \to 0 
 
    The optimal choice of ``window_length`` for a given sample size depends on 
    the (unknown) distribution. Typically, the smoother the density of the 
    distribution, the larger the optimal value of ``window_length`` [1]_. 
 
    The following options are available for the `method` parameter. 
 
    * ``'vasicek'`` uses the estimator presented in [1]_. This is 
      one of the first and most influential estimators of differential entropy. 
    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which 
      is not only consistent but, under some conditions, asymptotically normal. 
    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown 
      in simulation to have smaller bias and mean squared error than 
      the Vasicek estimator. 
    * ``'correa'`` uses the estimator presented in [5]_ based on local linear 
      regression. In a simulation study, it had consistently smaller mean 
      square error than the Vasiceck estimator, but it is more expensive to 
      compute. 
    * ``'auto'`` selects the method automatically (default). Currently, 
      this selects ``'van es'`` for very small samples (&lt;10), ``'ebrahimi'`` 
      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger 
      samples, but this behavior is subject to change in future versions. 
 
    All estimators are implemented as described in [6]_. 
 
    References 
    ---------- 
    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy. 
           Journal of the Royal Statistical Society: 
           Series B (Methodological), 38(1), 54-59. 
    .. [2] Crzcgorzewski, P., &amp; Wirczorkowski, R. (1999). Entropy-based 
           goodness-of-fit test for exponentiality. Communications in 
           Statistics-Theory and Methods, 28(5), 1183-1202. 
    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a 
           class of statistics based on spacings. Scandinavian Journal of 
           Statistics, 61-72. 
    .. [4] Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures 
           of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234. 
    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications 
           in Statistics-Theory and Methods, 24(10), 2439-2449. 
    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods. 
           Annals of Data Science, 2(2), 231-241. 
           https://link.springer.com/article/10.1007/s40745-015-0045-9 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import differential_entropy, norm 
 
    Entropy of a standard normal distribution: 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; values = rng.standard_normal(100) 
    &gt;&gt;&gt; differential_entropy(values) 
    1.3407817436640392 
 
    Compare with the true entropy: 
 
    &gt;&gt;&gt; float(norm.entropy()) 
    1.4189385332046727 
 
    For several sample sizes between 5 and 1000, compare the accuracy of 
    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically, 
    compare the root mean squared error (over 1000 trials) between the estimate 
    and the true differential entropy of the distribution. 
 
    &gt;&gt;&gt; from scipy import stats 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; def rmse(res, expected): 
    ...     '''Root mean squared error''' 
    ...     return np.sqrt(np.mean((res - expected)**2)) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; a, b = np.log10(5), np.log10(1000) 
    &gt;&gt;&gt; ns = np.round(np.logspace(a, b, 10)).astype(int) 
    &gt;&gt;&gt; reps = 1000  # number of repetitions for each sample size 
    &gt;&gt;&gt; expected = stats.expon.entropy() 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []} 
    &gt;&gt;&gt; for method in method_errors: 
    ...     for n in ns: 
    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng) 
    ...        res = stats.differential_entropy(rvs, method=method, axis=-1) 
    ...        error = rmse(res, expected) 
    ...        method_errors[method].append(error) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; for method, errors in method_errors.items(): 
    ...     plt.loglog(ns, errors, label=method) 
    &gt;&gt;&gt; 
    &gt;&gt;&gt; plt.legend() 
    &gt;&gt;&gt; plt.xlabel('sample size') 
    &gt;&gt;&gt; plt.ylabel('RMSE (1000 trials)') 
    &gt;&gt;&gt; plt.title('Entropy Estimator Error (Exponential Distribution)') 
 
    &quot;&quot;&quot;</span>
    <span class="s1">values </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">asarray</span><span class="s3">(</span><span class="s1">values</span><span class="s3">)</span>
    <span class="s1">values </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">moveaxis</span><span class="s3">(</span><span class="s1">values</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">, -</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s1">n </span><span class="s3">= </span><span class="s1">values</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]  </span><span class="s6"># number of observations</span>

    <span class="s2">if </span><span class="s1">window_length </span><span class="s2">is None</span><span class="s3">:</span>
        <span class="s1">window_length </span><span class="s3">= </span><span class="s1">math</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">(</span><span class="s1">math</span><span class="s3">.</span><span class="s1">sqrt</span><span class="s3">(</span><span class="s1">n</span><span class="s3">) + </span><span class="s5">0.5</span><span class="s3">)</span>

    <span class="s2">if not </span><span class="s5">2 </span><span class="s3">&lt;= </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">window_length </span><span class="s3">&lt; </span><span class="s1">n</span><span class="s3">:</span>
        <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
            <span class="s4">f&quot;Window length (</span><span class="s2">{</span><span class="s1">window_length</span><span class="s2">}</span><span class="s4">) must be positive and less &quot;</span>
            <span class="s4">f&quot;than half the sample size (</span><span class="s2">{</span><span class="s1">n</span><span class="s2">}</span><span class="s4">).&quot;</span><span class="s3">,</span>
        <span class="s3">)</span>

    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None and </span><span class="s1">base </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
        <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span><span class="s4">&quot;`base` must be a positive number or `None`.&quot;</span><span class="s3">)</span>

    <span class="s1">sorted_data </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sort</span><span class="s3">(</span><span class="s1">values</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">)</span>

    <span class="s1">methods </span><span class="s3">= {</span><span class="s4">&quot;vasicek&quot;</span><span class="s3">: </span><span class="s1">_vasicek_entropy</span><span class="s3">,</span>
               <span class="s4">&quot;van es&quot;</span><span class="s3">: </span><span class="s1">_van_es_entropy</span><span class="s3">,</span>
               <span class="s4">&quot;correa&quot;</span><span class="s3">: </span><span class="s1">_correa_entropy</span><span class="s3">,</span>
               <span class="s4">&quot;ebrahimi&quot;</span><span class="s3">: </span><span class="s1">_ebrahimi_entropy</span><span class="s3">,</span>
               <span class="s4">&quot;auto&quot;</span><span class="s3">: </span><span class="s1">_vasicek_entropy</span><span class="s3">}</span>
    <span class="s1">method </span><span class="s3">= </span><span class="s1">method</span><span class="s3">.</span><span class="s1">lower</span><span class="s3">()</span>
    <span class="s2">if </span><span class="s1">method </span><span class="s2">not in </span><span class="s1">methods</span><span class="s3">:</span>
        <span class="s1">message </span><span class="s3">= </span><span class="s4">f&quot;`method` must be one of </span><span class="s2">{</span><span class="s1">set</span><span class="s3">(</span><span class="s1">methods</span><span class="s3">)</span><span class="s2">}</span><span class="s4">&quot;</span>
        <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span><span class="s1">message</span><span class="s3">)</span>

    <span class="s2">if </span><span class="s1">method </span><span class="s3">== </span><span class="s4">&quot;auto&quot;</span><span class="s3">:</span>
        <span class="s2">if </span><span class="s1">n </span><span class="s3">&lt;= </span><span class="s5">10</span><span class="s3">:</span>
            <span class="s1">method </span><span class="s3">= </span><span class="s4">'van es'</span>
        <span class="s2">elif </span><span class="s1">n </span><span class="s3">&lt;= </span><span class="s5">1000</span><span class="s3">:</span>
            <span class="s1">method </span><span class="s3">= </span><span class="s4">'ebrahimi'</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">method </span><span class="s3">= </span><span class="s4">'vasicek'</span>

    <span class="s1">res </span><span class="s3">= </span><span class="s1">methods</span><span class="s3">[</span><span class="s1">method</span><span class="s3">](</span><span class="s1">sorted_data</span><span class="s3">, </span><span class="s1">window_length</span><span class="s3">)</span>

    <span class="s2">if </span><span class="s1">base </span><span class="s2">is not None</span><span class="s3">:</span>
        <span class="s1">res </span><span class="s3">/= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">base</span><span class="s3">)</span>

    <span class="s2">return </span><span class="s1">res</span>


<span class="s2">def </span><span class="s1">_pad_along_last_axis</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Pad the data for computing the rolling window difference.&quot;&quot;&quot;</span>
    <span class="s6"># scales a  bit better than method in _vasicek_like_entropy</span>
    <span class="s1">shape </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">)</span>
    <span class="s1">shape</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">] = </span><span class="s1">m</span>
    <span class="s1">Xl </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">broadcast_to</span><span class="s3">(</span><span class="s1">X</span><span class="s3">[..., [</span><span class="s5">0</span><span class="s3">]], </span><span class="s1">shape</span><span class="s3">)  </span><span class="s6"># [0] vs 0 to maintain shape</span>
    <span class="s1">Xr </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">broadcast_to</span><span class="s3">(</span><span class="s1">X</span><span class="s3">[..., [-</span><span class="s5">1</span><span class="s3">]], </span><span class="s1">shape</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">Xl</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">Xr</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">)</span>


<span class="s2">def </span><span class="s1">_vasicek_entropy</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Vasicek estimator as described in [6] Eq. 1.3.&quot;&quot;&quot;</span>
    <span class="s1">n </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">_pad_along_last_axis</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">)</span>
    <span class="s1">differences </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[..., </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">m</span><span class="s3">:] - </span><span class="s1">X</span><span class="s3">[..., : -</span><span class="s5">2 </span><span class="s3">* </span><span class="s1">m</span><span class="s3">:]</span>
    <span class="s1">logs </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">n</span><span class="s3">/(</span><span class="s5">2</span><span class="s3">*</span><span class="s1">m</span><span class="s3">) * </span><span class="s1">differences</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">np</span><span class="s3">.</span><span class="s1">mean</span><span class="s3">(</span><span class="s1">logs</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">)</span>


<span class="s2">def </span><span class="s1">_van_es_entropy</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the van Es estimator as described in [6].&quot;&quot;&quot;</span>
    <span class="s6"># No equation number, but referred to as HVE_mn.</span>
    <span class="s6"># Typo: there should be a log within the summation.</span>
    <span class="s1">n </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s1">difference </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[..., </span><span class="s1">m</span><span class="s3">:] - </span><span class="s1">X</span><span class="s3">[..., :-</span><span class="s1">m</span><span class="s3">]</span>
    <span class="s1">term1 </span><span class="s3">= </span><span class="s5">1</span><span class="s3">/(</span><span class="s1">n</span><span class="s3">-</span><span class="s1">m</span><span class="s3">) * </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">((</span><span class="s1">n</span><span class="s3">+</span><span class="s5">1</span><span class="s3">)/</span><span class="s1">m </span><span class="s3">* </span><span class="s1">difference</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s1">k </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">arange</span><span class="s3">(</span><span class="s1">m</span><span class="s3">, </span><span class="s1">n</span><span class="s3">+</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">term1 </span><span class="s3">+ </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s5">1</span><span class="s3">/</span><span class="s1">k</span><span class="s3">) + </span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">m</span><span class="s3">) - </span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">n</span><span class="s3">+</span><span class="s5">1</span><span class="s3">)</span>


<span class="s2">def </span><span class="s1">_ebrahimi_entropy</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Ebrahimi estimator as described in [6].&quot;&quot;&quot;</span>
    <span class="s6"># No equation number, but referred to as HE_mn</span>
    <span class="s1">n </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">_pad_along_last_axis</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">)</span>

    <span class="s1">differences </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[..., </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">m</span><span class="s3">:] - </span><span class="s1">X</span><span class="s3">[..., : -</span><span class="s5">2 </span><span class="s3">* </span><span class="s1">m</span><span class="s3">:]</span>

    <span class="s1">i </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">arange</span><span class="s3">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">n</span><span class="s3">+</span><span class="s5">1</span><span class="s3">).</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">float</span><span class="s3">)</span>
    <span class="s1">ci </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones_like</span><span class="s3">(</span><span class="s1">i</span><span class="s3">)*</span><span class="s5">2</span>
    <span class="s1">ci</span><span class="s3">[</span><span class="s1">i </span><span class="s3">&lt;= </span><span class="s1">m</span><span class="s3">] = </span><span class="s5">1 </span><span class="s3">+ (</span><span class="s1">i</span><span class="s3">[</span><span class="s1">i </span><span class="s3">&lt;= </span><span class="s1">m</span><span class="s3">] - </span><span class="s5">1</span><span class="s3">)/</span><span class="s1">m</span>
    <span class="s1">ci</span><span class="s3">[</span><span class="s1">i </span><span class="s3">&gt;= </span><span class="s1">n </span><span class="s3">- </span><span class="s1">m </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">] = </span><span class="s5">1 </span><span class="s3">+ (</span><span class="s1">n </span><span class="s3">- </span><span class="s1">i</span><span class="s3">[</span><span class="s1">i </span><span class="s3">&gt;= </span><span class="s1">n</span><span class="s3">-</span><span class="s1">m</span><span class="s3">+</span><span class="s5">1</span><span class="s3">])/</span><span class="s1">m</span>

    <span class="s1">logs </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">n </span><span class="s3">* </span><span class="s1">differences </span><span class="s3">/ (</span><span class="s1">ci </span><span class="s3">* </span><span class="s1">m</span><span class="s3">))</span>
    <span class="s2">return </span><span class="s1">np</span><span class="s3">.</span><span class="s1">mean</span><span class="s3">(</span><span class="s1">logs</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">)</span>


<span class="s2">def </span><span class="s1">_correa_entropy</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Correa estimator as described in [6].&quot;&quot;&quot;</span>
    <span class="s6"># No equation number, but referred to as HC_mn</span>
    <span class="s1">n </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">_pad_along_last_axis</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">m</span><span class="s3">)</span>

    <span class="s1">i </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">arange</span><span class="s3">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">n</span><span class="s3">+</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s1">dj </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">arange</span><span class="s3">(-</span><span class="s1">m</span><span class="s3">, </span><span class="s1">m</span><span class="s3">+</span><span class="s5">1</span><span class="s3">)[:, </span><span class="s2">None</span><span class="s3">]</span>
    <span class="s1">j </span><span class="s3">= </span><span class="s1">i </span><span class="s3">+ </span><span class="s1">dj</span>
    <span class="s1">j0 </span><span class="s3">= </span><span class="s1">j </span><span class="s3">+ </span><span class="s1">m </span><span class="s3">- </span><span class="s5">1  </span><span class="s6"># 0-indexed version of j</span>

    <span class="s1">Xibar </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">mean</span><span class="s3">(</span><span class="s1">X</span><span class="s3">[..., </span><span class="s1">j0</span><span class="s3">], </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">2</span><span class="s3">, </span><span class="s1">keepdims</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)</span>
    <span class="s1">difference </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[..., </span><span class="s1">j0</span><span class="s3">] - </span><span class="s1">Xibar</span>
    <span class="s1">num </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">difference</span><span class="s3">*</span><span class="s1">dj</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">2</span><span class="s3">)  </span><span class="s6"># dj is d-i</span>
    <span class="s1">den </span><span class="s3">= </span><span class="s1">n</span><span class="s3">*</span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">difference</span><span class="s3">**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">2</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s3">-</span><span class="s1">np</span><span class="s3">.</span><span class="s1">mean</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">num</span><span class="s3">/</span><span class="s1">den</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=-</span><span class="s5">1</span><span class="s3">)</span>
</pre>
</body>
</html>