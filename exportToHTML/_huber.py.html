<html>
<head>
<title>_huber.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_huber.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Manoj Kumar mks542@nyu.edu</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span>

<span class="s2">from </span><span class="s3">..</span><span class="s1">base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">_mask </span><span class="s2">import </span><span class="s1">axis0_safe_slice</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">_param_validation </span><span class="s2">import </span><span class="s1">Interval</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">extmath </span><span class="s2">import </span><span class="s1">safe_sparse_dot</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">optimize </span><span class="s2">import </span><span class="s1">_check_optimize_result</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">validation </span><span class="s2">import </span><span class="s1">_check_sample_weight</span>
<span class="s2">from </span><span class="s3">.</span><span class="s1">_base </span><span class="s2">import </span><span class="s1">LinearModel</span>


<span class="s2">def </span><span class="s1">_huber_loss_and_gradient</span><span class="s3">(</span><span class="s1">w</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">epsilon</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;Returns the Huber loss and the gradient. 
 
    Parameters 
    ---------- 
    w : ndarray, shape (n_features + 1,) or (n_features + 2,) 
        Feature vector. 
        w[:n_features] gives the coefficients 
        w[-1] gives the scale factor and if the intercept is fit w[-2] 
        gives the intercept factor. 
 
    X : ndarray of shape (n_samples, n_features) 
        Input data. 
 
    y : ndarray of shape (n_samples,) 
        Target vector. 
 
    epsilon : float 
        Robustness of the Huber estimator. 
 
    alpha : float 
        Regularization parameter. 
 
    sample_weight : ndarray of shape (n_samples,), default=None 
        Weight assigned to each sample. 
 
    Returns 
    ------- 
    loss : float 
        Huber loss. 
 
    gradient : ndarray, shape (len(w)) 
        Returns the derivative of the Huber loss with respect to each 
        coefficient, intercept and the scale as a vector. 
    &quot;&quot;&quot;</span>
    <span class="s1">_</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span>
    <span class="s1">fit_intercept </span><span class="s3">= </span><span class="s1">n_features </span><span class="s3">+ </span><span class="s5">2 </span><span class="s3">== </span><span class="s1">w</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">w</span><span class="s3">[-</span><span class="s5">2</span><span class="s3">]</span>
    <span class="s1">sigma </span><span class="s3">= </span><span class="s1">w</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s1">w </span><span class="s3">= </span><span class="s1">w</span><span class="s3">[:</span><span class="s1">n_features</span><span class="s3">]</span>
    <span class="s1">n_samples </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">sample_weight</span><span class="s3">)</span>

    <span class="s0"># Calculate the values where |y - X'w -c / sigma| &gt; epsilon</span>
    <span class="s0"># The values above this threshold are outliers.</span>
    <span class="s1">linear_loss </span><span class="s3">= </span><span class="s1">y </span><span class="s3">- </span><span class="s1">safe_sparse_dot</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">w</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">linear_loss </span><span class="s3">-= </span><span class="s1">intercept</span>
    <span class="s1">abs_linear_loss </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">(</span><span class="s1">linear_loss</span><span class="s3">)</span>
    <span class="s1">outliers_mask </span><span class="s3">= </span><span class="s1">abs_linear_loss </span><span class="s3">&gt; </span><span class="s1">epsilon </span><span class="s3">* </span><span class="s1">sigma</span>

    <span class="s0"># Calculate the linear loss due to the outliers.</span>
    <span class="s0"># This is equal to (2 * M * |y - X'w -c / sigma| - M**2) * sigma</span>
    <span class="s1">outliers </span><span class="s3">= </span><span class="s1">abs_linear_loss</span><span class="s3">[</span><span class="s1">outliers_mask</span><span class="s3">]</span>
    <span class="s1">num_outliers </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">count_nonzero</span><span class="s3">(</span><span class="s1">outliers_mask</span><span class="s3">)</span>
    <span class="s1">n_non_outliers </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">] - </span><span class="s1">num_outliers</span>

    <span class="s0"># n_sq_outliers includes the weight give to the outliers while</span>
    <span class="s0"># num_outliers is just the number of outliers.</span>
    <span class="s1">outliers_sw </span><span class="s3">= </span><span class="s1">sample_weight</span><span class="s3">[</span><span class="s1">outliers_mask</span><span class="s3">]</span>
    <span class="s1">n_sw_outliers </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">outliers_sw</span><span class="s3">)</span>
    <span class="s1">outlier_loss </span><span class="s3">= (</span>
        <span class="s5">2.0 </span><span class="s3">* </span><span class="s1">epsilon </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">outliers_sw </span><span class="s3">* </span><span class="s1">outliers</span><span class="s3">)</span>
        <span class="s3">- </span><span class="s1">sigma </span><span class="s3">* </span><span class="s1">n_sw_outliers </span><span class="s3">* </span><span class="s1">epsilon</span><span class="s3">**</span><span class="s5">2</span>
    <span class="s3">)</span>

    <span class="s0"># Calculate the quadratic loss due to the non-outliers.-</span>
    <span class="s0"># This is equal to |(y - X'w - c)**2 / sigma**2| * sigma</span>
    <span class="s1">non_outliers </span><span class="s3">= </span><span class="s1">linear_loss</span><span class="s3">[~</span><span class="s1">outliers_mask</span><span class="s3">]</span>
    <span class="s1">weighted_non_outliers </span><span class="s3">= </span><span class="s1">sample_weight</span><span class="s3">[~</span><span class="s1">outliers_mask</span><span class="s3">] * </span><span class="s1">non_outliers</span>
    <span class="s1">weighted_loss </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">(</span><span class="s1">weighted_non_outliers</span><span class="s3">.</span><span class="s1">T</span><span class="s3">, </span><span class="s1">non_outliers</span><span class="s3">)</span>
    <span class="s1">squared_loss </span><span class="s3">= </span><span class="s1">weighted_loss </span><span class="s3">/ </span><span class="s1">sigma</span>

    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">grad </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">n_features </span><span class="s3">+ </span><span class="s5">2</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">grad </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">n_features </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">)</span>

    <span class="s0"># Gradient due to the squared loss.</span>
    <span class="s1">X_non_outliers </span><span class="s3">= -</span><span class="s1">axis0_safe_slice</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, ~</span><span class="s1">outliers_mask</span><span class="s3">, </span><span class="s1">n_non_outliers</span><span class="s3">)</span>
    <span class="s1">grad</span><span class="s3">[:</span><span class="s1">n_features</span><span class="s3">] = (</span>
        <span class="s5">2.0 </span><span class="s3">/ </span><span class="s1">sigma </span><span class="s3">* </span><span class="s1">safe_sparse_dot</span><span class="s3">(</span><span class="s1">weighted_non_outliers</span><span class="s3">, </span><span class="s1">X_non_outliers</span><span class="s3">)</span>
    <span class="s3">)</span>

    <span class="s0"># Gradient due to the linear loss.</span>
    <span class="s1">signed_outliers </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones_like</span><span class="s3">(</span><span class="s1">outliers</span><span class="s3">)</span>
    <span class="s1">signed_outliers_mask </span><span class="s3">= </span><span class="s1">linear_loss</span><span class="s3">[</span><span class="s1">outliers_mask</span><span class="s3">] &lt; </span><span class="s5">0</span>
    <span class="s1">signed_outliers</span><span class="s3">[</span><span class="s1">signed_outliers_mask</span><span class="s3">] = -</span><span class="s5">1.0</span>
    <span class="s1">X_outliers </span><span class="s3">= </span><span class="s1">axis0_safe_slice</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">outliers_mask</span><span class="s3">, </span><span class="s1">num_outliers</span><span class="s3">)</span>
    <span class="s1">sw_outliers </span><span class="s3">= </span><span class="s1">sample_weight</span><span class="s3">[</span><span class="s1">outliers_mask</span><span class="s3">] * </span><span class="s1">signed_outliers</span>
    <span class="s1">grad</span><span class="s3">[:</span><span class="s1">n_features</span><span class="s3">] -= </span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">epsilon </span><span class="s3">* (</span><span class="s1">safe_sparse_dot</span><span class="s3">(</span><span class="s1">sw_outliers</span><span class="s3">, </span><span class="s1">X_outliers</span><span class="s3">))</span>

    <span class="s0"># Gradient due to the penalty.</span>
    <span class="s1">grad</span><span class="s3">[:</span><span class="s1">n_features</span><span class="s3">] += </span><span class="s1">alpha </span><span class="s3">* </span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">w</span>

    <span class="s0"># Gradient due to sigma.</span>
    <span class="s1">grad</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">] = </span><span class="s1">n_samples</span>
    <span class="s1">grad</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">] -= </span><span class="s1">n_sw_outliers </span><span class="s3">* </span><span class="s1">epsilon</span><span class="s3">**</span><span class="s5">2</span>
    <span class="s1">grad</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">] -= </span><span class="s1">squared_loss </span><span class="s3">/ </span><span class="s1">sigma</span>

    <span class="s0"># Gradient due to the intercept.</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">grad</span><span class="s3">[-</span><span class="s5">2</span><span class="s3">] = -</span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">weighted_non_outliers</span><span class="s3">) / </span><span class="s1">sigma</span>
        <span class="s1">grad</span><span class="s3">[-</span><span class="s5">2</span><span class="s3">] -= </span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">epsilon </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">sw_outliers</span><span class="s3">)</span>

    <span class="s1">loss </span><span class="s3">= </span><span class="s1">n_samples </span><span class="s3">* </span><span class="s1">sigma </span><span class="s3">+ </span><span class="s1">squared_loss </span><span class="s3">+ </span><span class="s1">outlier_loss</span>
    <span class="s1">loss </span><span class="s3">+= </span><span class="s1">alpha </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">(</span><span class="s1">w</span><span class="s3">, </span><span class="s1">w</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">loss</span><span class="s3">, </span><span class="s1">grad</span>


<span class="s2">class </span><span class="s1">HuberRegressor</span><span class="s3">(</span><span class="s1">LinearModel</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">BaseEstimator</span><span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;L2-regularized linear regression model that is robust to outliers. 
 
    The Huber Regressor optimizes the squared loss for the samples where 
    ``|(y - Xw - c) / sigma| &lt; epsilon`` and the absolute loss for the samples 
    where ``|(y - Xw - c) / sigma| &gt; epsilon``, where the model coefficients 
    ``w``, the intercept ``c`` and the scale ``sigma`` are parameters 
    to be optimized. The parameter sigma makes sure that if y is scaled up 
    or down by a certain factor, one does not need to rescale epsilon to 
    achieve the same robustness. Note that this does not take into account 
    the fact that the different features of X may be of different scales. 
 
    The Huber loss function has the advantage of not being heavily influenced 
    by the outliers while not completely ignoring their effect. 
 
    Read more in the :ref:`User Guide &lt;huber_regression&gt;` 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    epsilon : float, default=1.35 
        The parameter epsilon controls the number of samples that should be 
        classified as outliers. The smaller the epsilon, the more robust it is 
        to outliers. Epsilon must be in the range `[1, inf)`. 
 
    max_iter : int, default=100 
        Maximum number of iterations that 
        ``scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)`` should run for. 
 
    alpha : float, default=0.0001 
        Strength of the squared L2 regularization. Note that the penalty is 
        equal to ``alpha * ||w||^2``. 
        Must be in the range `[0, inf)`. 
 
    warm_start : bool, default=False 
        This is useful if the stored attributes of a previously used model 
        has to be reused. If set to False, then the coefficients will 
        be rewritten for every call to fit. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
 
    fit_intercept : bool, default=True 
        Whether or not to fit the intercept. This can be set to False 
        if the data is already centered around the origin. 
 
    tol : float, default=1e-05 
        The iteration will stop when 
        ``max{|proj g_i | i = 1, ..., n}`` &lt;= ``tol`` 
        where pg_i is the i-th component of the projected gradient. 
 
    Attributes 
    ---------- 
    coef_ : array, shape (n_features,) 
        Features got by optimizing the L2-regularized Huber loss. 
 
    intercept_ : float 
        Bias. 
 
    scale_ : float 
        The value by which ``|y - Xw - c|`` is scaled down. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_iter_ : int 
        Number of iterations that 
        ``scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)`` has run for. 
 
        .. versionchanged:: 0.20 
 
            In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed 
            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``. 
 
    outliers_ : array, shape (n_samples,) 
        A boolean mask which is set to True where the samples are identified 
        as outliers. 
 
    See Also 
    -------- 
    RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm. 
    TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model. 
    SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD. 
 
    References 
    ---------- 
    .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics 
           Concomitant scale estimates, pg 172 
    .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression. 
           https://statweb.stanford.edu/~owen/reports/hhu.pdf 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.linear_model import HuberRegressor, LinearRegression 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; rng = np.random.RandomState(0) 
    &gt;&gt;&gt; X, y, coef = make_regression( 
    ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0) 
    &gt;&gt;&gt; X[:4] = rng.uniform(10, 20, (4, 2)) 
    &gt;&gt;&gt; y[:4] = rng.uniform(10, 20, 4) 
    &gt;&gt;&gt; huber = HuberRegressor().fit(X, y) 
    &gt;&gt;&gt; huber.score(X, y) 
    -7.284... 
    &gt;&gt;&gt; huber.predict(X[:1,]) 
    array([806.7200...]) 
    &gt;&gt;&gt; linear = LinearRegression().fit(X, y) 
    &gt;&gt;&gt; print(&quot;True coefficients:&quot;, coef) 
    True coefficients: [20.4923...  34.1698...] 
    &gt;&gt;&gt; print(&quot;Huber coefficients:&quot;, huber.coef_) 
    Huber coefficients: [17.7906... 31.0106...] 
    &gt;&gt;&gt; print(&quot;Linear Regression coefficients:&quot;, linear.coef_) 
    Linear Regression coefficients: [-1.9221...  7.0226...] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s3">: </span><span class="s1">dict </span><span class="s3">= {</span>
        <span class="s6">&quot;epsilon&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">1.0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;max_iter&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;alpha&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;warm_start&quot;</span><span class="s3">: [</span><span class="s6">&quot;boolean&quot;</span><span class="s3">],</span>
        <span class="s6">&quot;fit_intercept&quot;</span><span class="s3">: [</span><span class="s6">&quot;boolean&quot;</span><span class="s3">],</span>
        <span class="s6">&quot;tol&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">0.0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
    <span class="s3">}</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s3">*,</span>
        <span class="s1">epsilon</span><span class="s3">=</span><span class="s5">1.35</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s5">0.0001</span><span class="s3">,</span>
        <span class="s1">warm_start</span><span class="s3">=</span><span class="s2">False</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-05</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">epsilon </span><span class="s3">= </span><span class="s1">epsilon</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter </span><span class="s3">= </span><span class="s1">max_iter</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">alpha </span><span class="s3">= </span><span class="s1">alpha</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">warm_start </span><span class="s3">= </span><span class="s1">warm_start</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">fit_intercept </span><span class="s3">= </span><span class="s1">fit_intercept</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">tol </span><span class="s3">= </span><span class="s1">tol</span>

    <span class="s3">@</span><span class="s1">_fit_context</span><span class="s3">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)</span>
    <span class="s2">def </span><span class="s1">fit</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">):</span>
        <span class="s4">&quot;&quot;&quot;Fit the model according to the given training data. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like, shape (n_samples,) 
            Target vector relative to X. 
 
        sample_weight : array-like, shape (n_samples,) 
            Weight given to each sample. 
 
        Returns 
        ------- 
        self : object 
            Fitted `HuberRegressor` estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_validate_data</span><span class="s3">(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">,</span>
            <span class="s1">accept_sparse</span><span class="s3">=[</span><span class="s6">&quot;csr&quot;</span><span class="s3">],</span>
            <span class="s1">y_numeric</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
            <span class="s1">dtype</span><span class="s3">=[</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float64</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">],</span>
        <span class="s3">)</span>

        <span class="s1">sample_weight </span><span class="s3">= </span><span class="s1">_check_sample_weight</span><span class="s3">(</span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">)</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">warm_start </span><span class="s2">and </span><span class="s1">hasattr</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s6">&quot;coef_&quot;</span><span class="s3">):</span>
            <span class="s1">parameters </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">self</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, [</span><span class="s1">self</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">scale_</span><span class="s3">]))</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">fit_intercept</span><span class="s3">:</span>
                <span class="s1">parameters </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">] + </span><span class="s5">2</span><span class="s3">)</span>
            <span class="s2">else</span><span class="s3">:</span>
                <span class="s1">parameters </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">] + </span><span class="s5">1</span><span class="s3">)</span>
            <span class="s0"># Make sure to initialize the scale parameter to a strictly</span>
            <span class="s0"># positive value:</span>
            <span class="s1">parameters</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">] = </span><span class="s5">1</span>

        <span class="s0"># Sigma or the scale factor should be non-negative.</span>
        <span class="s0"># Setting it to be zero might cause undefined bounds hence we set it</span>
        <span class="s0"># to a value close to zero.</span>
        <span class="s1">bounds </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">tile</span><span class="s3">([-</span><span class="s1">np</span><span class="s3">.</span><span class="s1">inf</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">inf</span><span class="s3">], (</span><span class="s1">parameters</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">], </span><span class="s5">1</span><span class="s3">))</span>
        <span class="s1">bounds</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">][</span><span class="s5">0</span><span class="s3">] = </span><span class="s1">np</span><span class="s3">.</span><span class="s1">finfo</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float64</span><span class="s3">).</span><span class="s1">eps </span><span class="s3">* </span><span class="s5">10</span>

        <span class="s1">opt_res </span><span class="s3">= </span><span class="s1">optimize</span><span class="s3">.</span><span class="s1">minimize</span><span class="s3">(</span>
            <span class="s1">_huber_loss_and_gradient</span><span class="s3">,</span>
            <span class="s1">parameters</span><span class="s3">,</span>
            <span class="s1">method</span><span class="s3">=</span><span class="s6">&quot;L-BFGS-B&quot;</span><span class="s3">,</span>
            <span class="s1">jac</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
            <span class="s1">args</span><span class="s3">=(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">epsilon</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">),</span>
            <span class="s1">options</span><span class="s3">={</span><span class="s6">&quot;maxiter&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter</span><span class="s3">, </span><span class="s6">&quot;gtol&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">tol</span><span class="s3">, </span><span class="s6">&quot;iprint&quot;</span><span class="s3">: -</span><span class="s5">1</span><span class="s3">},</span>
            <span class="s1">bounds</span><span class="s3">=</span><span class="s1">bounds</span><span class="s3">,</span>
        <span class="s3">)</span>

        <span class="s1">parameters </span><span class="s3">= </span><span class="s1">opt_res</span><span class="s3">.</span><span class="s1">x</span>

        <span class="s2">if </span><span class="s1">opt_res</span><span class="s3">.</span><span class="s1">status </span><span class="s3">== </span><span class="s5">2</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s6">&quot;HuberRegressor convergence failed: l-BFGS-b solver terminated with %s&quot;</span>
                <span class="s3">% </span><span class="s1">opt_res</span><span class="s3">.</span><span class="s1">message</span>
            <span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter_ </span><span class="s3">= </span><span class="s1">_check_optimize_result</span><span class="s3">(</span><span class="s6">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">opt_res</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter</span><span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">scale_ </span><span class="s3">= </span><span class="s1">parameters</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">fit_intercept</span><span class="s3">:</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">= </span><span class="s1">parameters</span><span class="s3">[-</span><span class="s5">2</span><span class="s3">]</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">= </span><span class="s5">0.0</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">coef_ </span><span class="s3">= </span><span class="s1">parameters</span><span class="s3">[: </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">]]</span>

        <span class="s1">residual </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">(</span><span class="s1">y </span><span class="s3">- </span><span class="s1">safe_sparse_dot</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">) - </span><span class="s1">self</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">outliers_ </span><span class="s3">= </span><span class="s1">residual </span><span class="s3">&gt; </span><span class="s1">self</span><span class="s3">.</span><span class="s1">scale_ </span><span class="s3">* </span><span class="s1">self</span><span class="s3">.</span><span class="s1">epsilon</span>
        <span class="s2">return </span><span class="s1">self</span>
</pre>
</body>
</html>