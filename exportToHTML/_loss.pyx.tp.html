<html>
<head>
<title>_loss.pyx.tp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #bcbec4;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_loss.pyx.tp</font>
</center></td></tr></table>
<pre><span class="s0">{{py:</span>

<span class="s0">&quot;&quot;&quot;</span>
<span class="s0">Template file to easily generate loops over samples using Tempita</span>
<span class="s0">(https://github.com/cython/cython/blob/master/Cython/Tempita/_tempita.py).</span>

<span class="s0">Generated file: _loss.pyx</span>

<span class="s0">Each loss class is generated by a cdef functions on single samples.</span>
<span class="s0">The keywords between double braces are substituted in setup.py.</span>
<span class="s0">&quot;&quot;&quot;</span>

<span class="s0">doc_HalfSquaredError = (</span>
    <span class="s0">&quot;&quot;&quot;Half Squared Error with identity link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true and y_pred all real numbers</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = raw_prediction</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_AbsoluteError = (</span>
    <span class="s0">&quot;&quot;&quot;Absolute Error with identity link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true and y_pred all real numbers</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = raw_prediction</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_PinballLoss = (</span>
    <span class="s0">&quot;&quot;&quot;Quantile Loss aka Pinball Loss with identity link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true and y_pred all real numbers</span>
    <span class="s0">quantile in (0, 1)</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = raw_prediction</span>

    <span class="s0">Note: 2 * cPinballLoss(quantile=0.5) equals cAbsoluteError()</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_HuberLoss = (</span>
    <span class="s0">&quot;&quot;&quot;Huber Loss with identity link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true and y_pred all real numbers</span>
    <span class="s0">delta in positive real numbers</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = raw_prediction</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_HalfPoissonLoss = (</span>
    <span class="s0">&quot;&quot;&quot;Half Poisson deviance loss with log-link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true in non-negative real numbers</span>
    <span class="s0">y_pred in positive real numbers</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = exp(raw_prediction)</span>

    <span class="s0">Half Poisson deviance with log-link is</span>
        <span class="s0">y_true * log(y_true/y_pred) + y_pred - y_true</span>
        <span class="s0">= y_true * log(y_true) - y_true * raw_prediction</span>
          <span class="s0">+ exp(raw_prediction) - y_true</span>

    <span class="s0">Dropping constant terms, this gives:</span>
        <span class="s0">exp(raw_prediction) - y_true * raw_prediction</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_HalfGammaLoss = (</span>
    <span class="s0">&quot;&quot;&quot;Half Gamma deviance loss with log-link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true and y_pred in positive real numbers</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = exp(raw_prediction)</span>

    <span class="s0">Half Gamma deviance with log-link is</span>
        <span class="s0">log(y_pred/y_true) + y_true/y_pred - 1</span>
        <span class="s0">= raw_prediction - log(y_true) + y_true * exp(-raw_prediction) - 1</span>

    <span class="s0">Dropping constant terms, this gives:</span>
        <span class="s0">raw_prediction + y_true * exp(-raw_prediction)</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_HalfTweedieLoss = (</span>
    <span class="s0">&quot;&quot;&quot;Half Tweedie deviance loss with log-link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true in real numbers if p &lt;= 0</span>
    <span class="s0">y_true in non-negative real numbers if 0 &lt; p &lt; 2</span>
    <span class="s0">y_true in positive real numbers if p &gt;= 2</span>
    <span class="s0">y_pred and power in positive real numbers</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = exp(raw_prediction)</span>

    <span class="s0">Half Tweedie deviance with log-link and p=power is</span>
        <span class="s0">max(y_true, 0)**(2-p) / (1-p) / (2-p)</span>
        <span class="s0">- y_true * y_pred**(1-p) / (1-p)</span>
        <span class="s0">+ y_pred**(2-p) / (2-p)</span>
        <span class="s0">= max(y_true, 0)**(2-p) / (1-p) / (2-p)</span>
        <span class="s0">- y_true * exp((1-p) * raw_prediction) / (1-p)</span>
        <span class="s0">+ exp((2-p) * raw_prediction) / (2-p)</span>

    <span class="s0">Dropping constant terms, this gives:</span>
        <span class="s0">exp((2-p) * raw_prediction) / (2-p)</span>
        <span class="s0">- y_true * exp((1-p) * raw_prediction) / (1-p)</span>

    <span class="s0">Notes:</span>
    <span class="s0">- Poisson with p=1 and and Gamma with p=2 have different terms dropped such</span>
      <span class="s0">that cHalfTweedieLoss is not continuous in p=power at p=1 and p=2.</span>
    <span class="s0">- While the Tweedie distribution only exists for p&lt;=0 or p&gt;=1, the range</span>
      <span class="s0">0&lt;p&lt;1 still gives a strictly consistent scoring function for the</span>
      <span class="s0">expectation.</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_HalfTweedieLossIdentity = (</span>
    <span class="s0">&quot;&quot;&quot;Half Tweedie deviance loss with identity link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true in real numbers if p &lt;= 0</span>
    <span class="s0">y_true in non-negative real numbers if 0 &lt; p &lt; 2</span>
    <span class="s0">y_true in positive real numbers if p &gt;= 2</span>
    <span class="s0">y_pred and power in positive real numbers, y_pred may be negative for p=0.</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = raw_prediction</span>

    <span class="s0">Half Tweedie deviance with identity link and p=power is</span>
        <span class="s0">max(y_true, 0)**(2-p) / (1-p) / (2-p)</span>
        <span class="s0">- y_true * y_pred**(1-p) / (1-p)</span>
        <span class="s0">+ y_pred**(2-p) / (2-p)</span>

    <span class="s0">Notes:</span>
    <span class="s0">- Here, we do not drop constant terms in contrast to the version with log-link.</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_HalfBinomialLoss = (</span>
    <span class="s0">&quot;&quot;&quot;Half Binomial deviance loss with logit link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true in [0, 1]</span>
    <span class="s0">y_pred in (0, 1), i.e. boundaries excluded</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = expit(raw_prediction)</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0">doc_ExponentialLoss = (</span>
    <span class="s0">&quot;&quot;&quot;&quot;Exponential loss with (half) logit link</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true in [0, 1]</span>
    <span class="s0">y_pred in (0, 1), i.e. boundaries excluded</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = expit(2 * raw_prediction)</span>
    <span class="s0">&quot;&quot;&quot;</span>
<span class="s0">)</span>

<span class="s0"># loss class name, docstring, param,</span>
<span class="s0"># cy_loss, cy_loss_grad,</span>
<span class="s0"># cy_grad, cy_grad_hess,</span>
<span class="s0">class_list = [</span>
    <span class="s0">(&quot;CyHalfSquaredError&quot;, doc_HalfSquaredError, None,</span>
     <span class="s0">&quot;closs_half_squared_error&quot;, None,</span>
     <span class="s0">&quot;cgradient_half_squared_error&quot;, &quot;cgrad_hess_half_squared_error&quot;),</span>
    <span class="s0">(&quot;CyAbsoluteError&quot;, doc_AbsoluteError, None,</span>
     <span class="s0">&quot;closs_absolute_error&quot;, None,</span>
     <span class="s0">&quot;cgradient_absolute_error&quot;, &quot;cgrad_hess_absolute_error&quot;),</span>
    <span class="s0">(&quot;CyPinballLoss&quot;, doc_PinballLoss, &quot;quantile&quot;,</span>
     <span class="s0">&quot;closs_pinball_loss&quot;, None,</span>
     <span class="s0">&quot;cgradient_pinball_loss&quot;, &quot;cgrad_hess_pinball_loss&quot;),</span>
     <span class="s0">(&quot;CyHuberLoss&quot;, doc_HuberLoss, &quot;delta&quot;,</span>
     <span class="s0">&quot;closs_huber_loss&quot;, None,</span>
     <span class="s0">&quot;cgradient_huber_loss&quot;, &quot;cgrad_hess_huber_loss&quot;),</span>
    <span class="s0">(&quot;CyHalfPoissonLoss&quot;, doc_HalfPoissonLoss, None,</span>
     <span class="s0">&quot;closs_half_poisson&quot;, &quot;closs_grad_half_poisson&quot;,</span>
     <span class="s0">&quot;cgradient_half_poisson&quot;, &quot;cgrad_hess_half_poisson&quot;),</span>
    <span class="s0">(&quot;CyHalfGammaLoss&quot;, doc_HalfGammaLoss, None,</span>
     <span class="s0">&quot;closs_half_gamma&quot;, &quot;closs_grad_half_gamma&quot;,</span>
     <span class="s0">&quot;cgradient_half_gamma&quot;, &quot;cgrad_hess_half_gamma&quot;),</span>
    <span class="s0">(&quot;CyHalfTweedieLoss&quot;, doc_HalfTweedieLoss, &quot;power&quot;,</span>
     <span class="s0">&quot;closs_half_tweedie&quot;, &quot;closs_grad_half_tweedie&quot;,</span>
     <span class="s0">&quot;cgradient_half_tweedie&quot;, &quot;cgrad_hess_half_tweedie&quot;),</span>
    <span class="s0">(&quot;CyHalfTweedieLossIdentity&quot;, doc_HalfTweedieLossIdentity, &quot;power&quot;,</span>
     <span class="s0">&quot;closs_half_tweedie_identity&quot;, &quot;closs_grad_half_tweedie_identity&quot;,</span>
     <span class="s0">&quot;cgradient_half_tweedie_identity&quot;, &quot;cgrad_hess_half_tweedie_identity&quot;),</span>
    <span class="s0">(&quot;CyHalfBinomialLoss&quot;, doc_HalfBinomialLoss, None,</span>
     <span class="s0">&quot;closs_half_binomial&quot;, &quot;closs_grad_half_binomial&quot;,</span>
     <span class="s0">&quot;cgradient_half_binomial&quot;, &quot;cgrad_hess_half_binomial&quot;),</span>
     <span class="s0">(&quot;CyExponentialLoss&quot;, doc_ExponentialLoss, None,</span>
     <span class="s0">&quot;closs_exponential&quot;, &quot;closs_grad_exponential&quot;,</span>
     <span class="s0">&quot;cgradient_exponential&quot;, &quot;cgrad_hess_exponential&quot;),</span>
<span class="s0">]</span>
<span class="s0">}}</span>

<span class="s0"># Design:</span>
<span class="s0"># See https://github.com/scikit-learn/scikit-learn/issues/15123 for reasons.</span>
<span class="s0"># a) Merge link functions into loss functions for speed and numerical</span>
<span class="s0">#    stability, i.e. use raw_prediction instead of y_pred in signature.</span>
<span class="s0"># b) Pure C functions (nogil) calculate single points (single sample)</span>
<span class="s0"># c) Wrap C functions in a loop to get Python functions operating on ndarrays.</span>
<span class="s0">#   - Write loops manually---use Tempita for this.</span>
<span class="s0">#     Reason: There is still some performance overhead when using a wrapper</span>
<span class="s0">#     function &quot;wrap&quot; that carries out the loop and gets as argument a function</span>
<span class="s0">#     pointer to one of the C functions from b), e.g.</span>
<span class="s0">#     wrap(closs_half_poisson, y_true, ...)</span>
<span class="s0">#   - Pass n_threads as argument to prange and propagate option to all callers.</span>
<span class="s0"># d) Provide classes (Cython extension types) per loss (names start with Cy) in</span>
<span class="s0">#    order to have semantical structured objects.</span>
<span class="s0">#    - Member functions for single points just call the C function from b).</span>
<span class="s0">#      These are used e.g. in SGD `_plain_sgd`.</span>
<span class="s0">#    - Member functions operating on ndarrays, see c), looping over calls to C</span>
<span class="s0">#      functions from b).</span>
<span class="s0"># e) Provide convenience Python classes that compose from these extension types</span>
<span class="s0">#    elsewhere (see loss.py)</span>
<span class="s0">#    - Example: loss.gradient calls CyLoss.gradient but does some input</span>
<span class="s0">#      checking like None -&gt; np.empty().</span>
<span class="s0">#</span>
<span class="s0"># Note: We require 1-dim ndarrays to be contiguous.</span>

<span class="s0">from cython.parallel import parallel, prange</span>
<span class="s0">import numpy as np</span>

<span class="s0">from libc.math cimport exp, fabs, log, log1p, pow</span>
<span class="s0">from libc.stdlib cimport malloc, free</span>


<span class="s0"># -------------------------------------</span>
<span class="s0"># Helper functions</span>
<span class="s0"># -------------------------------------</span>
<span class="s0"># Numerically stable version of log(1 + exp(x)) for double precision, see Eq. (10) of</span>
<span class="s0"># https://cran.r-project.org/web/packages/Rmpfr/vignettes/log1mexp-note.pdf</span>
<span class="s0"># Note: The only important cutoff is at x = 18. All others are to save computation</span>
<span class="s0"># time. Compared to the reference, we add the additional case distinction x &lt;= -2 in</span>
<span class="s0"># order to use log instead of log1p for improved performance. As with the other</span>
<span class="s0"># cutoffs, this is accurate within machine precision of double.</span>
<span class="s0">cdef inline double log1pexp(double x) noexcept nogil:</span>
    <span class="s0">if x &lt;= -37:</span>
        <span class="s0">return exp(x)</span>
    <span class="s0">elif x &lt;= -2:</span>
        <span class="s0">return log1p(exp(x))</span>
    <span class="s0">elif x &lt;= 18:</span>
        <span class="s0">return log(1. + exp(x))</span>
    <span class="s0">elif x &lt;= 33.3:</span>
        <span class="s0">return x + exp(-x)</span>
    <span class="s0">else:</span>
        <span class="s0">return x</span>


<span class="s0">cdef inline void sum_exp_minus_max(</span>
    <span class="s0">const int i,</span>
    <span class="s0">const floating_in[:, :] raw_prediction,  # IN</span>
    <span class="s0">floating_in *p                           # OUT</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0"># Thread local buffers are used to store results of this function via p.</span>
    <span class="s0"># The results are stored as follows:</span>
    <span class="s0">#     p[k] = exp(raw_prediction_i_k - max_value) for k = 0 to n_classes-1</span>
    <span class="s0">#     p[-2] = max(raw_prediction_i_k, k = 0 to n_classes-1)</span>
    <span class="s0">#     p[-1] = sum(p[k], k = 0 to n_classes-1) = sum of exponentials</span>
    <span class="s0"># len(p) must be n_classes + 2</span>
    <span class="s0"># Notes:</span>
    <span class="s0"># - Using &quot;by reference&quot; arguments doesn't work well, therefore we use a</span>
    <span class="s0">#   longer p, see https://github.com/cython/cython/issues/1863</span>
    <span class="s0"># - i needs to be passed (and stays constant) because otherwise Cython does</span>
    <span class="s0">#   not generate optimal code, see</span>
    <span class="s0">#   https://github.com/scikit-learn/scikit-learn/issues/17299</span>
    <span class="s0"># - We do not normalize p by calculating p[k] = p[k] / sum_exps.</span>
    <span class="s0">#   This helps to save one loop over k.</span>
    <span class="s0">cdef:</span>
        <span class="s0">int k</span>
        <span class="s0">int n_classes = raw_prediction.shape[1]</span>
        <span class="s0">double max_value = raw_prediction[i, 0]</span>
        <span class="s0">double sum_exps = 0</span>
    <span class="s0">for k in range(1, n_classes):</span>
        <span class="s0"># Compute max value of array for numerical stability</span>
        <span class="s0">if max_value &lt; raw_prediction[i, k]:</span>
            <span class="s0">max_value = raw_prediction[i, k]</span>

    <span class="s0">for k in range(n_classes):</span>
        <span class="s0">p[k] = exp(raw_prediction[i, k] - max_value)</span>
        <span class="s0">sum_exps += p[k]</span>

    <span class="s0">p[n_classes] = max_value     # same as p[-2]</span>
    <span class="s0">p[n_classes + 1] = sum_exps  # same as p[-1]</span>


<span class="s0"># -------------------------------------</span>
<span class="s0"># Single point inline C functions</span>
<span class="s0"># -------------------------------------</span>
<span class="s0"># Half Squared Error</span>
<span class="s0">cdef inline double closs_half_squared_error(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return 0.5 * (raw_prediction - y_true) * (raw_prediction - y_true)</span>


<span class="s0">cdef inline double cgradient_half_squared_error(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return raw_prediction - y_true</span>


<span class="s0">cdef inline double_pair cgrad_hess_half_squared_error(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">gh.val1 = raw_prediction - y_true  # gradient</span>
    <span class="s0">gh.val2 = 1.                       # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Absolute Error</span>
<span class="s0">cdef inline double closs_absolute_error(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return fabs(raw_prediction - y_true)</span>


<span class="s0">cdef inline double cgradient_absolute_error(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return 1. if raw_prediction &gt; y_true else -1.</span>


<span class="s0">cdef inline double_pair cgrad_hess_absolute_error(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0"># Note that exact hessian = 0 almost everywhere. Optimization routines like</span>
    <span class="s0"># in HGBT, however, need a hessian &gt; 0. Therefore, we assign 1.</span>
    <span class="s0">gh.val1 = 1. if raw_prediction &gt; y_true else -1.  # gradient</span>
    <span class="s0">gh.val2 = 1.                                      # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Quantile Loss / Pinball Loss</span>
<span class="s0">cdef inline double closs_pinball_loss(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double quantile</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return (quantile * (y_true - raw_prediction) if y_true &gt;= raw_prediction</span>
            <span class="s0">else (1. - quantile) * (raw_prediction - y_true))</span>


<span class="s0">cdef inline double cgradient_pinball_loss(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double quantile</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return -quantile if y_true &gt;=raw_prediction else 1. - quantile</span>


<span class="s0">cdef inline double_pair cgrad_hess_pinball_loss(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double quantile</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0"># Note that exact hessian = 0 almost everywhere. Optimization routines like</span>
    <span class="s0"># in HGBT, however, need a hessian &gt; 0. Therefore, we assign 1.</span>
    <span class="s0">gh.val1 = -quantile if y_true &gt;=raw_prediction else 1. - quantile  # gradient</span>
    <span class="s0">gh.val2 = 1.                                                       # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Huber Loss</span>
<span class="s0">cdef inline double closs_huber_loss(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double delta,</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double abserr = fabs(y_true - raw_prediction)</span>
    <span class="s0">if abserr &lt;= delta:</span>
        <span class="s0">return 0.5 * abserr**2</span>
    <span class="s0">else:</span>
        <span class="s0">return delta * (abserr - 0.5 * delta)</span>


<span class="s0">cdef inline double cgradient_huber_loss(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double delta,</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double res = raw_prediction - y_true</span>
    <span class="s0">if fabs(res) &lt;= delta:</span>
        <span class="s0">return res</span>
    <span class="s0">else:</span>
        <span class="s0">return delta if res &gt;=0 else -delta</span>


<span class="s0">cdef inline double_pair cgrad_hess_huber_loss(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double delta,</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">gh.val2 = raw_prediction - y_true               # used as temporary</span>
    <span class="s0">if fabs(gh.val2) &lt;= delta:</span>
        <span class="s0">gh.val1 = gh.val2                           # gradient</span>
        <span class="s0">gh.val2 = 1                                 # hessian</span>
    <span class="s0">else:</span>
        <span class="s0">gh.val1 = delta if gh.val2 &gt;=0 else -delta  # gradient</span>
        <span class="s0">gh.val2 = 0                                 # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Half Poisson Deviance with Log-Link, dropping constant terms</span>
<span class="s0">cdef inline double closs_half_poisson(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return exp(raw_prediction) - y_true * raw_prediction</span>


<span class="s0">cdef inline double cgradient_half_poisson(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0"># y_pred - y_true</span>
    <span class="s0">return exp(raw_prediction) - y_true</span>


<span class="s0">cdef inline double_pair closs_grad_half_poisson(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair lg</span>
    <span class="s0">lg.val2 = exp(raw_prediction)                # used as temporary</span>
    <span class="s0">lg.val1 = lg.val2 - y_true * raw_prediction  # loss</span>
    <span class="s0">lg.val2 -= y_true                            # gradient</span>
    <span class="s0">return lg</span>


<span class="s0">cdef inline double_pair cgrad_hess_half_poisson(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">gh.val2 = exp(raw_prediction)  # hessian</span>
    <span class="s0">gh.val1 = gh.val2 - y_true     # gradient</span>
    <span class="s0">return gh</span>


<span class="s0"># Half Gamma Deviance with Log-Link, dropping constant terms</span>
<span class="s0">cdef inline double closs_half_gamma(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return raw_prediction + y_true * exp(-raw_prediction)</span>


<span class="s0">cdef inline double cgradient_half_gamma(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">return 1. - y_true * exp(-raw_prediction)</span>


<span class="s0">cdef inline double_pair closs_grad_half_gamma(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair lg</span>
    <span class="s0">lg.val2 = exp(-raw_prediction)               # used as temporary</span>
    <span class="s0">lg.val1 = raw_prediction + y_true * lg.val2  # loss</span>
    <span class="s0">lg.val2 = 1. - y_true * lg.val2              # gradient</span>
    <span class="s0">return lg</span>


<span class="s0">cdef inline double_pair cgrad_hess_half_gamma(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">gh.val2 = exp(-raw_prediction)   # used as temporary</span>
    <span class="s0">gh.val1 = 1. - y_true * gh.val2  # gradient</span>
    <span class="s0">gh.val2 *= y_true                # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Half Tweedie Deviance with Log-Link, dropping constant terms</span>
<span class="s0"># Note that by dropping constants this is no longer continuous in parameter power.</span>
<span class="s0">cdef inline double closs_half_tweedie(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">return closs_half_squared_error(y_true, exp(raw_prediction))</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">return closs_half_poisson(y_true, raw_prediction)</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">return closs_half_gamma(y_true, raw_prediction)</span>
    <span class="s0">else:</span>
        <span class="s0">return (exp((2. - power) * raw_prediction) / (2. - power)</span>
                <span class="s0">- y_true * exp((1. - power) * raw_prediction) / (1. - power))</span>


<span class="s0">cdef inline double cgradient_half_tweedie(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double exp1</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">exp1 = exp(raw_prediction)</span>
        <span class="s0">return exp1 * (exp1 - y_true)</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">return cgradient_half_poisson(y_true, raw_prediction)</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">return cgradient_half_gamma(y_true, raw_prediction)</span>
    <span class="s0">else:</span>
        <span class="s0">return (exp((2. - power) * raw_prediction)</span>
                <span class="s0">- y_true * exp((1. - power) * raw_prediction))</span>


<span class="s0">cdef inline double_pair closs_grad_half_tweedie(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair lg</span>
    <span class="s0">cdef double exp1, exp2</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">exp1 = exp(raw_prediction)</span>
        <span class="s0">lg.val1 = closs_half_squared_error(y_true, exp1)  # loss</span>
        <span class="s0">lg.val2 = exp1 * (exp1 - y_true)                  # gradient</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">return closs_grad_half_poisson(y_true, raw_prediction)</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">return closs_grad_half_gamma(y_true, raw_prediction)</span>
    <span class="s0">else:</span>
        <span class="s0">exp1 = exp((1. - power) * raw_prediction)</span>
        <span class="s0">exp2 = exp((2. - power) * raw_prediction)</span>
        <span class="s0">lg.val1 = exp2 / (2. - power) - y_true * exp1 / (1. - power)  # loss</span>
        <span class="s0">lg.val2 = exp2 - y_true * exp1                                # gradient</span>
    <span class="s0">return lg</span>


<span class="s0">cdef inline double_pair cgrad_hess_half_tweedie(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">cdef double exp1, exp2</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">exp1 = exp(raw_prediction)</span>
        <span class="s0">gh.val1 = exp1 * (exp1 - y_true)      # gradient</span>
        <span class="s0">gh.val2 = exp1 * (2 * exp1 - y_true)  # hessian</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">return cgrad_hess_half_poisson(y_true, raw_prediction)</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">return cgrad_hess_half_gamma(y_true, raw_prediction)</span>
    <span class="s0">else:</span>
        <span class="s0">exp1 = exp((1. - power) * raw_prediction)</span>
        <span class="s0">exp2 = exp((2. - power) * raw_prediction)</span>
        <span class="s0">gh.val1 = exp2 - y_true * exp1                                # gradient</span>
        <span class="s0">gh.val2 = (2. - power) * exp2 - (1. - power) * y_true * exp1  # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Half Tweedie Deviance with identity link, without dropping constant terms!</span>
<span class="s0"># Therefore, best loss value is zero.</span>
<span class="s0">cdef inline double closs_half_tweedie_identity(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double tmp</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">return closs_half_squared_error(y_true, raw_prediction)</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">if y_true == 0:</span>
            <span class="s0">return raw_prediction</span>
        <span class="s0">else:</span>
            <span class="s0">return y_true * log(y_true/raw_prediction) + raw_prediction - y_true</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">return log(raw_prediction/y_true) + y_true/raw_prediction - 1.</span>
    <span class="s0">else:</span>
        <span class="s0">tmp = pow(raw_prediction, 1. - power)</span>
        <span class="s0">tmp = raw_prediction * tmp / (2. - power) - y_true * tmp / (1. - power)</span>
        <span class="s0">if y_true &gt; 0:</span>
            <span class="s0">tmp += pow(y_true, 2. - power) / ((1. - power) * (2. - power))</span>
        <span class="s0">return tmp</span>


<span class="s0">cdef inline double cgradient_half_tweedie_identity(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">return raw_prediction - y_true</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">return 1. - y_true / raw_prediction</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">return (raw_prediction - y_true) / (raw_prediction * raw_prediction)</span>
    <span class="s0">else:</span>
        <span class="s0">return pow(raw_prediction, -power) * (raw_prediction - y_true)</span>


<span class="s0">cdef inline double_pair closs_grad_half_tweedie_identity(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair lg</span>
    <span class="s0">cdef double tmp</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">lg.val2 = raw_prediction - y_true  # gradient</span>
        <span class="s0">lg.val1 = 0.5 * lg.val2 * lg.val2  # loss</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">if y_true == 0:</span>
            <span class="s0">lg.val1 = raw_prediction</span>
        <span class="s0">else:</span>
            <span class="s0">lg.val1 = (y_true * log(y_true/raw_prediction)  # loss</span>
                       <span class="s0">+ raw_prediction - y_true)</span>
        <span class="s0">lg.val2 = 1. - y_true / raw_prediction              # gradient</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">lg.val1 = log(raw_prediction/y_true) + y_true/raw_prediction - 1.  # loss</span>
        <span class="s0">tmp = raw_prediction * raw_prediction</span>
        <span class="s0">lg.val2 = (raw_prediction - y_true) / tmp                          # gradient</span>
    <span class="s0">else:</span>
        <span class="s0">tmp = pow(raw_prediction, 1. - power)</span>
        <span class="s0">lg.val1 = (raw_prediction * tmp / (2. - power)  # loss</span>
                   <span class="s0">- y_true * tmp / (1. - power))</span>
        <span class="s0">if y_true &gt; 0:</span>
            <span class="s0">lg.val1 += (pow(y_true, 2. - power)</span>
                        <span class="s0">/ ((1. - power) * (2. - power)))</span>
        <span class="s0">lg.val2 = tmp * (1. - y_true / raw_prediction)    # gradient</span>
    <span class="s0">return lg</span>


<span class="s0">cdef inline double_pair cgrad_hess_half_tweedie_identity(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction,</span>
    <span class="s0">double power</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">cdef double tmp</span>
    <span class="s0">if power == 0.:</span>
        <span class="s0">gh.val1 = raw_prediction - y_true  # gradient</span>
        <span class="s0">gh.val2 = 1.                       # hessian</span>
    <span class="s0">elif power == 1.:</span>
        <span class="s0">gh.val1 = 1. - y_true / raw_prediction                # gradient</span>
        <span class="s0">gh.val2 = y_true / (raw_prediction * raw_prediction)  # hessian</span>
    <span class="s0">elif power == 2.:</span>
        <span class="s0">tmp = raw_prediction * raw_prediction</span>
        <span class="s0">gh.val1 = (raw_prediction - y_true) / tmp             # gradient</span>
        <span class="s0">gh.val2 = (-1. + 2. * y_true / raw_prediction) / tmp  # hessian</span>
    <span class="s0">else:</span>
        <span class="s0">tmp = pow(raw_prediction, -power)</span>
        <span class="s0">gh.val1 = tmp * (raw_prediction - y_true)                         # gradient</span>
        <span class="s0">gh.val2 = tmp * ((1. - power) + power * y_true / raw_prediction)  # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># Half Binomial deviance with logit-link, aka log-loss or binary cross entropy</span>
<span class="s0">cdef inline double closs_half_binomial(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0"># log1p(exp(raw_prediction)) - y_true * raw_prediction</span>
    <span class="s0">return log1pexp(raw_prediction) - y_true * raw_prediction</span>


<span class="s0">cdef inline double cgradient_half_binomial(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0"># gradient = y_pred - y_true = expit(raw_prediction) - y_true</span>
    <span class="s0"># Numerically more stable, see http://fa.bianp.net/blog/2019/evaluate_logistic/</span>
    <span class="s0">#     if raw_prediction &lt; 0:</span>
    <span class="s0">#         exp_tmp = exp(raw_prediction)</span>
    <span class="s0">#         return ((1 - y_true) * exp_tmp - y_true) / (1 + exp_tmp)</span>
    <span class="s0">#     else:</span>
    <span class="s0">#         exp_tmp = exp(-raw_prediction)</span>
    <span class="s0">#         return ((1 - y_true) - y_true * exp_tmp) / (1 + exp_tmp)</span>
    <span class="s0"># Note that optimal speed would be achieved, at the cost of precision, by</span>
    <span class="s0">#     return expit(raw_prediction) - y_true</span>
    <span class="s0"># i.e. no &quot;if else&quot; and an own inline implementation of expit instead of</span>
    <span class="s0">#     from scipy.special.cython_special cimport expit</span>
    <span class="s0"># The case distinction raw_prediction &lt; 0 in the stable implementation does not</span>
    <span class="s0"># provide significant better precision apart from protecting overflow of exp(..).</span>
    <span class="s0"># The branch (if else), however, can incur runtime costs of up to 30%.</span>
    <span class="s0"># Instead, we help branch prediction by almost always ending in the first if clause</span>
    <span class="s0"># and making the second branch (else) a bit simpler. This has the exact same</span>
    <span class="s0"># precision but is faster than the stable implementation.</span>
    <span class="s0"># As branching criteria, we use the same cutoff as in log1pexp. Note that the</span>
    <span class="s0"># maximal value to get gradient = -1 with y_true = 1 is -37.439198610162731</span>
    <span class="s0"># (based on mpmath), and scipy.special.logit(np.finfo(float).eps) ~ -36.04365.</span>
    <span class="s0">cdef double exp_tmp</span>
    <span class="s0">if raw_prediction &gt; -37:</span>
        <span class="s0">exp_tmp = exp(-raw_prediction)</span>
        <span class="s0">return ((1 - y_true) - y_true * exp_tmp) / (1 + exp_tmp)</span>
    <span class="s0">else:</span>
        <span class="s0"># expit(raw_prediction) = exp(raw_prediction) for raw_prediction &lt;= -37</span>
        <span class="s0">return exp(raw_prediction) - y_true</span>


<span class="s0">cdef inline double_pair closs_grad_half_binomial(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair lg</span>
    <span class="s0"># Same if else conditions as in log1pexp.</span>
    <span class="s0">if raw_prediction &lt;= -37:</span>
        <span class="s0">lg.val2 = exp(raw_prediction)  # used as temporary</span>
        <span class="s0">lg.val1 = lg.val2 - y_true * raw_prediction                  # loss</span>
        <span class="s0">lg.val2 -= y_true                                            # gradient</span>
    <span class="s0">elif raw_prediction &lt;= -2:</span>
        <span class="s0">lg.val2 = exp(raw_prediction)  # used as temporary</span>
        <span class="s0">lg.val1 = log1p(lg.val2) - y_true * raw_prediction           # loss</span>
        <span class="s0">lg.val2 = ((1 - y_true) * lg.val2 - y_true) / (1 + lg.val2)  # gradient</span>
    <span class="s0">elif raw_prediction &lt;= 18:</span>
        <span class="s0">lg.val2 = exp(-raw_prediction)  # used as temporary</span>
        <span class="s0"># log1p(exp(x)) = log(1 + exp(x)) = x + log1p(exp(-x))</span>
        <span class="s0">lg.val1 = log1p(lg.val2) + (1 - y_true) * raw_prediction     # loss</span>
        <span class="s0">lg.val2 = ((1 - y_true) - y_true * lg.val2) / (1 + lg.val2)  # gradient</span>
    <span class="s0">else:</span>
        <span class="s0">lg.val2 = exp(-raw_prediction)  # used as temporary</span>
        <span class="s0">lg.val1 = lg.val2 + (1 - y_true) * raw_prediction            # loss</span>
        <span class="s0">lg.val2 = ((1 - y_true) - y_true * lg.val2) / (1 + lg.val2)  # gradient</span>
    <span class="s0">return lg</span>


<span class="s0">cdef inline double_pair cgrad_hess_half_binomial(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0"># with y_pred = expit(raw)</span>
    <span class="s0"># hessian = y_pred * (1 - y_pred) = exp( raw) / (1 + exp( raw))**2</span>
    <span class="s0">#                                 = exp(-raw) / (1 + exp(-raw))**2</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0"># See comment in cgradient_half_binomial.</span>
    <span class="s0">if raw_prediction &gt; -37:</span>
        <span class="s0">gh.val2 = exp(-raw_prediction)  # used as temporary</span>
        <span class="s0">gh.val1 = ((1 - y_true) - y_true * gh.val2) / (1 + gh.val2)  # gradient</span>
        <span class="s0">gh.val2 = gh.val2 / (1 + gh.val2)**2                         # hessian</span>
    <span class="s0">else:</span>
        <span class="s0">gh.val2 = exp(raw_prediction)  # = 1. order Taylor in exp(raw_prediction)</span>
        <span class="s0">gh.val1 = gh.val2 - y_true</span>
    <span class="s0">return gh</span>


<span class="s0"># Exponential loss with (half) logit-link, aka boosting loss</span>
<span class="s0">cdef inline double closs_exponential(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double tmp = exp(raw_prediction)</span>
    <span class="s0">return y_true / tmp + (1 - y_true) * tmp</span>


<span class="s0">cdef inline double cgradient_exponential(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double tmp = exp(raw_prediction)</span>
    <span class="s0">return -y_true / tmp + (1 - y_true) * tmp</span>


<span class="s0">cdef inline double_pair closs_grad_exponential(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">cdef double_pair lg</span>
    <span class="s0">lg.val2 = exp(raw_prediction)  # used as temporary</span>

    <span class="s0">lg.val1 =  y_true / lg.val2 + (1 - y_true) * lg.val2  # loss</span>
    <span class="s0">lg.val2 = -y_true / lg.val2 + (1 - y_true) * lg.val2  # gradient</span>
    <span class="s0">return lg</span>


<span class="s0">cdef inline double_pair cgrad_hess_exponential(</span>
    <span class="s0">double y_true,</span>
    <span class="s0">double raw_prediction</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0"># Note that hessian = loss</span>
    <span class="s0">cdef double_pair gh</span>
    <span class="s0">gh.val2 = exp(raw_prediction)  # used as temporary</span>

    <span class="s0">gh.val1 = -y_true / gh.val2 + (1 - y_true) * gh.val2  # gradient</span>
    <span class="s0">gh.val2 =  y_true / gh.val2 + (1 - y_true) * gh.val2  # hessian</span>
    <span class="s0">return gh</span>


<span class="s0"># ---------------------------------------------------</span>
<span class="s0"># Extension Types for Loss Functions of 1-dim targets</span>
<span class="s0"># ---------------------------------------------------</span>
<span class="s0">cdef class CyLossFunction:</span>
    <span class="s0">&quot;&quot;&quot;Base class for convex loss functions.&quot;&quot;&quot;</span>

    <span class="s0">cdef double cy_loss(self, double y_true, double raw_prediction) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Compute the loss for a single sample.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : double</span>
            <span class="s0">Observed, true target value.</span>
        <span class="s0">raw_prediction : double</span>
            <span class="s0">Raw prediction value (in link space).</span>

        <span class="s0">Returns</span>
        <span class="s0">-------</span>
        <span class="s0">double</span>
            <span class="s0">The loss evaluated at `y_true` and `raw_prediction`.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">pass</span>

    <span class="s0">cdef double cy_gradient(self, double y_true, double raw_prediction) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient of loss w.r.t. raw_prediction for a single sample.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : double</span>
            <span class="s0">Observed, true target value.</span>
        <span class="s0">raw_prediction : double</span>
            <span class="s0">Raw prediction value (in link space).</span>

        <span class="s0">Returns</span>
        <span class="s0">-------</span>
        <span class="s0">double</span>
            <span class="s0">The derivative of the loss function w.r.t. `raw_prediction`.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">pass</span>

    <span class="s0">cdef double_pair cy_grad_hess(</span>
        <span class="s0">self, double y_true, double raw_prediction</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient and hessian.</span>

        <span class="s0">Gradient and hessian of loss w.r.t. raw_prediction for a single sample.</span>

        <span class="s0">This is usually diagonal in raw_prediction_i and raw_prediction_j.</span>
        <span class="s0">Therefore, we return the diagonal element i=j.</span>

        <span class="s0">For a loss with a non-canonical link, this might implement the diagonal</span>
        <span class="s0">of the Fisher matrix (=expected hessian) instead of the hessian.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : double</span>
            <span class="s0">Observed, true target value.</span>
        <span class="s0">raw_prediction : double</span>
            <span class="s0">Raw prediction value (in link space).</span>

        <span class="s0">Returns</span>
        <span class="s0">-------</span>
        <span class="s0">double_pair</span>
            <span class="s0">Gradient and hessian of the loss function w.r.t. `raw_prediction`.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">pass</span>

    <span class="s0">def loss(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] loss_out,             # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the point-wise loss value for each input.</span>

        <span class="s0">The point-wise loss is written to `loss_out` and no array is returned.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : array of shape (n_samples,)</span>
            <span class="s0">Observed, true target values.</span>
        <span class="s0">raw_prediction : array of shape (n_samples,)</span>
            <span class="s0">Raw prediction values (in link space).</span>
        <span class="s0">sample_weight : array of shape (n_samples,) or None</span>
            <span class="s0">Sample weights.</span>
        <span class="s0">loss_out : array of shape (n_samples,)</span>
            <span class="s0">A location into which the result is stored.</span>
        <span class="s0">n_threads : int</span>
            <span class="s0">Number of threads used by OpenMP (if any).</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">pass</span>

    <span class="s0">def gradient(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] gradient_out,         # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient of loss w.r.t raw_prediction for each input.</span>

        <span class="s0">The gradient is written to `gradient_out` and no array is returned.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : array of shape (n_samples,)</span>
            <span class="s0">Observed, true target values.</span>
        <span class="s0">raw_prediction : array of shape (n_samples,)</span>
            <span class="s0">Raw prediction values (in link space).</span>
        <span class="s0">sample_weight : array of shape (n_samples,) or None</span>
            <span class="s0">Sample weights.</span>
        <span class="s0">gradient_out : array of shape (n_samples,)</span>
            <span class="s0">A location into which the result is stored.</span>
        <span class="s0">n_threads : int</span>
            <span class="s0">Number of threads used by OpenMP (if any).</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">pass</span>

    <span class="s0">def loss_gradient(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] loss_out,             # OUT</span>
        <span class="s0">floating_out[::1] gradient_out,         # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">&quot;&quot;&quot;Compute loss and gradient of loss w.r.t raw_prediction.</span>

        <span class="s0">The loss and gradient are written to `loss_out` and `gradient_out` and no arrays</span>
        <span class="s0">are returned.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : array of shape (n_samples,)</span>
            <span class="s0">Observed, true target values.</span>
        <span class="s0">raw_prediction : array of shape (n_samples,)</span>
            <span class="s0">Raw prediction values (in link space).</span>
        <span class="s0">sample_weight : array of shape (n_samples,) or None</span>
            <span class="s0">Sample weights.</span>
        <span class="s0">loss_out : array of shape (n_samples,) or None</span>
            <span class="s0">A location into which the element-wise loss is stored.</span>
        <span class="s0">gradient_out : array of shape (n_samples,)</span>
            <span class="s0">A location into which the gradient is stored.</span>
        <span class="s0">n_threads : int</span>
            <span class="s0">Number of threads used by OpenMP (if any).</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">self.loss(y_true, raw_prediction, sample_weight, loss_out, n_threads)</span>
        <span class="s0">self.gradient(y_true, raw_prediction, sample_weight, gradient_out, n_threads)</span>

    <span class="s0">def gradient_hessian(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] gradient_out,         # OUT</span>
        <span class="s0">floating_out[::1] hessian_out,          # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient and hessian of loss w.r.t raw_prediction.</span>

        <span class="s0">The gradient and hessian are written to `gradient_out` and `hessian_out` and no</span>
        <span class="s0">arrays are returned.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">y_true : array of shape (n_samples,)</span>
            <span class="s0">Observed, true target values.</span>
        <span class="s0">raw_prediction : array of shape (n_samples,)</span>
            <span class="s0">Raw prediction values (in link space).</span>
        <span class="s0">sample_weight : array of shape (n_samples,) or None</span>
            <span class="s0">Sample weights.</span>
        <span class="s0">gradient_out : array of shape (n_samples,)</span>
            <span class="s0">A location into which the gradient is stored.</span>
        <span class="s0">hessian_out : array of shape (n_samples,)</span>
            <span class="s0">A location into which the hessian is stored.</span>
        <span class="s0">n_threads : int</span>
            <span class="s0">Number of threads used by OpenMP (if any).</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">pass</span>


<span class="s0">{{for name, docstring, param, closs, closs_grad, cgrad, cgrad_hess, in class_list}}</span>
<span class="s0">{{py:</span>
<span class="s0">if param is None:</span>
    <span class="s0">with_param = &quot;&quot;</span>
<span class="s0">else:</span>
    <span class="s0">with_param = &quot;, self.&quot; + param</span>
<span class="s0">}}</span>

<span class="s0">cdef class {{name}}(CyLossFunction):</span>
    <span class="s0">&quot;&quot;&quot;{{docstring}}&quot;&quot;&quot;</span>

    <span class="s0">{{if param is not None}}</span>
    <span class="s0">def __init__(self, {{param}}):</span>
        <span class="s0">self.{{param}} = {{param}}</span>
    <span class="s0">{{endif}}</span>

    <span class="s0">cdef inline double cy_loss(self, double y_true, double raw_prediction) noexcept nogil:</span>
        <span class="s0">return {{closs}}(y_true, raw_prediction{{with_param}})</span>

    <span class="s0">cdef inline double cy_gradient(self, double y_true, double raw_prediction) noexcept nogil:</span>
        <span class="s0">return {{cgrad}}(y_true, raw_prediction{{with_param}})</span>

    <span class="s0">cdef inline double_pair cy_grad_hess(self, double y_true, double raw_prediction) noexcept nogil:</span>
        <span class="s0">return {{cgrad_hess}}(y_true, raw_prediction{{with_param}})</span>

    <span class="s0">def loss(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] loss_out,             # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">loss_out[i] = {{closs}}(y_true[i], raw_prediction[i]{{with_param}})</span>
        <span class="s0">else:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">loss_out[i] = sample_weight[i] * {{closs}}(y_true[i], raw_prediction[i]{{with_param}})</span>

    <span class="s0">{{if closs_grad is not None}}</span>
    <span class="s0">def loss_gradient(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] loss_out,             # OUT</span>
        <span class="s0">floating_out[::1] gradient_out,         # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">double_pair dbl2</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">dbl2 = {{closs_grad}}(y_true[i], raw_prediction[i]{{with_param}})</span>
                <span class="s0">loss_out[i] = dbl2.val1</span>
                <span class="s0">gradient_out[i] = dbl2.val2</span>
        <span class="s0">else:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">dbl2 = {{closs_grad}}(y_true[i], raw_prediction[i]{{with_param}})</span>
                <span class="s0">loss_out[i] = sample_weight[i] * dbl2.val1</span>
                <span class="s0">gradient_out[i] = sample_weight[i] * dbl2.val2</span>

    <span class="s0">{{endif}}</span>

    <span class="s0">def gradient(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] gradient_out,         # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">gradient_out[i] = {{cgrad}}(y_true[i], raw_prediction[i]{{with_param}})</span>
        <span class="s0">else:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">gradient_out[i] = sample_weight[i] * {{cgrad}}(y_true[i], raw_prediction[i]{{with_param}})</span>

    <span class="s0">def gradient_hessian(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,          # IN</span>
        <span class="s0">const floating_in[::1] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,   # IN</span>
        <span class="s0">floating_out[::1] gradient_out,         # OUT</span>
        <span class="s0">floating_out[::1] hessian_out,          # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">double_pair dbl2</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">dbl2 = {{cgrad_hess}}(y_true[i], raw_prediction[i]{{with_param}})</span>
                <span class="s0">gradient_out[i] = dbl2.val1</span>
                <span class="s0">hessian_out[i] = dbl2.val2</span>
        <span class="s0">else:</span>
            <span class="s0">for i in prange(</span>
                <span class="s0">n_samples, schedule='static', nogil=True, num_threads=n_threads</span>
            <span class="s0">):</span>
                <span class="s0">dbl2 = {{cgrad_hess}}(y_true[i], raw_prediction[i]{{with_param}})</span>
                <span class="s0">gradient_out[i] = sample_weight[i] * dbl2.val1</span>
                <span class="s0">hessian_out[i] = sample_weight[i] * dbl2.val2</span>

<span class="s0">{{endfor}}</span>


<span class="s0"># The multinomial deviance loss is also known as categorical cross-entropy or</span>
<span class="s0"># multinomial log-likelihood</span>
<span class="s0">cdef class CyHalfMultinomialLoss(CyLossFunction):</span>
    <span class="s0">&quot;&quot;&quot;Half Multinomial deviance loss with multinomial logit link.</span>

    <span class="s0">Domain:</span>
    <span class="s0">y_true in {0, 1, 2, 3, .., n_classes - 1}</span>
    <span class="s0">y_pred in (0, 1)**n_classes, i.e. interval with boundaries excluded</span>

    <span class="s0">Link:</span>
    <span class="s0">y_pred = softmax(raw_prediction)</span>

    <span class="s0">Note: Label encoding is built-in, i.e. {0, 1, 2, 3, .., n_classes - 1} is</span>
    <span class="s0">mapped to (y_true == k) for k = 0 .. n_classes - 1 which is either 0 or 1.</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0"># Note that we do not assume memory alignment/contiguity of 2d arrays.</span>
    <span class="s0"># There seems to be little benefit in doing so. Benchmarks proofing the</span>
    <span class="s0"># opposite are welcome.</span>
    <span class="s0">def loss(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,           # IN</span>
        <span class="s0">const floating_in[:, :] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,    # IN</span>
        <span class="s0">floating_out[::1] loss_out,              # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i, k</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">int n_classes = raw_prediction.shape[1]</span>
            <span class="s0">floating_in max_value, sum_exps</span>
            <span class="s0">floating_in*  p  # temporary buffer</span>

        <span class="s0"># We assume n_samples &gt; n_classes. In this case having the inner loop</span>
        <span class="s0"># over n_classes is a good default.</span>
        <span class="s0"># TODO: If every memoryview is contiguous and raw_prediction is</span>
        <span class="s0">#       f-contiguous, can we write a better algo (loops) to improve</span>
        <span class="s0">#       performance?</span>
        <span class="s0">if sample_weight is None:</span>
            <span class="s0"># inner loop over n_classes</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0"># Define private buffer variables as each thread might use its</span>
                <span class="s0"># own.</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">max_value = p[n_classes]     # p[-2]</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>
                    <span class="s0">loss_out[i] = log(sum_exps) + max_value</span>

                    <span class="s0"># label encoded y_true</span>
                    <span class="s0">k = int(y_true[i])</span>
                    <span class="s0">loss_out[i] -= raw_prediction[i, k]</span>

                <span class="s0">free(p)</span>
        <span class="s0">else:</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">max_value = p[n_classes]     # p[-2]</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>
                    <span class="s0">loss_out[i] = log(sum_exps) + max_value</span>

                    <span class="s0"># label encoded y_true</span>
                    <span class="s0">k = int(y_true[i])</span>
                    <span class="s0">loss_out[i] -= raw_prediction[i, k]</span>

                    <span class="s0">loss_out[i] *= sample_weight[i]</span>

                <span class="s0">free(p)</span>

    <span class="s0">def loss_gradient(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,           # IN</span>
        <span class="s0">const floating_in[:, :] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,    # IN</span>
        <span class="s0">floating_out[::1] loss_out,              # OUT</span>
        <span class="s0">floating_out[:, :] gradient_out,         # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i, k</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">int n_classes = raw_prediction.shape[1]</span>
            <span class="s0">floating_in max_value, sum_exps</span>
            <span class="s0">floating_in*  p  # temporary buffer</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0"># inner loop over n_classes</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0"># Define private buffer variables as each thread might use its</span>
                <span class="s0"># own.</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">max_value = p[n_classes]  # p[-2]</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>
                    <span class="s0">loss_out[i] = log(sum_exps) + max_value</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0"># label decode y_true</span>
                        <span class="s0">if y_true[i] == k:</span>
                            <span class="s0">loss_out[i] -= raw_prediction[i, k]</span>
                        <span class="s0">p[k] /= sum_exps  # p_k = y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = p_k - (y_true == k)</span>
                        <span class="s0">gradient_out[i, k] = p[k] - (y_true[i] == k)</span>

                <span class="s0">free(p)</span>
        <span class="s0">else:</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">max_value = p[n_classes]  # p[-2]</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>
                    <span class="s0">loss_out[i] = log(sum_exps) + max_value</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0"># label decode y_true</span>
                        <span class="s0">if y_true[i] == k:</span>
                            <span class="s0">loss_out[i] -= raw_prediction[i, k]</span>
                        <span class="s0">p[k] /= sum_exps  # p_k = y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = (p_k - (y_true == k)) * sw</span>
                        <span class="s0">gradient_out[i, k] = (p[k] - (y_true[i] == k)) * sample_weight[i]</span>

                    <span class="s0">loss_out[i] *= sample_weight[i]</span>

                <span class="s0">free(p)</span>

    <span class="s0">def gradient(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,           # IN</span>
        <span class="s0">const floating_in[:, :] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,    # IN</span>
        <span class="s0">floating_out[:, :] gradient_out,         # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i, k</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">int n_classes = raw_prediction.shape[1]</span>
            <span class="s0">floating_in sum_exps</span>
            <span class="s0">floating_in*  p  # temporary buffer</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0"># inner loop over n_classes</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0"># Define private buffer variables as each thread might use its</span>
                <span class="s0"># own.</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0">p[k] /= sum_exps  # p_k = y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = y_pred_k - (y_true == k)</span>
                        <span class="s0">gradient_out[i, k] = p[k] - (y_true[i] == k)</span>

                <span class="s0">free(p)</span>
        <span class="s0">else:</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0">p[k] /= sum_exps  # p_k = y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = (p_k - (y_true == k)) * sw</span>
                        <span class="s0">gradient_out[i, k] = (p[k] - (y_true[i] == k)) * sample_weight[i]</span>

                <span class="s0">free(p)</span>

    <span class="s0">def gradient_hessian(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,           # IN</span>
        <span class="s0">const floating_in[:, :] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,    # IN</span>
        <span class="s0">floating_out[:, :] gradient_out,         # OUT</span>
        <span class="s0">floating_out[:, :] hessian_out,          # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i, k</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">int n_classes = raw_prediction.shape[1]</span>
            <span class="s0">floating_in sum_exps</span>
            <span class="s0">floating_in* p  # temporary buffer</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0"># inner loop over n_classes</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0"># Define private buffer variables as each thread might use its</span>
                <span class="s0"># own.</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0">p[k] /= sum_exps  # p_k = y_pred_k = prob of class k</span>
                        <span class="s0"># hessian_k = p_k * (1 - p_k)</span>
                        <span class="s0"># gradient_k = p_k - (y_true == k)</span>
                        <span class="s0">gradient_out[i, k] = p[k] - (y_true[i] == k)</span>
                        <span class="s0">hessian_out[i, k] = p[k] * (1. - p[k])</span>

                <span class="s0">free(p)</span>
        <span class="s0">else:</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0">p[k] /= sum_exps  # p_k = y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = (p_k - (y_true == k)) * sw</span>
                        <span class="s0"># hessian_k = p_k * (1 - p_k) * sw</span>
                        <span class="s0">gradient_out[i, k] = (p[k] - (y_true[i] == k)) * sample_weight[i]</span>
                        <span class="s0">hessian_out[i, k] = (p[k] * (1. - p[k])) * sample_weight[i]</span>

                <span class="s0">free(p)</span>

    <span class="s0"># This method simplifies the implementation of hessp in linear models,</span>
    <span class="s0"># i.e. the matrix-vector product of the full hessian, not only of the</span>
    <span class="s0"># diagonal (in the classes) approximation as implemented above.</span>
    <span class="s0">def gradient_proba(</span>
        <span class="s0">self,</span>
        <span class="s0">const floating_in[::1] y_true,           # IN</span>
        <span class="s0">const floating_in[:, :] raw_prediction,  # IN</span>
        <span class="s0">const floating_in[::1] sample_weight,    # IN</span>
        <span class="s0">floating_out[:, :] gradient_out,         # OUT</span>
        <span class="s0">floating_out[:, :] proba_out,            # OUT</span>
        <span class="s0">int n_threads=1</span>
    <span class="s0">):</span>
        <span class="s0">cdef:</span>
            <span class="s0">int i, k</span>
            <span class="s0">int n_samples = y_true.shape[0]</span>
            <span class="s0">int n_classes = raw_prediction.shape[1]</span>
            <span class="s0">floating_in sum_exps</span>
            <span class="s0">floating_in*  p  # temporary buffer</span>

        <span class="s0">if sample_weight is None:</span>
            <span class="s0"># inner loop over n_classes</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0"># Define private buffer variables as each thread might use its</span>
                <span class="s0"># own.</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0">proba_out[i, k] = p[k] / sum_exps  # y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = y_pred_k - (y_true == k)</span>
                        <span class="s0">gradient_out[i, k] = proba_out[i, k] - (y_true[i] == k)</span>

                <span class="s0">free(p)</span>
        <span class="s0">else:</span>
            <span class="s0">with nogil, parallel(num_threads=n_threads):</span>
                <span class="s0">p = &lt;floating_in *&gt; malloc(sizeof(floating_in) * (n_classes + 2))</span>

                <span class="s0">for i in prange(n_samples, schedule='static'):</span>
                    <span class="s0">sum_exp_minus_max(i, raw_prediction, p)</span>
                    <span class="s0">sum_exps = p[n_classes + 1]  # p[-1]</span>

                    <span class="s0">for k in range(n_classes):</span>
                        <span class="s0">proba_out[i, k] = p[k] / sum_exps  # y_pred_k = prob of class k</span>
                        <span class="s0"># gradient_k = (p_k - (y_true == k)) * sw</span>
                        <span class="s0">gradient_out[i, k] = (proba_out[i, k] - (y_true[i] == k)) * sample_weight[i]</span>

                <span class="s0">free(p)</span>
</pre>
</body>
</html>