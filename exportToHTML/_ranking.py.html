<html>
<head>
<title>_ranking.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #6aab73;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_ranking.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Metrics to assess performance on classification task given scores. 
 
Functions named as ``*_score`` return a scalar value to maximize: the higher 
the better. 
 
Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize: 
the lower the better. 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s2">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s2">#          Arnaud Joly &lt;a.joly@ulg.ac.be&gt;</span>
<span class="s2">#          Jochen Wersdorfer &lt;jochen@wersdoerfer.de&gt;</span>
<span class="s2">#          Lars Buitinck</span>
<span class="s2">#          Joel Nothman &lt;joel.nothman@gmail.com&gt;</span>
<span class="s2">#          Noel Dawe &lt;noel@dawe.me&gt;</span>
<span class="s2">#          Michal Karbownik &lt;michakarbownik@gmail.com&gt;</span>
<span class="s2"># License: BSD 3 clause</span>


<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">sparse </span><span class="s3">import </span><span class="s1">csr_matrix</span><span class="s4">, </span><span class="s1">issparse</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">stats </span><span class="s3">import </span><span class="s1">rankdata</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">exceptions </span><span class="s3">import </span><span class="s1">UndefinedMetricWarning</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">preprocessing </span><span class="s3">import </span><span class="s1">label_binarize</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">assert_all_finite</span><span class="s4">,</span>
    <span class="s1">check_array</span><span class="s4">,</span>
    <span class="s1">check_consistent_length</span><span class="s4">,</span>
    <span class="s1">column_or_1d</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_encode </span><span class="s3">import </span><span class="s1">_encode</span><span class="s4">, </span><span class="s1">_unique</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s1">Hidden</span><span class="s4">, </span><span class="s1">Interval</span><span class="s4">, </span><span class="s1">StrOptions</span><span class="s4">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">extmath </span><span class="s3">import </span><span class="s1">stable_cumsum</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">fixes </span><span class="s3">import </span><span class="s1">trapezoid</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">multiclass </span><span class="s3">import </span><span class="s1">type_of_target</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">sparsefuncs </span><span class="s3">import </span><span class="s1">count_nonzero</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s1">_check_pos_label_consistency</span><span class="s4">, </span><span class="s1">_check_sample_weight</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_base </span><span class="s3">import </span><span class="s1">_average_binary_score</span><span class="s4">, </span><span class="s1">_average_multiclass_ovo_score</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span><span class="s5">&quot;x&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">], </span><span class="s5">&quot;y&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">]},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">auc</span><span class="s4">(</span><span class="s1">x</span><span class="s4">, </span><span class="s1">y</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Area Under the Curve (AUC) using the trapezoidal rule. 
 
    This is a general function, given points on a curve.  For computing the 
    area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative 
    way to summarize a precision-recall curve, see 
    :func:`average_precision_score`. 
 
    Parameters 
    ---------- 
    x : array-like of shape (n,) 
        X coordinates. These must be either monotonic increasing or monotonic 
        decreasing. 
    y : array-like of shape (n,) 
        Y coordinates. 
 
    Returns 
    ------- 
    auc : float 
        Area Under the Curve. 
 
    See Also 
    -------- 
    roc_auc_score : Compute the area under the ROC curve. 
    average_precision_score : Compute average precision from prediction scores. 
    precision_recall_curve : Compute precision-recall pairs for different 
        probability thresholds. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn import metrics 
    &gt;&gt;&gt; y = np.array([1, 1, 2, 2]) 
    &gt;&gt;&gt; pred = np.array([0.1, 0.4, 0.35, 0.8]) 
    &gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2) 
    &gt;&gt;&gt; metrics.auc(fpr, tpr) 
    np.float64(0.75) 
    &quot;&quot;&quot;</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">x</span><span class="s4">, </span><span class="s1">y</span><span class="s4">)</span>
    <span class="s1">x </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">x</span><span class="s4">)</span>
    <span class="s1">y </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">x</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] &lt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;At least 2 points are needed to compute area under curve, but x.shape = %s&quot;</span>
            <span class="s4">% </span><span class="s1">x</span><span class="s4">.</span><span class="s1">shape</span>
        <span class="s4">)</span>

    <span class="s1">direction </span><span class="s4">= </span><span class="s6">1</span>
    <span class="s1">dx </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">x</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">dx </span><span class="s4">&lt; </span><span class="s6">0</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">dx </span><span class="s4">&lt;= </span><span class="s6">0</span><span class="s4">):</span>
            <span class="s1">direction </span><span class="s4">= -</span><span class="s6">1</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;x is neither increasing nor decreasing : {}.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">x</span><span class="s4">))</span>

    <span class="s1">area </span><span class="s4">= </span><span class="s1">direction </span><span class="s4">* </span><span class="s1">trapezoid</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">x</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">area</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">memmap</span><span class="s4">):</span>
        <span class="s2"># Reductions such as .sum used internally in trapezoid do not return a</span>
        <span class="s2"># scalar by default for numpy.memmap instances contrary to</span>
        <span class="s2"># regular numpy.ndarray instances.</span>
        <span class="s1">area </span><span class="s4">= </span><span class="s1">area</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">.</span><span class="s1">type</span><span class="s4">(</span><span class="s1">area</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">area</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">}), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">average_precision_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute average precision (AP) from prediction scores. 
 
    AP summarizes a precision-recall curve as the weighted mean of precisions 
    achieved at each threshold, with the increase in recall from the previous 
    threshold used as the weight: 
 
    .. math:: 
        \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n 
 
    where :math:`P_n` and :math:`R_n` are the precision and recall at the nth 
    threshold [1]_. This implementation is not interpolated and is different 
    from computing the area under the precision-recall curve with the 
    trapezoidal rule, which uses linear interpolation and can be too 
    optimistic. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) or (n_samples, n_classes) 
        True binary labels or binary label indicators. 
 
    y_score : array-like of shape (n_samples,) or (n_samples, n_classes) 
        Target scores, can either be probability estimates of the positive 
        class, confidence values, or non-thresholded measure of decisions 
        (as returned by :term:`decision_function` on some classifiers). 
 
    average : {'micro', 'samples', 'weighted', 'macro'} or None, \ 
            default='macro' 
        If ``None``, the scores for each class are returned. Otherwise, 
        this determines the type of averaging performed on the data: 
 
        ``'micro'``: 
            Calculate metrics globally by considering each element of the label 
            indicator matrix as a label. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average, weighted 
            by support (the number of true instances for each label). 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average. 
 
        Will be ignored when ``y_true`` is binary. 
 
    pos_label : int, float, bool or str, default=1 
        The label of the positive class. Only applied to binary ``y_true``. 
        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    average_precision : float 
        Average precision score. 
 
    See Also 
    -------- 
    roc_auc_score : Compute the area under the ROC curve. 
    precision_recall_curve : Compute precision-recall pairs for different 
        probability thresholds. 
 
    Notes 
    ----- 
    .. versionchanged:: 0.19 
      Instead of linearly interpolating between operating points, precisions 
      are weighted by the change in recall since the last operating point. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Average precision 
           &lt;https://en.wikipedia.org/w/index.php?title=Information_retrieval&amp; 
           oldid=793358396#Average_precision&gt;`_ 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import average_precision_score 
    &gt;&gt;&gt; y_true = np.array([0, 0, 1, 1]) 
    &gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8]) 
    &gt;&gt;&gt; average_precision_score(y_true, y_scores) 
    np.float64(0.83...) 
    &gt;&gt;&gt; y_true = np.array([0, 0, 1, 1, 2, 2]) 
    &gt;&gt;&gt; y_scores = np.array([ 
    ...     [0.7, 0.2, 0.1], 
    ...     [0.4, 0.3, 0.3], 
    ...     [0.1, 0.8, 0.1], 
    ...     [0.2, 0.3, 0.5], 
    ...     [0.4, 0.4, 0.2], 
    ...     [0.1, 0.2, 0.7], 
    ... ]) 
    &gt;&gt;&gt; average_precision_score(y_true, y_scores) 
    np.float64(0.77...) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">_binary_uninterpolated_average_precision</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span>
    <span class="s4">):</span>
        <span class="s1">precision</span><span class="s4">, </span><span class="s1">recall</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">precision_recall_curve</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
        <span class="s4">)</span>
        <span class="s2"># Return the step function integral</span>
        <span class="s2"># The following works because the last entry of precision is</span>
        <span class="s2"># guaranteed to be 1, as returned by precision_recall_curve</span>
        <span class="s3">return </span><span class="s4">-</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">recall</span><span class="s4">) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">precision</span><span class="s4">)[:-</span><span class="s6">1</span><span class="s4">])</span>

    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>

    <span class="s2"># Convert to Python primitive type to avoid NumPy type / Python str</span>
    <span class="s2"># comparison. See https://github.com/numpy/numpy/issues/6784</span>
    <span class="s1">present_labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">).</span><span class="s1">tolist</span><span class="s4">()</span>

    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">present_labels</span><span class="s4">) == </span><span class="s6">2 </span><span class="s3">and </span><span class="s1">pos_label </span><span class="s3">not in </span><span class="s1">present_labels</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">f&quot;pos_label=</span><span class="s3">{</span><span class="s1">pos_label</span><span class="s3">} </span><span class="s5">is not a valid label. It should be &quot;</span>
                <span class="s5">f&quot;one of </span><span class="s3">{</span><span class="s1">present_labels</span><span class="s3">}</span><span class="s5">&quot;</span>
            <span class="s4">)</span>

    <span class="s3">elif </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multilabel-indicator&quot; </span><span class="s3">and </span><span class="s1">pos_label </span><span class="s4">!= </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Parameter pos_label is fixed to 1 for multilabel-indicator y_true. &quot;</span>
            <span class="s5">&quot;Do not set pos_label or set pos_label to 1.&quot;</span>
        <span class="s4">)</span>

    <span class="s3">elif </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">pos_label </span><span class="s4">!= </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Parameter pos_label is fixed to 1 for multiclass y_true. &quot;</span>
                <span class="s5">&quot;Do not set pos_label or set pos_label to 1.&quot;</span>
            <span class="s4">)</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">label_binarize</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s1">present_labels</span><span class="s4">)</span>

    <span class="s1">average_precision </span><span class="s4">= </span><span class="s1">partial</span><span class="s4">(</span>
        <span class="s1">_binary_uninterpolated_average_precision</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span>
    <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">_average_binary_score</span><span class="s4">(</span>
        <span class="s1">average_precision</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
    <span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">det_curve</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute error rates for different probability thresholds. 
 
    .. note:: 
       This metric is used for evaluation of ranking and error tradeoffs of 
       a binary classification task. 
 
    Read more in the :ref:`User Guide &lt;det_curve&gt;`. 
 
    .. versionadded:: 0.24 
 
    Parameters 
    ---------- 
    y_true : ndarray of shape (n_samples,) 
        True binary labels. If labels are not either {-1, 1} or {0, 1}, then 
        pos_label should be explicitly given. 
 
    y_score : ndarray of shape of (n_samples,) 
        Target scores, can either be probability estimates of the positive 
        class, confidence values, or non-thresholded measure of decisions 
        (as returned by &quot;decision_function&quot; on some classifiers). 
 
    pos_label : int, float, bool or str, default=None 
        The label of the positive class. 
        When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1}, 
        ``pos_label`` is set to 1, otherwise an error will be raised. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    fpr : ndarray of shape (n_thresholds,) 
        False positive rate (FPR) such that element i is the false positive 
        rate of predictions with score &gt;= thresholds[i]. This is occasionally 
        referred to as false acceptance probability or fall-out. 
 
    fnr : ndarray of shape (n_thresholds,) 
        False negative rate (FNR) such that element i is the false negative 
        rate of predictions with score &gt;= thresholds[i]. This is occasionally 
        referred to as false rejection or miss rate. 
 
    thresholds : ndarray of shape (n_thresholds,) 
        Decreasing score values. 
 
    See Also 
    -------- 
    DetCurveDisplay.from_estimator : Plot DET curve given an estimator and 
        some data. 
    DetCurveDisplay.from_predictions : Plot DET curve given the true and 
        predicted labels. 
    DetCurveDisplay : DET curve visualization. 
    roc_curve : Compute Receiver operating characteristic (ROC) curve. 
    precision_recall_curve : Compute precision-recall curve. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import det_curve 
    &gt;&gt;&gt; y_true = np.array([0, 0, 1, 1]) 
    &gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8]) 
    &gt;&gt;&gt; fpr, fnr, thresholds = det_curve(y_true, y_scores) 
    &gt;&gt;&gt; fpr 
    array([0.5, 0.5, 0. ]) 
    &gt;&gt;&gt; fnr 
    array([0. , 0.5, 0.5]) 
    &gt;&gt;&gt; thresholds 
    array([0.35, 0.4 , 0.8 ]) 
    &quot;&quot;&quot;</span>
    <span class="s1">fps</span><span class="s4">, </span><span class="s1">tps</span><span class="s4">, </span><span class="s1">thresholds </span><span class="s4">= </span><span class="s1">_binary_clf_curve</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
    <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)) != </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Only one class present in y_true. Detection error &quot;</span>
            <span class="s5">&quot;tradeoff curve is not defined in that case.&quot;</span>
        <span class="s4">)</span>

    <span class="s1">fns </span><span class="s4">= </span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">] - </span><span class="s1">tps</span>
    <span class="s1">p_count </span><span class="s4">= </span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]</span>
    <span class="s1">n_count </span><span class="s4">= </span><span class="s1">fps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]</span>

    <span class="s2"># start with false positives zero</span>
    <span class="s1">first_ind </span><span class="s4">= (</span>
        <span class="s1">fps</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">fps</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">side</span><span class="s4">=</span><span class="s5">&quot;right&quot;</span><span class="s4">) - </span><span class="s6">1</span>
        <span class="s3">if </span><span class="s1">fps</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">fps</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">side</span><span class="s4">=</span><span class="s5">&quot;right&quot;</span><span class="s4">) &gt; </span><span class="s6">0</span>
        <span class="s3">else None</span>
    <span class="s4">)</span>
    <span class="s2"># stop with false negatives zero</span>
    <span class="s1">last_ind </span><span class="s4">= </span><span class="s1">tps</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]) + </span><span class="s6">1</span>
    <span class="s1">sl </span><span class="s4">= </span><span class="s1">slice</span><span class="s4">(</span><span class="s1">first_ind</span><span class="s4">, </span><span class="s1">last_ind</span><span class="s4">)</span>

    <span class="s2"># reverse the output such that list of false positives is decreasing</span>
    <span class="s3">return </span><span class="s4">(</span><span class="s1">fps</span><span class="s4">[</span><span class="s1">sl</span><span class="s4">][::-</span><span class="s6">1</span><span class="s4">] / </span><span class="s1">n_count</span><span class="s4">, </span><span class="s1">fns</span><span class="s4">[</span><span class="s1">sl</span><span class="s4">][::-</span><span class="s6">1</span><span class="s4">] / </span><span class="s1">p_count</span><span class="s4">, </span><span class="s1">thresholds</span><span class="s4">[</span><span class="s1">sl</span><span class="s4">][::-</span><span class="s6">1</span><span class="s4">])</span>


<span class="s3">def </span><span class="s1">_binary_roc_auc_score</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">max_fpr</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Binary roc auc score.&quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)) != </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Only one class present in y_true. ROC AUC score &quot;</span>
            <span class="s5">&quot;is not defined in that case.&quot;</span>
        <span class="s4">)</span>

    <span class="s1">fpr</span><span class="s4">, </span><span class="s1">tpr</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">roc_curve</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">max_fpr </span><span class="s3">is None or </span><span class="s1">max_fpr </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">auc</span><span class="s4">(</span><span class="s1">fpr</span><span class="s4">, </span><span class="s1">tpr</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">max_fpr </span><span class="s4">&lt;= </span><span class="s6">0 </span><span class="s3">or </span><span class="s1">max_fpr </span><span class="s4">&gt; </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Expected max_fpr in range (0, 1], got: %r&quot; </span><span class="s4">% </span><span class="s1">max_fpr</span><span class="s4">)</span>

    <span class="s2"># Add a single point at max_fpr by linear interpolation</span>
    <span class="s1">stop </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">fpr</span><span class="s4">, </span><span class="s1">max_fpr</span><span class="s4">, </span><span class="s5">&quot;right&quot;</span><span class="s4">)</span>
    <span class="s1">x_interp </span><span class="s4">= [</span><span class="s1">fpr</span><span class="s4">[</span><span class="s1">stop </span><span class="s4">- </span><span class="s6">1</span><span class="s4">], </span><span class="s1">fpr</span><span class="s4">[</span><span class="s1">stop</span><span class="s4">]]</span>
    <span class="s1">y_interp </span><span class="s4">= [</span><span class="s1">tpr</span><span class="s4">[</span><span class="s1">stop </span><span class="s4">- </span><span class="s6">1</span><span class="s4">], </span><span class="s1">tpr</span><span class="s4">[</span><span class="s1">stop</span><span class="s4">]]</span>
    <span class="s1">tpr </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">tpr</span><span class="s4">[:</span><span class="s1">stop</span><span class="s4">], </span><span class="s1">np</span><span class="s4">.</span><span class="s1">interp</span><span class="s4">(</span><span class="s1">max_fpr</span><span class="s4">, </span><span class="s1">x_interp</span><span class="s4">, </span><span class="s1">y_interp</span><span class="s4">))</span>
    <span class="s1">fpr </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">fpr</span><span class="s4">[:</span><span class="s1">stop</span><span class="s4">], </span><span class="s1">max_fpr</span><span class="s4">)</span>
    <span class="s1">partial_auc </span><span class="s4">= </span><span class="s1">auc</span><span class="s4">(</span><span class="s1">fpr</span><span class="s4">, </span><span class="s1">tpr</span><span class="s4">)</span>

    <span class="s2"># McClish correction: standardize result to be 0.5 if non-discriminant</span>
    <span class="s2"># and 1 if maximal</span>
    <span class="s1">min_area </span><span class="s4">= </span><span class="s6">0.5 </span><span class="s4">* </span><span class="s1">max_fpr</span><span class="s4">**</span><span class="s6">2</span>
    <span class="s1">max_area </span><span class="s4">= </span><span class="s1">max_fpr</span>
    <span class="s3">return </span><span class="s6">0.5 </span><span class="s4">* (</span><span class="s6">1 </span><span class="s4">+ (</span><span class="s1">partial_auc </span><span class="s4">- </span><span class="s1">min_area</span><span class="s4">) / (</span><span class="s1">max_area </span><span class="s4">- </span><span class="s1">min_area</span><span class="s4">))</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">}), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;max_fpr&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;right&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;multi_class&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;raise&quot;</span><span class="s4">, </span><span class="s5">&quot;ovr&quot;</span><span class="s4">, </span><span class="s5">&quot;ovo&quot;</span><span class="s4">})],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">roc_auc_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_score</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;macro&quot;</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">max_fpr</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">multi_class</span><span class="s4">=</span><span class="s5">&quot;raise&quot;</span><span class="s4">,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \ 
    from prediction scores. 
 
    Note: this implementation can be used with binary, multiclass and 
    multilabel classification, but some restrictions apply (see Parameters). 
 
    Read more in the :ref:`User Guide &lt;roc_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) or (n_samples, n_classes) 
        True labels or binary label indicators. The binary and multiclass cases 
        expect labels with shape (n_samples,) while the multilabel case expects 
        binary label indicators with shape (n_samples, n_classes). 
 
    y_score : array-like of shape (n_samples,) or (n_samples, n_classes) 
        Target scores. 
 
        * In the binary case, it corresponds to an array of shape 
          `(n_samples,)`. Both probability estimates and non-thresholded 
          decision values can be provided. The probability estimates correspond 
          to the **probability of the class with the greater label**, 
          i.e. `estimator.classes_[1]` and thus 
          `estimator.predict_proba(X, y)[:, 1]`. The decision values 
          corresponds to the output of `estimator.decision_function(X, y)`. 
          See more information in the :ref:`User guide &lt;roc_auc_binary&gt;`; 
        * In the multiclass case, it corresponds to an array of shape 
          `(n_samples, n_classes)` of probability estimates provided by the 
          `predict_proba` method. The probability estimates **must** 
          sum to 1 across the possible classes. In addition, the order of the 
          class scores must correspond to the order of ``labels``, 
          if provided, or else to the numerical or lexicographical order of 
          the labels in ``y_true``. See more information in the 
          :ref:`User guide &lt;roc_auc_multiclass&gt;`; 
        * In the multilabel case, it corresponds to an array of shape 
          `(n_samples, n_classes)`. Probability estimates are provided by the 
          `predict_proba` method and the non-thresholded decision values by 
          the `decision_function` method. The probability estimates correspond 
          to the **probability of the class with the greater label for each 
          output** of the classifier. See more information in the 
          :ref:`User guide &lt;roc_auc_multilabel&gt;`. 
 
    average : {'micro', 'macro', 'samples', 'weighted'} or None, \ 
            default='macro' 
        If ``None``, the scores for each class are returned. 
        Otherwise, this determines the type of averaging performed on the data. 
        Note: multiclass ROC AUC currently only handles the 'macro' and 
        'weighted' averages. For multiclass targets, `average=None` is only 
        implemented for `multi_class='ovr'` and `average='micro'` is only 
        implemented for `multi_class='ovr'`. 
 
        ``'micro'``: 
            Calculate metrics globally by considering each element of the label 
            indicator matrix as a label. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average, weighted 
            by support (the number of true instances for each label). 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average. 
 
        Will be ignored when ``y_true`` is binary. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    max_fpr : float &gt; 0 and &lt;= 1, default=None 
        If not ``None``, the standardized partial AUC [2]_ over the range 
        [0, max_fpr] is returned. For the multiclass case, ``max_fpr``, 
        should be either equal to ``None`` or ``1.0`` as AUC ROC partial 
        computation currently is not supported for multiclass. 
 
    multi_class : {'raise', 'ovr', 'ovo'}, default='raise' 
        Only used for multiclass targets. Determines the type of configuration 
        to use. The default value raises an error, so either 
        ``'ovr'`` or ``'ovo'`` must be passed explicitly. 
 
        ``'ovr'``: 
            Stands for One-vs-rest. Computes the AUC of each class 
            against the rest [3]_ [4]_. This 
            treats the multiclass case in the same way as the multilabel case. 
            Sensitive to class imbalance even when ``average == 'macro'``, 
            because class imbalance affects the composition of each of the 
            'rest' groupings. 
        ``'ovo'``: 
            Stands for One-vs-one. Computes the average AUC of all 
            possible pairwise combinations of classes [5]_. 
            Insensitive to class imbalance when 
            ``average == 'macro'``. 
 
    labels : array-like of shape (n_classes,), default=None 
        Only used for multiclass targets. List of labels that index the 
        classes in ``y_score``. If ``None``, the numerical or lexicographical 
        order of the labels in ``y_true`` is used. 
 
    Returns 
    ------- 
    auc : float 
        Area Under the Curve score. 
 
    See Also 
    -------- 
    average_precision_score : Area under the precision-recall curve. 
    roc_curve : Compute Receiver operating characteristic (ROC) curve. 
    RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic 
        (ROC) curve given an estimator and some data. 
    RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic 
        (ROC) curve given the true and predicted values. 
 
    Notes 
    ----- 
    The Gini Coefficient is a summary measure of the ranking ability of binary 
    classifiers. It is expressed using the area under of the ROC as follows: 
 
    G = 2 * AUC - 1 
 
    Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation 
    will ensure that random guessing will yield a score of 0 in expectation, and it is 
    upper bounded by 1. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Receiver operating characteristic 
            &lt;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&gt;`_ 
 
    .. [2] `Analyzing a portion of the ROC curve. McClish, 1989 
            &lt;https://www.ncbi.nlm.nih.gov/pubmed/2668680&gt;`_ 
 
    .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving 
           probability estimation trees (Section 6.2), CeDER Working Paper 
           #IS-00-04, Stern School of Business, New York University. 
 
    .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern 
            Recognition Letters, 27(8), 861-874. 
            &lt;https://www.sciencedirect.com/science/article/pii/S016786550500303X&gt;`_ 
 
    .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area 
            Under the ROC Curve for Multiple Class Classification Problems. 
            Machine Learning, 45(2), 171-186. 
            &lt;http://link.springer.com/article/10.1023/A:1010920819831&gt;`_ 
    .. [6] `Wikipedia entry for the Gini coefficient 
            &lt;https://en.wikipedia.org/wiki/Gini_coefficient&gt;`_ 
 
    Examples 
    -------- 
    Binary case: 
 
    &gt;&gt;&gt; from sklearn.datasets import load_breast_cancer 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
    &gt;&gt;&gt; from sklearn.metrics import roc_auc_score 
    &gt;&gt;&gt; X, y = load_breast_cancer(return_X_y=True) 
    &gt;&gt;&gt; clf = LogisticRegression(solver=&quot;liblinear&quot;, random_state=0).fit(X, y) 
    &gt;&gt;&gt; roc_auc_score(y, clf.predict_proba(X)[:, 1]) 
    np.float64(0.99...) 
    &gt;&gt;&gt; roc_auc_score(y, clf.decision_function(X)) 
    np.float64(0.99...) 
 
    Multiclass case: 
 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; clf = LogisticRegression(solver=&quot;liblinear&quot;).fit(X, y) 
    &gt;&gt;&gt; roc_auc_score(y, clf.predict_proba(X), multi_class='ovr') 
    np.float64(0.99...) 
 
    Multilabel case: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.datasets import make_multilabel_classification 
    &gt;&gt;&gt; from sklearn.multioutput import MultiOutputClassifier 
    &gt;&gt;&gt; X, y = make_multilabel_classification(random_state=0) 
    &gt;&gt;&gt; clf = MultiOutputClassifier(clf).fit(X, y) 
    &gt;&gt;&gt; # get a list of n_output containing probability arrays of shape 
    &gt;&gt;&gt; # (n_samples, n_classes) 
    &gt;&gt;&gt; y_pred = clf.predict_proba(X) 
    &gt;&gt;&gt; # extract the positive columns for each output 
    &gt;&gt;&gt; y_pred = np.transpose([pred[:, 1] for pred in y_pred]) 
    &gt;&gt;&gt; roc_auc_score(y, y_pred, average=None) 
    array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...]) 
    &gt;&gt;&gt; from sklearn.linear_model import RidgeClassifierCV 
    &gt;&gt;&gt; clf = RidgeClassifierCV().fit(X, y) 
    &gt;&gt;&gt; roc_auc_score(y, clf.decision_function(X), average=None) 
    array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s3">None</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multiclass&quot; </span><span class="s3">or </span><span class="s4">(</span>
        <span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot; </span><span class="s3">and </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">2 </span><span class="s3">and </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] &gt; </span><span class="s6">2</span>
    <span class="s4">):</span>
        <span class="s2"># do not support partial ROC computation for multiclass</span>
        <span class="s3">if </span><span class="s1">max_fpr </span><span class="s3">is not None and </span><span class="s1">max_fpr </span><span class="s4">!= </span><span class="s6">1.0</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Partial AUC computation not available in &quot;</span>
                <span class="s5">&quot;multiclass setting, 'max_fpr' must be&quot;</span>
                <span class="s5">&quot; set to `None`, received `max_fpr={0}` &quot;</span>
                <span class="s5">&quot;instead&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">max_fpr</span><span class="s4">)</span>
            <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">multi_class </span><span class="s4">== </span><span class="s5">&quot;raise&quot;</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;multi_class must be in ('ovo', 'ovr')&quot;</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">_multiclass_roc_auc_score</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">, </span><span class="s1">multi_class</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">sample_weight</span>
        <span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">label_binarize</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">)[:, </span><span class="s6">0</span><span class="s4">]</span>
        <span class="s3">return </span><span class="s1">_average_binary_score</span><span class="s4">(</span>
            <span class="s1">partial</span><span class="s4">(</span><span class="s1">_binary_roc_auc_score</span><span class="s4">, </span><span class="s1">max_fpr</span><span class="s4">=</span><span class="s1">max_fpr</span><span class="s4">),</span>
            <span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">y_score</span><span class="s4">,</span>
            <span class="s1">average</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:  </span><span class="s2"># multilabel-indicator</span>
        <span class="s3">return </span><span class="s1">_average_binary_score</span><span class="s4">(</span>
            <span class="s1">partial</span><span class="s4">(</span><span class="s1">_binary_roc_auc_score</span><span class="s4">, </span><span class="s1">max_fpr</span><span class="s4">=</span><span class="s1">max_fpr</span><span class="s4">),</span>
            <span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">y_score</span><span class="s4">,</span>
            <span class="s1">average</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s4">)</span>


<span class="s3">def </span><span class="s1">_multiclass_roc_auc_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">, </span><span class="s1">multi_class</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">sample_weight</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Multiclass roc auc score. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True multiclass labels. 
 
    y_score : array-like of shape (n_samples, n_classes) 
        Target scores corresponding to probability estimates of a sample 
        belonging to a particular class 
 
    labels : array-like of shape (n_classes,) or None 
        List of labels to index ``y_score`` used for multiclass. If ``None``, 
        the lexical order of ``y_true`` is used to index ``y_score``. 
 
    multi_class : {'ovr', 'ovo'} 
        Determines the type of multiclass configuration to use. 
        ``'ovr'``: 
            Calculate metrics for the multiclass case using the one-vs-rest 
            approach. 
        ``'ovo'``: 
            Calculate metrics for the multiclass case using the one-vs-one 
            approach. 
 
    average : {'micro', 'macro', 'weighted'} 
        Determines the type of averaging performed on the pairwise binary 
        metric scores 
        ``'micro'``: 
            Calculate metrics for the binarized-raveled classes. Only supported 
            for `multi_class='ovr'`. 
 
        .. versionadded:: 1.2 
 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean. This does not take label imbalance into account. Classes 
            are assumed to be uniformly distributed. 
        ``'weighted'``: 
            Calculate metrics for each label, taking into account the 
            prevalence of the classes. 
 
    sample_weight : array-like of shape (n_samples,) or None 
        Sample weights. 
 
    &quot;&quot;&quot;</span>
    <span class="s2"># validation of the input y_score</span>
    <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">allclose</span><span class="s4">(</span><span class="s6">1</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Target scores need to be probabilities for multiclass &quot;</span>
            <span class="s5">&quot;roc_auc, i.e. they should sum up to 1.0 over classes&quot;</span>
        <span class="s4">)</span>

    <span class="s2"># validation for multiclass parameter specifications</span>
    <span class="s1">average_options </span><span class="s4">= (</span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">multi_class </span><span class="s4">== </span><span class="s5">&quot;ovr&quot;</span><span class="s4">:</span>
        <span class="s1">average_options </span><span class="s4">= (</span><span class="s5">&quot;micro&quot;</span><span class="s4">,) + </span><span class="s1">average_options</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s3">not in </span><span class="s1">average_options</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;average must be one of {0} for multiclass problems&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">average_options</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s1">multiclass_options </span><span class="s4">= (</span><span class="s5">&quot;ovo&quot;</span><span class="s4">, </span><span class="s5">&quot;ovr&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">multi_class </span><span class="s3">not in </span><span class="s1">multiclass_options</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;multi_class='{0}' is not supported &quot;</span>
            <span class="s5">&quot;for multiclass ROC AUC, multi_class must be &quot;</span>
            <span class="s5">&quot;in {1}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">multi_class</span><span class="s4">, </span><span class="s1">multiclass_options</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">average </span><span class="s3">is None and </span><span class="s1">multi_class </span><span class="s4">== </span><span class="s5">&quot;ovo&quot;</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span><span class="s4">(</span>
            <span class="s5">&quot;average=None is not implemented for multi_class='ovo'.&quot;</span>
        <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">labels </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">classes </span><span class="s4">= </span><span class="s1">_unique</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">) != </span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Parameter 'labels' must be unique&quot;</span><span class="s4">)</span>
        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array_equal</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Parameter 'labels' must be ordered&quot;</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">) != </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Number of given labels, {0}, not equal to the number &quot;</span>
                <span class="s5">&quot;of columns in 'y_score', {1}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">), </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">])</span>
            <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">setdiff1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">)):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;'y_true' contains labels not in parameter 'labels'&quot;</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">classes </span><span class="s4">= </span><span class="s1">_unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">) != </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Number of classes in y_true not equal to the number of &quot;</span>
                <span class="s5">&quot;columns in 'y_score'&quot;</span>
            <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">multi_class </span><span class="s4">== </span><span class="s5">&quot;ovo&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;sample_weight is not supported &quot;</span>
                <span class="s5">&quot;for multiclass one-vs-one ROC AUC, &quot;</span>
                <span class="s5">&quot;'sample_weight' must be None in this case.&quot;</span>
            <span class="s4">)</span>
        <span class="s1">y_true_encoded </span><span class="s4">= </span><span class="s1">_encode</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">uniques</span><span class="s4">=</span><span class="s1">classes</span><span class="s4">)</span>
        <span class="s2"># Hand &amp; Till (2001) implementation (ovo)</span>
        <span class="s3">return </span><span class="s1">_average_multiclass_ovo_score</span><span class="s4">(</span>
            <span class="s1">_binary_roc_auc_score</span><span class="s4">, </span><span class="s1">y_true_encoded</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">average</span><span class="s4">=</span><span class="s1">average</span>
        <span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s2"># ovr is same as multi-label</span>
        <span class="s1">y_true_multilabel </span><span class="s4">= </span><span class="s1">label_binarize</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s1">classes</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">_average_binary_score</span><span class="s4">(</span>
            <span class="s1">_binary_roc_auc_score</span><span class="s4">,</span>
            <span class="s1">y_true_multilabel</span><span class="s4">,</span>
            <span class="s1">y_score</span><span class="s4">,</span>
            <span class="s1">average</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s4">)</span>


<span class="s3">def </span><span class="s1">_binary_clf_curve</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Calculate true and false positives per binary classification threshold. 
 
    Parameters 
    ---------- 
    y_true : ndarray of shape (n_samples,) 
        True targets of binary classification. 
 
    y_score : ndarray of shape (n_samples,) 
        Estimated probabilities or output of a decision function. 
 
    pos_label : int, float, bool or str, default=None 
        The label of the positive class. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    fps : ndarray of shape (n_thresholds,) 
        A count of false positives, at index i being the number of negative 
        samples assigned a score &gt;= thresholds[i]. The total number of 
        negative samples is equal to fps[-1] (thus true negatives are given by 
        fps[-1] - fps). 
 
    tps : ndarray of shape (n_thresholds,) 
        An increasing count of true positives, at index i being the number 
        of positive samples assigned a score &gt;= thresholds[i]. The total 
        number of positive samples is equal to tps[-1] (thus false negatives 
        are given by tps[-1] - tps). 
 
    thresholds : ndarray of shape (n_thresholds,) 
        Decreasing score values. 
    &quot;&quot;&quot;</span>
    <span class="s2"># Check to make sure y_true is valid</span>
    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s3">if not </span><span class="s4">(</span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot; </span><span class="s3">or </span><span class="s4">(</span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multiclass&quot; </span><span class="s3">and </span><span class="s1">pos_label </span><span class="s3">is not None</span><span class="s4">)):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;{0} format is not supported&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">))</span>

    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">)</span>
    <span class="s1">assert_all_finite</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">assert_all_finite</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">)</span>

    <span class="s2"># Filter out zero-weighted samples, as they should not impact the result</span>
    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s1">nonzero_weight_mask </span><span class="s4">= </span><span class="s1">sample_weight </span><span class="s4">!= </span><span class="s6">0</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">[</span><span class="s1">nonzero_weight_mask</span><span class="s4">]</span>
        <span class="s1">y_score </span><span class="s4">= </span><span class="s1">y_score</span><span class="s4">[</span><span class="s1">nonzero_weight_mask</span><span class="s4">]</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">sample_weight</span><span class="s4">[</span><span class="s1">nonzero_weight_mask</span><span class="s4">]</span>

    <span class="s1">pos_label </span><span class="s4">= </span><span class="s1">_check_pos_label_consistency</span><span class="s4">(</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s2"># make y_true a boolean vector</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">y_true </span><span class="s4">== </span><span class="s1">pos_label</span>

    <span class="s2"># sort scores and corresponding truth values</span>
    <span class="s1">desc_score_indices </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">argsort</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">kind</span><span class="s4">=</span><span class="s5">&quot;mergesort&quot;</span><span class="s4">)[::-</span><span class="s6">1</span><span class="s4">]</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">y_score</span><span class="s4">[</span><span class="s1">desc_score_indices</span><span class="s4">]</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">[</span><span class="s1">desc_score_indices</span><span class="s4">]</span>
    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">weight </span><span class="s4">= </span><span class="s1">sample_weight</span><span class="s4">[</span><span class="s1">desc_score_indices</span><span class="s4">]</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">weight </span><span class="s4">= </span><span class="s6">1.0</span>

    <span class="s2"># y_score typically has many tied values. Here we extract</span>
    <span class="s2"># the indices associated with the distinct values. We also</span>
    <span class="s2"># concatenate a value for the end of the curve.</span>
    <span class="s1">distinct_value_indices </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">where</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">))[</span><span class="s6">0</span><span class="s4">]</span>
    <span class="s1">threshold_idxs </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">r_</span><span class="s4">[</span><span class="s1">distinct_value_indices</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">size </span><span class="s4">- </span><span class="s6">1</span><span class="s4">]</span>

    <span class="s2"># accumulate the true positives with decreasing threshold</span>
    <span class="s1">tps </span><span class="s4">= </span><span class="s1">stable_cumsum</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">* </span><span class="s1">weight</span><span class="s4">)[</span><span class="s1">threshold_idxs</span><span class="s4">]</span>
    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s2"># express fps as a cumsum to ensure fps is increasing even in</span>
        <span class="s2"># the presence of floating point errors</span>
        <span class="s1">fps </span><span class="s4">= </span><span class="s1">stable_cumsum</span><span class="s4">((</span><span class="s6">1 </span><span class="s4">- </span><span class="s1">y_true</span><span class="s4">) * </span><span class="s1">weight</span><span class="s4">)[</span><span class="s1">threshold_idxs</span><span class="s4">]</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">fps </span><span class="s4">= </span><span class="s6">1 </span><span class="s4">+ </span><span class="s1">threshold_idxs </span><span class="s4">- </span><span class="s1">tps</span>
    <span class="s3">return </span><span class="s1">fps</span><span class="s4">, </span><span class="s1">tps</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">[</span><span class="s1">threshold_idxs</span><span class="s4">]</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s1">Hidden</span><span class="s4">(</span><span class="s3">None</span><span class="s4">)],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;drop_intermediate&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;probas_pred&quot;</span><span class="s4">: [</span>
            <span class="s5">&quot;array-like&quot;</span><span class="s4">,</span>
            <span class="s1">Hidden</span><span class="s4">(</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;deprecated&quot;</span><span class="s4">})),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">precision_recall_curve</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_score</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">drop_intermediate</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
    <span class="s1">probas_pred</span><span class="s4">=</span><span class="s5">&quot;deprecated&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute precision-recall pairs for different probability thresholds. 
 
    Note: this implementation is restricted to the binary classification task. 
 
    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of 
    true positives and ``fp`` the number of false positives. The precision is 
    intuitively the ability of the classifier not to label as positive a sample 
    that is negative. 
 
    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of 
    true positives and ``fn`` the number of false negatives. The recall is 
    intuitively the ability of the classifier to find all the positive samples. 
 
    The last precision and recall values are 1. and 0. respectively and do not 
    have a corresponding threshold. This ensures that the graph starts on the 
    y axis. 
 
    The first precision and recall values are precision=class balance and recall=1.0 
    which corresponds to a classifier that always predicts the positive class. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True binary labels. If labels are not either {-1, 1} or {0, 1}, then 
        pos_label should be explicitly given. 
 
    y_score : array-like of shape (n_samples,) 
        Target scores, can either be probability estimates of the positive 
        class, or non-thresholded measure of decisions (as returned by 
        `decision_function` on some classifiers). 
 
    pos_label : int, float, bool or str, default=None 
        The label of the positive class. 
        When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1}, 
        ``pos_label`` is set to 1, otherwise an error will be raised. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    drop_intermediate : bool, default=False 
        Whether to drop some suboptimal thresholds which would not appear 
        on a plotted precision-recall curve. This is useful in order to create 
        lighter precision-recall curves. 
 
        .. versionadded:: 1.3 
 
    probas_pred : array-like of shape (n_samples,) 
        Target scores, can either be probability estimates of the positive 
        class, or non-thresholded measure of decisions (as returned by 
        `decision_function` on some classifiers). 
 
        .. deprecated:: 1.5 
            `probas_pred` is deprecated and will be removed in 1.7. Use 
            `y_score` instead. 
 
    Returns 
    ------- 
    precision : ndarray of shape (n_thresholds + 1,) 
        Precision values such that element i is the precision of 
        predictions with score &gt;= thresholds[i] and the last element is 1. 
 
    recall : ndarray of shape (n_thresholds + 1,) 
        Decreasing recall values such that element i is the recall of 
        predictions with score &gt;= thresholds[i] and the last element is 0. 
 
    thresholds : ndarray of shape (n_thresholds,) 
        Increasing thresholds on the decision function used to compute 
        precision and recall where `n_thresholds = len(np.unique(probas_pred))`. 
 
    See Also 
    -------- 
    PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given 
        a binary classifier. 
    PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve 
        using predictions from a binary classifier. 
    average_precision_score : Compute average precision from prediction scores. 
    det_curve: Compute error rates for different probability thresholds. 
    roc_curve : Compute Receiver operating characteristic (ROC) curve. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import precision_recall_curve 
    &gt;&gt;&gt; y_true = np.array([0, 0, 1, 1]) 
    &gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8]) 
    &gt;&gt;&gt; precision, recall, thresholds = precision_recall_curve( 
    ...     y_true, y_scores) 
    &gt;&gt;&gt; precision 
    array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ]) 
    &gt;&gt;&gt; recall 
    array([1. , 1. , 0.5, 0.5, 0. ]) 
    &gt;&gt;&gt; thresholds 
    array([0.1 , 0.35, 0.4 , 0.8 ]) 
    &quot;&quot;&quot;</span>
    <span class="s2"># TODO(1.7): remove in 1.7 and reset y_score to be required</span>
    <span class="s2"># Note: validate params will raise an error if probas_pred is not array-like,</span>
    <span class="s2"># or &quot;deprecated&quot;</span>
    <span class="s3">if </span><span class="s1">y_score </span><span class="s3">is not None and not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">probas_pred</span><span class="s4">, </span><span class="s1">str</span><span class="s4">):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;`probas_pred` and `y_score` cannot be both specified. Please use `y_score`&quot;</span>
            <span class="s5">&quot; only as `probas_pred` is deprecated in v1.5 and will be removed in v1.7.&quot;</span>
        <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_score </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s4">(</span>
                <span class="s5">&quot;probas_pred was deprecated in version 1.5 and will be removed in 1.7.&quot;</span>
                <span class="s5">&quot;Please use ``y_score`` instead.&quot;</span>
            <span class="s4">),</span>
            <span class="s1">FutureWarning</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">y_score </span><span class="s4">= </span><span class="s1">probas_pred</span>

    <span class="s1">fps</span><span class="s4">, </span><span class="s1">tps</span><span class="s4">, </span><span class="s1">thresholds </span><span class="s4">= </span><span class="s1">_binary_clf_curve</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
    <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">drop_intermediate </span><span class="s3">and </span><span class="s1">len</span><span class="s4">(</span><span class="s1">fps</span><span class="s4">) &gt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s2"># Drop thresholds corresponding to points where true positives (tps)</span>
        <span class="s2"># do not change from the previous or subsequent point. This will keep</span>
        <span class="s2"># only the first and last point for each tps value. All points</span>
        <span class="s2"># with the same tps value have the same recall and thus x coordinate.</span>
        <span class="s2"># They appear as a vertical line on the plot.</span>
        <span class="s1">optimal_idxs </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">where</span><span class="s4">(</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">concatenate</span><span class="s4">(</span>
                <span class="s4">[[</span><span class="s3">True</span><span class="s4">], </span><span class="s1">np</span><span class="s4">.</span><span class="s1">logical_or</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">[:-</span><span class="s6">1</span><span class="s4">]), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">[</span><span class="s6">1</span><span class="s4">:])), [</span><span class="s3">True</span><span class="s4">]]</span>
            <span class="s4">)</span>
        <span class="s4">)[</span><span class="s6">0</span><span class="s4">]</span>
        <span class="s1">fps </span><span class="s4">= </span><span class="s1">fps</span><span class="s4">[</span><span class="s1">optimal_idxs</span><span class="s4">]</span>
        <span class="s1">tps </span><span class="s4">= </span><span class="s1">tps</span><span class="s4">[</span><span class="s1">optimal_idxs</span><span class="s4">]</span>
        <span class="s1">thresholds </span><span class="s4">= </span><span class="s1">thresholds</span><span class="s4">[</span><span class="s1">optimal_idxs</span><span class="s4">]</span>

    <span class="s1">ps </span><span class="s4">= </span><span class="s1">tps </span><span class="s4">+ </span><span class="s1">fps</span>
    <span class="s2"># Initialize the result array with zeros to make sure that precision[ps == 0]</span>
    <span class="s2"># does not contain uninitialized values.</span>
    <span class="s1">precision </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">)</span>
    <span class="s1">np</span><span class="s4">.</span><span class="s1">divide</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">, </span><span class="s1">ps</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">precision</span><span class="s4">, </span><span class="s1">where</span><span class="s4">=(</span><span class="s1">ps </span><span class="s4">!= </span><span class="s6">0</span><span class="s4">))</span>

    <span class="s2"># When no positive label in y_true, recall is set to 1 for all thresholds</span>
    <span class="s2"># tps[-1] == 0 &lt;=&gt; y_true == all negative labels</span>
    <span class="s3">if </span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">] == </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s5">&quot;No positive class found in y_true, &quot;</span>
            <span class="s5">&quot;recall is set to one for all thresholds.&quot;</span>
        <span class="s4">)</span>
        <span class="s1">recall </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones_like</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">recall </span><span class="s4">= </span><span class="s1">tps </span><span class="s4">/ </span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]</span>

    <span class="s2"># reverse the outputs so recall is decreasing</span>
    <span class="s1">sl </span><span class="s4">= </span><span class="s1">slice</span><span class="s4">(</span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, -</span><span class="s6">1</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">hstack</span><span class="s4">((</span><span class="s1">precision</span><span class="s4">[</span><span class="s1">sl</span><span class="s4">], </span><span class="s6">1</span><span class="s4">)), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">hstack</span><span class="s4">((</span><span class="s1">recall</span><span class="s4">[</span><span class="s1">sl</span><span class="s4">], </span><span class="s6">0</span><span class="s4">)), </span><span class="s1">thresholds</span><span class="s4">[</span><span class="s1">sl</span><span class="s4">]</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;drop_intermediate&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">roc_curve</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">drop_intermediate</span><span class="s4">=</span><span class="s3">True</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Receiver operating characteristic (ROC). 
 
    Note: this implementation is restricted to the binary classification task. 
 
    Read more in the :ref:`User Guide &lt;roc_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True binary labels. If labels are not either {-1, 1} or {0, 1}, then 
        pos_label should be explicitly given. 
 
    y_score : array-like of shape (n_samples,) 
        Target scores, can either be probability estimates of the positive 
        class, confidence values, or non-thresholded measure of decisions 
        (as returned by &quot;decision_function&quot; on some classifiers). 
 
    pos_label : int, float, bool or str, default=None 
        The label of the positive class. 
        When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1}, 
        ``pos_label`` is set to 1, otherwise an error will be raised. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    drop_intermediate : bool, default=True 
        Whether to drop some suboptimal thresholds which would not appear 
        on a plotted ROC curve. This is useful in order to create lighter 
        ROC curves. 
 
        .. versionadded:: 0.17 
           parameter *drop_intermediate*. 
 
    Returns 
    ------- 
    fpr : ndarray of shape (&gt;2,) 
        Increasing false positive rates such that element i is the false 
        positive rate of predictions with score &gt;= `thresholds[i]`. 
 
    tpr : ndarray of shape (&gt;2,) 
        Increasing true positive rates such that element `i` is the true 
        positive rate of predictions with score &gt;= `thresholds[i]`. 
 
    thresholds : ndarray of shape (n_thresholds,) 
        Decreasing thresholds on the decision function used to compute 
        fpr and tpr. `thresholds[0]` represents no instances being predicted 
        and is arbitrarily set to `np.inf`. 
 
    See Also 
    -------- 
    RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic 
        (ROC) curve given an estimator and some data. 
    RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic 
        (ROC) curve given the true and predicted values. 
    det_curve: Compute error rates for different probability thresholds. 
    roc_auc_score : Compute the area under the ROC curve. 
 
    Notes 
    ----- 
    Since the thresholds are sorted from low to high values, they 
    are reversed upon returning them to ensure they correspond to both ``fpr`` 
    and ``tpr``, which are sorted in reversed order during their calculation. 
 
    An arbitrary threshold is added for the case `tpr=0` and `fpr=0` to 
    ensure that the curve starts at `(0, 0)`. This threshold corresponds to the 
    `np.inf`. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Receiver operating characteristic 
            &lt;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&gt;`_ 
 
    .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition 
           Letters, 2006, 27(8):861-874. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn import metrics 
    &gt;&gt;&gt; y = np.array([1, 1, 2, 2]) 
    &gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8]) 
    &gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2) 
    &gt;&gt;&gt; fpr 
    array([0. , 0. , 0.5, 0.5, 1. ]) 
    &gt;&gt;&gt; tpr 
    array([0. , 0.5, 0.5, 1. , 1. ]) 
    &gt;&gt;&gt; thresholds 
    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ]) 
    &quot;&quot;&quot;</span>
    <span class="s1">fps</span><span class="s4">, </span><span class="s1">tps</span><span class="s4">, </span><span class="s1">thresholds </span><span class="s4">= </span><span class="s1">_binary_clf_curve</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
    <span class="s4">)</span>

    <span class="s2"># Attempt to drop thresholds corresponding to points in between and</span>
    <span class="s2"># collinear with other points. These are always suboptimal and do not</span>
    <span class="s2"># appear on a plotted ROC curve (and thus do not affect the AUC).</span>
    <span class="s2"># Here np.diff(_, 2) is used as a &quot;second derivative&quot; to tell if there</span>
    <span class="s2"># is a corner at the point. Both fps and tps must be tested to handle</span>
    <span class="s2"># thresholds with multiple data points (which are combined in</span>
    <span class="s2"># _binary_clf_curve). This keeps all cases where the point should be kept,</span>
    <span class="s2"># but does not drop more complicated cases like fps = [1, 3, 7],</span>
    <span class="s2"># tps = [1, 2, 4]; there is no harm in keeping too many thresholds.</span>
    <span class="s3">if </span><span class="s1">drop_intermediate </span><span class="s3">and </span><span class="s1">len</span><span class="s4">(</span><span class="s1">fps</span><span class="s4">) &gt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s1">optimal_idxs </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">where</span><span class="s4">(</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">r_</span><span class="s4">[</span><span class="s3">True</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">logical_or</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">fps</span><span class="s4">, </span><span class="s6">2</span><span class="s4">), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">tps</span><span class="s4">, </span><span class="s6">2</span><span class="s4">)), </span><span class="s3">True</span><span class="s4">]</span>
        <span class="s4">)[</span><span class="s6">0</span><span class="s4">]</span>
        <span class="s1">fps </span><span class="s4">= </span><span class="s1">fps</span><span class="s4">[</span><span class="s1">optimal_idxs</span><span class="s4">]</span>
        <span class="s1">tps </span><span class="s4">= </span><span class="s1">tps</span><span class="s4">[</span><span class="s1">optimal_idxs</span><span class="s4">]</span>
        <span class="s1">thresholds </span><span class="s4">= </span><span class="s1">thresholds</span><span class="s4">[</span><span class="s1">optimal_idxs</span><span class="s4">]</span>

    <span class="s2"># Add an extra threshold position</span>
    <span class="s2"># to make sure that the curve starts at (0, 0)</span>
    <span class="s1">tps </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">r_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">, </span><span class="s1">tps</span><span class="s4">]</span>
    <span class="s1">fps </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">r_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">, </span><span class="s1">fps</span><span class="s4">]</span>
    <span class="s2"># get dtype of `y_score` even if it is an array-like</span>
    <span class="s1">thresholds </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">r_</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s1">thresholds</span><span class="s4">]</span>

    <span class="s3">if </span><span class="s1">fps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">] &lt;= </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s5">&quot;No negative samples in y_true, false positive value should be meaningless&quot;</span><span class="s4">,</span>
            <span class="s1">UndefinedMetricWarning</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">fpr </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">repeat</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span><span class="s4">, </span><span class="s1">fps</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">fpr </span><span class="s4">= </span><span class="s1">fps </span><span class="s4">/ </span><span class="s1">fps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]</span>

    <span class="s3">if </span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">] &lt;= </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s5">&quot;No positive samples in y_true, true positive value should be meaningless&quot;</span><span class="s4">,</span>
            <span class="s1">UndefinedMetricWarning</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">tpr </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">repeat</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span><span class="s4">, </span><span class="s1">tps</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">tpr </span><span class="s4">= </span><span class="s1">tps </span><span class="s4">/ </span><span class="s1">tps</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]</span>

    <span class="s3">return </span><span class="s1">fpr</span><span class="s4">, </span><span class="s1">tpr</span><span class="s4">, </span><span class="s1">thresholds</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">label_ranking_average_precision_score</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute ranking-based average precision. 
 
    Label ranking average precision (LRAP) is the average over each ground 
    truth label assigned to each sample, of the ratio of true vs. total 
    labels with lower score. 
 
    This metric is used in multilabel ranking problem, where the goal 
    is to give better rank to the labels associated to each sample. 
 
    The obtained score is always strictly greater than 0 and 
    the best value is 1. 
 
    Read more in the :ref:`User Guide &lt;label_ranking_average_precision&gt;`. 
 
    Parameters 
    ---------- 
    y_true : {array-like, sparse matrix} of shape (n_samples, n_labels) 
        True binary labels in binary indicator format. 
 
    y_score : array-like of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates of the positive 
        class, confidence values, or non-thresholded measure of decisions 
        (as returned by &quot;decision_function&quot; on some classifiers). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
        .. versionadded:: 0.20 
 
    Returns 
    ------- 
    score : float 
        Ranking-based average precision score. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import label_ranking_average_precision_score 
    &gt;&gt;&gt; y_true = np.array([[1, 0, 0], [0, 0, 1]]) 
    &gt;&gt;&gt; y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]]) 
    &gt;&gt;&gt; label_ranking_average_precision_score(y_true, y_score) 
    np.float64(0.416...) 
    &quot;&quot;&quot;</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s5">&quot;csr&quot;</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape </span><span class="s4">!= </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;y_true and y_score have different shape&quot;</span><span class="s4">)</span>

    <span class="s2"># Handle badly formatted array and the degenerate case with one label</span>
    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">!= </span><span class="s5">&quot;multilabel-indicator&quot; </span><span class="s3">and not </span><span class="s4">(</span>
        <span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot; </span><span class="s3">and </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">2</span>
    <span class="s4">):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;{0} format is not supported&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">))</span>

    <span class="s3">if not </span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">):</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">csr_matrix</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s1">y_score </span><span class="s4">= -</span><span class="s1">y_score</span>

    <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_labels </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span>

    <span class="s1">out </span><span class="s4">= </span><span class="s6">0.0</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s4">, (</span><span class="s1">start</span><span class="s4">, </span><span class="s1">stop</span><span class="s4">) </span><span class="s3">in </span><span class="s1">enumerate</span><span class="s4">(</span><span class="s1">zip</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">indptr</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">indptr</span><span class="s4">[</span><span class="s6">1</span><span class="s4">:])):</span>
        <span class="s1">relevant </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">indices</span><span class="s4">[</span><span class="s1">start</span><span class="s4">:</span><span class="s1">stop</span><span class="s4">]</span>

        <span class="s3">if </span><span class="s1">relevant</span><span class="s4">.</span><span class="s1">size </span><span class="s4">== </span><span class="s6">0 </span><span class="s3">or </span><span class="s1">relevant</span><span class="s4">.</span><span class="s1">size </span><span class="s4">== </span><span class="s1">n_labels</span><span class="s4">:</span>
            <span class="s2"># If all labels are relevant or unrelevant, the score is also</span>
            <span class="s2"># equal to 1. The label ranking has no meaning.</span>
            <span class="s1">aux </span><span class="s4">= </span><span class="s6">1.0</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">scores_i </span><span class="s4">= </span><span class="s1">y_score</span><span class="s4">[</span><span class="s1">i</span><span class="s4">]</span>
            <span class="s1">rank </span><span class="s4">= </span><span class="s1">rankdata</span><span class="s4">(</span><span class="s1">scores_i</span><span class="s4">, </span><span class="s5">&quot;max&quot;</span><span class="s4">)[</span><span class="s1">relevant</span><span class="s4">]</span>
            <span class="s1">L </span><span class="s4">= </span><span class="s1">rankdata</span><span class="s4">(</span><span class="s1">scores_i</span><span class="s4">[</span><span class="s1">relevant</span><span class="s4">], </span><span class="s5">&quot;max&quot;</span><span class="s4">)</span>
            <span class="s1">aux </span><span class="s4">= (</span><span class="s1">L </span><span class="s4">/ </span><span class="s1">rank</span><span class="s4">).</span><span class="s1">mean</span><span class="s4">()</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">aux </span><span class="s4">= </span><span class="s1">aux </span><span class="s4">* </span><span class="s1">sample_weight</span><span class="s4">[</span><span class="s1">i</span><span class="s4">]</span>
        <span class="s1">out </span><span class="s4">+= </span><span class="s1">aux</span>

    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">out </span><span class="s4">/= </span><span class="s1">n_samples</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">out </span><span class="s4">/= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">return </span><span class="s1">out</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">coverage_error</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Coverage error measure. 
 
    Compute how far we need to go through the ranked scores to cover all 
    true labels. The best value is equal to the average number 
    of labels in ``y_true`` per sample. 
 
    Ties in ``y_scores`` are broken by giving maximal rank that would have 
    been assigned to all tied values. 
 
    Note: Our implementation's score is 1 greater than the one given in 
    Tsoumakas et al., 2010. This extends it to handle the degenerate case 
    in which an instance has 0 true labels. 
 
    Read more in the :ref:`User Guide &lt;coverage_error&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples, n_labels) 
        True binary labels in binary indicator format. 
 
    y_score : array-like of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates of the positive 
        class, confidence values, or non-thresholded measure of decisions 
        (as returned by &quot;decision_function&quot; on some classifiers). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    coverage_error : float 
        The coverage error. 
 
    References 
    ---------- 
    .. [1] Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010). 
           Mining multi-label data. In Data mining and knowledge discovery 
           handbook (pp. 667-685). Springer US. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import coverage_error 
    &gt;&gt;&gt; y_true = [[1, 0, 0], [0, 1, 1]] 
    &gt;&gt;&gt; y_score = [[1, 0, 0], [0, 1, 1]] 
    &gt;&gt;&gt; coverage_error(y_true, y_score) 
    np.float64(1.5) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">!= </span><span class="s5">&quot;multilabel-indicator&quot;</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;{0} format is not supported&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">))</span>

    <span class="s3">if </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape </span><span class="s4">!= </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;y_true and y_score have different shape&quot;</span><span class="s4">)</span>

    <span class="s1">y_score_mask </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ma</span><span class="s4">.</span><span class="s1">masked_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">mask</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">logical_not</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">))</span>
    <span class="s1">y_min_relevant </span><span class="s4">= </span><span class="s1">y_score_mask</span><span class="s4">.</span><span class="s1">min</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">).</span><span class="s1">reshape</span><span class="s4">((-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">))</span>
    <span class="s1">coverage </span><span class="s4">= (</span><span class="s1">y_score </span><span class="s4">&gt;= </span><span class="s1">y_min_relevant</span><span class="s4">).</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
    <span class="s1">coverage </span><span class="s4">= </span><span class="s1">coverage</span><span class="s4">.</span><span class="s1">filled</span><span class="s4">(</span><span class="s6">0</span><span class="s4">)</span>

    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">coverage</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">label_ranking_loss</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Ranking loss measure. 
 
    Compute the average number of label pairs that are incorrectly ordered 
    given y_score weighted by the size of the label set and the number of 
    labels not in the label set. 
 
    This is similar to the error set size, but weighted by the number of 
    relevant and irrelevant labels. The best performance is achieved with 
    a ranking loss of zero. 
 
    Read more in the :ref:`User Guide &lt;label_ranking_loss&gt;`. 
 
    .. versionadded:: 0.17 
       A function *label_ranking_loss* 
 
    Parameters 
    ---------- 
    y_true : {array-like, sparse matrix} of shape (n_samples, n_labels) 
        True binary labels in binary indicator format. 
 
    y_score : array-like of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates of the positive 
        class, confidence values, or non-thresholded measure of decisions 
        (as returned by &quot;decision_function&quot; on some classifiers). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    loss : float 
        Average number of label pairs that are incorrectly ordered given 
        y_score weighted by the size of the label set and the number of labels not 
        in the label set. 
 
    References 
    ---------- 
    .. [1] Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010). 
           Mining multi-label data. In Data mining and knowledge discovery 
           handbook (pp. 667-685). Springer US. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import label_ranking_loss 
    &gt;&gt;&gt; y_true = [[1, 0, 0], [0, 0, 1]] 
    &gt;&gt;&gt; y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]] 
    &gt;&gt;&gt; label_ranking_loss(y_true, y_score) 
    np.float64(0.75...) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s5">&quot;csr&quot;</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s4">(</span><span class="s5">&quot;multilabel-indicator&quot;</span><span class="s4">,):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;{0} format is not supported&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">))</span>

    <span class="s3">if </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape </span><span class="s4">!= </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;y_true and y_score have different shape&quot;</span><span class="s4">)</span>

    <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_labels </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span>

    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">csr_matrix</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s1">loss </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">)</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s4">, (</span><span class="s1">start</span><span class="s4">, </span><span class="s1">stop</span><span class="s4">) </span><span class="s3">in </span><span class="s1">enumerate</span><span class="s4">(</span><span class="s1">zip</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">indptr</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">indptr</span><span class="s4">[</span><span class="s6">1</span><span class="s4">:])):</span>
        <span class="s2"># Sort and bin the label scores</span>
        <span class="s1">unique_scores</span><span class="s4">, </span><span class="s1">unique_inverse </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">[</span><span class="s1">i</span><span class="s4">], </span><span class="s1">return_inverse</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
        <span class="s1">true_at_reversed_rank </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span>
            <span class="s1">unique_inverse</span><span class="s4">[</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">indices</span><span class="s4">[</span><span class="s1">start</span><span class="s4">:</span><span class="s1">stop</span><span class="s4">]], </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">unique_scores</span><span class="s4">)</span>
        <span class="s4">)</span>
        <span class="s1">all_at_reversed_rank </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">unique_inverse</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">unique_scores</span><span class="s4">))</span>
        <span class="s1">false_at_reversed_rank </span><span class="s4">= </span><span class="s1">all_at_reversed_rank </span><span class="s4">- </span><span class="s1">true_at_reversed_rank</span>

        <span class="s2"># if the scores are ordered, it's possible to count the number of</span>
        <span class="s2"># incorrectly ordered paires in linear time by cumulatively counting</span>
        <span class="s2"># how many false labels of a given score have a score higher than the</span>
        <span class="s2"># accumulated true labels with lower score.</span>
        <span class="s1">loss</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">true_at_reversed_rank</span><span class="s4">.</span><span class="s1">cumsum</span><span class="s4">(), </span><span class="s1">false_at_reversed_rank</span><span class="s4">)</span>

    <span class="s1">n_positives </span><span class="s4">= </span><span class="s1">count_nonzero</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
    <span class="s3">with </span><span class="s1">np</span><span class="s4">.</span><span class="s1">errstate</span><span class="s4">(</span><span class="s1">divide</span><span class="s4">=</span><span class="s5">&quot;ignore&quot;</span><span class="s4">, </span><span class="s1">invalid</span><span class="s4">=</span><span class="s5">&quot;ignore&quot;</span><span class="s4">):</span>
        <span class="s1">loss </span><span class="s4">/= (</span><span class="s1">n_labels </span><span class="s4">- </span><span class="s1">n_positives</span><span class="s4">) * </span><span class="s1">n_positives</span>

    <span class="s2"># When there is no positive or no negative labels, those values should</span>
    <span class="s2"># be consider as correct, i.e. the ranking doesn't matter.</span>
    <span class="s1">loss</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">logical_or</span><span class="s4">(</span><span class="s1">n_positives </span><span class="s4">== </span><span class="s6">0</span><span class="s4">, </span><span class="s1">n_positives </span><span class="s4">== </span><span class="s1">n_labels</span><span class="s4">)] = </span><span class="s6">0.0</span>

    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">loss</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>


<span class="s3">def </span><span class="s1">_dcg_sample_scores</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">k</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">log_base</span><span class="s4">=</span><span class="s6">2</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Discounted Cumulative Gain. 
 
    Sum the true scores ranked in the order induced by the predicted scores, 
    after applying a logarithmic discount. 
 
    This ranking metric yields a high value if true labels are ranked high by 
    ``y_score``. 
 
    Parameters 
    ---------- 
    y_true : ndarray of shape (n_samples, n_labels) 
        True targets of multilabel classification, or true scores of entities 
        to be ranked. 
 
    y_score : ndarray of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates, confidence values, 
        or non-thresholded measure of decisions (as returned by 
        &quot;decision_function&quot; on some classifiers). 
 
    k : int, default=None 
        Only consider the highest k scores in the ranking. If `None`, use all 
        outputs. 
 
    log_base : float, default=2 
        Base of the logarithm used for the discount. A low value means a 
        sharper discount (top results are more important). 
 
    ignore_ties : bool, default=False 
        Assume that there are no ties in y_score (which is likely to be the 
        case if y_score is continuous) for efficiency gains. 
 
    Returns 
    ------- 
    discounted_cumulative_gain : ndarray of shape (n_samples,) 
        The DCG score for each sample. 
 
    See Also 
    -------- 
    ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted 
        Cumulative Gain (the DCG obtained for a perfect ranking), in order to 
        have a score between 0 and 1. 
    &quot;&quot;&quot;</span>
    <span class="s1">discount </span><span class="s4">= </span><span class="s6">1 </span><span class="s4">/ (</span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">arange</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]) + </span><span class="s6">2</span><span class="s4">) / </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">log_base</span><span class="s4">))</span>
    <span class="s3">if </span><span class="s1">k </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">discount</span><span class="s4">[</span><span class="s1">k</span><span class="s4">:] = </span><span class="s6">0</span>
    <span class="s3">if </span><span class="s1">ignore_ties</span><span class="s4">:</span>
        <span class="s1">ranking </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">argsort</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">)[:, ::-</span><span class="s6">1</span><span class="s4">]</span>
        <span class="s1">ranked </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">arange</span><span class="s4">(</span><span class="s1">ranking</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">])[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">], </span><span class="s1">ranking</span><span class="s4">]</span>
        <span class="s1">cumulative_gains </span><span class="s4">= </span><span class="s1">discount</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">ranked</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">discount_cumsum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">cumsum</span><span class="s4">(</span><span class="s1">discount</span><span class="s4">)</span>
        <span class="s1">cumulative_gains </span><span class="s4">= [</span>
            <span class="s1">_tie_averaged_dcg</span><span class="s4">(</span><span class="s1">y_t</span><span class="s4">, </span><span class="s1">y_s</span><span class="s4">, </span><span class="s1">discount_cumsum</span><span class="s4">)</span>
            <span class="s3">for </span><span class="s1">y_t</span><span class="s4">, </span><span class="s1">y_s </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">)</span>
        <span class="s4">]</span>
        <span class="s1">cumulative_gains </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">cumulative_gains</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">cumulative_gains</span>


<span class="s3">def </span><span class="s1">_tie_averaged_dcg</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">discount_cumsum</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Compute DCG by averaging over possible permutations of ties. 
 
    The gain (`y_true`) of an index falling inside a tied group (in the order 
    induced by `y_score`) is replaced by the average gain within this group. 
    The discounted gain for a tied group is then the average `y_true` within 
    this group times the sum of discounts of the corresponding ranks. 
 
    This amounts to averaging scores for all possible orderings of the tied 
    groups. 
 
    (note in the case of dcg@k the discount is 0 after index k) 
 
    Parameters 
    ---------- 
    y_true : ndarray 
        The true relevance scores. 
 
    y_score : ndarray 
        Predicted scores. 
 
    discount_cumsum : ndarray 
        Precomputed cumulative sum of the discounts. 
 
    Returns 
    ------- 
    discounted_cumulative_gain : float 
        The discounted cumulative gain. 
 
    References 
    ---------- 
    McSherry, F., &amp; Najork, M. (2008, March). Computing information retrieval 
    performance measures efficiently in the presence of tied scores. In 
    European conference on information retrieval (pp. 414-421). Springer, 
    Berlin, Heidelberg. 
    &quot;&quot;&quot;</span>
    <span class="s1">_</span><span class="s4">, </span><span class="s1">inv</span><span class="s4">, </span><span class="s1">counts </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(-</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">return_inverse</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">return_counts</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s1">ranked </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">counts</span><span class="s4">))</span>
    <span class="s1">np</span><span class="s4">.</span><span class="s1">add</span><span class="s4">.</span><span class="s1">at</span><span class="s4">(</span><span class="s1">ranked</span><span class="s4">, </span><span class="s1">inv</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">ranked </span><span class="s4">/= </span><span class="s1">counts</span>
    <span class="s1">groups </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">cumsum</span><span class="s4">(</span><span class="s1">counts</span><span class="s4">) - </span><span class="s6">1</span>
    <span class="s1">discount_sums </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">counts</span><span class="s4">))</span>
    <span class="s1">discount_sums</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] = </span><span class="s1">discount_cumsum</span><span class="s4">[</span><span class="s1">groups</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]]</span>
    <span class="s1">discount_sums</span><span class="s4">[</span><span class="s6">1</span><span class="s4">:] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">diff</span><span class="s4">(</span><span class="s1">discount_cumsum</span><span class="s4">[</span><span class="s1">groups</span><span class="s4">])</span>
    <span class="s3">return </span><span class="s4">(</span><span class="s1">ranked </span><span class="s4">* </span><span class="s1">discount_sums</span><span class="s4">).</span><span class="s1">sum</span><span class="s4">()</span>


<span class="s3">def </span><span class="s1">_check_dcg_target_type</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">):</span>
    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s1">supported_fmt </span><span class="s4">= (</span>
        <span class="s5">&quot;multilabel-indicator&quot;</span><span class="s4">,</span>
        <span class="s5">&quot;continuous-multioutput&quot;</span><span class="s4">,</span>
        <span class="s5">&quot;multiclass-multioutput&quot;</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s1">supported_fmt</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Only {} formats are supported. Got {} instead&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                <span class="s1">supported_fmt</span><span class="s4">, </span><span class="s1">y_type</span>
            <span class="s4">)</span>
        <span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;k&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;log_base&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;neither&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;ignore_ties&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">dcg_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">k</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">log_base</span><span class="s4">=</span><span class="s6">2</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s3">False</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Discounted Cumulative Gain. 
 
    Sum the true scores ranked in the order induced by the predicted scores, 
    after applying a logarithmic discount. 
 
    This ranking metric yields a high value if true labels are ranked high by 
    ``y_score``. 
 
    Usually the Normalized Discounted Cumulative Gain (NDCG, computed by 
    ndcg_score) is preferred. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples, n_labels) 
        True targets of multilabel classification, or true scores of entities 
        to be ranked. 
 
    y_score : array-like of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates, confidence values, 
        or non-thresholded measure of decisions (as returned by 
        &quot;decision_function&quot; on some classifiers). 
 
    k : int, default=None 
        Only consider the highest k scores in the ranking. If None, use all 
        outputs. 
 
    log_base : float, default=2 
        Base of the logarithm used for the discount. A low value means a 
        sharper discount (top results are more important). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. If `None`, all samples are given the same weight. 
 
    ignore_ties : bool, default=False 
        Assume that there are no ties in y_score (which is likely to be the 
        case if y_score is continuous) for efficiency gains. 
 
    Returns 
    ------- 
    discounted_cumulative_gain : float 
        The averaged sample DCG scores. 
 
    See Also 
    -------- 
    ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted 
        Cumulative Gain (the DCG obtained for a perfect ranking), in order to 
        have a score between 0 and 1. 
 
    References 
    ---------- 
    `Wikipedia entry for Discounted Cumulative Gain 
    &lt;https://en.wikipedia.org/wiki/Discounted_cumulative_gain&gt;`_. 
 
    Jarvelin, K., &amp; Kekalainen, J. (2002). 
    Cumulated gain-based evaluation of IR techniques. ACM Transactions on 
    Information Systems (TOIS), 20(4), 422-446. 
 
    Wang, Y., Wang, L., Li, Y., He, D., Chen, W., &amp; Liu, T. Y. (2013, May). 
    A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th 
    Annual Conference on Learning Theory (COLT 2013). 
 
    McSherry, F., &amp; Najork, M. (2008, March). Computing information retrieval 
    performance measures efficiently in the presence of tied scores. In 
    European conference on information retrieval (pp. 414-421). Springer, 
    Berlin, Heidelberg. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import dcg_score 
    &gt;&gt;&gt; # we have ground-truth relevance of some answers to a query: 
    &gt;&gt;&gt; true_relevance = np.asarray([[10, 0, 0, 1, 5]]) 
    &gt;&gt;&gt; # we predict scores for the answers 
    &gt;&gt;&gt; scores = np.asarray([[.1, .2, .3, 4, 70]]) 
    &gt;&gt;&gt; dcg_score(true_relevance, scores) 
    np.float64(9.49...) 
    &gt;&gt;&gt; # we can set k to truncate the sum; only top k answers contribute 
    &gt;&gt;&gt; dcg_score(true_relevance, scores, k=2) 
    np.float64(5.63...) 
    &gt;&gt;&gt; # now we have some ties in our prediction 
    &gt;&gt;&gt; scores = np.asarray([[1, 0, 0, 0, 1]]) 
    &gt;&gt;&gt; # by default ties are averaged, so here we get the average true 
    &gt;&gt;&gt; # relevance of our top predictions: (10 + 5) / 2 = 7.5 
    &gt;&gt;&gt; dcg_score(true_relevance, scores, k=1) 
    np.float64(7.5) 
    &gt;&gt;&gt; # we can choose to ignore ties for faster results, but only 
    &gt;&gt;&gt; # if we know there aren't ties in our scores, otherwise we get 
    &gt;&gt;&gt; # wrong results: 
    &gt;&gt;&gt; dcg_score(true_relevance, 
    ...           scores, k=1, ignore_ties=True) 
    np.float64(5.0) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">_check_dcg_target_type</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span>
        <span class="s1">_dcg_sample_scores</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">k</span><span class="s4">=</span><span class="s1">k</span><span class="s4">, </span><span class="s1">log_base</span><span class="s4">=</span><span class="s1">log_base</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s1">ignore_ties</span>
        <span class="s4">),</span>
        <span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
    <span class="s4">)</span>


<span class="s3">def </span><span class="s1">_ndcg_sample_scores</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">k</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Normalized Discounted Cumulative Gain. 
 
    Sum the true scores ranked in the order induced by the predicted scores, 
    after applying a logarithmic discount. Then divide by the best possible 
    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between 
    0 and 1. 
 
    This ranking metric yields a high value if true labels are ranked high by 
    ``y_score``. 
 
    Parameters 
    ---------- 
    y_true : ndarray of shape (n_samples, n_labels) 
        True targets of multilabel classification, or true scores of entities 
        to be ranked. 
 
    y_score : ndarray of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates, confidence values, 
        or non-thresholded measure of decisions (as returned by 
        &quot;decision_function&quot; on some classifiers). 
 
    k : int, default=None 
        Only consider the highest k scores in the ranking. If None, use all 
        outputs. 
 
    ignore_ties : bool, default=False 
        Assume that there are no ties in y_score (which is likely to be the 
        case if y_score is continuous) for efficiency gains. 
 
    Returns 
    ------- 
    normalized_discounted_cumulative_gain : ndarray of shape (n_samples,) 
        The NDCG score for each sample (float in [0., 1.]). 
 
    See Also 
    -------- 
    dcg_score : Discounted Cumulative Gain (not normalized). 
 
    &quot;&quot;&quot;</span>
    <span class="s1">gain </span><span class="s4">= </span><span class="s1">_dcg_sample_scores</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">k</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s1">ignore_ties</span><span class="s4">)</span>
    <span class="s2"># Here we use the order induced by y_true so we can ignore ties since</span>
    <span class="s2"># the gain associated to tied indices is the same (permuting ties doesn't</span>
    <span class="s2"># change the value of the re-ordered y_true)</span>
    <span class="s1">normalizing_gain </span><span class="s4">= </span><span class="s1">_dcg_sample_scores</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">k</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s1">all_irrelevant </span><span class="s4">= </span><span class="s1">normalizing_gain </span><span class="s4">== </span><span class="s6">0</span>
    <span class="s1">gain</span><span class="s4">[</span><span class="s1">all_irrelevant</span><span class="s4">] = </span><span class="s6">0</span>
    <span class="s1">gain</span><span class="s4">[~</span><span class="s1">all_irrelevant</span><span class="s4">] /= </span><span class="s1">normalizing_gain</span><span class="s4">[~</span><span class="s1">all_irrelevant</span><span class="s4">]</span>
    <span class="s3">return </span><span class="s1">gain</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;k&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;ignore_ties&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">ndcg_score</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">k</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute Normalized Discounted Cumulative Gain. 
 
    Sum the true scores ranked in the order induced by the predicted scores, 
    after applying a logarithmic discount. Then divide by the best possible 
    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between 
    0 and 1. 
 
    This ranking metric returns a high value if true labels are ranked high by 
    ``y_score``. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples, n_labels) 
        True targets of multilabel classification, or true scores of entities 
        to be ranked. Negative values in `y_true` may result in an output 
        that is not between 0 and 1. 
 
    y_score : array-like of shape (n_samples, n_labels) 
        Target scores, can either be probability estimates, confidence values, 
        or non-thresholded measure of decisions (as returned by 
        &quot;decision_function&quot; on some classifiers). 
 
    k : int, default=None 
        Only consider the highest k scores in the ranking. If `None`, use all 
        outputs. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. If `None`, all samples are given the same weight. 
 
    ignore_ties : bool, default=False 
        Assume that there are no ties in y_score (which is likely to be the 
        case if y_score is continuous) for efficiency gains. 
 
    Returns 
    ------- 
    normalized_discounted_cumulative_gain : float in [0., 1.] 
        The averaged NDCG scores for all samples. 
 
    See Also 
    -------- 
    dcg_score : Discounted Cumulative Gain (not normalized). 
 
    References 
    ---------- 
    `Wikipedia entry for Discounted Cumulative Gain 
    &lt;https://en.wikipedia.org/wiki/Discounted_cumulative_gain&gt;`_ 
 
    Jarvelin, K., &amp; Kekalainen, J. (2002). 
    Cumulated gain-based evaluation of IR techniques. ACM Transactions on 
    Information Systems (TOIS), 20(4), 422-446. 
 
    Wang, Y., Wang, L., Li, Y., He, D., Chen, W., &amp; Liu, T. Y. (2013, May). 
    A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th 
    Annual Conference on Learning Theory (COLT 2013) 
 
    McSherry, F., &amp; Najork, M. (2008, March). Computing information retrieval 
    performance measures efficiently in the presence of tied scores. In 
    European conference on information retrieval (pp. 414-421). Springer, 
    Berlin, Heidelberg. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import ndcg_score 
    &gt;&gt;&gt; # we have ground-truth relevance of some answers to a query: 
    &gt;&gt;&gt; true_relevance = np.asarray([[10, 0, 0, 1, 5]]) 
    &gt;&gt;&gt; # we predict some scores (relevance) for the answers 
    &gt;&gt;&gt; scores = np.asarray([[.1, .2, .3, 4, 70]]) 
    &gt;&gt;&gt; ndcg_score(true_relevance, scores) 
    np.float64(0.69...) 
    &gt;&gt;&gt; scores = np.asarray([[.05, 1.1, 1., .5, .0]]) 
    &gt;&gt;&gt; ndcg_score(true_relevance, scores) 
    np.float64(0.49...) 
    &gt;&gt;&gt; # we can set k to truncate the sum; only top k answers contribute. 
    &gt;&gt;&gt; ndcg_score(true_relevance, scores, k=4) 
    np.float64(0.35...) 
    &gt;&gt;&gt; # the normalization takes k into account so a perfect answer 
    &gt;&gt;&gt; # would still get 1.0 
    &gt;&gt;&gt; ndcg_score(true_relevance, true_relevance, k=4) 
    np.float64(1.0...) 
    &gt;&gt;&gt; # now we have some ties in our prediction 
    &gt;&gt;&gt; scores = np.asarray([[1, 0, 0, 0, 1]]) 
    &gt;&gt;&gt; # by default ties are averaged, so here we get the average (normalized) 
    &gt;&gt;&gt; # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75 
    &gt;&gt;&gt; ndcg_score(true_relevance, scores, k=1) 
    np.float64(0.75...) 
    &gt;&gt;&gt; # we can choose to ignore ties for faster results, but only 
    &gt;&gt;&gt; # if we know there aren't ties in our scores, otherwise we get 
    &gt;&gt;&gt; # wrong results: 
    &gt;&gt;&gt; ndcg_score(true_relevance, 
    ...           scores, k=1, ignore_ties=True) 
    np.float64(0.5...) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() &lt; </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;ndcg_score should not be used on negative y_true values.&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">&gt; </span><span class="s6">1 </span><span class="s3">and </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] &lt;= </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Computing NDCG is only meaningful when there is more than 1 document. &quot;</span>
            <span class="s5">f&quot;Got </span><span class="s3">{</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span><span class="s3">} </span><span class="s5">instead.&quot;</span>
        <span class="s4">)</span>
    <span class="s1">_check_dcg_target_type</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">gain </span><span class="s4">= </span><span class="s1">_ndcg_sample_scores</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">k</span><span class="s4">=</span><span class="s1">k</span><span class="s4">, </span><span class="s1">ignore_ties</span><span class="s4">=</span><span class="s1">ignore_ties</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">gain</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_score&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;k&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;normalize&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">top_k_accuracy_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, *, </span><span class="s1">k</span><span class="s4">=</span><span class="s6">2</span><span class="s4">, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Top-k Accuracy classification score. 
 
    This metric computes the number of times where the correct label is among 
    the top `k` labels predicted (ranked by predicted scores). Note that the 
    multilabel case isn't covered here. 
 
    Read more in the :ref:`User Guide &lt;top_k_accuracy_score&gt;` 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True labels. 
 
    y_score : array-like of shape (n_samples,) or (n_samples, n_classes) 
        Target scores. These can be either probability estimates or 
        non-thresholded decision values (as returned by 
        :term:`decision_function` on some classifiers). 
        The binary case expects scores with shape (n_samples,) while the 
        multiclass case expects scores with shape (n_samples, n_classes). 
        In the multiclass case, the order of the class scores must 
        correspond to the order of ``labels``, if provided, or else to 
        the numerical or lexicographical order of the labels in ``y_true``. 
        If ``y_true`` does not contain all the labels, ``labels`` must be 
        provided. 
 
    k : int, default=2 
        Number of most likely outcomes considered to find the correct label. 
 
    normalize : bool, default=True 
        If `True`, return the fraction of correctly classified samples. 
        Otherwise, return the number of correctly classified samples. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. If `None`, all samples are given the same weight. 
 
    labels : array-like of shape (n_classes,), default=None 
        Multiclass only. List of labels that index the classes in ``y_score``. 
        If ``None``, the numerical or lexicographical order of the labels in 
        ``y_true`` is used. If ``y_true`` does not contain all the labels, 
        ``labels`` must be provided. 
 
    Returns 
    ------- 
    score : float 
        The top-k accuracy score. The best performance is 1 with 
        `normalize == True` and the number of samples with 
        `normalize == False`. 
 
    See Also 
    -------- 
    accuracy_score : Compute the accuracy score. By default, the function will 
        return the fraction of correct predictions divided by the total number 
        of predictions. 
 
    Notes 
    ----- 
    In cases where two or more labels are assigned equal predicted scores, 
    the labels with the highest indices will be chosen first. This might 
    impact the result if the correct label falls after the threshold because 
    of that. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import top_k_accuracy_score 
    &gt;&gt;&gt; y_true = np.array([0, 1, 2, 2]) 
    &gt;&gt;&gt; y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2 
    ...                     [0.3, 0.4, 0.2],  # 1 is in top 2 
    ...                     [0.2, 0.4, 0.3],  # 2 is in top 2 
    ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2 
    &gt;&gt;&gt; top_k_accuracy_score(y_true, y_score, k=2) 
    np.float64(0.75) 
    &gt;&gt;&gt; # Not normalizing gives the number of &quot;correctly&quot; classified samples 
    &gt;&gt;&gt; top_k_accuracy_score(y_true, y_score, k=2, normalize=False) 
    np.int64(3) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s3">None</span><span class="s4">)</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot; </span><span class="s3">and </span><span class="s1">labels </span><span class="s3">is not None and </span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">) &gt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s1">y_type </span><span class="s4">= </span><span class="s5">&quot;multiclass&quot;</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s4">{</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">}:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">f&quot;y type must be 'binary' or 'multiclass', got '</span><span class="s3">{</span><span class="s1">y_type</span><span class="s3">}</span><span class="s5">' instead.&quot;</span>
        <span class="s4">)</span>
    <span class="s1">y_score </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">2 </span><span class="s3">and </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] != </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;`y_true` is binary while y_score is 2d with&quot;</span>
                <span class="s5">f&quot; </span><span class="s3">{</span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span><span class="s3">} </span><span class="s5">classes. If `y_true` does not contain all the&quot;</span>
                <span class="s5">&quot; labels, `labels` must be provided.&quot;</span>
            <span class="s4">)</span>
        <span class="s1">y_score </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">)</span>

    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">y_score_n_classes </span><span class="s4">= </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] </span><span class="s3">if </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">2 </span><span class="s3">else </span><span class="s6">2</span>

    <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">classes </span><span class="s4">= </span><span class="s1">_unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_classes </span><span class="s4">!= </span><span class="s1">y_score_n_classes</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">f&quot;Number of classes in 'y_true' (</span><span class="s3">{</span><span class="s1">n_classes</span><span class="s3">}</span><span class="s5">) not equal &quot;</span>
                <span class="s5">f&quot;to the number of classes in 'y_score' (</span><span class="s3">{</span><span class="s1">y_score_n_classes</span><span class="s3">}</span><span class="s5">).&quot;</span>
                <span class="s5">&quot;You can provide a list of all known classes by assigning it &quot;</span>
                <span class="s5">&quot;to the `labels` parameter.&quot;</span>
            <span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">classes </span><span class="s4">= </span><span class="s1">_unique</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">n_labels </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_classes </span><span class="s4">!= </span><span class="s1">n_labels</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Parameter 'labels' must be unique.&quot;</span><span class="s4">)</span>

        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array_equal</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Parameter 'labels' must be ordered.&quot;</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_classes </span><span class="s4">!= </span><span class="s1">y_score_n_classes</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">f&quot;Number of given labels (</span><span class="s3">{</span><span class="s1">n_classes</span><span class="s3">}</span><span class="s5">) not equal to the &quot;</span>
                <span class="s5">f&quot;number of classes in 'y_score' (</span><span class="s3">{</span><span class="s1">y_score_n_classes</span><span class="s3">}</span><span class="s5">).&quot;</span>
            <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">setdiff1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">)):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;'y_true' contains labels not in parameter 'labels'.&quot;</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">k </span><span class="s4">&gt;= </span><span class="s1">n_classes</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s4">(</span>
                <span class="s5">f&quot;'k' (</span><span class="s3">{</span><span class="s1">k</span><span class="s3">}</span><span class="s5">) greater than or equal to 'n_classes' (</span><span class="s3">{</span><span class="s1">n_classes</span><span class="s3">}</span><span class="s5">) &quot;</span>
                <span class="s5">&quot;will result in a perfect score and is therefore meaningless.&quot;</span>
            <span class="s4">),</span>
            <span class="s1">UndefinedMetricWarning</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s1">y_true_encoded </span><span class="s4">= </span><span class="s1">_encode</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">uniques</span><span class="s4">=</span><span class="s1">classes</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">k </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s1">threshold </span><span class="s4">= </span><span class="s6">0.5 </span><span class="s3">if </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() &gt;= </span><span class="s6">0 </span><span class="s3">and </span><span class="s1">y_score</span><span class="s4">.</span><span class="s1">max</span><span class="s4">() &lt;= </span><span class="s6">1 </span><span class="s3">else </span><span class="s6">0</span>
            <span class="s1">y_pred </span><span class="s4">= (</span><span class="s1">y_score </span><span class="s4">&gt; </span><span class="s1">threshold</span><span class="s4">).</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">int64</span><span class="s4">)</span>
            <span class="s1">hits </span><span class="s4">= </span><span class="s1">y_pred </span><span class="s4">== </span><span class="s1">y_true_encoded</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">hits </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones_like</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">bool_</span><span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">:</span>
        <span class="s1">sorted_pred </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">argsort</span><span class="s4">(</span><span class="s1">y_score</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">kind</span><span class="s4">=</span><span class="s5">&quot;mergesort&quot;</span><span class="s4">)[:, ::-</span><span class="s6">1</span><span class="s4">]</span>
        <span class="s1">hits </span><span class="s4">= (</span><span class="s1">y_true_encoded </span><span class="s4">== </span><span class="s1">sorted_pred</span><span class="s4">[:, :</span><span class="s1">k</span><span class="s4">].</span><span class="s1">T</span><span class="s4">).</span><span class="s1">any</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">normalize</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">hits</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">hits</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">hits</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
</pre>
</body>
</html>