<html>
<head>
<title>test_glm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #6aab73;}
.s5 { color: #2aacb8;}
.s6 { color: #5f826b; font-style: italic;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_glm.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: The scikit-learn developers</span>
<span class="s0"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">functools </span><span class="s2">import </span><span class="s1">partial</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">import </span><span class="s1">scipy</span>
<span class="s2">from </span><span class="s1">numpy</span><span class="s3">.</span><span class="s1">testing </span><span class="s2">import </span><span class="s1">assert_allclose</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">linalg</span>
<span class="s2">from </span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">optimize </span><span class="s2">import </span><span class="s1">minimize</span><span class="s3">, </span><span class="s1">root</span>

<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">_loss </span><span class="s2">import </span><span class="s1">HalfBinomialLoss</span><span class="s3">, </span><span class="s1">HalfPoissonLoss</span><span class="s3">, </span><span class="s1">HalfTweedieLoss</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">_loss</span><span class="s3">.</span><span class="s1">link </span><span class="s2">import </span><span class="s1">IdentityLink</span><span class="s3">, </span><span class="s1">LogLink</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">base </span><span class="s2">import </span><span class="s1">clone</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">datasets </span><span class="s2">import </span><span class="s1">make_low_rank_matrix</span><span class="s3">, </span><span class="s1">make_regression</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">linear_model </span><span class="s2">import </span><span class="s3">(</span>
    <span class="s1">GammaRegressor</span><span class="s3">,</span>
    <span class="s1">PoissonRegressor</span><span class="s3">,</span>
    <span class="s1">Ridge</span><span class="s3">,</span>
    <span class="s1">TweedieRegressor</span><span class="s3">,</span>
<span class="s3">)</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">linear_model</span><span class="s3">.</span><span class="s1">_glm </span><span class="s2">import </span><span class="s1">_GeneralizedLinearRegressor</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">linear_model</span><span class="s3">.</span><span class="s1">_glm</span><span class="s3">.</span><span class="s1">_newton_solver </span><span class="s2">import </span><span class="s1">NewtonCholeskySolver</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">linear_model</span><span class="s3">.</span><span class="s1">_linear_loss </span><span class="s2">import </span><span class="s1">LinearModelLoss</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">metrics </span><span class="s2">import </span><span class="s1">d2_tweedie_score</span><span class="s3">, </span><span class="s1">mean_poisson_deviance</span>
<span class="s2">from </span><span class="s1">sklearn</span><span class="s3">.</span><span class="s1">model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>

<span class="s1">SOLVERS </span><span class="s3">= [</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">]</span>


<span class="s2">class </span><span class="s1">BinomialRegressor</span><span class="s3">(</span><span class="s1">_GeneralizedLinearRegressor</span><span class="s3">):</span>
    <span class="s2">def </span><span class="s1">_get_loss</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s1">HalfBinomialLoss</span><span class="s3">()</span>


<span class="s2">def </span><span class="s1">_special_minimize</span><span class="s3">(</span><span class="s1">fun</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">tol_NM</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">):</span>
    <span class="s0"># Find good starting point by Nelder-Mead</span>
    <span class="s1">res_NM </span><span class="s3">= </span><span class="s1">minimize</span><span class="s3">(</span>
        <span class="s1">fun</span><span class="s3">, </span><span class="s1">x</span><span class="s3">, </span><span class="s1">method</span><span class="s3">=</span><span class="s4">&quot;Nelder-Mead&quot;</span><span class="s3">, </span><span class="s1">options</span><span class="s3">={</span><span class="s4">&quot;xatol&quot;</span><span class="s3">: </span><span class="s1">tol_NM</span><span class="s3">, </span><span class="s4">&quot;fatol&quot;</span><span class="s3">: </span><span class="s1">tol_NM</span><span class="s3">}</span>
    <span class="s3">)</span>
    <span class="s0"># Now refine via root finding on the gradient of the function, which is</span>
    <span class="s0"># more precise than minimizing the function itself.</span>
    <span class="s1">res </span><span class="s3">= </span><span class="s1">root</span><span class="s3">(</span>
        <span class="s1">grad</span><span class="s3">,</span>
        <span class="s1">res_NM</span><span class="s3">.</span><span class="s1">x</span><span class="s3">,</span>
        <span class="s1">method</span><span class="s3">=</span><span class="s4">&quot;lm&quot;</span><span class="s3">,</span>
        <span class="s1">options</span><span class="s3">={</span><span class="s4">&quot;ftol&quot;</span><span class="s3">: </span><span class="s1">tol</span><span class="s3">, </span><span class="s4">&quot;xtol&quot;</span><span class="s3">: </span><span class="s1">tol</span><span class="s3">, </span><span class="s4">&quot;gtol&quot;</span><span class="s3">: </span><span class="s1">tol</span><span class="s3">},</span>
    <span class="s3">)</span>
    <span class="s2">return </span><span class="s1">res</span><span class="s3">.</span><span class="s1">x</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">fixture</span><span class="s3">(</span><span class="s1">scope</span><span class="s3">=</span><span class="s4">&quot;module&quot;</span><span class="s3">)</span>
<span class="s2">def </span><span class="s1">regression_data</span><span class="s3">():</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">= </span><span class="s1">make_regression</span><span class="s3">(</span>
        <span class="s1">n_samples</span><span class="s3">=</span><span class="s5">107</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">=</span><span class="s5">10</span><span class="s3">, </span><span class="s1">n_informative</span><span class="s3">=</span><span class="s5">80</span><span class="s3">, </span><span class="s1">noise</span><span class="s3">=</span><span class="s5">0.5</span><span class="s3">, </span><span class="s1">random_state</span><span class="s3">=</span><span class="s5">2</span>
    <span class="s3">)</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">fixture</span><span class="s3">(</span>
    <span class="s1">params</span><span class="s3">=</span><span class="s1">itertools</span><span class="s3">.</span><span class="s1">product</span><span class="s3">(</span>
        <span class="s3">[</span><span class="s4">&quot;long&quot;</span><span class="s3">, </span><span class="s4">&quot;wide&quot;</span><span class="s3">],</span>
        <span class="s3">[</span>
            <span class="s1">BinomialRegressor</span><span class="s3">(),</span>
            <span class="s1">PoissonRegressor</span><span class="s3">(),</span>
            <span class="s1">GammaRegressor</span><span class="s3">(),</span>
            <span class="s0"># TweedieRegressor(power=3.0),  # too difficult</span>
            <span class="s0"># TweedieRegressor(power=0, link=&quot;log&quot;),  # too difficult</span>
            <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">1.5</span><span class="s3">),</span>
        <span class="s3">],</span>
    <span class="s3">),</span>
    <span class="s1">ids</span><span class="s3">=</span><span class="s2">lambda </span><span class="s1">param</span><span class="s3">: </span><span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">param</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span><span class="s2">}</span><span class="s4">-</span><span class="s2">{</span><span class="s1">param</span><span class="s3">[</span><span class="s5">1</span><span class="s3">]</span><span class="s2">}</span><span class="s4">&quot;</span><span class="s3">,</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">glm_dataset</span><span class="s3">(</span><span class="s1">global_random_seed</span><span class="s3">, </span><span class="s1">request</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Dataset with GLM solutions, well conditioned X. 
 
    This is inspired by ols_ridge_dataset in test_ridge.py. 
 
    The construction is based on the SVD decomposition of X = U S V'. 
 
    Parameters 
    ---------- 
    type : {&quot;long&quot;, &quot;wide&quot;} 
        If &quot;long&quot;, then n_samples &gt; n_features. 
        If &quot;wide&quot;, then n_features &gt; n_samples. 
    model : a GLM model 
 
    For &quot;wide&quot;, we return the minimum norm solution: 
 
        min ||w||_2 subject to w = argmin deviance(X, y, w) 
 
    Note that the deviance is always minimized if y = inverse_link(X w) is possible to 
    achieve, which it is in the wide data case. Therefore, we can construct the 
    solution with minimum norm like (wide) OLS: 
 
        min ||w||_2 subject to link(y) = raw_prediction = X w 
 
    Returns 
    ------- 
    model : GLM model 
    X : ndarray 
        Last column of 1, i.e. intercept. 
    y : ndarray 
    coef_unpenalized : ndarray 
        Minimum norm solutions, i.e. min sum(loss(w)) (with minimum ||w||_2 in 
        case of ambiguity) 
        Last coefficient is intercept. 
    coef_penalized : ndarray 
        GLM solution with alpha=l2_reg_strength=1, i.e. 
        min 1/n * sum(loss) + ||w[:-1]||_2^2. 
        Last coefficient is intercept. 
    l2_reg_strength : float 
        Always equal 1. 
    &quot;&quot;&quot;</span>
    <span class="s1">data_type</span><span class="s3">, </span><span class="s1">model </span><span class="s3">= </span><span class="s1">request</span><span class="s3">.</span><span class="s1">param</span>
    <span class="s0"># Make larger dim more than double as big as the smaller one.</span>
    <span class="s0"># This helps when constructing singular matrices like (X, X).</span>
    <span class="s2">if </span><span class="s1">data_type </span><span class="s3">== </span><span class="s4">&quot;long&quot;</span><span class="s3">:</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s5">12</span><span class="s3">, </span><span class="s5">4</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s5">4</span><span class="s3">, </span><span class="s5">12</span>
    <span class="s1">k </span><span class="s3">= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">)</span>
    <span class="s1">rng </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">random</span><span class="s3">.</span><span class="s1">RandomState</span><span class="s3">(</span><span class="s1">global_random_seed</span><span class="s3">)</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">make_low_rank_matrix</span><span class="s3">(</span>
        <span class="s1">n_samples</span><span class="s3">=</span><span class="s1">n_samples</span><span class="s3">,</span>
        <span class="s1">n_features</span><span class="s3">=</span><span class="s1">n_features</span><span class="s3">,</span>
        <span class="s1">effective_rank</span><span class="s3">=</span><span class="s1">k</span><span class="s3">,</span>
        <span class="s1">tail_strength</span><span class="s3">=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">=</span><span class="s1">rng</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">X</span><span class="s3">[:, -</span><span class="s5">1</span><span class="s3">] = </span><span class="s5">1  </span><span class="s0"># last columns acts as intercept</span>
    <span class="s1">U</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s1">Vt </span><span class="s3">= </span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">svd</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">full_matrices</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">all</span><span class="s3">(</span><span class="s1">s </span><span class="s3">&gt; </span><span class="s5">1e-3</span><span class="s3">)  </span><span class="s0"># to be sure</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">max</span><span class="s3">(</span><span class="s1">s</span><span class="s3">) / </span><span class="s1">np</span><span class="s3">.</span><span class="s1">min</span><span class="s3">(</span><span class="s1">s</span><span class="s3">) &lt; </span><span class="s5">100  </span><span class="s0"># condition number of X</span>

    <span class="s2">if </span><span class="s1">data_type </span><span class="s3">== </span><span class="s4">&quot;long&quot;</span><span class="s3">:</span>
        <span class="s1">coef_unpenalized </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">uniform</span><span class="s3">(</span><span class="s1">low</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">high</span><span class="s3">=</span><span class="s5">3</span><span class="s3">, </span><span class="s1">size</span><span class="s3">=</span><span class="s1">n_features</span><span class="s3">)</span>
        <span class="s1">coef_unpenalized </span><span class="s3">*= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">choice</span><span class="s3">([-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">], </span><span class="s1">size</span><span class="s3">=</span><span class="s1">n_features</span><span class="s3">)</span>
        <span class="s1">raw_prediction </span><span class="s3">= </span><span class="s1">X </span><span class="s3">@ </span><span class="s1">coef_unpenalized</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">raw_prediction </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">uniform</span><span class="s3">(</span><span class="s1">low</span><span class="s3">=-</span><span class="s5">3</span><span class="s3">, </span><span class="s1">high</span><span class="s3">=</span><span class="s5">3</span><span class="s3">, </span><span class="s1">size</span><span class="s3">=</span><span class="s1">n_samples</span><span class="s3">)</span>
        <span class="s0"># minimum norm solution min ||w||_2 such that raw_prediction = X w:</span>
        <span class="s0"># w = X'(XX')^-1 raw_prediction = V s^-1 U' raw_prediction</span>
        <span class="s1">coef_unpenalized </span><span class="s3">= </span><span class="s1">Vt</span><span class="s3">.</span><span class="s1">T </span><span class="s3">@ </span><span class="s1">np</span><span class="s3">.</span><span class="s1">diag</span><span class="s3">(</span><span class="s5">1 </span><span class="s3">/ </span><span class="s1">s</span><span class="s3">) @ </span><span class="s1">U</span><span class="s3">.</span><span class="s1">T </span><span class="s3">@ </span><span class="s1">raw_prediction</span>

    <span class="s1">linear_loss </span><span class="s3">= </span><span class="s1">LinearModelLoss</span><span class="s3">(</span><span class="s1">base_loss</span><span class="s3">=</span><span class="s1">model</span><span class="s3">.</span><span class="s1">_get_loss</span><span class="s3">(), </span><span class="s1">fit_intercept</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)</span>
    <span class="s1">sw </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">full</span><span class="s3">(</span><span class="s1">shape</span><span class="s3">=</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">fill_value</span><span class="s3">=</span><span class="s5">1 </span><span class="s3">/ </span><span class="s1">n_samples</span><span class="s3">)</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">base_loss</span><span class="s3">.</span><span class="s1">link</span><span class="s3">.</span><span class="s1">inverse</span><span class="s3">(</span><span class="s1">raw_prediction</span><span class="s3">)</span>

    <span class="s0"># Add penalty l2_reg_strength * ||coef||_2^2 for l2_reg_strength=1 and solve with</span>
    <span class="s0"># optimizer. Note that the problem is well conditioned such that we get accurate</span>
    <span class="s0"># results.</span>
    <span class="s1">l2_reg_strength </span><span class="s3">= </span><span class="s5">1</span>
    <span class="s1">fun </span><span class="s3">= </span><span class="s1">partial</span><span class="s3">(</span>
        <span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">loss</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">],</span>
        <span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s1">l2_reg_strength</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">grad </span><span class="s3">= </span><span class="s1">partial</span><span class="s3">(</span>
        <span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">gradient</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">],</span>
        <span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s1">l2_reg_strength</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">coef_penalized_with_intercept </span><span class="s3">= </span><span class="s1">_special_minimize</span><span class="s3">(</span>
        <span class="s1">fun</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">coef_unpenalized</span><span class="s3">, </span><span class="s1">tol_NM</span><span class="s3">=</span><span class="s5">1e-6</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-14</span>
    <span class="s3">)</span>

    <span class="s1">linear_loss </span><span class="s3">= </span><span class="s1">LinearModelLoss</span><span class="s3">(</span><span class="s1">base_loss</span><span class="s3">=</span><span class="s1">model</span><span class="s3">.</span><span class="s1">_get_loss</span><span class="s3">(), </span><span class="s1">fit_intercept</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">fun </span><span class="s3">= </span><span class="s1">partial</span><span class="s3">(</span>
        <span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">loss</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">],</span>
        <span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s1">l2_reg_strength</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">grad </span><span class="s3">= </span><span class="s1">partial</span><span class="s3">(</span>
        <span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">gradient</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">],</span>
        <span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s1">l2_reg_strength</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">coef_penalized_without_intercept </span><span class="s3">= </span><span class="s1">_special_minimize</span><span class="s3">(</span>
        <span class="s1">fun</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">coef_unpenalized</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">tol_NM</span><span class="s3">=</span><span class="s5">1e-6</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-14</span>
    <span class="s3">)</span>

    <span class="s0"># To be sure</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">coef_penalized_with_intercept</span><span class="s3">) &lt; </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span>
        <span class="s1">coef_unpenalized</span>
    <span class="s3">)</span>

    <span class="s2">return </span><span class="s3">(</span>
        <span class="s1">model</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">,</span>
        <span class="s1">coef_unpenalized</span><span class="s3">,</span>
        <span class="s1">coef_penalized_with_intercept</span><span class="s3">,</span>
        <span class="s1">coef_penalized_without_intercept</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">,</span>
    <span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">False</span><span class="s3">, </span><span class="s2">True</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_regression</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">glm_dataset</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that GLM converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    &quot;&quot;&quot;</span>
    <span class="s1">model</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">coef_with_intercept</span><span class="s3">, </span><span class="s1">coef_without_intercept</span><span class="s3">, </span><span class="s1">alpha </span><span class="s3">= </span><span class="s1">glm_dataset</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s1">model </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># remove intercept</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef_with_intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef_without_intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s5">0</span>

    <span class="s1">model</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s1">rtol </span><span class="s3">= </span><span class="s5">5e-5 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">1e-9</span>
    <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>

    <span class="s0"># Same with sample_weight.</span>
    <span class="s1">model </span><span class="s3">= (</span>
        <span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">(</span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]))</span>
    <span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_regression_hstacked_X</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">glm_dataset</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that GLM converges for all solvers to correct solution on hstacked data. 
 
    We work with a simple constructed data set with known solution. 
    Fit on [X] with alpha is the same as fit on [X, X]/2 with alpha/2. 
    For long X, [X, X] is still a long but singular matrix. 
    &quot;&quot;&quot;</span>
    <span class="s1">model</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">coef_with_intercept</span><span class="s3">, </span><span class="s1">coef_without_intercept</span><span class="s3">, </span><span class="s1">alpha </span><span class="s3">= </span><span class="s1">glm_dataset</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha </span><span class="s3">/ </span><span class="s5">2</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s1">model </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># remove intercept</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s5">0.5 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">matrix_rank</span><span class="s3">(</span><span class="s1">X</span><span class="s3">) &lt;= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">- </span><span class="s5">1</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef_with_intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef_without_intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s5">0</span>

    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s0"># XXX: Investigate if the ConvergenceWarning that can appear in some</span>
        <span class="s0"># cases should be considered a bug or not. In the mean time we don't</span>
        <span class="s0"># fail when the assertions below pass irrespective of the presence of</span>
        <span class="s0"># the warning.</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">model</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s1">rtol </span><span class="s3">= </span><span class="s5">2e-4 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">5e-9</span>
    <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">coef</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_regression_vstacked_X</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">glm_dataset</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that GLM converges for all solvers to correct solution on vstacked data. 
 
    We work with a simple constructed data set with known solution. 
    Fit on [X] with alpha is the same as fit on [X], [y] 
                                                [X], [y] with 1 * alpha. 
    It is the same alpha as the average loss stays the same. 
    For wide X, [X', X'] is a singular matrix. 
    &quot;&quot;&quot;</span>
    <span class="s1">model</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">coef_with_intercept</span><span class="s3">, </span><span class="s1">coef_without_intercept</span><span class="s3">, </span><span class="s1">alpha </span><span class="s3">= </span><span class="s1">glm_dataset</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s1">model </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># remove intercept</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">0</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">matrix_rank</span><span class="s3">(</span><span class="s1">X</span><span class="s3">) &lt;= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">)</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">y</span><span class="s3">, </span><span class="s1">y</span><span class="s3">]</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef_with_intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef_without_intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s5">0</span>
    <span class="s1">model</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s1">rtol </span><span class="s3">= </span><span class="s5">3e-5 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">5e-9</span>
    <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_regression_unpenalized</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">glm_dataset</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that unpenalized GLM converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    Note: This checks the minimum norm solution for wide X, i.e. 
    n_samples &lt; n_features: 
        min ||w||_2 subject to w = argmin deviance(X, y, w) 
    &quot;&quot;&quot;</span>
    <span class="s1">model</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">= </span><span class="s1">glm_dataset</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span>
    <span class="s1">alpha </span><span class="s3">= </span><span class="s5">0  </span><span class="s0"># unpenalized</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s1">model </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">X </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># remove intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s5">0</span>

    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s2">if </span><span class="s1">solver</span><span class="s3">.</span><span class="s1">startswith</span><span class="s3">(</span><span class="s4">&quot;newton&quot;</span><span class="s3">) </span><span class="s2">and </span><span class="s1">n_samples </span><span class="s3">&lt; </span><span class="s1">n_features</span><span class="s3">:</span>
            <span class="s0"># The newton solvers should warn and automatically fallback to LBFGS</span>
            <span class="s0"># in this case. The model should still converge.</span>
            <span class="s1">warnings</span><span class="s3">.</span><span class="s1">filterwarnings</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category</span><span class="s3">=</span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">LinAlgWarning</span><span class="s3">)</span>
        <span class="s0"># XXX: Investigate if the ConvergenceWarning that can appear in some</span>
        <span class="s0"># cases should be considered a bug or not. In the mean time we don't</span>
        <span class="s0"># fail when the assertions below pass irrespective of the presence of</span>
        <span class="s0"># the warning.</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">filterwarnings</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category</span><span class="s3">=</span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">model</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s0"># FIXME: `assert_allclose(model.coef_, coef)` should work for all cases but fails</span>
    <span class="s0"># for the wide/fat case with n_features &gt; n_samples. Most current GLM solvers do</span>
    <span class="s0"># NOT return the minimum norm solution with fit_intercept=True.</span>
    <span class="s2">if </span><span class="s1">n_samples </span><span class="s3">&gt; </span><span class="s1">n_features</span><span class="s3">:</span>
        <span class="s1">rtol </span><span class="s3">= </span><span class="s5">5e-5 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">1e-7</span>
        <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s0"># As it is an underdetermined problem, prediction = y. The following shows that</span>
        <span class="s0"># we get a solution, i.e. a (non-unique) minimum of the objective function ...</span>
        <span class="s1">rtol </span><span class="s3">= </span><span class="s5">5e-5</span>
        <span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">:</span>
            <span class="s1">rtol </span><span class="s3">= </span><span class="s5">5e-4</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X</span><span class="s3">), </span><span class="s1">y</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>

        <span class="s1">norm_solution </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">])</span>
        <span class="s1">norm_model </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">])</span>
        <span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">:</span>
            <span class="s0"># XXX: This solver shows random behaviour. Sometimes it finds solutions</span>
            <span class="s0"># with norm_model &lt;= norm_solution! So we check conditionally.</span>
            <span class="s2">if </span><span class="s1">norm_model </span><span class="s3">&lt; (</span><span class="s5">1 </span><span class="s3">+ </span><span class="s5">1e-12</span><span class="s3">) * </span><span class="s1">norm_solution</span><span class="s3">:</span>
                <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">)</span>
                <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
        <span class="s2">elif </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">and </span><span class="s1">fit_intercept</span><span class="s3">:</span>
            <span class="s0"># But it is not the minimum norm solution. Otherwise the norms would be</span>
            <span class="s0"># equal.</span>
            <span class="s2">assert </span><span class="s1">norm_model </span><span class="s3">&gt; (</span><span class="s5">1 </span><span class="s3">+ </span><span class="s5">1e-12</span><span class="s3">) * </span><span class="s1">norm_solution</span>

            <span class="s0"># See https://github.com/scikit-learn/scikit-learn/issues/23670.</span>
            <span class="s0"># Note: Even adding a tiny penalty does not give the minimal norm solution.</span>
            <span class="s0"># XXX: We could have naively expected LBFGS to find the minimal norm</span>
            <span class="s0"># solution by adding a very small penalty. Even that fails for a reason we</span>
            <span class="s0"># do not properly understand at this point.</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s0"># When `fit_intercept=False`, LBFGS naturally converges to the minimum norm</span>
            <span class="s0"># solution on this problem.</span>
            <span class="s0"># XXX: Do we have any theoretical guarantees why this should be the case?</span>
            <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
            <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_regression_unpenalized_hstacked_X</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">glm_dataset</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that unpenalized GLM converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    GLM fit on [X] is the same as fit on [X, X]/2. 
    For long X, [X, X] is a singular matrix and we check against the minimum norm 
    solution: 
        min ||w||_2 subject to w = argmin deviance(X, y, w) 
    &quot;&quot;&quot;</span>
    <span class="s1">model</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">= </span><span class="s1">glm_dataset</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span>
    <span class="s1">alpha </span><span class="s3">= </span><span class="s5">0  </span><span class="s0"># unpenalized</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s1">model </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s2">if </span><span class="s1">n_samples </span><span class="s3">&gt; </span><span class="s1">n_features</span><span class="s3">:</span>
            <span class="s1">X </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># remove intercept</span>
            <span class="s1">X </span><span class="s3">= </span><span class="s5">0.5 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s0"># To know the minimum norm solution, we keep one intercept column and do</span>
            <span class="s0"># not divide by 2. Later on, we must take special care.</span>
            <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">c_</span><span class="s3">[</span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">X</span><span class="s3">[:, -</span><span class="s5">1</span><span class="s3">]]</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s5">0</span>
        <span class="s1">X </span><span class="s3">= </span><span class="s5">0.5 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">matrix_rank</span><span class="s3">(</span><span class="s1">X</span><span class="s3">) &lt;= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">)</span>

    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s2">if </span><span class="s1">solver</span><span class="s3">.</span><span class="s1">startswith</span><span class="s3">(</span><span class="s4">&quot;newton&quot;</span><span class="s3">):</span>
            <span class="s0"># The newton solvers should warn and automatically fallback to LBFGS</span>
            <span class="s0"># in this case. The model should still converge.</span>
            <span class="s1">warnings</span><span class="s3">.</span><span class="s1">filterwarnings</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category</span><span class="s3">=</span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">LinAlgWarning</span><span class="s3">)</span>
        <span class="s0"># XXX: Investigate if the ConvergenceWarning that can appear in some</span>
        <span class="s0"># cases should be considered a bug or not. In the mean time we don't</span>
        <span class="s0"># fail when the assertions below pass irrespective of the presence of</span>
        <span class="s0"># the warning.</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">filterwarnings</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category</span><span class="s3">=</span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">model</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s2">if </span><span class="s1">fit_intercept </span><span class="s2">and </span><span class="s1">n_samples </span><span class="s3">&lt; </span><span class="s1">n_features</span><span class="s3">:</span>
        <span class="s0"># Here we take special care.</span>
        <span class="s1">model_intercept </span><span class="s3">= </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_</span>
        <span class="s1">model_coef </span><span class="s3">= </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># exclude the other intercept term.</span>
        <span class="s0"># For minimum norm solution, we would have</span>
        <span class="s0"># assert model.intercept_ == pytest.approx(model.coef_[-1])</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">model_intercept </span><span class="s3">= </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_</span>
        <span class="s1">model_coef </span><span class="s3">= </span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span>

    <span class="s2">if </span><span class="s1">n_samples </span><span class="s3">&gt; </span><span class="s1">n_features</span><span class="s3">:</span>
        <span class="s2">assert </span><span class="s1">model_intercept </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">)</span>
        <span class="s1">rtol </span><span class="s3">= </span><span class="s5">1e-4</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model_coef</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">coef</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s0"># As it is an underdetermined problem, prediction = y. The following shows that</span>
        <span class="s0"># we get a solution, i.e. a (non-unique) minimum of the objective function ...</span>
        <span class="s1">rtol </span><span class="s3">= </span><span class="s5">1e-6 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">5e-6</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X</span><span class="s3">), </span><span class="s1">y</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
        <span class="s2">if </span><span class="s3">(</span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">and </span><span class="s1">fit_intercept</span><span class="s3">) </span><span class="s2">or </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">:</span>
            <span class="s0"># Same as in test_glm_regression_unpenalized.</span>
            <span class="s0"># But it is not the minimum norm solution. Otherwise the norms would be</span>
            <span class="s0"># equal.</span>
            <span class="s1">norm_solution </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span>
                <span class="s5">0.5 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">]</span>
            <span class="s3">)</span>
            <span class="s1">norm_model </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">])</span>
            <span class="s2">assert </span><span class="s1">norm_model </span><span class="s3">&gt; (</span><span class="s5">1 </span><span class="s3">+ </span><span class="s5">1e-12</span><span class="s3">) * </span><span class="s1">norm_solution</span>
            <span class="s0"># For minimum norm solution, we would have</span>
            <span class="s0"># assert model.intercept_ == pytest.approx(model.coef_[-1])</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s2">assert </span><span class="s1">model_intercept </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s5">5e-6</span><span class="s3">)</span>
            <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model_coef</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">coef</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-4</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_regression_unpenalized_vstacked_X</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">glm_dataset</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that unpenalized GLM converges for all solvers to correct solution. 
 
    We work with a simple constructed data set with known solution. 
    GLM fit on [X] is the same as fit on [X], [y] 
                                         [X], [y]. 
    For wide X, [X', X'] is a singular matrix and we check against the minimum norm 
    solution: 
        min ||w||_2 subject to w = argmin deviance(X, y, w) 
    &quot;&quot;&quot;</span>
    <span class="s1">model</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ </span><span class="s3">= </span><span class="s1">glm_dataset</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span>
    <span class="s1">alpha </span><span class="s3">= </span><span class="s5">0  </span><span class="s0"># unpenalized</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s1">model </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">model</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(**</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">X </span><span class="s3">= </span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">]  </span><span class="s0"># remove intercept</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>
        <span class="s1">coef </span><span class="s3">= </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">intercept </span><span class="s3">= </span><span class="s5">0</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">((</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X</span><span class="s3">), </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">0</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">matrix_rank</span><span class="s3">(</span><span class="s1">X</span><span class="s3">) &lt;= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">)</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">y</span><span class="s3">, </span><span class="s1">y</span><span class="s3">]</span>

    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s2">if </span><span class="s1">solver</span><span class="s3">.</span><span class="s1">startswith</span><span class="s3">(</span><span class="s4">&quot;newton&quot;</span><span class="s3">) </span><span class="s2">and </span><span class="s1">n_samples </span><span class="s3">&lt; </span><span class="s1">n_features</span><span class="s3">:</span>
            <span class="s0"># The newton solvers should warn and automatically fallback to LBFGS</span>
            <span class="s0"># in this case. The model should still converge.</span>
            <span class="s1">warnings</span><span class="s3">.</span><span class="s1">filterwarnings</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category</span><span class="s3">=</span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">LinAlgWarning</span><span class="s3">)</span>
        <span class="s0"># XXX: Investigate if the ConvergenceWarning that can appear in some</span>
        <span class="s0"># cases should be considered a bug or not. In the mean time we don't</span>
        <span class="s0"># fail when the assertions below pass irrespective of the presence of</span>
        <span class="s0"># the warning.</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">filterwarnings</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category</span><span class="s3">=</span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">model</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s2">if </span><span class="s1">n_samples </span><span class="s3">&gt; </span><span class="s1">n_features</span><span class="s3">:</span>
        <span class="s1">rtol </span><span class="s3">= </span><span class="s5">5e-5 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">1e-6</span>
        <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s0"># As it is an underdetermined problem, prediction = y. The following shows that</span>
        <span class="s0"># we get a solution, i.e. a (non-unique) minimum of the objective function ...</span>
        <span class="s1">rtol </span><span class="s3">= </span><span class="s5">1e-6 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">5e-6</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X</span><span class="s3">), </span><span class="s1">y</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>

        <span class="s1">norm_solution </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">])</span>
        <span class="s1">norm_model </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">])</span>
        <span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">:</span>
            <span class="s0"># XXX: This solver shows random behaviour. Sometimes it finds solutions</span>
            <span class="s0"># with norm_model &lt;= norm_solution! So we check conditionally.</span>
            <span class="s2">if not </span><span class="s3">(</span><span class="s1">norm_model </span><span class="s3">&gt; (</span><span class="s5">1 </span><span class="s3">+ </span><span class="s5">1e-12</span><span class="s3">) * </span><span class="s1">norm_solution</span><span class="s3">):</span>
                <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">)</span>
                <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-4</span><span class="s3">)</span>
        <span class="s2">elif </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">and </span><span class="s1">fit_intercept</span><span class="s3">:</span>
            <span class="s0"># Same as in test_glm_regression_unpenalized.</span>
            <span class="s0"># But it is not the minimum norm solution. Otherwise the norms would be</span>
            <span class="s0"># equal.</span>
            <span class="s2">assert </span><span class="s1">norm_model </span><span class="s3">&gt; (</span><span class="s5">1 </span><span class="s3">+ </span><span class="s5">1e-12</span><span class="s3">) * </span><span class="s1">norm_solution</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">rtol </span><span class="s3">= </span><span class="s5">1e-5 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;newton-cholesky&quot; </span><span class="s2">else </span><span class="s5">1e-4</span>
            <span class="s2">assert </span><span class="s1">model</span><span class="s3">.</span><span class="s1">intercept_ </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">intercept</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
            <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">model</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>


<span class="s2">def </span><span class="s1">test_sample_weights_validation</span><span class="s3">():</span>
    <span class="s6">&quot;&quot;&quot;Test the raised errors in the validation of sample_weight.&quot;&quot;&quot;</span>
    <span class="s0"># scalar value but not positive</span>
    <span class="s1">X </span><span class="s3">= [[</span><span class="s5">1</span><span class="s3">]]</span>
    <span class="s1">y </span><span class="s3">= [</span><span class="s5">1</span><span class="s3">]</span>
    <span class="s1">weights </span><span class="s3">= </span><span class="s5">0</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">_GeneralizedLinearRegressor</span><span class="s3">()</span>

    <span class="s0"># Positive weights are accepted</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>

    <span class="s0"># 2d array</span>
    <span class="s1">weights </span><span class="s3">= [[</span><span class="s5">0</span><span class="s3">]]</span>
    <span class="s2">with </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">raises</span><span class="s3">(</span><span class="s1">ValueError</span><span class="s3">, </span><span class="s1">match</span><span class="s3">=</span><span class="s4">&quot;must be 1D array or scalar&quot;</span><span class="s3">):</span>
        <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">weights</span><span class="s3">)</span>

    <span class="s0"># 1d but wrong length</span>
    <span class="s1">weights </span><span class="s3">= [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">0</span><span class="s3">]</span>
    <span class="s1">msg </span><span class="s3">= </span><span class="s4">r&quot;sample_weight.shape == \(2,\), expected \(1,\)!&quot;</span>
    <span class="s2">with </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">raises</span><span class="s3">(</span><span class="s1">ValueError</span><span class="s3">, </span><span class="s1">match</span><span class="s3">=</span><span class="s1">msg</span><span class="s3">):</span>
        <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">weights</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span>
    <span class="s4">&quot;glm&quot;</span><span class="s3">,</span>
    <span class="s3">[</span>
        <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">3</span><span class="s3">),</span>
        <span class="s1">PoissonRegressor</span><span class="s3">(),</span>
        <span class="s1">GammaRegressor</span><span class="s3">(),</span>
        <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">1.5</span><span class="s3">),</span>
    <span class="s3">],</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">test_glm_wrong_y_range</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">):</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">])</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[</span><span class="s5">1</span><span class="s3">], [</span><span class="s5">1</span><span class="s3">]])</span>
    <span class="s1">msg </span><span class="s3">= </span><span class="s4">r&quot;Some value\(s\) of y are out of the valid range of the loss&quot;</span>
    <span class="s2">with </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">raises</span><span class="s3">(</span><span class="s1">ValueError</span><span class="s3">, </span><span class="s1">match</span><span class="s3">=</span><span class="s1">msg</span><span class="s3">):</span>
        <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">False</span><span class="s3">, </span><span class="s2">True</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_glm_identity_regression</span><span class="s3">(</span><span class="s1">fit_intercept</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test GLM regression with identity link on a simple dataset.&quot;&quot;&quot;</span>
    <span class="s1">coef </span><span class="s3">= [</span><span class="s5">1.0</span><span class="s3">, </span><span class="s5">2.0</span><span class="s3">]</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">], [</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">]]).</span><span class="s1">T</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">)</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">_GeneralizedLinearRegressor</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">[:, </span><span class="s5">1</span><span class="s3">:], </span><span class="s1">y</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">[</span><span class="s5">1</span><span class="s3">:], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-10</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">[</span><span class="s5">0</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-10</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">False</span><span class="s3">, </span><span class="s2">True</span><span class="s3">])</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;alpha&quot;</span><span class="s3">, [</span><span class="s5">0.0</span><span class="s3">, </span><span class="s5">1.0</span><span class="s3">])</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span>
    <span class="s4">&quot;GLMEstimator&quot;</span><span class="s3">, [</span><span class="s1">_GeneralizedLinearRegressor</span><span class="s3">, </span><span class="s1">PoissonRegressor</span><span class="s3">, </span><span class="s1">GammaRegressor</span><span class="s3">]</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">test_glm_sample_weight_consistency</span><span class="s3">(</span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">GLMEstimator</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that the impact of sample_weight is consistent&quot;&quot;&quot;</span>
    <span class="s1">rng </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">random</span><span class="s3">.</span><span class="s1">RandomState</span><span class="s3">(</span><span class="s5">0</span><span class="s3">)</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s5">10</span><span class="s3">, </span><span class="s5">5</span>

    <span class="s1">X </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">rand</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">)</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">rand</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">)</span>
    <span class="s1">glm_params </span><span class="s3">= </span><span class="s1">dict</span><span class="s3">(</span><span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">)</span>

    <span class="s1">glm </span><span class="s3">= </span><span class="s1">GLMEstimator</span><span class="s3">(**</span><span class="s1">glm_params</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s1">coef </span><span class="s3">= </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">.</span><span class="s1">copy</span><span class="s3">()</span>

    <span class="s0"># sample_weight=np.ones(..) should be equivalent to sample_weight=None</span>
    <span class="s1">sample_weight </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">(</span><span class="s1">y</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">)</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sample_weight</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">)</span>

    <span class="s0"># sample_weight are normalized to 1 so, scaling them has no effect</span>
    <span class="s1">sample_weight </span><span class="s3">= </span><span class="s5">2 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">(</span><span class="s1">y</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">)</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sample_weight</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">)</span>

    <span class="s0"># setting one element of sample_weight to 0 is equivalent to removing</span>
    <span class="s0"># the corresponding sample</span>
    <span class="s1">sample_weight </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">(</span><span class="s1">y</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">)</span>
    <span class="s1">sample_weight</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">] = </span><span class="s5">0</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sample_weight</span><span class="s3">)</span>
    <span class="s1">coef1 </span><span class="s3">= </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">.</span><span class="s1">copy</span><span class="s3">()</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">y</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">])</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef1</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-12</span><span class="s3">)</span>

    <span class="s0"># check that multiplying sample_weight by 2 is equivalent</span>
    <span class="s0"># to repeating corresponding samples twice</span>
    <span class="s1">X2 </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">([</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X</span><span class="s3">[: </span><span class="s1">n_samples </span><span class="s3">// </span><span class="s5">2</span><span class="s3">]], </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">0</span><span class="s3">)</span>
    <span class="s1">y2 </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">concatenate</span><span class="s3">([</span><span class="s1">y</span><span class="s3">, </span><span class="s1">y</span><span class="s3">[: </span><span class="s1">n_samples </span><span class="s3">// </span><span class="s5">2</span><span class="s3">]])</span>
    <span class="s1">sample_weight_1 </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">(</span><span class="s1">len</span><span class="s3">(</span><span class="s1">y</span><span class="s3">))</span>
    <span class="s1">sample_weight_1</span><span class="s3">[: </span><span class="s1">n_samples </span><span class="s3">// </span><span class="s5">2</span><span class="s3">] = </span><span class="s5">2</span>

    <span class="s1">glm1 </span><span class="s3">= </span><span class="s1">GLMEstimator</span><span class="s3">(**</span><span class="s1">glm_params</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sample_weight_1</span><span class="s3">)</span>

    <span class="s1">glm2 </span><span class="s3">= </span><span class="s1">GLMEstimator</span><span class="s3">(**</span><span class="s1">glm_params</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X2</span><span class="s3">, </span><span class="s1">y2</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">glm2</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span>
    <span class="s4">&quot;estimator&quot;</span><span class="s3">,</span>
    <span class="s3">[</span>
        <span class="s1">PoissonRegressor</span><span class="s3">(),</span>
        <span class="s1">GammaRegressor</span><span class="s3">(),</span>
        <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">3.0</span><span class="s3">),</span>
        <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">link</span><span class="s3">=</span><span class="s4">&quot;log&quot;</span><span class="s3">),</span>
        <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">1.5</span><span class="s3">),</span>
        <span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">4.5</span><span class="s3">),</span>
    <span class="s3">],</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">test_glm_log_regression</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">estimator</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test GLM regression with log link on a simple dataset.&quot;&quot;&quot;</span>
    <span class="s1">coef </span><span class="s3">= [</span><span class="s5">0.2</span><span class="s3">, -</span><span class="s5">0.1</span><span class="s3">]</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">4</span><span class="s3">], [</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">]]).</span><span class="s1">T</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">exp</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">))</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">clone</span><span class="s3">(</span><span class="s1">estimator</span><span class="s3">).</span><span class="s1">set_params</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-8</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s2">if </span><span class="s1">fit_intercept</span><span class="s3">:</span>
        <span class="s1">res </span><span class="s3">= </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">[:, :-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">y</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">res</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-6</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">res</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-6</span><span class="s3">)</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">res </span><span class="s3">= </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
        <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">res</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">coef</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">2e-6</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, </span><span class="s1">SOLVERS</span><span class="s3">)</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_warm_start</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">global_random_seed</span><span class="s3">):</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features </span><span class="s3">= </span><span class="s5">100</span><span class="s3">, </span><span class="s5">10</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">= </span><span class="s1">make_regression</span><span class="s3">(</span>
        <span class="s1">n_samples</span><span class="s3">=</span><span class="s1">n_samples</span><span class="s3">,</span>
        <span class="s1">n_features</span><span class="s3">=</span><span class="s1">n_features</span><span class="s3">,</span>
        <span class="s1">n_informative</span><span class="s3">=</span><span class="s1">n_features </span><span class="s3">- </span><span class="s5">2</span><span class="s3">,</span>
        <span class="s1">bias</span><span class="s3">=</span><span class="s1">fit_intercept </span><span class="s3">* </span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">noise</span><span class="s3">=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">=</span><span class="s1">global_random_seed</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">(</span><span class="s1">y</span><span class="s3">)  </span><span class="s0"># Poisson requires non-negative targets.</span>
    <span class="s1">alpha </span><span class="s3">= </span><span class="s5">1</span>
    <span class="s1">params </span><span class="s3">= {</span>
        <span class="s4">&quot;solver&quot;</span><span class="s3">: </span><span class="s1">solver</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s3">: </span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s3">: </span><span class="s5">1e-10</span><span class="s3">,</span>
    <span class="s3">}</span>

    <span class="s1">glm1 </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span><span class="s1">warm_start</span><span class="s3">=</span><span class="s2">False</span><span class="s3">, </span><span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">, **</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s1">glm1</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s1">glm2 </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span><span class="s1">warm_start</span><span class="s3">=</span><span class="s2">True</span><span class="s3">, </span><span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">, **</span><span class="s1">params</span><span class="s3">)</span>
    <span class="s0"># As we intentionally set max_iter=1 such that the solver should raise a</span>
    <span class="s0"># ConvergenceWarning.</span>
    <span class="s2">with </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">warns</span><span class="s3">(</span><span class="s1">ConvergenceWarning</span><span class="s3">):</span>
        <span class="s1">glm2</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s1">linear_loss </span><span class="s3">= </span><span class="s1">LinearModelLoss</span><span class="s3">(</span>
        <span class="s1">base_loss</span><span class="s3">=</span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">_get_loss</span><span class="s3">(),</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">sw </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">full_like</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">fill_value</span><span class="s3">=</span><span class="s5">1 </span><span class="s3">/ </span><span class="s1">n_samples</span><span class="s3">)</span>

    <span class="s1">objective_glm1 </span><span class="s3">= </span><span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">loss</span><span class="s3">(</span>
        <span class="s1">coef</span><span class="s3">=</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">] </span><span class="s2">if </span><span class="s1">fit_intercept </span><span class="s2">else </span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">objective_glm2 </span><span class="s3">= </span><span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">loss</span><span class="s3">(</span>
        <span class="s1">coef</span><span class="s3">=</span><span class="s1">np</span><span class="s3">.</span><span class="s1">r_</span><span class="s3">[</span><span class="s1">glm2</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">glm2</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">] </span><span class="s2">if </span><span class="s1">fit_intercept </span><span class="s2">else </span><span class="s1">glm2</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">,</span>
        <span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">,</span>
        <span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">,</span>
        <span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">objective_glm1 </span><span class="s3">&lt; </span><span class="s1">objective_glm2</span>

    <span class="s1">glm2</span><span class="s3">.</span><span class="s1">set_params</span><span class="s3">(</span><span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1000</span><span class="s3">)</span>
    <span class="s1">glm2</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s0"># The two models are not exactly identical since the lbfgs solver</span>
    <span class="s0"># computes the approximate hessian from previous iterations, which</span>
    <span class="s0"># will not be strictly identical in the case of a warm start.</span>
    <span class="s1">rtol </span><span class="s3">= </span><span class="s5">2e-4 </span><span class="s2">if </span><span class="s1">solver </span><span class="s3">== </span><span class="s4">&quot;lbfgs&quot; </span><span class="s2">else </span><span class="s5">1e-9</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">glm2</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm1</span><span class="s3">.</span><span class="s1">score</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">), </span><span class="s1">glm2</span><span class="s3">.</span><span class="s1">score</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">), </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-5</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;n_samples, n_features&quot;</span><span class="s3">, [(</span><span class="s5">100</span><span class="s3">, </span><span class="s5">10</span><span class="s3">), (</span><span class="s5">10</span><span class="s3">, </span><span class="s5">100</span><span class="s3">)])</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;fit_intercept&quot;</span><span class="s3">, [</span><span class="s2">True</span><span class="s3">, </span><span class="s2">False</span><span class="s3">])</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s3">, [</span><span class="s2">None</span><span class="s3">, </span><span class="s2">True</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_normal_ridge_comparison</span><span class="s3">(</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">fit_intercept</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">request</span>
<span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Compare with Ridge regression for Normal distributions.&quot;&quot;&quot;</span>
    <span class="s1">test_size </span><span class="s3">= </span><span class="s5">10</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">= </span><span class="s1">make_regression</span><span class="s3">(</span>
        <span class="s1">n_samples</span><span class="s3">=</span><span class="s1">n_samples </span><span class="s3">+ </span><span class="s1">test_size</span><span class="s3">,</span>
        <span class="s1">n_features</span><span class="s3">=</span><span class="s1">n_features</span><span class="s3">,</span>
        <span class="s1">n_informative</span><span class="s3">=</span><span class="s1">n_features </span><span class="s3">- </span><span class="s5">2</span><span class="s3">,</span>
        <span class="s1">noise</span><span class="s3">=</span><span class="s5">0.5</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">=</span><span class="s5">42</span><span class="s3">,</span>
    <span class="s3">)</span>

    <span class="s2">if </span><span class="s1">n_samples </span><span class="s3">&gt; </span><span class="s1">n_features</span><span class="s3">:</span>
        <span class="s1">ridge_params </span><span class="s3">= {</span><span class="s4">&quot;solver&quot;</span><span class="s3">: </span><span class="s4">&quot;svd&quot;</span><span class="s3">}</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">ridge_params </span><span class="s3">= {</span><span class="s4">&quot;solver&quot;</span><span class="s3">: </span><span class="s4">&quot;saga&quot;</span><span class="s3">, </span><span class="s4">&quot;max_iter&quot;</span><span class="s3">: </span><span class="s5">1000000</span><span class="s3">, </span><span class="s4">&quot;tol&quot;</span><span class="s3">: </span><span class="s5">1e-7</span><span class="s3">}</span>

    <span class="s3">(</span>
        <span class="s1">X_train</span><span class="s3">,</span>
        <span class="s1">X_test</span><span class="s3">,</span>
        <span class="s1">y_train</span><span class="s3">,</span>
        <span class="s1">y_test</span><span class="s3">,</span>
    <span class="s3">) = </span><span class="s1">train_test_split</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">test_size</span><span class="s3">=</span><span class="s1">test_size</span><span class="s3">, </span><span class="s1">random_state</span><span class="s3">=</span><span class="s5">0</span><span class="s3">)</span>

    <span class="s1">alpha </span><span class="s3">= </span><span class="s5">1.0</span>
    <span class="s2">if </span><span class="s1">sample_weight </span><span class="s2">is None</span><span class="s3">:</span>
        <span class="s1">sw_train </span><span class="s3">= </span><span class="s2">None</span>
        <span class="s1">alpha_ridge </span><span class="s3">= </span><span class="s1">alpha </span><span class="s3">* </span><span class="s1">n_samples</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">sw_train </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">random</span><span class="s3">.</span><span class="s1">RandomState</span><span class="s3">(</span><span class="s5">0</span><span class="s3">).</span><span class="s1">rand</span><span class="s3">(</span><span class="s1">len</span><span class="s3">(</span><span class="s1">y_train</span><span class="s3">))</span>
        <span class="s1">alpha_ridge </span><span class="s3">= </span><span class="s1">alpha </span><span class="s3">* </span><span class="s1">sw_train</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">()</span>

    <span class="s0"># GLM has 1/(2*n) * Loss + 1/2*L2, Ridge has Loss + L2</span>
    <span class="s1">ridge </span><span class="s3">= </span><span class="s1">Ridge</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha_ridge</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">=</span><span class="s5">42</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s3">**</span><span class="s1">ridge_params</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">ridge</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw_train</span><span class="s3">)</span>

    <span class="s1">glm </span><span class="s3">= </span><span class="s1">_GeneralizedLinearRegressor</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s1">alpha</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s1">fit_intercept</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">300</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-5</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s1">sw_train</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">.</span><span class="s1">shape </span><span class="s3">== (</span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">],)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">ridge</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, </span><span class="s1">atol</span><span class="s3">=</span><span class="s5">5e-5</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">ridge</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-5</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_train</span><span class="s3">), </span><span class="s1">ridge</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_train</span><span class="s3">), </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">2e-4</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_test</span><span class="s3">), </span><span class="s1">ridge</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_test</span><span class="s3">), </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">2e-4</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;solver&quot;</span><span class="s3">, [</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_poisson_glmnet</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Compare Poisson regression with L2 regularization and LogLink to glmnet&quot;&quot;&quot;</span>
    <span class="s0"># library(&quot;glmnet&quot;)</span>
    <span class="s0"># options(digits=10)</span>
    <span class="s0"># df &lt;- data.frame(a=c(-2,-1,1,2), b=c(0,0,1,1), y=c(0,1,1,2))</span>
    <span class="s0"># x &lt;- data.matrix(df[,c(&quot;a&quot;, &quot;b&quot;)])</span>
    <span class="s0"># y &lt;- df$y</span>
    <span class="s0"># fit &lt;- glmnet(x=x, y=y, alpha=0, intercept=T, family=&quot;poisson&quot;,</span>
    <span class="s0">#               standardize=F, thresh=1e-10, nlambda=10000)</span>
    <span class="s0"># coef(fit, s=1)</span>
    <span class="s0"># (Intercept) -0.12889386979</span>
    <span class="s0"># a            0.29019207995</span>
    <span class="s0"># b            0.03741173122</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[-</span><span class="s5">2</span><span class="s3">, -</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">], [</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">]]).</span><span class="s1">T</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">])</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">fit_intercept</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-7</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s5">300</span><span class="s3">,</span>
        <span class="s1">solver</span><span class="s3">=</span><span class="s1">solver</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">glm</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">intercept_</span><span class="s3">, -</span><span class="s5">0.12889386979</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-5</span><span class="s3">)</span>
    <span class="s1">assert_allclose</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">coef_</span><span class="s3">, [</span><span class="s5">0.29019207995</span><span class="s3">, </span><span class="s5">0.03741173122</span><span class="s3">], </span><span class="s1">rtol</span><span class="s3">=</span><span class="s5">1e-5</span><span class="s3">)</span>


<span class="s2">def </span><span class="s1">test_convergence_warning</span><span class="s3">(</span><span class="s1">regression_data</span><span class="s3">):</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">= </span><span class="s1">regression_data</span>

    <span class="s1">est </span><span class="s3">= </span><span class="s1">_GeneralizedLinearRegressor</span><span class="s3">(</span><span class="s1">max_iter</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">=</span><span class="s5">1e-20</span><span class="s3">)</span>
    <span class="s2">with </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">warns</span><span class="s3">(</span><span class="s1">ConvergenceWarning</span><span class="s3">):</span>
        <span class="s1">est</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span>
    <span class="s4">&quot;name, link_class&quot;</span><span class="s3">, [(</span><span class="s4">&quot;identity&quot;</span><span class="s3">, </span><span class="s1">IdentityLink</span><span class="s3">), (</span><span class="s4">&quot;log&quot;</span><span class="s3">, </span><span class="s1">LogLink</span><span class="s3">)]</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">test_tweedie_link_argument</span><span class="s3">(</span><span class="s1">name</span><span class="s3">, </span><span class="s1">link_class</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test GLM link argument set as string.&quot;&quot;&quot;</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">0.5</span><span class="s3">])  </span><span class="s0"># in range of all distributions</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[</span><span class="s5">1</span><span class="s3">], [</span><span class="s5">2</span><span class="s3">]])</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">link</span><span class="s3">=</span><span class="s1">name</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">_base_loss</span><span class="s3">.</span><span class="s1">link</span><span class="s3">, </span><span class="s1">link_class</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span>
    <span class="s4">&quot;power, expected_link_class&quot;</span><span class="s3">,</span>
    <span class="s3">[</span>
        <span class="s3">(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">IdentityLink</span><span class="s3">),  </span><span class="s0"># normal</span>
        <span class="s3">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">LogLink</span><span class="s3">),  </span><span class="s0"># poisson</span>
        <span class="s3">(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">LogLink</span><span class="s3">),  </span><span class="s0"># gamma</span>
        <span class="s3">(</span><span class="s5">3</span><span class="s3">, </span><span class="s1">LogLink</span><span class="s3">),  </span><span class="s0"># inverse-gaussian</span>
    <span class="s3">],</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">test_tweedie_link_auto</span><span class="s3">(</span><span class="s1">power</span><span class="s3">, </span><span class="s1">expected_link_class</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that link='auto' delivers the expected link function&quot;&quot;&quot;</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">0.1</span><span class="s3">, </span><span class="s5">0.5</span><span class="s3">])  </span><span class="s0"># in range of all distributions</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[</span><span class="s5">1</span><span class="s3">], [</span><span class="s5">2</span><span class="s3">]])</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">link</span><span class="s3">=</span><span class="s4">&quot;auto&quot;</span><span class="s3">, </span><span class="s1">power</span><span class="s3">=</span><span class="s1">power</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">glm</span><span class="s3">.</span><span class="s1">_base_loss</span><span class="s3">.</span><span class="s1">link</span><span class="s3">, </span><span class="s1">expected_link_class</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;power&quot;</span><span class="s3">, [</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1.5</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s3">])</span>
<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;link&quot;</span><span class="s3">, [</span><span class="s4">&quot;log&quot;</span><span class="s3">, </span><span class="s4">&quot;identity&quot;</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_tweedie_score</span><span class="s3">(</span><span class="s1">regression_data</span><span class="s3">, </span><span class="s1">power</span><span class="s3">, </span><span class="s1">link</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test that GLM score equals d2_tweedie_score for Tweedie losses.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">y </span><span class="s3">= </span><span class="s1">regression_data</span>
    <span class="s0"># make y positive</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">(</span><span class="s1">y</span><span class="s3">) + </span><span class="s5">1.0</span>
    <span class="s1">glm </span><span class="s3">= </span><span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s1">power</span><span class="s3">, </span><span class="s1">link</span><span class="s3">=</span><span class="s1">link</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">score</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">) == </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span>
        <span class="s1">d2_tweedie_score</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">glm</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X</span><span class="s3">), </span><span class="s1">power</span><span class="s3">=</span><span class="s1">power</span><span class="s3">)</span>
    <span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span>
    <span class="s4">&quot;estimator, value&quot;</span><span class="s3">,</span>
    <span class="s3">[</span>
        <span class="s3">(</span><span class="s1">PoissonRegressor</span><span class="s3">(), </span><span class="s2">True</span><span class="s3">),</span>
        <span class="s3">(</span><span class="s1">GammaRegressor</span><span class="s3">(), </span><span class="s2">True</span><span class="s3">),</span>
        <span class="s3">(</span><span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">1.5</span><span class="s3">), </span><span class="s2">True</span><span class="s3">),</span>
        <span class="s3">(</span><span class="s1">TweedieRegressor</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">0</span><span class="s3">), </span><span class="s2">False</span><span class="s3">),</span>
    <span class="s3">],</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">test_tags</span><span class="s3">(</span><span class="s1">estimator</span><span class="s3">, </span><span class="s1">value</span><span class="s3">):</span>
    <span class="s2">assert </span><span class="s1">estimator</span><span class="s3">.</span><span class="s1">_get_tags</span><span class="s3">()[</span><span class="s4">&quot;requires_positive_y&quot;</span><span class="s3">] </span><span class="s2">is </span><span class="s1">value</span>


<span class="s2">def </span><span class="s1">test_linalg_warning_with_newton_solver</span><span class="s3">(</span><span class="s1">global_random_seed</span><span class="s3">):</span>
    <span class="s1">newton_solver </span><span class="s3">= </span><span class="s4">&quot;newton-cholesky&quot;</span>
    <span class="s1">rng </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">random</span><span class="s3">.</span><span class="s1">RandomState</span><span class="s3">(</span><span class="s1">global_random_seed</span><span class="s3">)</span>
    <span class="s0"># Use at least 20 samples to reduce the likelihood of getting a degenerate</span>
    <span class="s0"># dataset for any global_random_seed.</span>
    <span class="s1">X_orig </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">normal</span><span class="s3">(</span><span class="s1">size</span><span class="s3">=(</span><span class="s5">20</span><span class="s3">, </span><span class="s5">3</span><span class="s3">))</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">poisson</span><span class="s3">(</span>
        <span class="s1">np</span><span class="s3">.</span><span class="s1">exp</span><span class="s3">(</span><span class="s1">X_orig </span><span class="s3">@ </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones</span><span class="s3">(</span><span class="s1">X_orig</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">])), </span><span class="s1">size</span><span class="s3">=</span><span class="s1">X_orig</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>
    <span class="s3">).</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float64</span><span class="s3">)</span>

    <span class="s0"># Collinear variation of the same input features.</span>
    <span class="s1">X_collinear </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">hstack</span><span class="s3">([</span><span class="s1">X_orig</span><span class="s3">] * </span><span class="s5">10</span><span class="s3">)</span>

    <span class="s0"># Let's consider the deviance of a constant baseline on this problem.</span>
    <span class="s1">baseline_pred </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">full_like</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">y</span><span class="s3">.</span><span class="s1">mean</span><span class="s3">())</span>
    <span class="s1">constant_model_deviance </span><span class="s3">= </span><span class="s1">mean_poisson_deviance</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">baseline_pred</span><span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">constant_model_deviance </span><span class="s3">&gt; </span><span class="s5">1.0</span>

    <span class="s0"># No warning raised on well-conditioned design, even without regularization.</span>
    <span class="s1">tol </span><span class="s3">= </span><span class="s5">1e-10</span>
    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;error&quot;</span><span class="s3">)</span>
        <span class="s1">reg </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">=</span><span class="s1">newton_solver</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">=</span><span class="s5">0.0</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">=</span><span class="s1">tol</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X_orig</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s1">original_newton_deviance </span><span class="s3">= </span><span class="s1">mean_poisson_deviance</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">reg</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_orig</span><span class="s3">))</span>

    <span class="s0"># On this dataset, we should have enough data points to not make it</span>
    <span class="s0"># possible to get a near zero deviance (for the any of the admissible</span>
    <span class="s0"># random seeds). This will make it easier to interpret meaning of rtol in</span>
    <span class="s0"># the subsequent assertions:</span>
    <span class="s2">assert </span><span class="s1">original_newton_deviance </span><span class="s3">&gt; </span><span class="s5">0.2</span>

    <span class="s0"># We check that the model could successfully fit information in X_orig to</span>
    <span class="s0"># improve upon the constant baseline by a large margin (when evaluated on</span>
    <span class="s0"># the traing set).</span>
    <span class="s2">assert </span><span class="s1">constant_model_deviance </span><span class="s3">- </span><span class="s1">original_newton_deviance </span><span class="s3">&gt; </span><span class="s5">0.1</span>

    <span class="s0"># LBFGS is robust to a collinear design because its approximation of the</span>
    <span class="s0"># Hessian is Symmeric Positive Definite by construction. Let's record its</span>
    <span class="s0"># solution</span>
    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;error&quot;</span><span class="s3">)</span>
        <span class="s1">reg </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">=</span><span class="s5">0.0</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">=</span><span class="s1">tol</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X_collinear</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>
    <span class="s1">collinear_lbfgs_deviance </span><span class="s3">= </span><span class="s1">mean_poisson_deviance</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">reg</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_collinear</span><span class="s3">))</span>

    <span class="s0"># The LBFGS solution on the collinear is expected to reach a comparable</span>
    <span class="s0"># solution to the Newton solution on the original data.</span>
    <span class="s1">rtol </span><span class="s3">= </span><span class="s5">1e-6</span>
    <span class="s2">assert </span><span class="s1">collinear_lbfgs_deviance </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span><span class="s1">original_newton_deviance</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span><span class="s3">)</span>

    <span class="s0"># Fitting a Newton solver on the collinear version of the training data</span>
    <span class="s0"># without regularization should raise an informative warning and fallback</span>
    <span class="s0"># to the LBFGS solver.</span>
    <span class="s1">msg </span><span class="s3">= (</span>
        <span class="s4">&quot;The inner solver of .*Newton.*Solver stumbled upon a singular or very &quot;</span>
        <span class="s4">&quot;ill-conditioned Hessian matrix&quot;</span>
    <span class="s3">)</span>
    <span class="s2">with </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">warns</span><span class="s3">(</span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">LinAlgWarning</span><span class="s3">, </span><span class="s1">match</span><span class="s3">=</span><span class="s1">msg</span><span class="s3">):</span>
        <span class="s1">reg </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">=</span><span class="s1">newton_solver</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">=</span><span class="s5">0.0</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">=</span><span class="s1">tol</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span>
            <span class="s1">X_collinear</span><span class="s3">, </span><span class="s1">y</span>
        <span class="s3">)</span>
    <span class="s0"># As a result we should still automatically converge to a good solution.</span>
    <span class="s1">collinear_newton_deviance </span><span class="s3">= </span><span class="s1">mean_poisson_deviance</span><span class="s3">(</span><span class="s1">y</span><span class="s3">, </span><span class="s1">reg</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_collinear</span><span class="s3">))</span>
    <span class="s2">assert </span><span class="s1">collinear_newton_deviance </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span>
        <span class="s1">original_newton_deviance</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span>
    <span class="s3">)</span>

    <span class="s0"># Increasing the regularization slightly should make the problem go away:</span>
    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;error&quot;</span><span class="s3">, </span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">LinAlgWarning</span><span class="s3">)</span>
        <span class="s1">reg </span><span class="s3">= </span><span class="s1">PoissonRegressor</span><span class="s3">(</span><span class="s1">solver</span><span class="s3">=</span><span class="s1">newton_solver</span><span class="s3">, </span><span class="s1">alpha</span><span class="s3">=</span><span class="s5">1e-10</span><span class="s3">).</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X_collinear</span><span class="s3">, </span><span class="s1">y</span><span class="s3">)</span>

    <span class="s0"># The slightly penalized model on the collinear data should be close enough</span>
    <span class="s0"># to the unpenalized model on the original data.</span>
    <span class="s1">penalized_collinear_newton_deviance </span><span class="s3">= </span><span class="s1">mean_poisson_deviance</span><span class="s3">(</span>
        <span class="s1">y</span><span class="s3">, </span><span class="s1">reg</span><span class="s3">.</span><span class="s1">predict</span><span class="s3">(</span><span class="s1">X_collinear</span><span class="s3">)</span>
    <span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">penalized_collinear_newton_deviance </span><span class="s3">== </span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">approx</span><span class="s3">(</span>
        <span class="s1">original_newton_deviance</span><span class="s3">, </span><span class="s1">rel</span><span class="s3">=</span><span class="s1">rtol</span>
    <span class="s3">)</span>


<span class="s3">@</span><span class="s1">pytest</span><span class="s3">.</span><span class="s1">mark</span><span class="s3">.</span><span class="s1">parametrize</span><span class="s3">(</span><span class="s4">&quot;verbose&quot;</span><span class="s3">, [</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">])</span>
<span class="s2">def </span><span class="s1">test_newton_solver_verbosity</span><span class="s3">(</span><span class="s1">capsys</span><span class="s3">, </span><span class="s1">verbose</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Test the std output of verbose newton solvers.&quot;&quot;&quot;</span>
    <span class="s1">y </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">], </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">)</span>
    <span class="s1">X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([[</span><span class="s5">1.0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">], [</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">]], </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">)</span>
    <span class="s1">linear_loss </span><span class="s3">= </span><span class="s1">LinearModelLoss</span><span class="s3">(</span><span class="s1">base_loss</span><span class="s3">=</span><span class="s1">HalfPoissonLoss</span><span class="s3">(), </span><span class="s1">fit_intercept</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">sol </span><span class="s3">= </span><span class="s1">NewtonCholeskySolver</span><span class="s3">(</span>
        <span class="s1">coef</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">init_zero_coef</span><span class="s3">(</span><span class="s1">X</span><span class="s3">),</span>
        <span class="s1">linear_loss</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">=</span><span class="s1">verbose</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">solve</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s2">None</span><span class="s3">)  </span><span class="s0"># returns array([0., 0.69314758])</span>
    <span class="s1">captured </span><span class="s3">= </span><span class="s1">capsys</span><span class="s3">.</span><span class="s1">readouterr</span><span class="s3">()</span>

    <span class="s2">if </span><span class="s1">verbose </span><span class="s3">== </span><span class="s5">0</span><span class="s3">:</span>
        <span class="s2">assert </span><span class="s1">captured</span><span class="s3">.</span><span class="s1">out </span><span class="s3">== </span><span class="s4">&quot;&quot;</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">msg </span><span class="s3">= [</span>
            <span class="s4">&quot;Newton iter=1&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;Check Convergence&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;1. max |gradient|&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;2. Newton decrement&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;Solver did converge at loss = &quot;</span><span class="s3">,</span>
        <span class="s3">]</span>
        <span class="s2">for </span><span class="s1">m </span><span class="s2">in </span><span class="s1">msg</span><span class="s3">:</span>
            <span class="s2">assert </span><span class="s1">m </span><span class="s2">in </span><span class="s1">captured</span><span class="s3">.</span><span class="s1">out</span>

    <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">2</span><span class="s3">:</span>
        <span class="s1">msg </span><span class="s3">= [</span><span class="s4">&quot;Backtracking Line Search&quot;</span><span class="s3">, </span><span class="s4">&quot;line search iteration=&quot;</span><span class="s3">]</span>
        <span class="s2">for </span><span class="s1">m </span><span class="s2">in </span><span class="s1">msg</span><span class="s3">:</span>
            <span class="s2">assert </span><span class="s1">m </span><span class="s2">in </span><span class="s1">captured</span><span class="s3">.</span><span class="s1">out</span>

    <span class="s0"># Set the Newton solver to a state with a completely wrong Newton step.</span>
    <span class="s1">sol </span><span class="s3">= </span><span class="s1">NewtonCholeskySolver</span><span class="s3">(</span>
        <span class="s1">coef</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">init_zero_coef</span><span class="s3">(</span><span class="s1">X</span><span class="s3">),</span>
        <span class="s1">linear_loss</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">=</span><span class="s1">verbose</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">setup</span><span class="s3">(</span><span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">iteration </span><span class="s3">= </span><span class="s5">1</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">update_gradient_hessian</span><span class="s3">(</span><span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">coef_newton </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">1.0</span><span class="s3">, </span><span class="s5">0</span><span class="s3">])</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">gradient_times_newton </span><span class="s3">= </span><span class="s1">sol</span><span class="s3">.</span><span class="s1">gradient </span><span class="s3">@ </span><span class="s1">sol</span><span class="s3">.</span><span class="s1">coef_newton</span>
    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">sol</span><span class="s3">.</span><span class="s1">line_search</span><span class="s3">(</span><span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
        <span class="s1">captured </span><span class="s3">= </span><span class="s1">capsys</span><span class="s3">.</span><span class="s1">readouterr</span><span class="s3">()</span>
    <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">1</span><span class="s3">:</span>
        <span class="s2">assert </span><span class="s3">(</span>
            <span class="s4">&quot;Line search did not converge and resorts to lbfgs instead.&quot; </span><span class="s2">in </span><span class="s1">captured</span><span class="s3">.</span><span class="s1">out</span>
        <span class="s3">)</span>

    <span class="s0"># Set the Newton solver to a state with bad Newton step such that the loss</span>
    <span class="s0"># improvement in line search is tiny.</span>
    <span class="s1">sol </span><span class="s3">= </span><span class="s1">NewtonCholeskySolver</span><span class="s3">(</span>
        <span class="s1">coef</span><span class="s3">=</span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">1e-12</span><span class="s3">, </span><span class="s5">0.69314758</span><span class="s3">]),</span>
        <span class="s1">linear_loss</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">=</span><span class="s1">verbose</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">setup</span><span class="s3">(</span><span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">iteration </span><span class="s3">= </span><span class="s5">1</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">update_gradient_hessian</span><span class="s3">(</span><span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">coef_newton </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">([</span><span class="s5">1e-6</span><span class="s3">, </span><span class="s5">0</span><span class="s3">])</span>
    <span class="s1">sol</span><span class="s3">.</span><span class="s1">gradient_times_newton </span><span class="s3">= </span><span class="s1">sol</span><span class="s3">.</span><span class="s1">gradient </span><span class="s3">@ </span><span class="s1">sol</span><span class="s3">.</span><span class="s1">coef_newton</span>
    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">sol</span><span class="s3">.</span><span class="s1">line_search</span><span class="s3">(</span><span class="s1">X</span><span class="s3">=</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">=</span><span class="s2">None</span><span class="s3">)</span>
        <span class="s1">captured </span><span class="s3">= </span><span class="s1">capsys</span><span class="s3">.</span><span class="s1">readouterr</span><span class="s3">()</span>
    <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">2</span><span class="s3">:</span>
        <span class="s1">msg </span><span class="s3">= [</span>
            <span class="s4">&quot;line search iteration=&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;check loss improvement &lt;= armijo term:&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;check loss |improvement| &lt;= eps * |loss_old|:&quot;</span><span class="s3">,</span>
            <span class="s4">&quot;check sum(|gradient|) &lt; sum(|gradient_old|):&quot;</span><span class="s3">,</span>
        <span class="s3">]</span>
        <span class="s2">for </span><span class="s1">m </span><span class="s2">in </span><span class="s1">msg</span><span class="s3">:</span>
            <span class="s2">assert </span><span class="s1">m </span><span class="s2">in </span><span class="s1">captured</span><span class="s3">.</span><span class="s1">out</span>

    <span class="s0"># Test for a case with negative hessian. We badly initialize coef for a Tweedie</span>
    <span class="s0"># loss with non-canonical link, e.g. Inverse Gaussian deviance with a log link.</span>
    <span class="s1">linear_loss </span><span class="s3">= </span><span class="s1">LinearModelLoss</span><span class="s3">(</span>
        <span class="s1">base_loss</span><span class="s3">=</span><span class="s1">HalfTweedieLoss</span><span class="s3">(</span><span class="s1">power</span><span class="s3">=</span><span class="s5">3</span><span class="s3">), </span><span class="s1">fit_intercept</span><span class="s3">=</span><span class="s2">False</span>
    <span class="s3">)</span>
    <span class="s1">sol </span><span class="s3">= </span><span class="s1">NewtonCholeskySolver</span><span class="s3">(</span>
        <span class="s1">coef</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">.</span><span class="s1">init_zero_coef</span><span class="s3">(</span><span class="s1">X</span><span class="s3">) + </span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">linear_loss</span><span class="s3">=</span><span class="s1">linear_loss</span><span class="s3">,</span>
        <span class="s1">l2_reg_strength</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">=</span><span class="s1">verbose</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s2">with </span><span class="s1">warnings</span><span class="s3">.</span><span class="s1">catch_warnings</span><span class="s3">():</span>
        <span class="s1">warnings</span><span class="s3">.</span><span class="s1">simplefilter</span><span class="s3">(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">ConvergenceWarning</span><span class="s3">)</span>
        <span class="s1">sol</span><span class="s3">.</span><span class="s1">solve</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s2">None</span><span class="s3">)</span>
    <span class="s1">captured </span><span class="s3">= </span><span class="s1">capsys</span><span class="s3">.</span><span class="s1">readouterr</span><span class="s3">()</span>
    <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">1</span><span class="s3">:</span>
        <span class="s2">assert </span><span class="s3">(</span>
            <span class="s4">&quot;The inner solver detected a pointwise Hessian with many negative values&quot;</span>
            <span class="s4">&quot; and resorts to lbfgs instead.&quot; </span><span class="s2">in </span><span class="s1">captured</span><span class="s3">.</span><span class="s1">out</span>
        <span class="s3">)</span>
</pre>
</body>
</html>