<html>
<head>
<title>loss.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
loss.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
This module contains loss classes suitable for fitting. 
 
It is not part of the public API. 
Specific losses are used for regression, binary classification or multiclass 
classification. 
&quot;&quot;&quot;</span>

<span class="s2"># Goals:</span>
<span class="s2"># - Provide a common private module for loss functions/classes.</span>
<span class="s2"># - To be used in:</span>
<span class="s2">#   - LogisticRegression</span>
<span class="s2">#   - PoissonRegressor, GammaRegressor, TweedieRegressor</span>
<span class="s2">#   - HistGradientBoostingRegressor, HistGradientBoostingClassifier</span>
<span class="s2">#   - GradientBoostingRegressor, GradientBoostingClassifier</span>
<span class="s2">#   - SGDRegressor, SGDClassifier</span>
<span class="s2"># - Replace link module of GLMs.</span>

<span class="s3">import </span><span class="s1">numbers</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">special </span><span class="s3">import </span><span class="s1">xlogy</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">utils </span><span class="s3">import </span><span class="s1">check_scalar</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">stats </span><span class="s3">import </span><span class="s1">_weighted_percentile</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_loss </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">CyAbsoluteError</span><span class="s4">,</span>
    <span class="s1">CyExponentialLoss</span><span class="s4">,</span>
    <span class="s1">CyHalfBinomialLoss</span><span class="s4">,</span>
    <span class="s1">CyHalfGammaLoss</span><span class="s4">,</span>
    <span class="s1">CyHalfMultinomialLoss</span><span class="s4">,</span>
    <span class="s1">CyHalfPoissonLoss</span><span class="s4">,</span>
    <span class="s1">CyHalfSquaredError</span><span class="s4">,</span>
    <span class="s1">CyHalfTweedieLoss</span><span class="s4">,</span>
    <span class="s1">CyHalfTweedieLossIdentity</span><span class="s4">,</span>
    <span class="s1">CyHuberLoss</span><span class="s4">,</span>
    <span class="s1">CyPinballLoss</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">link </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">HalfLogitLink</span><span class="s4">,</span>
    <span class="s1">IdentityLink</span><span class="s4">,</span>
    <span class="s1">Interval</span><span class="s4">,</span>
    <span class="s1">LogitLink</span><span class="s4">,</span>
    <span class="s1">LogLink</span><span class="s4">,</span>
    <span class="s1">MultinomialLogit</span><span class="s4">,</span>
<span class="s4">)</span>


<span class="s2"># Note: The shape of raw_prediction for multiclass classifications are</span>
<span class="s2"># - GradientBoostingClassifier: (n_samples, n_classes)</span>
<span class="s2"># - HistGradientBoostingClassifier: (n_classes, n_samples)</span>
<span class="s2">#</span>
<span class="s2"># Note: Instead of inheritance like</span>
<span class="s2">#</span>
<span class="s2">#    class BaseLoss(BaseLink, CyLossFunction):</span>
<span class="s2">#    ...</span>
<span class="s2">#</span>
<span class="s2">#    # Note: Naturally, we would inherit in the following order</span>
<span class="s2">#    #     class HalfSquaredError(IdentityLink, CyHalfSquaredError, BaseLoss)</span>
<span class="s2">#    #   But because of https://github.com/cython/cython/issues/4350 we set BaseLoss as</span>
<span class="s2">#    #   the last one. This, of course, changes the MRO.</span>
<span class="s2">#    class HalfSquaredError(IdentityLink, CyHalfSquaredError, BaseLoss):</span>
<span class="s2">#</span>
<span class="s2"># we use composition. This way we improve maintainability by avoiding the above</span>
<span class="s2"># mentioned Cython edge case and have easier to understand code (which method calls</span>
<span class="s2"># which code).</span>
<span class="s3">class </span><span class="s1">BaseLoss</span><span class="s4">:</span>
    <span class="s0">&quot;&quot;&quot;Base class for a loss function of 1-dimensional targets. 
 
    Conventions: 
 
        - y_true.shape = sample_weight.shape = (n_samples,) 
        - y_pred.shape = raw_prediction.shape = (n_samples,) 
        - If is_multiclass is true (multiclass classification), then 
          y_pred.shape = raw_prediction.shape = (n_samples, n_classes) 
          Note that this corresponds to the return value of decision_function. 
 
    y_true, y_pred, sample_weight and raw_prediction must either be all float64 
    or all float32. 
    gradient and hessian must be either both float64 or both float32. 
 
    Note that y_pred = link.inverse(raw_prediction). 
 
    Specific loss classes can inherit specific link classes to satisfy 
    BaseLink's abstractmethods. 
 
    Parameters 
    ---------- 
    sample_weight : {None, ndarray} 
        If sample_weight is None, the hessian might be constant. 
    n_classes : {None, int} 
        The number of classes for classification, else None. 
 
    Attributes 
    ---------- 
    closs: CyLossFunction 
    link : BaseLink 
    interval_y_true : Interval 
        Valid interval for y_true 
    interval_y_pred : Interval 
        Valid Interval for y_pred 
    differentiable : bool 
        Indicates whether or not loss function is differentiable in 
        raw_prediction everywhere. 
    need_update_leaves_values : bool 
        Indicates whether decision trees in gradient boosting need to uptade 
        leave values after having been fit to the (negative) gradients. 
    approx_hessian : bool 
        Indicates whether the hessian is approximated or exact. If, 
        approximated, it should be larger or equal to the exact one. 
    constant_hessian : bool 
        Indicates whether the hessian is one for this loss. 
    is_multiclass : bool 
        Indicates whether n_classes &gt; 2 is allowed. 
    &quot;&quot;&quot;</span>

    <span class="s2"># For gradient boosted decision trees:</span>
    <span class="s2"># This variable indicates whether the loss requires the leaves values to</span>
    <span class="s2"># be updated once the tree has been trained. The trees are trained to</span>
    <span class="s2"># predict a Newton-Raphson step (see grower._finalize_leaf()). But for</span>
    <span class="s2"># some losses (e.g. least absolute deviation) we need to adjust the tree</span>
    <span class="s2"># values to account for the &quot;line search&quot; of the gradient descent</span>
    <span class="s2"># procedure. See the original paper Greedy Function Approximation: A</span>
    <span class="s2"># Gradient Boosting Machine by Friedman</span>
    <span class="s2"># (https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) for the theory.</span>
    <span class="s1">differentiable </span><span class="s4">= </span><span class="s3">True</span>
    <span class="s1">need_update_leaves_values </span><span class="s4">= </span><span class="s3">False</span>
    <span class="s1">is_multiclass </span><span class="s4">= </span><span class="s3">False</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">closs</span><span class="s4">, </span><span class="s1">link</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">closs </span><span class="s4">= </span><span class="s1">closs</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">link </span><span class="s4">= </span><span class="s1">link</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">approx_hessian </span><span class="s4">= </span><span class="s3">False</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_hessian </span><span class="s4">= </span><span class="s3">False</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes </span><span class="s4">= </span><span class="s1">n_classes</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(-</span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">interval_y_pred</span>

    <span class="s3">def </span><span class="s1">in_y_true_range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return True if y is in the valid range of y_true. 
 
        Parameters 
        ---------- 
        y : ndarray 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true</span><span class="s4">.</span><span class="s1">includes</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">in_y_pred_range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return True if y is in the valid range of y_pred. 
 
        Parameters 
        ---------- 
        y : ndarray 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">includes</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">loss</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">raw_prediction</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">loss_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the pointwise loss value for each input. 
 
        Parameters 
        ---------- 
        y_true : C-contiguous array of shape (n_samples,) 
            Observed, true target values. 
        raw_prediction : C-contiguous array of shape (n_samples,) or array of \ 
            shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
        sample_weight : None or C-contiguous array of shape (n_samples,) 
            Sample weights. 
        loss_out : None or C-contiguous array of shape (n_samples,) 
            A location into which the result is stored. If None, a new array 
            might be created. 
        n_threads : int, default=1 
            Might use openmp thread parallelism. 
 
        Returns 
        ------- 
        loss : array of shape (n_samples,) 
            Element-wise loss function. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">loss_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">loss_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s2"># Be graceful to shape (n_samples, 1) -&gt; (n_samples,)</span>
        <span class="s3">if </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">loss</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">loss_out</span><span class="s4">=</span><span class="s1">loss_out</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">n_threads</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">loss_out</span>

    <span class="s3">def </span><span class="s1">loss_gradient</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">raw_prediction</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">loss_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">gradient_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute loss and gradient w.r.t. raw_prediction for each input. 
 
        Parameters 
        ---------- 
        y_true : C-contiguous array of shape (n_samples,) 
            Observed, true target values. 
        raw_prediction : C-contiguous array of shape (n_samples,) or array of \ 
            shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
        sample_weight : None or C-contiguous array of shape (n_samples,) 
            Sample weights. 
        loss_out : None or C-contiguous array of shape (n_samples,) 
            A location into which the loss is stored. If None, a new array 
            might be created. 
        gradient_out : None or C-contiguous array of shape (n_samples,) or array \ 
            of shape (n_samples, n_classes) 
            A location into which the gradient is stored. If None, a new array 
            might be created. 
        n_threads : int, default=1 
            Might use openmp thread parallelism. 
 
        Returns 
        ------- 
        loss : array of shape (n_samples,) 
            Element-wise loss function. 
 
        gradient : array of shape (n_samples,) or (n_samples, n_classes) 
            Element-wise gradients. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">loss_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">gradient_out </span><span class="s3">is None</span><span class="s4">:</span>
                <span class="s1">loss_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
                <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">loss_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">gradient_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">loss_out</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>

        <span class="s2"># Be graceful to shape (n_samples, 1) -&gt; (n_samples,)</span>
        <span class="s3">if </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">loss_gradient</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">loss_out</span><span class="s4">=</span><span class="s1">loss_out</span><span class="s4">,</span>
            <span class="s1">gradient_out</span><span class="s4">=</span><span class="s1">gradient_out</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">n_threads</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">loss_out</span><span class="s4">, </span><span class="s1">gradient_out</span>

    <span class="s3">def </span><span class="s1">gradient</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">raw_prediction</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">gradient_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient of loss w.r.t raw_prediction for each input. 
 
        Parameters 
        ---------- 
        y_true : C-contiguous array of shape (n_samples,) 
            Observed, true target values. 
        raw_prediction : C-contiguous array of shape (n_samples,) or array of \ 
            shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
        sample_weight : None or C-contiguous array of shape (n_samples,) 
            Sample weights. 
        gradient_out : None or C-contiguous array of shape (n_samples,) or array \ 
            of shape (n_samples, n_classes) 
            A location into which the result is stored. If None, a new array 
            might be created. 
        n_threads : int, default=1 
            Might use openmp thread parallelism. 
 
        Returns 
        ------- 
        gradient : array of shape (n_samples,) or (n_samples, n_classes) 
            Element-wise gradients. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">gradient_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>

        <span class="s2"># Be graceful to shape (n_samples, 1) -&gt; (n_samples,)</span>
        <span class="s3">if </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">gradient</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">gradient_out</span><span class="s4">=</span><span class="s1">gradient_out</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">n_threads</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">gradient_out</span>

    <span class="s3">def </span><span class="s1">gradient_hessian</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">raw_prediction</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">gradient_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">hessian_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient and hessian of loss w.r.t raw_prediction. 
 
        Parameters 
        ---------- 
        y_true : C-contiguous array of shape (n_samples,) 
            Observed, true target values. 
        raw_prediction : C-contiguous array of shape (n_samples,) or array of \ 
            shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
        sample_weight : None or C-contiguous array of shape (n_samples,) 
            Sample weights. 
        gradient_out : None or C-contiguous array of shape (n_samples,) or array \ 
            of shape (n_samples, n_classes) 
            A location into which the gradient is stored. If None, a new array 
            might be created. 
        hessian_out : None or C-contiguous array of shape (n_samples,) or array \ 
            of shape (n_samples, n_classes) 
            A location into which the hessian is stored. If None, a new array 
            might be created. 
        n_threads : int, default=1 
            Might use openmp thread parallelism. 
 
        Returns 
        ------- 
        gradient : arrays of shape (n_samples,) or (n_samples, n_classes) 
            Element-wise gradients. 
 
        hessian : arrays of shape (n_samples,) or (n_samples, n_classes) 
            Element-wise hessians. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">gradient_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">hessian_out </span><span class="s3">is None</span><span class="s4">:</span>
                <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
                <span class="s1">hessian_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">hessian_out</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">hessian_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">hessian_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">gradient_out</span><span class="s4">)</span>

        <span class="s2"># Be graceful to shape (n_samples, 1) -&gt; (n_samples,)</span>
        <span class="s3">if </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">gradient_out</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">hessian_out</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">hessian_out</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">hessian_out </span><span class="s4">= </span><span class="s1">hessian_out</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">gradient_hessian</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">gradient_out</span><span class="s4">=</span><span class="s1">gradient_out</span><span class="s4">,</span>
            <span class="s1">hessian_out</span><span class="s4">=</span><span class="s1">hessian_out</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">n_threads</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">gradient_out</span><span class="s4">, </span><span class="s1">hessian_out</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">raw_prediction</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the weighted average loss. 
 
        Parameters 
        ---------- 
        y_true : C-contiguous array of shape (n_samples,) 
            Observed, true target values. 
        raw_prediction : C-contiguous array of shape (n_samples,) or array of \ 
            shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
        sample_weight : None or C-contiguous array of shape (n_samples,) 
            Sample weights. 
        n_threads : int, default=1 
            Might use openmp thread parallelism. 
 
        Returns 
        ------- 
        loss : float 
            Mean or averaged loss function. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">loss</span><span class="s4">(</span>
                <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
                <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
                <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
                <span class="s1">loss_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
                <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">n_threads</span><span class="s4">,</span>
            <span class="s4">),</span>
            <span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">fit_intercept_only</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute raw_prediction of an intercept-only model. 
 
        This can be used as initial estimates of predictions, i.e. before the 
        first iteration in fit. 
 
        Parameters 
        ---------- 
        y_true : array-like of shape (n_samples,) 
            Observed, true target values. 
        sample_weight : None or array of shape (n_samples,) 
            Sample weights. 
 
        Returns 
        ------- 
        raw_prediction : numpy scalar or array of shape (n_classes,) 
            Raw predictions of an intercept-only model. 
        &quot;&quot;&quot;</span>
        <span class="s2"># As default, take weighted average of the target over the samples</span>
        <span class="s2"># axis=0 and then transform into link-scale (raw_prediction).</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)</span>
        <span class="s1">eps </span><span class="s4">= </span><span class="s5">10 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">).</span><span class="s1">eps</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">low </span><span class="s4">== -</span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">:</span>
            <span class="s1">a_min </span><span class="s4">= </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">low_inclusive</span><span class="s4">:</span>
            <span class="s1">a_min </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">low</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">a_min </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">low </span><span class="s4">+ </span><span class="s1">eps</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">high </span><span class="s4">== </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">:</span>
            <span class="s1">a_max </span><span class="s4">= </span><span class="s3">None</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">high_inclusive</span><span class="s4">:</span>
            <span class="s1">a_max </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">high</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">a_max </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred</span><span class="s4">.</span><span class="s1">high </span><span class="s4">- </span><span class="s1">eps</span>

        <span class="s3">if </span><span class="s1">a_min </span><span class="s3">is None and </span><span class="s1">a_max </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">link</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">link</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">clip</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">a_min</span><span class="s4">, </span><span class="s1">a_max</span><span class="s4">))</span>

    <span class="s3">def </span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate term dropped in loss. 
 
        With this term added, the loss of perfect predictions is zero. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">init_gradient_and_hessian</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s6">&quot;F&quot;</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Initialize arrays for gradients and hessians. 
 
        Unless hessians are constant, arrays are initialized with undefined values. 
 
        Parameters 
        ---------- 
        n_samples : int 
            The number of samples, usually passed to `fit()`. 
        dtype : {np.float64, np.float32}, default=np.float64 
            The dtype of the arrays gradient and hessian. 
        order : {'C', 'F'}, default='F' 
            Order of the arrays gradient and hessian. The default 'F' makes the arrays 
            contiguous along samples. 
 
        Returns 
        ------- 
        gradient : C-contiguous array of shape (n_samples,) or array of shape \ 
            (n_samples, n_classes) 
            Empty array (allocated but not initialized) to be used as argument 
            gradient_out. 
        hessian : C-contiguous array of shape (n_samples,), array of shape 
            (n_samples, n_classes) or shape (1,) 
            Empty (allocated but not initialized) array to be used as argument 
            hessian_out. 
            If constant_hessian is True (e.g. `HalfSquaredError`), the array is 
            initialized to ``1``. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">dtype </span><span class="s3">not in </span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s6">&quot;Valid options for 'dtype' are np.float32 and np.float64. &quot;</span>
                <span class="s6">f&quot;Got dtype=</span><span class="s3">{</span><span class="s1">dtype</span><span class="s3">} </span><span class="s6">instead.&quot;</span>
            <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">is_multiclass</span><span class="s4">:</span>
            <span class="s1">shape </span><span class="s4">= (</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">shape </span><span class="s4">= (</span><span class="s1">n_samples</span><span class="s4">,)</span>
        <span class="s1">gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">(</span><span class="s1">shape</span><span class="s4">=</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s1">order</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">constant_hessian</span><span class="s4">:</span>
            <span class="s2"># If the hessians are constant, we consider them equal to 1.</span>
            <span class="s2"># - This is correct for HalfSquaredError</span>
            <span class="s2"># - For AbsoluteError, hessians are actually 0, but they are</span>
            <span class="s2">#   always ignored anyway.</span>
            <span class="s1">hessian </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">(</span><span class="s1">shape</span><span class="s4">=(</span><span class="s5">1</span><span class="s4">,), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">hessian </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">(</span><span class="s1">shape</span><span class="s4">=</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">order</span><span class="s4">=</span><span class="s1">order</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">gradient</span><span class="s4">, </span><span class="s1">hessian</span>


<span class="s2"># Note: Naturally, we would inherit in the following order</span>
<span class="s2">#         class HalfSquaredError(IdentityLink, CyHalfSquaredError, BaseLoss)</span>
<span class="s2">#       But because of https://github.com/cython/cython/issues/4350 we</span>
<span class="s2">#       set BaseLoss as the last one. This, of course, changes the MRO.</span>
<span class="s3">class </span><span class="s1">HalfSquaredError</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Half squared error with identity link, for regression. 
 
    Domain: 
    y_true and y_pred all real numbers 
 
    Link: 
    y_pred = raw_prediction 
 
    For a given sample x_i, half squared error is defined as:: 
 
        loss(x_i) = 0.5 * (y_true_i - raw_prediction_i)**2 
 
    The factor of 0.5 simplifies the computation of gradients and results in a 
    unit hessian (and is consistent with what is done in LightGBM). It is also 
    half the Normal distribution deviance. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfSquaredError</span><span class="s4">(), </span><span class="s1">link</span><span class="s4">=</span><span class="s1">IdentityLink</span><span class="s4">())</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_hessian </span><span class="s4">= </span><span class="s1">sample_weight </span><span class="s3">is None</span>


<span class="s3">class </span><span class="s1">AbsoluteError</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Absolute error with identity link, for regression. 
 
    Domain: 
    y_true and y_pred all real numbers 
 
    Link: 
    y_pred = raw_prediction 
 
    For a given sample x_i, the absolute error is defined as:: 
 
        loss(x_i) = |y_true_i - raw_prediction_i| 
 
    Note that the exact hessian = 0 almost everywhere (except at one point, therefore 
    differentiable = False). Optimization routines like in HGBT, however, need a 
    hessian &gt; 0. Therefore, we assign 1. 
    &quot;&quot;&quot;</span>

    <span class="s1">differentiable </span><span class="s4">= </span><span class="s3">False</span>
    <span class="s1">need_update_leaves_values </span><span class="s4">= </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">closs</span><span class="s4">=</span><span class="s1">CyAbsoluteError</span><span class="s4">(), </span><span class="s1">link</span><span class="s4">=</span><span class="s1">IdentityLink</span><span class="s4">())</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">approx_hessian </span><span class="s4">= </span><span class="s3">True</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_hessian </span><span class="s4">= </span><span class="s1">sample_weight </span><span class="s3">is None</span>

    <span class="s3">def </span><span class="s1">fit_intercept_only</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute raw_prediction of an intercept-only model. 
 
        This is the weighted median of the target, i.e. over the samples 
        axis=0. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">median</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">_weighted_percentile</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s5">50</span><span class="s4">)</span>


<span class="s3">class </span><span class="s1">PinballLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Quantile loss aka pinball loss, for regression. 
 
    Domain: 
    y_true and y_pred all real numbers 
    quantile in (0, 1) 
 
    Link: 
    y_pred = raw_prediction 
 
    For a given sample x_i, the pinball loss is defined as:: 
 
        loss(x_i) = rho_{quantile}(y_true_i - raw_prediction_i) 
 
        rho_{quantile}(u) = u * (quantile - 1_{u&lt;0}) 
                          = -u *(1 - quantile)  if u &lt; 0 
                             u * quantile       if u &gt;= 0 
 
    Note: 2 * PinballLoss(quantile=0.5) equals AbsoluteError(). 
 
    Note that the exact hessian = 0 almost everywhere (except at one point, therefore 
    differentiable = False). Optimization routines like in HGBT, however, need a 
    hessian &gt; 0. Therefore, we assign 1. 
 
    Additional Attributes 
    --------------------- 
    quantile : float 
        The quantile level of the quantile to be estimated. Must be in range (0, 1). 
    &quot;&quot;&quot;</span>

    <span class="s1">differentiable </span><span class="s4">= </span><span class="s3">False</span>
    <span class="s1">need_update_leaves_values </span><span class="s4">= </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">quantile</span><span class="s4">=</span><span class="s5">0.5</span><span class="s4">):</span>
        <span class="s1">check_scalar</span><span class="s4">(</span>
            <span class="s1">quantile</span><span class="s4">,</span>
            <span class="s6">&quot;quantile&quot;</span><span class="s4">,</span>
            <span class="s1">target_type</span><span class="s4">=</span><span class="s1">numbers</span><span class="s4">.</span><span class="s1">Real</span><span class="s4">,</span>
            <span class="s1">min_val</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
            <span class="s1">max_val</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
            <span class="s1">include_boundaries</span><span class="s4">=</span><span class="s6">&quot;neither&quot;</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyPinballLoss</span><span class="s4">(</span><span class="s1">quantile</span><span class="s4">=</span><span class="s1">float</span><span class="s4">(</span><span class="s1">quantile</span><span class="s4">)),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">IdentityLink</span><span class="s4">(),</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">approx_hessian </span><span class="s4">= </span><span class="s3">True</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_hessian </span><span class="s4">= </span><span class="s1">sample_weight </span><span class="s3">is None</span>

    <span class="s3">def </span><span class="s1">fit_intercept_only</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute raw_prediction of an intercept-only model. 
 
        This is the weighted median of the target, i.e. over the samples 
        axis=0. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">percentile</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s5">100 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">quantile</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">_weighted_percentile</span><span class="s4">(</span>
                <span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s5">100 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">quantile</span>
            <span class="s4">)</span>


<span class="s3">class </span><span class="s1">HuberLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Huber loss, for regression. 
 
    Domain: 
    y_true and y_pred all real numbers 
    quantile in (0, 1) 
 
    Link: 
    y_pred = raw_prediction 
 
    For a given sample x_i, the Huber loss is defined as:: 
 
        loss(x_i) = 1/2 * abserr**2            if abserr &lt;= delta 
                    delta * (abserr - delta/2) if abserr &gt; delta 
 
        abserr = |y_true_i - raw_prediction_i| 
        delta = quantile(abserr, self.quantile) 
 
    Note: HuberLoss(quantile=1) equals HalfSquaredError and HuberLoss(quantile=0) 
    equals delta * (AbsoluteError() - delta/2). 
 
    Additional Attributes 
    --------------------- 
    quantile : float 
        The quantile level which defines the breaking point `delta` to distinguish 
        between absolute error and squared error. Must be in range (0, 1). 
 
     Reference 
    --------- 
    .. [1] Friedman, J.H. (2001). :doi:`Greedy function approximation: A gradient 
      boosting machine &lt;10.1214/aos/1013203451&gt;`. 
      Annals of Statistics, 29, 1189-1232. 
    &quot;&quot;&quot;</span>

    <span class="s1">differentiable </span><span class="s4">= </span><span class="s3">False</span>
    <span class="s1">need_update_leaves_values </span><span class="s4">= </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">quantile</span><span class="s4">=</span><span class="s5">0.9</span><span class="s4">, </span><span class="s1">delta</span><span class="s4">=</span><span class="s5">0.5</span><span class="s4">):</span>
        <span class="s1">check_scalar</span><span class="s4">(</span>
            <span class="s1">quantile</span><span class="s4">,</span>
            <span class="s6">&quot;quantile&quot;</span><span class="s4">,</span>
            <span class="s1">target_type</span><span class="s4">=</span><span class="s1">numbers</span><span class="s4">.</span><span class="s1">Real</span><span class="s4">,</span>
            <span class="s1">min_val</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
            <span class="s1">max_val</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
            <span class="s1">include_boundaries</span><span class="s4">=</span><span class="s6">&quot;neither&quot;</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">quantile </span><span class="s4">= </span><span class="s1">quantile  </span><span class="s2"># This is better stored outside of Cython.</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHuberLoss</span><span class="s4">(</span><span class="s1">delta</span><span class="s4">=</span><span class="s1">float</span><span class="s4">(</span><span class="s1">delta</span><span class="s4">)),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">IdentityLink</span><span class="s4">(),</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">approx_hessian </span><span class="s4">= </span><span class="s3">True</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_hessian </span><span class="s4">= </span><span class="s3">False</span>

    <span class="s3">def </span><span class="s1">fit_intercept_only</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute raw_prediction of an intercept-only model. 
 
        This is the weighted median of the target, i.e. over the samples 
        axis=0. 
        &quot;&quot;&quot;</span>
        <span class="s2"># See formula before algo 4 in Friedman (2001), but we apply it to y_true,</span>
        <span class="s2"># not to the residual y_true - raw_prediction. An estimator like</span>
        <span class="s2"># HistGradientBoostingRegressor might then call it on the residual, e.g.</span>
        <span class="s2"># fit_intercept_only(y_true - raw_prediction).</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">median </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">percentile</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s5">50</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">median </span><span class="s4">= </span><span class="s1">_weighted_percentile</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s5">50</span><span class="s4">)</span>
        <span class="s1">diff </span><span class="s4">= </span><span class="s1">y_true </span><span class="s4">- </span><span class="s1">median</span>
        <span class="s1">term </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sign</span><span class="s4">(</span><span class="s1">diff</span><span class="s4">) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">minimum</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">delta</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">diff</span><span class="s4">))</span>
        <span class="s3">return </span><span class="s1">median </span><span class="s4">+ </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">term</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>


<span class="s3">class </span><span class="s1">HalfPoissonLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Half Poisson deviance loss with log-link, for regression. 
 
    Domain: 
    y_true in non-negative real numbers 
    y_pred in positive real numbers 
 
    Link: 
    y_pred = exp(raw_prediction) 
 
    For a given sample x_i, half the Poisson deviance is defined as:: 
 
        loss(x_i) = y_true_i * log(y_true_i/exp(raw_prediction_i)) 
                    - y_true_i + exp(raw_prediction_i) 
 
    Half the Poisson deviance is actually the negative log-likelihood up to 
    constant terms (not involving raw_prediction) and simplifies the 
    computation of the gradients. 
    We also skip the constant term `y_true_i * log(y_true_i) - y_true_i`. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfPoissonLoss</span><span class="s4">(), </span><span class="s1">link</span><span class="s4">=</span><span class="s1">LogLink</span><span class="s4">())</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">True</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">term </span><span class="s4">= </span><span class="s1">xlogy</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">) - </span><span class="s1">y_true</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">term </span><span class="s4">*= </span><span class="s1">sample_weight</span>
        <span class="s3">return </span><span class="s1">term</span>


<span class="s3">class </span><span class="s1">HalfGammaLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Half Gamma deviance loss with log-link, for regression. 
 
    Domain: 
    y_true and y_pred in positive real numbers 
 
    Link: 
    y_pred = exp(raw_prediction) 
 
    For a given sample x_i, half Gamma deviance loss is defined as:: 
 
        loss(x_i) = log(exp(raw_prediction_i)/y_true_i) 
                    + y_true/exp(raw_prediction_i) - 1 
 
    Half the Gamma deviance is actually proportional to the negative log- 
    likelihood up to constant terms (not involving raw_prediction) and 
    simplifies the computation of the gradients. 
    We also skip the constant term `-log(y_true_i) - 1`. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfGammaLoss</span><span class="s4">(), </span><span class="s1">link</span><span class="s4">=</span><span class="s1">LogLink</span><span class="s4">())</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">term </span><span class="s4">= -</span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">) - </span><span class="s5">1</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">term </span><span class="s4">*= </span><span class="s1">sample_weight</span>
        <span class="s3">return </span><span class="s1">term</span>


<span class="s3">class </span><span class="s1">HalfTweedieLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Half Tweedie deviance loss with log-link, for regression. 
 
    Domain: 
    y_true in real numbers for power &lt;= 0 
    y_true in non-negative real numbers for 0 &lt; power &lt; 2 
    y_true in positive real numbers for 2 &lt;= power 
    y_pred in positive real numbers 
    power in real numbers 
 
    Link: 
    y_pred = exp(raw_prediction) 
 
    For a given sample x_i, half Tweedie deviance loss with p=power is defined 
    as:: 
 
        loss(x_i) = max(y_true_i, 0)**(2-p) / (1-p) / (2-p) 
                    - y_true_i * exp(raw_prediction_i)**(1-p) / (1-p) 
                    + exp(raw_prediction_i)**(2-p) / (2-p) 
 
    Taking the limits for p=0, 1, 2 gives HalfSquaredError with a log link, 
    HalfPoissonLoss and HalfGammaLoss. 
 
    We also skip constant terms, but those are different for p=0, 1, 2. 
    Therefore, the loss is not continuous in `power`. 
 
    Note furthermore that although no Tweedie distribution exists for 
    0 &lt; power &lt; 1, it still gives a strictly consistent scoring function for 
    the expectation. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">power</span><span class="s4">=</span><span class="s5">1.5</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfTweedieLoss</span><span class="s4">(</span><span class="s1">power</span><span class="s4">=</span><span class="s1">float</span><span class="s4">(</span><span class="s1">power</span><span class="s4">)),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">LogLink</span><span class="s4">(),</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">&lt;= </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(-</span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">&lt; </span><span class="s5">2</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">True</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">HalfSquaredError</span><span class="s4">().</span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span>
                <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
            <span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">== </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">HalfPoissonLoss</span><span class="s4">().</span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span>
                <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
            <span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">== </span><span class="s5">2</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">HalfGammaLoss</span><span class="s4">().</span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span>
                <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">p </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power</span>
            <span class="s1">term </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">power</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">maximum</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s5">0</span><span class="s4">), </span><span class="s5">2 </span><span class="s4">- </span><span class="s1">p</span><span class="s4">) / (</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">p</span><span class="s4">) / (</span><span class="s5">2 </span><span class="s4">- </span><span class="s1">p</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">term </span><span class="s4">*= </span><span class="s1">sample_weight</span>
            <span class="s3">return </span><span class="s1">term</span>


<span class="s3">class </span><span class="s1">HalfTweedieLossIdentity</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Half Tweedie deviance loss with identity link, for regression. 
 
    Domain: 
    y_true in real numbers for power &lt;= 0 
    y_true in non-negative real numbers for 0 &lt; power &lt; 2 
    y_true in positive real numbers for 2 &lt;= power 
    y_pred in positive real numbers for power != 0 
    y_pred in real numbers for power = 0 
    power in real numbers 
 
    Link: 
    y_pred = raw_prediction 
 
    For a given sample x_i, half Tweedie deviance loss with p=power is defined 
    as:: 
 
        loss(x_i) = max(y_true_i, 0)**(2-p) / (1-p) / (2-p) 
                    - y_true_i * raw_prediction_i**(1-p) / (1-p) 
                    + raw_prediction_i**(2-p) / (2-p) 
 
    Note that the minimum value of this loss is 0. 
 
    Note furthermore that although no Tweedie distribution exists for 
    0 &lt; power &lt; 1, it still gives a strictly consistent scoring function for 
    the expectation. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">power</span><span class="s4">=</span><span class="s5">1.5</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfTweedieLossIdentity</span><span class="s4">(</span><span class="s1">power</span><span class="s4">=</span><span class="s1">float</span><span class="s4">(</span><span class="s1">power</span><span class="s4">)),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">IdentityLink</span><span class="s4">(),</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">&lt;= </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(-</span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">&lt; </span><span class="s5">2</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">True</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">power </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(-</span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>


<span class="s3">class </span><span class="s1">HalfBinomialLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Half Binomial deviance loss with logit link, for binary classification. 
 
    This is also know as binary cross entropy, log-loss and logistic loss. 
 
    Domain: 
    y_true in [0, 1], i.e. regression on the unit interval 
    y_pred in (0, 1), i.e. boundaries excluded 
 
    Link: 
    y_pred = expit(raw_prediction) 
 
    For a given sample x_i, half Binomial deviance is defined as the negative 
    log-likelihood of the Binomial/Bernoulli distribution and can be expressed 
    as:: 
 
        loss(x_i) = log(1 + exp(raw_pred_i)) - y_true_i * raw_pred_i 
 
    See The Elements of Statistical Learning, by Hastie, Tibshirani, Friedman, 
    section 4.4.1 (about logistic regression). 
 
    Note that the formulation works for classification, y = {0, 1}, as well as 
    logistic regression, y = [0, 1]. 
    If you add `constant_to_optimal_zero` to the loss, you get half the 
    Bernoulli/binomial deviance. 
 
    More details: Inserting the predicted probability y_pred = expit(raw_prediction) 
    in the loss gives the well known:: 
 
        loss(x_i) = - y_true_i * log(y_pred_i) - (1 - y_true_i) * log(1 - y_pred_i) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfBinomialLoss</span><span class="s4">(),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">LogitLink</span><span class="s4">(),</span>
            <span class="s1">n_classes</span><span class="s4">=</span><span class="s5">2</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">True</span><span class="s4">, </span><span class="s3">True</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s2"># This is non-zero only if y_true is neither 0 nor 1.</span>
        <span class="s1">term </span><span class="s4">= </span><span class="s1">xlogy</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">) + </span><span class="s1">xlogy</span><span class="s4">(</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">y_true</span><span class="s4">, </span><span class="s5">1 </span><span class="s4">- </span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">term </span><span class="s4">*= </span><span class="s1">sample_weight</span>
        <span class="s3">return </span><span class="s1">term</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">raw_prediction</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict probabilities. 
 
        Parameters 
        ---------- 
        raw_prediction : array of shape (n_samples,) or (n_samples, 1) 
            Raw prediction values (in link space). 
 
        Returns 
        ------- 
        proba : array of shape (n_samples, 2) 
            Element-wise class probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Be graceful to shape (n_samples, 1) -&gt; (n_samples,)</span>
        <span class="s3">if </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>
        <span class="s1">proba </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s5">2</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s1">proba</span><span class="s4">[:, </span><span class="s5">1</span><span class="s4">] = </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">inverse</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
        <span class="s1">proba</span><span class="s4">[:, </span><span class="s5">0</span><span class="s4">] = </span><span class="s5">1 </span><span class="s4">- </span><span class="s1">proba</span><span class="s4">[:, </span><span class="s5">1</span><span class="s4">]</span>
        <span class="s3">return </span><span class="s1">proba</span>


<span class="s3">class </span><span class="s1">HalfMultinomialLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Categorical cross-entropy loss, for multiclass classification. 
 
    Domain: 
    y_true in {0, 1, 2, 3, .., n_classes - 1} 
    y_pred has n_classes elements, each element in (0, 1) 
 
    Link: 
    y_pred = softmax(raw_prediction) 
 
    Note: We assume y_true to be already label encoded. The inverse link is 
    softmax. But the full link function is the symmetric multinomial logit 
    function. 
 
    For a given sample x_i, the categorical cross-entropy loss is defined as 
    the negative log-likelihood of the multinomial distribution, it 
    generalizes the binary cross-entropy to more than 2 classes:: 
 
        loss_i = log(sum(exp(raw_pred_{i, k}), k=0..n_classes-1)) 
                - sum(y_true_{i, k} * raw_pred_{i, k}, k=0..n_classes-1) 
 
    See [1]. 
 
    Note that for the hessian, we calculate only the diagonal part in the 
    classes: If the full hessian for classes k and l and sample i is H_i_k_l, 
    we calculate H_i_k_k, i.e. k=l. 
 
    Reference 
    --------- 
    .. [1] :arxiv:`Simon, Noah, J. Friedman and T. Hastie. 
        &quot;A Blockwise Descent Algorithm for Group-penalized Multiresponse and 
        Multinomial Regression&quot;. 
        &lt;1311.6529&gt;` 
    &quot;&quot;&quot;</span>

    <span class="s1">is_multiclass </span><span class="s4">= </span><span class="s3">True</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">=</span><span class="s5">3</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyHalfMultinomialLoss</span><span class="s4">(),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">MultinomialLogit</span><span class="s4">(),</span>
            <span class="s1">n_classes</span><span class="s4">=</span><span class="s1">n_classes</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">, </span><span class="s3">True</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_pred </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">False</span><span class="s4">, </span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">in_y_true_range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return True if y is in the valid range of y_true. 
 
        Parameters 
        ---------- 
        y : ndarray 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true</span><span class="s4">.</span><span class="s1">includes</span><span class="s4">(</span><span class="s1">y</span><span class="s4">) </span><span class="s3">and </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">y</span><span class="s4">.</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">int</span><span class="s4">) == </span><span class="s1">y</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">fit_intercept_only</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute raw_prediction of an intercept-only model. 
 
        This is the softmax of the weighted average of the target, i.e. over 
        the samples axis=0. 
        &quot;&quot;&quot;</span>
        <span class="s1">out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s1">eps </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">).</span><span class="s1">eps</span>
        <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_classes</span><span class="s4">):</span>
            <span class="s1">out</span><span class="s4">[</span><span class="s1">k</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">== </span><span class="s1">k</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)</span>
            <span class="s1">out</span><span class="s4">[</span><span class="s1">k</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">clip</span><span class="s4">(</span><span class="s1">out</span><span class="s4">[</span><span class="s1">k</span><span class="s4">], </span><span class="s1">eps</span><span class="s4">, </span><span class="s5">1 </span><span class="s4">- </span><span class="s1">eps</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">link</span><span class="s4">(</span><span class="s1">out</span><span class="s4">[</span><span class="s3">None</span><span class="s4">, :]).</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s5">1</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">raw_prediction</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict probabilities. 
 
        Parameters 
        ---------- 
        raw_prediction : array of shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
 
        Returns 
        ------- 
        proba : array of shape (n_samples, n_classes) 
            Element-wise class probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">inverse</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">gradient_proba</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">raw_prediction</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">gradient_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">proba_out</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">n_threads</span><span class="s4">=</span><span class="s5">1</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute gradient and class probabilities fow raw_prediction. 
 
        Parameters 
        ---------- 
        y_true : C-contiguous array of shape (n_samples,) 
            Observed, true target values. 
        raw_prediction : array of shape (n_samples, n_classes) 
            Raw prediction values (in link space). 
        sample_weight : None or C-contiguous array of shape (n_samples,) 
            Sample weights. 
        gradient_out : None or array of shape (n_samples, n_classes) 
            A location into which the gradient is stored. If None, a new array 
            might be created. 
        proba_out : None or array of shape (n_samples, n_classes) 
            A location into which the class probabilities are stored. If None, 
            a new array might be created. 
        n_threads : int, default=1 
            Might use openmp thread parallelism. 
 
        Returns 
        ------- 
        gradient : array of shape (n_samples, n_classes) 
            Element-wise gradients. 
 
        proba : array of shape (n_samples, n_classes) 
            Element-wise class probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">gradient_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">proba_out </span><span class="s3">is None</span><span class="s4">:</span>
                <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
                <span class="s1">proba_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">gradient_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">proba_out</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">proba_out </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">proba_out </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty_like</span><span class="s4">(</span><span class="s1">gradient_out</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">closs</span><span class="s4">.</span><span class="s1">gradient_proba</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">gradient_out</span><span class="s4">=</span><span class="s1">gradient_out</span><span class="s4">,</span>
            <span class="s1">proba_out</span><span class="s4">=</span><span class="s1">proba_out</span><span class="s4">,</span>
            <span class="s1">n_threads</span><span class="s4">=</span><span class="s1">n_threads</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">gradient_out</span><span class="s4">, </span><span class="s1">proba_out</span>


<span class="s3">class </span><span class="s1">ExponentialLoss</span><span class="s4">(</span><span class="s1">BaseLoss</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Exponential loss with (half) logit link, for binary classification. 
 
    This is also know as boosting loss. 
 
    Domain: 
    y_true in [0, 1], i.e. regression on the unit interval 
    y_pred in (0, 1), i.e. boundaries excluded 
 
    Link: 
    y_pred = expit(2 * raw_prediction) 
 
    For a given sample x_i, the exponential loss is defined as:: 
 
        loss(x_i) = y_true_i * exp(-raw_pred_i)) + (1 - y_true_i) * exp(raw_pred_i) 
 
    See: 
    - J. Friedman, T. Hastie, R. Tibshirani. 
      &quot;Additive logistic regression: a statistical view of boosting (With discussion 
      and a rejoinder by the authors).&quot; Ann. Statist. 28 (2) 337 - 407, April 2000. 
      https://doi.org/10.1214/aos/1016218223 
    - A. Buja, W. Stuetzle, Y. Shen. (2005). 
      &quot;Loss Functions for Binary Class Probability Estimation and Classification: 
      Structure and Applications.&quot; 
 
    Note that the formulation works for classification, y = {0, 1}, as well as 
    &quot;exponential logistic&quot; regression, y = [0, 1]. 
    Note that this is a proper scoring rule, but without it's canonical link. 
 
    More details: Inserting the predicted probability 
    y_pred = expit(2 * raw_prediction) in the loss gives:: 
 
        loss(x_i) = y_true_i * sqrt((1 - y_pred_i) / y_pred_i) 
            + (1 - y_true_i) * sqrt(y_pred_i / (1 - y_pred_i)) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">closs</span><span class="s4">=</span><span class="s1">CyExponentialLoss</span><span class="s4">(),</span>
            <span class="s1">link</span><span class="s4">=</span><span class="s1">HalfLogitLink</span><span class="s4">(),</span>
            <span class="s1">n_classes</span><span class="s4">=</span><span class="s5">2</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">interval_y_true </span><span class="s4">= </span><span class="s1">Interval</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s3">True</span><span class="s4">, </span><span class="s3">True</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">constant_to_optimal_zero</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s2"># This is non-zero only if y_true is neither 0 nor 1.</span>
        <span class="s1">term </span><span class="s4">= -</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">* (</span><span class="s5">1 </span><span class="s4">- </span><span class="s1">y_true</span><span class="s4">))</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">term </span><span class="s4">*= </span><span class="s1">sample_weight</span>
        <span class="s3">return </span><span class="s1">term</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">raw_prediction</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict probabilities. 
 
        Parameters 
        ---------- 
        raw_prediction : array of shape (n_samples,) or (n_samples, 1) 
            Raw prediction values (in link space). 
 
        Returns 
        ------- 
        proba : array of shape (n_samples, 2) 
            Element-wise class probabilities. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Be graceful to shape (n_samples, 1) -&gt; (n_samples,)</span>
        <span class="s3">if </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s5">2 </span><span class="s3">and </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s1">raw_prediction </span><span class="s4">= </span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s5">1</span><span class="s4">)</span>
        <span class="s1">proba </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s5">2</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s1">proba</span><span class="s4">[:, </span><span class="s5">1</span><span class="s4">] = </span><span class="s1">self</span><span class="s4">.</span><span class="s1">link</span><span class="s4">.</span><span class="s1">inverse</span><span class="s4">(</span><span class="s1">raw_prediction</span><span class="s4">)</span>
        <span class="s1">proba</span><span class="s4">[:, </span><span class="s5">0</span><span class="s4">] = </span><span class="s5">1 </span><span class="s4">- </span><span class="s1">proba</span><span class="s4">[:, </span><span class="s5">1</span><span class="s4">]</span>
        <span class="s3">return </span><span class="s1">proba</span>


<span class="s1">_LOSSES </span><span class="s4">= {</span>
    <span class="s6">&quot;squared_error&quot;</span><span class="s4">: </span><span class="s1">HalfSquaredError</span><span class="s4">,</span>
    <span class="s6">&quot;absolute_error&quot;</span><span class="s4">: </span><span class="s1">AbsoluteError</span><span class="s4">,</span>
    <span class="s6">&quot;pinball_loss&quot;</span><span class="s4">: </span><span class="s1">PinballLoss</span><span class="s4">,</span>
    <span class="s6">&quot;huber_loss&quot;</span><span class="s4">: </span><span class="s1">HuberLoss</span><span class="s4">,</span>
    <span class="s6">&quot;poisson_loss&quot;</span><span class="s4">: </span><span class="s1">HalfPoissonLoss</span><span class="s4">,</span>
    <span class="s6">&quot;gamma_loss&quot;</span><span class="s4">: </span><span class="s1">HalfGammaLoss</span><span class="s4">,</span>
    <span class="s6">&quot;tweedie_loss&quot;</span><span class="s4">: </span><span class="s1">HalfTweedieLoss</span><span class="s4">,</span>
    <span class="s6">&quot;binomial_loss&quot;</span><span class="s4">: </span><span class="s1">HalfBinomialLoss</span><span class="s4">,</span>
    <span class="s6">&quot;multinomial_loss&quot;</span><span class="s4">: </span><span class="s1">HalfMultinomialLoss</span><span class="s4">,</span>
    <span class="s6">&quot;exponential_loss&quot;</span><span class="s4">: </span><span class="s1">ExponentialLoss</span><span class="s4">,</span>
<span class="s4">}</span>
</pre>
</body>
</html>