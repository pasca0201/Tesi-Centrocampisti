<html>
<head>
<title>kernels.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernels.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;A set of kernels that can be combined by operators and used in Gaussian processes.&quot;&quot;&quot;</span>

<span class="s2"># Kernels for Gaussian process regression and classification.</span>
<span class="s2">#</span>
<span class="s2"># The kernels in this module allow kernel-engineering, i.e., they can be</span>
<span class="s2"># combined via the &quot;+&quot; and &quot;*&quot; operators or be exponentiated with a scalar</span>
<span class="s2"># via &quot;**&quot;. These sum and product expressions can also contain scalar values,</span>
<span class="s2"># which are automatically converted to a constant kernel.</span>
<span class="s2">#</span>
<span class="s2"># All kernels allow (analytic) gradient-based hyperparameter optimization.</span>
<span class="s2"># The space of hyperparameters can be specified by giving lower und upper</span>
<span class="s2"># boundaries for the value of each hyperparameter (the search space is thus</span>
<span class="s2"># rectangular). Instead of specifying bounds, hyperparameters can also be</span>
<span class="s2"># declared to be &quot;fixed&quot;, which causes these hyperparameters to be excluded from</span>
<span class="s2"># optimization.</span>


<span class="s2"># Author: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s2"># Note: this module is strongly inspired by the kernel module of the george</span>
<span class="s2">#       package.</span>

<span class="s3">import </span><span class="s1">math</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABCMeta</span><span class="s4">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">collections </span><span class="s3">import </span><span class="s1">namedtuple</span>
<span class="s3">from </span><span class="s1">inspect </span><span class="s3">import </span><span class="s1">signature</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">spatial</span><span class="s4">.</span><span class="s1">distance </span><span class="s3">import </span><span class="s1">cdist</span><span class="s4">, </span><span class="s1">pdist</span><span class="s4">, </span><span class="s1">squareform</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">special </span><span class="s3">import </span><span class="s1">gamma</span><span class="s4">, </span><span class="s1">kv</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">base </span><span class="s3">import </span><span class="s1">clone</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">metrics</span><span class="s4">.</span><span class="s1">pairwise </span><span class="s3">import </span><span class="s1">pairwise_kernels</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s1">_num_samples</span>


<span class="s3">def </span><span class="s1">_check_length_scale</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">length_scale</span><span class="s4">):</span>
    <span class="s1">length_scale </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">squeeze</span><span class="s4">(</span><span class="s1">length_scale</span><span class="s4">).</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">float</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ndim</span><span class="s4">(</span><span class="s1">length_scale</span><span class="s4">) &gt; </span><span class="s5">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;length_scale cannot be of dimension greater than 1&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ndim</span><span class="s4">(</span><span class="s1">length_scale</span><span class="s4">) == </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] != </span><span class="s1">length_scale</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s6">&quot;Anisotropic kernel must have the same number of &quot;</span>
            <span class="s6">&quot;dimensions as data (%d!=%d)&quot; </span><span class="s4">% (</span><span class="s1">length_scale</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">])</span>
        <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">length_scale</span>


<span class="s3">class </span><span class="s1">Hyperparameter</span><span class="s4">(</span>
    <span class="s1">namedtuple</span><span class="s4">(</span>
        <span class="s6">&quot;Hyperparameter&quot;</span><span class="s4">, (</span><span class="s6">&quot;name&quot;</span><span class="s4">, </span><span class="s6">&quot;value_type&quot;</span><span class="s4">, </span><span class="s6">&quot;bounds&quot;</span><span class="s4">, </span><span class="s6">&quot;n_elements&quot;</span><span class="s4">, </span><span class="s6">&quot;fixed&quot;</span><span class="s4">)</span>
    <span class="s4">)</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;A kernel hyperparameter's specification in form of a namedtuple. 
 
    .. versionadded:: 0.18 
 
    Attributes 
    ---------- 
    name : str 
        The name of the hyperparameter. Note that a kernel using a 
        hyperparameter with name &quot;x&quot; must have the attributes self.x and 
        self.x_bounds 
 
    value_type : str 
        The type of the hyperparameter. Currently, only &quot;numeric&quot; 
        hyperparameters are supported. 
 
    bounds : pair of floats &gt;= 0 or &quot;fixed&quot; 
        The lower and upper bound on the parameter. If n_elements&gt;1, a pair 
        of 1d array with n_elements each may be given alternatively. If 
        the string &quot;fixed&quot; is passed as bounds, the hyperparameter's value 
        cannot be changed. 
 
    n_elements : int, default=1 
        The number of elements of the hyperparameter value. Defaults to 1, 
        which corresponds to a scalar hyperparameter. n_elements &gt; 1 
        corresponds to a hyperparameter which is vector-valued, 
        such as, e.g., anisotropic length-scales. 
 
    fixed : bool, default=None 
        Whether the value of this hyperparameter is fixed, i.e., cannot be 
        changed during hyperparameter tuning. If None is passed, the &quot;fixed&quot; is 
        derived based on the given bounds. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import ConstantKernel 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import Hyperparameter 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=50, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = ConstantKernel(constant_value=1.0, 
    ...    constant_value_bounds=(0.0, 10.0)) 
 
    We can access each hyperparameter: 
 
    &gt;&gt;&gt; for hyperparameter in kernel.hyperparameters: 
    ...    print(hyperparameter) 
    Hyperparameter(name='constant_value', value_type='numeric', 
    bounds=array([[ 0., 10.]]), n_elements=1, fixed=False) 
 
    &gt;&gt;&gt; params = kernel.get_params() 
    &gt;&gt;&gt; for key in sorted(params): print(f&quot;{key} : {params[key]}&quot;) 
    constant_value : 1.0 
    constant_value_bounds : (0.0, 10.0) 
    &quot;&quot;&quot;</span>

    <span class="s2"># A raw namedtuple is very memory efficient as it packs the attributes</span>
    <span class="s2"># in a struct to get rid of the __dict__ of attributes in particular it</span>
    <span class="s2"># does not copy the string for the keys on each instance.</span>
    <span class="s2"># By deriving a namedtuple class just to introduce the __init__ method we</span>
    <span class="s2"># would also reintroduce the __dict__ on the instance. By telling the</span>
    <span class="s2"># Python interpreter that this subclass uses static __slots__ instead of</span>
    <span class="s2"># dynamic attributes. Furthermore we don't need any additional slot in the</span>
    <span class="s2"># subclass so we set __slots__ to the empty tuple.</span>
    <span class="s1">__slots__ </span><span class="s4">= ()</span>

    <span class="s3">def </span><span class="s1">__new__</span><span class="s4">(</span><span class="s1">cls</span><span class="s4">, </span><span class="s1">name</span><span class="s4">, </span><span class="s1">value_type</span><span class="s4">, </span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">n_elements</span><span class="s4">=</span><span class="s5">1</span><span class="s4">, </span><span class="s1">fixed</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">str</span><span class="s4">) </span><span class="s3">or </span><span class="s1">bounds </span><span class="s4">!= </span><span class="s6">&quot;fixed&quot;</span><span class="s4">:</span>
            <span class="s1">bounds </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">bounds</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">n_elements </span><span class="s4">&gt; </span><span class="s5">1</span><span class="s4">:  </span><span class="s2"># vector-valued parameter</span>
                <span class="s3">if </span><span class="s1">bounds</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
                    <span class="s1">bounds </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">repeat</span><span class="s4">(</span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">n_elements</span><span class="s4">, </span><span class="s5">0</span><span class="s4">)</span>
                <span class="s3">elif </span><span class="s1">bounds</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] != </span><span class="s1">n_elements</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s6">&quot;Bounds on %s should have either 1 or &quot;</span>
                        <span class="s6">&quot;%d dimensions. Given are %d&quot;</span>
                        <span class="s4">% (</span><span class="s1">name</span><span class="s4">, </span><span class="s1">n_elements</span><span class="s4">, </span><span class="s1">bounds</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">])</span>
                    <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">fixed </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">fixed </span><span class="s4">= </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">str</span><span class="s4">) </span><span class="s3">and </span><span class="s1">bounds </span><span class="s4">== </span><span class="s6">&quot;fixed&quot;</span>
        <span class="s3">return </span><span class="s1">super</span><span class="s4">(</span><span class="s1">Hyperparameter</span><span class="s4">, </span><span class="s1">cls</span><span class="s4">).</span><span class="s1">__new__</span><span class="s4">(</span>
            <span class="s1">cls</span><span class="s4">, </span><span class="s1">name</span><span class="s4">, </span><span class="s1">value_type</span><span class="s4">, </span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">n_elements</span><span class="s4">, </span><span class="s1">fixed</span>
        <span class="s4">)</span>

    <span class="s2"># This is mainly a testing utility to check that two hyperparameters</span>
    <span class="s2"># are equal.</span>
    <span class="s3">def </span><span class="s1">__eq__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">other</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">name </span><span class="s4">== </span><span class="s1">other</span><span class="s4">.</span><span class="s1">name</span>
            <span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">value_type </span><span class="s4">== </span><span class="s1">other</span><span class="s4">.</span><span class="s1">value_type</span>
            <span class="s3">and </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">bounds </span><span class="s4">== </span><span class="s1">other</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">)</span>
            <span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_elements </span><span class="s4">== </span><span class="s1">other</span><span class="s4">.</span><span class="s1">n_elements</span>
            <span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">fixed </span><span class="s4">== </span><span class="s1">other</span><span class="s4">.</span><span class="s1">fixed</span>
        <span class="s4">)</span>


<span class="s3">class </span><span class="s1">Kernel</span><span class="s4">(</span><span class="s1">metaclass</span><span class="s4">=</span><span class="s1">ABCMeta</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Base class for all kernels. 
 
    .. versionadded:: 0.18 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import Kernel, RBF 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; class CustomKernel(Kernel): 
    ...     def __init__(self, length_scale=1.0): 
    ...         self.length_scale = length_scale 
    ...     def __call__(self, X, Y=None): 
    ...         if Y is None: 
    ...             Y = X 
    ...         return np.inner(X, X if Y is None else Y) ** 2 
    ...     def diag(self, X): 
    ...         return np.ones(X.shape[0]) 
    ...     def is_stationary(self): 
    ...         return True 
    &gt;&gt;&gt; kernel = CustomKernel(length_scale=2.0) 
    &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]]) 
    &gt;&gt;&gt; print(kernel(X)) 
    [[ 25 121] 
     [121 625]] 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">get_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">deep</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get parameters of this kernel. 
 
        Parameters 
        ---------- 
        deep : bool, default=True 
            If True, will return the parameters for this estimator and 
            contained subobjects that are estimators. 
 
        Returns 
        ------- 
        params : dict 
            Parameter names mapped to their values. 
        &quot;&quot;&quot;</span>
        <span class="s1">params </span><span class="s4">= </span><span class="s1">dict</span><span class="s4">()</span>

        <span class="s2"># introspect the constructor arguments to find the model parameters</span>
        <span class="s2"># to represent</span>
        <span class="s1">cls </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span>
        <span class="s1">init </span><span class="s4">= </span><span class="s1">getattr</span><span class="s4">(</span><span class="s1">cls</span><span class="s4">.</span><span class="s1">__init__</span><span class="s4">, </span><span class="s6">&quot;deprecated_original&quot;</span><span class="s4">, </span><span class="s1">cls</span><span class="s4">.</span><span class="s1">__init__</span><span class="s4">)</span>
        <span class="s1">init_sign </span><span class="s4">= </span><span class="s1">signature</span><span class="s4">(</span><span class="s1">init</span><span class="s4">)</span>
        <span class="s1">args</span><span class="s4">, </span><span class="s1">varargs </span><span class="s4">= [], []</span>
        <span class="s3">for </span><span class="s1">parameter </span><span class="s3">in </span><span class="s1">init_sign</span><span class="s4">.</span><span class="s1">parameters</span><span class="s4">.</span><span class="s1">values</span><span class="s4">():</span>
            <span class="s3">if </span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">kind </span><span class="s4">!= </span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">VAR_KEYWORD </span><span class="s3">and </span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">name </span><span class="s4">!= </span><span class="s6">&quot;self&quot;</span><span class="s4">:</span>
                <span class="s1">args</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">kind </span><span class="s4">== </span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">VAR_POSITIONAL</span><span class="s4">:</span>
                <span class="s1">varargs</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">parameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">varargs</span><span class="s4">) != </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">RuntimeError</span><span class="s4">(</span>
                <span class="s6">&quot;scikit-learn kernels should always &quot;</span>
                <span class="s6">&quot;specify their parameters in the signature&quot;</span>
                <span class="s6">&quot; of their __init__ (no varargs).&quot;</span>
                <span class="s6">&quot; %s doesn't follow this convention.&quot; </span><span class="s4">% (</span><span class="s1">cls</span><span class="s4">,)</span>
            <span class="s4">)</span>
        <span class="s3">for </span><span class="s1">arg </span><span class="s3">in </span><span class="s1">args</span><span class="s4">:</span>
            <span class="s1">params</span><span class="s4">[</span><span class="s1">arg</span><span class="s4">] = </span><span class="s1">getattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">arg</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">params</span>

    <span class="s3">def </span><span class="s1">set_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, **</span><span class="s1">params</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Set the parameters of this kernel. 
 
        The method works on simple kernels as well as on nested kernels. 
        The latter have parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` 
        so that it's possible to update each component of a nested object. 
 
        Returns 
        ------- 
        self 
        &quot;&quot;&quot;</span>
        <span class="s3">if not </span><span class="s1">params</span><span class="s4">:</span>
            <span class="s2"># Simple optimisation to gain speed (inspect is slow)</span>
            <span class="s3">return </span><span class="s1">self</span>
        <span class="s1">valid_params </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">(</span><span class="s1">deep</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
        <span class="s3">for </span><span class="s1">key</span><span class="s4">, </span><span class="s1">value </span><span class="s3">in </span><span class="s1">params</span><span class="s4">.</span><span class="s1">items</span><span class="s4">():</span>
            <span class="s1">split </span><span class="s4">= </span><span class="s1">key</span><span class="s4">.</span><span class="s1">split</span><span class="s4">(</span><span class="s6">&quot;__&quot;</span><span class="s4">, </span><span class="s5">1</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">split</span><span class="s4">) &gt; </span><span class="s5">1</span><span class="s4">:</span>
                <span class="s2"># nested objects case</span>
                <span class="s1">name</span><span class="s4">, </span><span class="s1">sub_name </span><span class="s4">= </span><span class="s1">split</span>
                <span class="s3">if </span><span class="s1">name </span><span class="s3">not in </span><span class="s1">valid_params</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s6">&quot;Invalid parameter %s for kernel %s. &quot;</span>
                        <span class="s6">&quot;Check the list of available parameters &quot;</span>
                        <span class="s6">&quot;with `kernel.get_params().keys()`.&quot; </span><span class="s4">% (</span><span class="s1">name</span><span class="s4">, </span><span class="s1">self</span><span class="s4">)</span>
                    <span class="s4">)</span>
                <span class="s1">sub_object </span><span class="s4">= </span><span class="s1">valid_params</span><span class="s4">[</span><span class="s1">name</span><span class="s4">]</span>
                <span class="s1">sub_object</span><span class="s4">.</span><span class="s1">set_params</span><span class="s4">(**{</span><span class="s1">sub_name</span><span class="s4">: </span><span class="s1">value</span><span class="s4">})</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s2"># simple objects case</span>
                <span class="s3">if </span><span class="s1">key </span><span class="s3">not in </span><span class="s1">valid_params</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s6">&quot;Invalid parameter %s for kernel %s. &quot;</span>
                        <span class="s6">&quot;Check the list of available parameters &quot;</span>
                        <span class="s6">&quot;with `kernel.get_params().keys()`.&quot;</span>
                        <span class="s4">% (</span><span class="s1">key</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">)</span>
                    <span class="s4">)</span>
                <span class="s1">setattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">key</span><span class="s4">, </span><span class="s1">value</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">clone_with_theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">theta</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns a clone of self with given hyperparameters theta. 
 
        Parameters 
        ---------- 
        theta : ndarray of shape (n_dims,) 
            The hyperparameters 
        &quot;&quot;&quot;</span>
        <span class="s1">cloned </span><span class="s4">= </span><span class="s1">clone</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">cloned</span><span class="s4">.</span><span class="s1">theta </span><span class="s4">= </span><span class="s1">theta</span>
        <span class="s3">return </span><span class="s1">cloned</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">n_dims</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the number of non-fixed hyperparameters of the kernel.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameters</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns a list of all hyperparameter specifications.&quot;&quot;&quot;</span>
        <span class="s1">r </span><span class="s4">= [</span>
            <span class="s1">getattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">attr</span><span class="s4">)</span>
            <span class="s3">for </span><span class="s1">attr </span><span class="s3">in </span><span class="s1">dir</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">attr</span><span class="s4">.</span><span class="s1">startswith</span><span class="s4">(</span><span class="s6">&quot;hyperparameter_&quot;</span><span class="s4">)</span>
        <span class="s4">]</span>
        <span class="s3">return </span><span class="s1">r</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Note that theta are typically the log-transformed values of the 
        kernel's hyperparameters as this representation of the search space 
        is more amenable for hyperparameter search, as hyperparameters like 
        length-scales naturally live on a log-scale. 
 
        Returns 
        ------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s1">theta </span><span class="s4">= []</span>
        <span class="s1">params </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">()</span>
        <span class="s3">for </span><span class="s1">hyperparameter </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameters</span><span class="s4">:</span>
            <span class="s3">if not </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s1">theta</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">params</span><span class="s4">[</span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">])</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">) &gt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">hstack</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([])</span>

    <span class="s4">@</span><span class="s1">theta</span><span class="s4">.</span><span class="s1">setter</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">theta</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Sets the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Parameters 
        ---------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s1">params </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">()</span>
        <span class="s1">i </span><span class="s4">= </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">hyperparameter </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameters</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s3">continue</span>
            <span class="s3">if </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">n_elements </span><span class="s4">&gt; </span><span class="s5">1</span><span class="s4">:</span>
                <span class="s2"># vector-valued parameter</span>
                <span class="s1">params</span><span class="s4">[</span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span>
                    <span class="s1">theta</span><span class="s4">[</span><span class="s1">i </span><span class="s4">: </span><span class="s1">i </span><span class="s4">+ </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">n_elements</span><span class="s4">]</span>
                <span class="s4">)</span>
                <span class="s1">i </span><span class="s4">+= </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">n_elements</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">params</span><span class="s4">[</span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">] = </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">[</span><span class="s1">i</span><span class="s4">])</span>
                <span class="s1">i </span><span class="s4">+= </span><span class="s5">1</span>

        <span class="s3">if </span><span class="s1">i </span><span class="s4">!= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s6">&quot;theta has not the correct number of entries.&quot;</span>
                <span class="s6">&quot; Should be %d; given are %d&quot; </span><span class="s4">% (</span><span class="s1">i</span><span class="s4">, </span><span class="s1">len</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">))</span>
            <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">set_params</span><span class="s4">(**</span><span class="s1">params</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">bounds</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the log-transformed bounds on the theta. 
 
        Returns 
        ------- 
        bounds : ndarray of shape (n_dims, 2) 
            The log-transformed bounds on the kernel's hyperparameters theta 
        &quot;&quot;&quot;</span>
        <span class="s1">bounds </span><span class="s4">= [</span>
            <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">bounds</span>
            <span class="s3">for </span><span class="s1">hyperparameter </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameters</span>
            <span class="s3">if not </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">fixed</span>
        <span class="s4">]</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">bounds</span><span class="s4">) &gt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">vstack</span><span class="s4">(</span><span class="s1">bounds</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([])</span>

    <span class="s3">def </span><span class="s1">__add__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">b</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
            <span class="s3">return </span><span class="s1">Sum</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">ConstantKernel</span><span class="s4">(</span><span class="s1">b</span><span class="s4">))</span>
        <span class="s3">return </span><span class="s1">Sum</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__radd__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">b</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
            <span class="s3">return </span><span class="s1">Sum</span><span class="s4">(</span><span class="s1">ConstantKernel</span><span class="s4">(</span><span class="s1">b</span><span class="s4">), </span><span class="s1">self</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">Sum</span><span class="s4">(</span><span class="s1">b</span><span class="s4">, </span><span class="s1">self</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__mul__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">b</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
            <span class="s3">return </span><span class="s1">Product</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">ConstantKernel</span><span class="s4">(</span><span class="s1">b</span><span class="s4">))</span>
        <span class="s3">return </span><span class="s1">Product</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__rmul__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">b</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
            <span class="s3">return </span><span class="s1">Product</span><span class="s4">(</span><span class="s1">ConstantKernel</span><span class="s4">(</span><span class="s1">b</span><span class="s4">), </span><span class="s1">self</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">Product</span><span class="s4">(</span><span class="s1">b</span><span class="s4">, </span><span class="s1">self</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__pow__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Exponentiation</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__eq__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">type</span><span class="s4">(</span><span class="s1">self</span><span class="s4">) != </span><span class="s1">type</span><span class="s4">(</span><span class="s1">b</span><span class="s4">):</span>
            <span class="s3">return False</span>
        <span class="s1">params_a </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">()</span>
        <span class="s1">params_b </span><span class="s4">= </span><span class="s1">b</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">()</span>
        <span class="s3">for </span><span class="s1">key </span><span class="s3">in </span><span class="s1">set</span><span class="s4">(</span><span class="s1">list</span><span class="s4">(</span><span class="s1">params_a</span><span class="s4">.</span><span class="s1">keys</span><span class="s4">()) + </span><span class="s1">list</span><span class="s4">(</span><span class="s1">params_b</span><span class="s4">.</span><span class="s1">keys</span><span class="s4">())):</span>
            <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">params_a</span><span class="s4">.</span><span class="s1">get</span><span class="s4">(</span><span class="s1">key</span><span class="s4">, </span><span class="s3">None</span><span class="s4">) != </span><span class="s1">params_b</span><span class="s4">.</span><span class="s1">get</span><span class="s4">(</span><span class="s1">key</span><span class="s4">, </span><span class="s3">None</span><span class="s4">)):</span>
                <span class="s3">return False</span>
        <span class="s3">return True</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0}({1})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s6">&quot;, &quot;</span><span class="s4">.</span><span class="s1">join</span><span class="s4">(</span><span class="s1">map</span><span class="s4">(</span><span class="s6">&quot;{0:.3g}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">))</span>
        <span class="s4">)</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Evaluate the kernel.&quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples,) 
            Left argument of the returned kernel k(X, Y) 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">requires_vector_input</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is defined on fixed-length feature 
        vectors or generic objects. Defaults to True for backward 
        compatibility.&quot;&quot;&quot;</span>
        <span class="s3">return True</span>

    <span class="s3">def </span><span class="s1">_check_bounds_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Called after fitting to warn if bounds may have been too tight.&quot;&quot;&quot;</span>
        <span class="s1">list_close </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isclose</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">).</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s1">idx </span><span class="s4">= </span><span class="s5">0</span>
        <span class="s3">for </span><span class="s1">hyp </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameters</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">hyp</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s3">continue</span>
            <span class="s3">for </span><span class="s1">dim </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">hyp</span><span class="s4">.</span><span class="s1">n_elements</span><span class="s4">):</span>
                <span class="s3">if </span><span class="s1">list_close</span><span class="s4">[</span><span class="s1">idx</span><span class="s4">, </span><span class="s5">0</span><span class="s4">]:</span>
                    <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                        <span class="s6">&quot;The optimal value found for &quot;</span>
                        <span class="s6">&quot;dimension %s of parameter %s is &quot;</span>
                        <span class="s6">&quot;close to the specified lower &quot;</span>
                        <span class="s6">&quot;bound %s. Decreasing the bound and&quot;</span>
                        <span class="s6">&quot; calling fit again may find a &quot;</span>
                        <span class="s6">&quot;better value.&quot; </span><span class="s4">% (</span><span class="s1">dim</span><span class="s4">, </span><span class="s1">hyp</span><span class="s4">.</span><span class="s1">name</span><span class="s4">, </span><span class="s1">hyp</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">[</span><span class="s1">dim</span><span class="s4">][</span><span class="s5">0</span><span class="s4">]),</span>
                        <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
                    <span class="s4">)</span>
                <span class="s3">elif </span><span class="s1">list_close</span><span class="s4">[</span><span class="s1">idx</span><span class="s4">, </span><span class="s5">1</span><span class="s4">]:</span>
                    <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                        <span class="s6">&quot;The optimal value found for &quot;</span>
                        <span class="s6">&quot;dimension %s of parameter %s is &quot;</span>
                        <span class="s6">&quot;close to the specified upper &quot;</span>
                        <span class="s6">&quot;bound %s. Increasing the bound and&quot;</span>
                        <span class="s6">&quot; calling fit again may find a &quot;</span>
                        <span class="s6">&quot;better value.&quot; </span><span class="s4">% (</span><span class="s1">dim</span><span class="s4">, </span><span class="s1">hyp</span><span class="s4">.</span><span class="s1">name</span><span class="s4">, </span><span class="s1">hyp</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">[</span><span class="s1">dim</span><span class="s4">][</span><span class="s5">1</span><span class="s4">]),</span>
                        <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
                    <span class="s4">)</span>
                <span class="s1">idx </span><span class="s4">+= </span><span class="s5">1</span>


<span class="s3">class </span><span class="s1">NormalizedKernelMixin</span><span class="s4">:</span>
    <span class="s0">&quot;&quot;&quot;Mixin for kernels which are normalized: k(X, X)=1. 
 
    .. versionadded:: 0.18 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">])</span>


<span class="s3">class </span><span class="s1">StationaryKernelMixin</span><span class="s4">:</span>
    <span class="s0">&quot;&quot;&quot;Mixin for kernels which are stationary: k(X, Y)= f(X-Y). 
 
    .. versionadded:: 0.18 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return True</span>


<span class="s3">class </span><span class="s1">GenericKernelMixin</span><span class="s4">:</span>
    <span class="s0">&quot;&quot;&quot;Mixin for kernels which operate on generic objects such as variable- 
    length sequences, trees, and graphs. 
 
    .. versionadded:: 0.22 
    &quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">requires_vector_input</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Whether the kernel works only on fixed-length feature vectors.&quot;&quot;&quot;</span>
        <span class="s3">return False</span>


<span class="s3">class </span><span class="s1">CompoundKernel</span><span class="s4">(</span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Kernel which is composed of a set of other kernels. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    kernels : list of Kernels 
        The other kernels 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import WhiteKernel 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import RBF 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import CompoundKernel 
    &gt;&gt;&gt; kernel = CompoundKernel( 
    ...     [WhiteKernel(noise_level=3.0), RBF(length_scale=2.0)]) 
    &gt;&gt;&gt; print(kernel.bounds) 
    [[-11.51292546  11.51292546] 
     [-11.51292546  11.51292546]] 
    &gt;&gt;&gt; print(kernel.n_dims) 
    2 
    &gt;&gt;&gt; print(kernel.theta) 
    [1.09861229 0.69314718] 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">kernels</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">kernels </span><span class="s4">= </span><span class="s1">kernels</span>

    <span class="s3">def </span><span class="s1">get_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">deep</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get parameters of this kernel. 
 
        Parameters 
        ---------- 
        deep : bool, default=True 
            If True, will return the parameters for this estimator and 
            contained subobjects that are estimators. 
 
        Returns 
        ------- 
        params : dict 
            Parameter names mapped to their values. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">kernels</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Note that theta are typically the log-transformed values of the 
        kernel's hyperparameters as this representation of the search space 
        is more amenable for hyperparameter search, as hyperparameters like 
        length-scales naturally live on a log-scale. 
 
        Returns 
        ------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">hstack</span><span class="s4">([</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">theta </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">])</span>

    <span class="s4">@</span><span class="s1">theta</span><span class="s4">.</span><span class="s1">setter</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">theta</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Sets the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Parameters 
        ---------- 
        theta : array of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s1">k_dims </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">n_dims</span>
        <span class="s3">for </span><span class="s1">i</span><span class="s4">, </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">enumerate</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">):</span>
            <span class="s1">kernel</span><span class="s4">.</span><span class="s1">theta </span><span class="s4">= </span><span class="s1">theta</span><span class="s4">[</span><span class="s1">i </span><span class="s4">* </span><span class="s1">k_dims </span><span class="s4">: (</span><span class="s1">i </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">) * </span><span class="s1">k_dims</span><span class="s4">]</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">bounds</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the log-transformed bounds on the theta. 
 
        Returns 
        ------- 
        bounds : array of shape (n_dims, 2) 
            The log-transformed bounds on the kernel's hyperparameters theta 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">vstack</span><span class="s4">([</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">bounds </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">])</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Note that this compound kernel returns the results of all simple kernel 
        stacked along an additional axis. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object, \ 
            default=None 
            Left argument of the returned kernel k(X, Y) 
 
        Y : array-like of shape (n_samples_X, n_features) or list of object, \ 
            default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            is evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of the 
            kernel hyperparameter is computed. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y, n_kernels) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape \ 
                (n_samples_X, n_samples_X, n_dims, n_kernels), optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= []</span>
            <span class="s1">K_grad </span><span class="s4">= []</span>
            <span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">:</span>
                <span class="s1">K_single</span><span class="s4">, </span><span class="s1">K_grad_single </span><span class="s4">= </span><span class="s1">kernel</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">)</span>
                <span class="s1">K</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">K_single</span><span class="s4">)</span>
                <span class="s1">K_grad</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">K_grad_single</span><span class="s4">[..., </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">])</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dstack</span><span class="s4">(</span><span class="s1">K</span><span class="s4">), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">concatenate</span><span class="s4">(</span><span class="s1">K_grad</span><span class="s4">, </span><span class="s5">3</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dstack</span><span class="s4">([</span><span class="s1">kernel</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">) </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">])</span>

    <span class="s3">def </span><span class="s1">__eq__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">type</span><span class="s4">(</span><span class="s1">self</span><span class="s4">) != </span><span class="s1">type</span><span class="s4">(</span><span class="s1">b</span><span class="s4">) </span><span class="s3">or </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">) != </span><span class="s1">len</span><span class="s4">(</span><span class="s1">b</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">):</span>
            <span class="s3">return False</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span>
            <span class="s4">[</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] == </span><span class="s1">b</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">))]</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">([</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">is_stationary</span><span class="s4">() </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">])</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">requires_vector_input</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is defined on discrete structures.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">([</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">requires_vector_input </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">])</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to `np.diag(self(X))`; however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Argument to the kernel. 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X, n_kernels) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">vstack</span><span class="s4">([</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">X</span><span class="s4">) </span><span class="s3">for </span><span class="s1">kernel </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernels</span><span class="s4">]).</span><span class="s1">T</span>


<span class="s3">class </span><span class="s1">KernelOperator</span><span class="s4">(</span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Base class for all kernel operators. 
 
    .. versionadded:: 0.18 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">k1</span><span class="s4">, </span><span class="s1">k2</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">k1 </span><span class="s4">= </span><span class="s1">k1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">k2 </span><span class="s4">= </span><span class="s1">k2</span>

    <span class="s3">def </span><span class="s1">get_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">deep</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get parameters of this kernel. 
 
        Parameters 
        ---------- 
        deep : bool, default=True 
            If True, will return the parameters for this estimator and 
            contained subobjects that are estimators. 
 
        Returns 
        ------- 
        params : dict 
            Parameter names mapped to their values. 
        &quot;&quot;&quot;</span>
        <span class="s1">params </span><span class="s4">= </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">k1</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">, </span><span class="s1">k2</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">deep</span><span class="s4">:</span>
            <span class="s1">deep_items </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">().</span><span class="s1">items</span><span class="s4">()</span>
            <span class="s1">params</span><span class="s4">.</span><span class="s1">update</span><span class="s4">((</span><span class="s6">&quot;k1__&quot; </span><span class="s4">+ </span><span class="s1">k</span><span class="s4">, </span><span class="s1">val</span><span class="s4">) </span><span class="s3">for </span><span class="s1">k</span><span class="s4">, </span><span class="s1">val </span><span class="s3">in </span><span class="s1">deep_items</span><span class="s4">)</span>
            <span class="s1">deep_items </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">().</span><span class="s1">items</span><span class="s4">()</span>
            <span class="s1">params</span><span class="s4">.</span><span class="s1">update</span><span class="s4">((</span><span class="s6">&quot;k2__&quot; </span><span class="s4">+ </span><span class="s1">k</span><span class="s4">, </span><span class="s1">val</span><span class="s4">) </span><span class="s3">for </span><span class="s1">k</span><span class="s4">, </span><span class="s1">val </span><span class="s3">in </span><span class="s1">deep_items</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">params</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameters</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns a list of all hyperparameter.&quot;&quot;&quot;</span>
        <span class="s1">r </span><span class="s4">= [</span>
            <span class="s1">Hyperparameter</span><span class="s4">(</span>
                <span class="s6">&quot;k1__&quot; </span><span class="s4">+ </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">,</span>
                <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">value_type</span><span class="s4">,</span>
                <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">,</span>
                <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">n_elements</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s3">for </span><span class="s1">hyperparameter </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">hyperparameters</span>
        <span class="s4">]</span>

        <span class="s3">for </span><span class="s1">hyperparameter </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">hyperparameters</span><span class="s4">:</span>
            <span class="s1">r</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span>
                <span class="s1">Hyperparameter</span><span class="s4">(</span>
                    <span class="s6">&quot;k2__&quot; </span><span class="s4">+ </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">,</span>
                    <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">value_type</span><span class="s4">,</span>
                    <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">,</span>
                    <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">n_elements</span><span class="s4">,</span>
                <span class="s4">)</span>
            <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">r</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Note that theta are typically the log-transformed values of the 
        kernel's hyperparameters as this representation of the search space 
        is more amenable for hyperparameter search, as hyperparameters like 
        length-scales naturally live on a log-scale. 
 
        Returns 
        ------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">theta</span><span class="s4">.</span><span class="s1">setter</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">theta</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Sets the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Parameters 
        ---------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s1">k1_dims </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">n_dims</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">theta </span><span class="s4">= </span><span class="s1">theta</span><span class="s4">[:</span><span class="s1">k1_dims</span><span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">theta </span><span class="s4">= </span><span class="s1">theta</span><span class="s4">[</span><span class="s1">k1_dims</span><span class="s4">:]</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">bounds</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the log-transformed bounds on the theta. 
 
        Returns 
        ------- 
        bounds : ndarray of shape (n_dims, 2) 
            The log-transformed bounds on the kernel's hyperparameters theta 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">.</span><span class="s1">size </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">bounds</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">.</span><span class="s1">size </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">bounds</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">vstack</span><span class="s4">((</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">))</span>

    <span class="s3">def </span><span class="s1">__eq__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">type</span><span class="s4">(</span><span class="s1">self</span><span class="s4">) != </span><span class="s1">type</span><span class="s4">(</span><span class="s1">b</span><span class="s4">):</span>
            <span class="s3">return False</span>
        <span class="s3">return </span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1 </span><span class="s4">== </span><span class="s1">b</span><span class="s4">.</span><span class="s1">k1 </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2 </span><span class="s4">== </span><span class="s1">b</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">) </span><span class="s3">or </span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">k1 </span><span class="s4">== </span><span class="s1">b</span><span class="s4">.</span><span class="s1">k2 </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2 </span><span class="s4">== </span><span class="s1">b</span><span class="s4">.</span><span class="s1">k1</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">is_stationary</span><span class="s4">() </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">is_stationary</span><span class="s4">()</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">requires_vector_input</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">requires_vector_input </span><span class="s3">or </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">requires_vector_input</span>


<span class="s3">class </span><span class="s1">Sum</span><span class="s4">(</span><span class="s1">KernelOperator</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;The `Sum` kernel takes two kernels :math:`k_1` and :math:`k_2` 
    and combines them via 
 
    .. math:: 
        k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y) 
 
    Note that the `__add__` magic method is overridden, so 
    `Sum(RBF(), RBF())` is equivalent to using the + operator 
    with `RBF() + RBF()`. 
 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    k1 : Kernel 
        The first base-kernel of the sum-kernel 
 
    k2 : Kernel 
        The second base-kernel of the sum-kernel 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import RBF, Sum, ConstantKernel 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = Sum(ConstantKernel(2), RBF()) 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    1.0 
    &gt;&gt;&gt; kernel 
    1.41**2 + RBF(length_scale=1) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Left argument of the returned kernel k(X, Y) 
 
        Y : array-like of shape (n_samples_X, n_features) or list of object,\ 
                default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            is evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s1">K1</span><span class="s4">, </span><span class="s1">K1_gradient </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
            <span class="s1">K2</span><span class="s4">, </span><span class="s1">K2_gradient </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
            <span class="s3">return </span><span class="s1">K1 </span><span class="s4">+ </span><span class="s1">K2</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dstack</span><span class="s4">((</span><span class="s1">K1_gradient</span><span class="s4">, </span><span class="s1">K2_gradient</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to `np.diag(self(X))`; however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Argument to the kernel. 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">X</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0} + {1}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">)</span>


<span class="s3">class </span><span class="s1">Product</span><span class="s4">(</span><span class="s1">KernelOperator</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;The `Product` kernel takes two kernels :math:`k_1` and :math:`k_2` 
    and combines them via 
 
    .. math:: 
        k_{prod}(X, Y) = k_1(X, Y) * k_2(X, Y) 
 
    Note that the `__mul__` magic method is overridden, so 
    `Product(RBF(), RBF())` is equivalent to using the * operator 
    with `RBF() * RBF()`. 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    k1 : Kernel 
        The first base-kernel of the product-kernel 
 
    k2 : Kernel 
        The second base-kernel of the product-kernel 
 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import (RBF, Product, 
    ...            ConstantKernel) 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = Product(ConstantKernel(2), RBF()) 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    1.0 
    &gt;&gt;&gt; kernel 
    1.41**2 * RBF(length_scale=1) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Left argument of the returned kernel k(X, Y) 
 
        Y : array-like of shape (n_samples_Y, n_features) or list of object,\ 
            default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            is evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s1">K1</span><span class="s4">, </span><span class="s1">K1_gradient </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
            <span class="s1">K2</span><span class="s4">, </span><span class="s1">K2_gradient </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
            <span class="s3">return </span><span class="s1">K1 </span><span class="s4">* </span><span class="s1">K2</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dstack</span><span class="s4">(</span>
                <span class="s4">(</span><span class="s1">K1_gradient </span><span class="s4">* </span><span class="s1">K2</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">], </span><span class="s1">K2_gradient </span><span class="s4">* </span><span class="s1">K1</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">])</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">) * </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Argument to the kernel. 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">X</span><span class="s4">) * </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0} * {1}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">k1</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">k2</span><span class="s4">)</span>


<span class="s3">class </span><span class="s1">Exponentiation</span><span class="s4">(</span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;The Exponentiation kernel takes one base kernel and a scalar parameter 
    :math:`p` and combines them via 
 
    .. math:: 
        k_{exp}(X, Y) = k(X, Y) ^p 
 
    Note that the `__pow__` magic method is overridden, so 
    `Exponentiation(RBF(), 2)` is equivalent to using the ** operator 
    with `RBF() ** 2`. 
 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    kernel : Kernel 
        The base kernel 
 
    exponent : float 
        The exponent for the base kernel 
 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import (RationalQuadratic, 
    ...            Exponentiation) 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = Exponentiation(RationalQuadratic(), exponent=2) 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, alpha=5, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    0.419... 
    &gt;&gt;&gt; gpr.predict(X[:1,:], return_std=True) 
    (array([635.5...]), array([0.559...])) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">kernel</span><span class="s4">, </span><span class="s1">exponent</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">kernel </span><span class="s4">= </span><span class="s1">kernel</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">exponent </span><span class="s4">= </span><span class="s1">exponent</span>

    <span class="s3">def </span><span class="s1">get_params</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">deep</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get parameters of this kernel. 
 
        Parameters 
        ---------- 
        deep : bool, default=True 
            If True, will return the parameters for this estimator and 
            contained subobjects that are estimators. 
 
        Returns 
        ------- 
        params : dict 
            Parameter names mapped to their values. 
        &quot;&quot;&quot;</span>
        <span class="s1">params </span><span class="s4">= </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">kernel</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">, </span><span class="s1">exponent</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">deep</span><span class="s4">:</span>
            <span class="s1">deep_items </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">get_params</span><span class="s4">().</span><span class="s1">items</span><span class="s4">()</span>
            <span class="s1">params</span><span class="s4">.</span><span class="s1">update</span><span class="s4">((</span><span class="s6">&quot;kernel__&quot; </span><span class="s4">+ </span><span class="s1">k</span><span class="s4">, </span><span class="s1">val</span><span class="s4">) </span><span class="s3">for </span><span class="s1">k</span><span class="s4">, </span><span class="s1">val </span><span class="s3">in </span><span class="s1">deep_items</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">params</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameters</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns a list of all hyperparameter.&quot;&quot;&quot;</span>
        <span class="s1">r </span><span class="s4">= []</span>
        <span class="s3">for </span><span class="s1">hyperparameter </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">hyperparameters</span><span class="s4">:</span>
            <span class="s1">r</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span>
                <span class="s1">Hyperparameter</span><span class="s4">(</span>
                    <span class="s6">&quot;kernel__&quot; </span><span class="s4">+ </span><span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">name</span><span class="s4">,</span>
                    <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">value_type</span><span class="s4">,</span>
                    <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">bounds</span><span class="s4">,</span>
                    <span class="s1">hyperparameter</span><span class="s4">.</span><span class="s1">n_elements</span><span class="s4">,</span>
                <span class="s4">)</span>
            <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">r</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Note that theta are typically the log-transformed values of the 
        kernel's hyperparameters as this representation of the search space 
        is more amenable for hyperparameter search, as hyperparameters like 
        length-scales naturally live on a log-scale. 
 
        Returns 
        ------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">theta</span>

    <span class="s4">@</span><span class="s1">theta</span><span class="s4">.</span><span class="s1">setter</span>
    <span class="s3">def </span><span class="s1">theta</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">theta</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Sets the (flattened, log-transformed) non-fixed hyperparameters. 
 
        Parameters 
        ---------- 
        theta : ndarray of shape (n_dims,) 
            The non-fixed, log-transformed hyperparameters of the kernel 
        &quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">theta </span><span class="s4">= </span><span class="s1">theta</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">bounds</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the log-transformed bounds on the theta. 
 
        Returns 
        ------- 
        bounds : ndarray of shape (n_dims, 2) 
            The log-transformed bounds on the kernel's hyperparameters theta 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">bounds</span>

    <span class="s3">def </span><span class="s1">__eq__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">b</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">type</span><span class="s4">(</span><span class="s1">self</span><span class="s4">) != </span><span class="s1">type</span><span class="s4">(</span><span class="s1">b</span><span class="s4">):</span>
            <span class="s3">return False</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel </span><span class="s4">== </span><span class="s1">b</span><span class="s4">.</span><span class="s1">kernel </span><span class="s3">and </span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent </span><span class="s4">== </span><span class="s1">b</span><span class="s4">.</span><span class="s1">exponent</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Left argument of the returned kernel k(X, Y) 
 
        Y : array-like of shape (n_samples_Y, n_features) or list of object,\ 
            default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            is evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
            <span class="s1">K_gradient </span><span class="s4">*= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent </span><span class="s4">* </span><span class="s1">K</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">] ** (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent </span><span class="s4">- </span><span class="s5">1</span><span class="s4">)</span>
            <span class="s3">return </span><span class="s1">K</span><span class="s4">**</span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent</span><span class="s4">, </span><span class="s1">K_gradient</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
            <span class="s3">return </span><span class="s1">K</span><span class="s4">**</span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Argument to the kernel. 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">X</span><span class="s4">) ** </span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0} ** {1}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">exponent</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">is_stationary</span><span class="s4">()</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">requires_vector_input</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is defined on discrete structures.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">.</span><span class="s1">requires_vector_input</span>


<span class="s3">class </span><span class="s1">ConstantKernel</span><span class="s4">(</span><span class="s1">StationaryKernelMixin</span><span class="s4">, </span><span class="s1">GenericKernelMixin</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Constant kernel. 
 
    Can be used as part of a product-kernel where it scales the magnitude of 
    the other factor (kernel) or as part of a sum-kernel, where it modifies 
    the mean of the Gaussian process. 
 
    .. math:: 
        k(x_1, x_2) = constant\\_value \\;\\forall\\; x_1, x_2 
 
    Adding a constant kernel is equivalent to adding a constant:: 
 
            kernel = RBF() + ConstantKernel(constant_value=2) 
 
    is the same as:: 
 
            kernel = RBF() + 2 
 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    constant_value : float, default=1.0 
        The constant value which defines the covariance: 
        k(x_1, x_2) = constant_value 
 
    constant_value_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on `constant_value`. 
        If set to &quot;fixed&quot;, `constant_value` cannot be changed during 
        hyperparameter tuning. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import RBF, ConstantKernel 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = RBF() + ConstantKernel(constant_value=2) 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, alpha=5, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    0.3696... 
    &gt;&gt;&gt; gpr.predict(X[:1,:], return_std=True) 
    (array([606.1...]), array([0.24...])) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">constant_value</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">, </span><span class="s1">constant_value_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">)):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value </span><span class="s4">= </span><span class="s1">constant_value</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value_bounds </span><span class="s4">= </span><span class="s1">constant_value_bounds</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_constant_value</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;constant_value&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Left argument of the returned kernel k(X, Y) 
 
        Y : array-like of shape (n_samples_X, n_features) or list of object, \ 
            default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            is evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \ 
            optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when eval_gradient 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">Y </span><span class="s4">= </span><span class="s1">X</span>
        <span class="s3">elif </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>

        <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">(</span>
            <span class="s4">(</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">)),</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">,</span>
            <span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">).</span><span class="s1">dtype</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_constant_value</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s4">(</span>
                    <span class="s1">K</span><span class="s4">,</span>
                    <span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">(</span>
                        <span class="s4">(</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s5">1</span><span class="s4">),</span>
                        <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">,</span>
                        <span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">).</span><span class="s1">dtype</span><span class="s4">,</span>
                    <span class="s4">),</span>
                <span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s5">0</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Argument to the kernel. 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">(</span>
            <span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">),</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">,</span>
            <span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">).</span><span class="s1">dtype</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0:.3g}**2&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">constant_value</span><span class="s4">))</span>


<span class="s3">class </span><span class="s1">WhiteKernel</span><span class="s4">(</span><span class="s1">StationaryKernelMixin</span><span class="s4">, </span><span class="s1">GenericKernelMixin</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;White kernel. 
 
    The main use-case of this kernel is as part of a sum-kernel where it 
    explains the noise of the signal as independently and identically 
    normally-distributed. The parameter noise_level equals the variance of this 
    noise. 
 
    .. math:: 
        k(x_1, x_2) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0 
 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    noise_level : float, default=1.0 
        Parameter controlling the noise level (variance) 
 
    noise_level_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'noise_level'. 
        If set to &quot;fixed&quot;, 'noise_level' cannot be changed during 
        hyperparameter tuning. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = DotProduct() + WhiteKernel(noise_level=0.5) 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    0.3680... 
    &gt;&gt;&gt; gpr.predict(X[:2,:], return_std=True) 
    (array([653.0..., 592.1... ]), array([316.6..., 316.6...])) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">noise_level</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">, </span><span class="s1">noise_level_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">)):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level </span><span class="s4">= </span><span class="s1">noise_level</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level_bounds </span><span class="s4">= </span><span class="s1">noise_level_bounds</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_noise_level</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;noise_level&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Left argument of the returned kernel k(X, Y) 
 
        Y : array-like of shape (n_samples_X, n_features) or list of object,\ 
            default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            is evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\ 
            optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when eval_gradient 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is not None and </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">eye</span><span class="s4">(</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">))</span>
            <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
                <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_noise_level</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                    <span class="s3">return </span><span class="s4">(</span>
                        <span class="s1">K</span><span class="s4">,</span>
                        <span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">eye</span><span class="s4">(</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">))[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">],</span>
                    <span class="s4">)</span>
                <span class="s3">else</span><span class="s4">:</span>
                    <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s5">0</span><span class="s4">))</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">K</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">)))</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples_X, n_features) or list of object 
            Argument to the kernel. 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">(</span>
            <span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level</span><span class="s4">).</span><span class="s1">dtype</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0}(noise_level={1:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">noise_level</span>
        <span class="s4">)</span>


<span class="s3">class </span><span class="s1">RBF</span><span class="s4">(</span><span class="s1">StationaryKernelMixin</span><span class="s4">, </span><span class="s1">NormalizedKernelMixin</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Radial basis function kernel (aka squared-exponential kernel). 
 
    The RBF kernel is a stationary kernel. It is also known as the 
    &quot;squared exponential&quot; kernel. It is parameterized by a length scale 
    parameter :math:`l&gt;0`, which can either be a scalar (isotropic variant 
    of the kernel) or a vector with the same number of dimensions as the inputs 
    X (anisotropic variant of the kernel). The kernel is given by: 
 
    .. math:: 
        k(x_i, x_j) = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right) 
 
    where :math:`l` is the length scale of the kernel and 
    :math:`d(\\cdot,\\cdot)` is the Euclidean distance. 
    For advice on how to set the length scale parameter, see e.g. [1]_. 
 
    This kernel is infinitely differentiable, which implies that GPs with this 
    kernel as covariance function have mean square derivatives of all orders, 
    and are thus very smooth. 
    See [2]_, Chapter 4, Section 4.2, for further details of the RBF kernel. 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    length_scale : float or ndarray of shape (n_features,), default=1.0 
        The length scale of the kernel. If a float, an isotropic kernel is 
        used. If an array, an anisotropic kernel is used where each dimension 
        of l defines the length-scale of the respective feature dimension. 
 
    length_scale_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'length_scale'. 
        If set to &quot;fixed&quot;, 'length_scale' cannot be changed during 
        hyperparameter tuning. 
 
    References 
    ---------- 
    .. [1] `David Duvenaud (2014). &quot;The Kernel Cookbook: 
        Advice on Covariance functions&quot;. 
        &lt;https://www.cs.toronto.edu/~duvenaud/cookbook/&gt;`_ 
 
    .. [2] `Carl Edward Rasmussen, Christopher K. I. Williams (2006). 
        &quot;Gaussian Processes for Machine Learning&quot;. The MIT Press. 
        &lt;http://www.gaussianprocess.org/gpml/&gt;`_ 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessClassifier 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import RBF 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; kernel = 1.0 * RBF(1.0) 
    &gt;&gt;&gt; gpc = GaussianProcessClassifier(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpc.score(X, y) 
    0.9866... 
    &gt;&gt;&gt; gpc.predict_proba(X[:2,:]) 
    array([[0.8354..., 0.03228..., 0.1322...], 
           [0.7906..., 0.0652..., 0.1441...]]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">length_scale</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">, </span><span class="s1">length_scale_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">)):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale </span><span class="s4">= </span><span class="s1">length_scale</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds </span><span class="s4">= </span><span class="s1">length_scale_bounds</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">anisotropic</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">iterable</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">) </span><span class="s3">and </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">) &gt; </span><span class="s5">1</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_length_scale</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span>
                <span class="s6">&quot;length_scale&quot;</span><span class="s4">,</span>
                <span class="s6">&quot;numeric&quot;</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds</span><span class="s4">,</span>
                <span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">),</span>
            <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;length_scale&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Y : ndarray of shape (n_samples_Y, n_features), default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            if evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">length_scale </span><span class="s4">= </span><span class="s1">_check_length_scale</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">pdist</span><span class="s4">(</span><span class="s1">X </span><span class="s4">/ </span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;sqeuclidean&quot;</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s5">0.5 </span><span class="s4">* </span><span class="s1">dists</span><span class="s4">)</span>
            <span class="s2"># convert from upper-triangular matrix to square matrix</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">squareform</span><span class="s4">(</span><span class="s1">K</span><span class="s4">)</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">fill_diagonal</span><span class="s4">(</span><span class="s1">K</span><span class="s4">, </span><span class="s5">1</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">cdist</span><span class="s4">(</span><span class="s1">X </span><span class="s4">/ </span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">Y </span><span class="s4">/ </span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;sqeuclidean&quot;</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s5">0.5 </span><span class="s4">* </span><span class="s1">dists</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_length_scale</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s2"># Hyperparameter l kept fixed</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>
            <span class="s3">elif not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic </span><span class="s3">or </span><span class="s1">length_scale</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] == </span><span class="s5">1</span><span class="s4">:</span>
                <span class="s1">K_gradient </span><span class="s4">= (</span><span class="s1">K </span><span class="s4">* </span><span class="s1">squareform</span><span class="s4">(</span><span class="s1">dists</span><span class="s4">))[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient</span>
            <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic</span><span class="s4">:</span>
                <span class="s2"># We need to recompute the pairwise dimension-wise distances</span>
                <span class="s1">K_gradient </span><span class="s4">= (</span><span class="s1">X</span><span class="s4">[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">, :] - </span><span class="s1">X</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">, :, :]) ** </span><span class="s5">2 </span><span class="s4">/ (</span>
                    <span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2</span>
                <span class="s4">)</span>
                <span class="s1">K_gradient </span><span class="s4">*= </span><span class="s1">K</span><span class="s4">[..., </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s6">&quot;{0}(length_scale=[{1}])&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">,</span>
                <span class="s6">&quot;, &quot;</span><span class="s4">.</span><span class="s1">join</span><span class="s4">(</span><span class="s1">map</span><span class="s4">(</span><span class="s6">&quot;{0:.3g}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)),</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:  </span><span class="s2"># isotropic</span>
            <span class="s3">return </span><span class="s6">&quot;{0}(length_scale={1:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)[</span><span class="s5">0</span><span class="s4">]</span>
            <span class="s4">)</span>


<span class="s3">class </span><span class="s1">Matern</span><span class="s4">(</span><span class="s1">RBF</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Matern kernel. 
 
    The class of Matern kernels is a generalization of the :class:`RBF`. 
    It has an additional parameter :math:`\\nu` which controls the 
    smoothness of the resulting function. The smaller :math:`\\nu`, 
    the less smooth the approximated function is. 
    As :math:`\\nu\\rightarrow\\infty`, the kernel becomes equivalent to 
    the :class:`RBF` kernel. When :math:`\\nu = 1/2`, the Matérn kernel 
    becomes identical to the absolute exponential kernel. 
    Important intermediate values are 
    :math:`\\nu=1.5` (once differentiable functions) 
    and :math:`\\nu=2.5` (twice differentiable functions). 
 
    The kernel is given by: 
 
    .. math:: 
         k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg( 
         \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j ) 
         \\Bigg)^\\nu K_\\nu\\Bigg( 
         \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg) 
 
 
 
    where :math:`d(\\cdot,\\cdot)` is the Euclidean distance, 
    :math:`K_{\\nu}(\\cdot)` is a modified Bessel function and 
    :math:`\\Gamma(\\cdot)` is the gamma function. 
    See [1]_, Chapter 4, Section 4.2, for details regarding the different 
    variants of the Matern kernel. 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    length_scale : float or ndarray of shape (n_features,), default=1.0 
        The length scale of the kernel. If a float, an isotropic kernel is 
        used. If an array, an anisotropic kernel is used where each dimension 
        of l defines the length-scale of the respective feature dimension. 
 
    length_scale_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'length_scale'. 
        If set to &quot;fixed&quot;, 'length_scale' cannot be changed during 
        hyperparameter tuning. 
 
    nu : float, default=1.5 
        The parameter nu controlling the smoothness of the learned function. 
        The smaller nu, the less smooth the approximated function is. 
        For nu=inf, the kernel becomes equivalent to the RBF kernel and for 
        nu=0.5 to the absolute exponential kernel. Important intermediate 
        values are nu=1.5 (once differentiable functions) and nu=2.5 
        (twice differentiable functions). Note that values of nu not in 
        [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost 
        (appr. 10 times higher) since they require to evaluate the modified 
        Bessel function. Furthermore, in contrast to l, nu is kept fixed to 
        its initial value and not optimized. 
 
    References 
    ---------- 
    .. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006). 
        &quot;Gaussian Processes for Machine Learning&quot;. The MIT Press. 
        &lt;http://www.gaussianprocess.org/gpml/&gt;`_ 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessClassifier 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import Matern 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; kernel = 1.0 * Matern(length_scale=1.0, nu=1.5) 
    &gt;&gt;&gt; gpc = GaussianProcessClassifier(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpc.score(X, y) 
    0.9866... 
    &gt;&gt;&gt; gpc.predict_proba(X[:2,:]) 
    array([[0.8513..., 0.0368..., 0.1117...], 
            [0.8086..., 0.0693..., 0.1220...]]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">length_scale</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">, </span><span class="s1">length_scale_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">), </span><span class="s1">nu</span><span class="s4">=</span><span class="s5">1.5</span><span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">length_scale_bounds</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">= </span><span class="s1">nu</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Y : ndarray of shape (n_samples_Y, n_features), default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            if evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">length_scale </span><span class="s4">= </span><span class="s1">_check_length_scale</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">pdist</span><span class="s4">(</span><span class="s1">X </span><span class="s4">/ </span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;euclidean&quot;</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">cdist</span><span class="s4">(</span><span class="s1">X </span><span class="s4">/ </span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">Y </span><span class="s4">/ </span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;euclidean&quot;</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s5">0.5</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s1">dists</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s5">1.5</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">dists </span><span class="s4">* </span><span class="s1">math</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">3</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= (</span><span class="s5">1.0 </span><span class="s4">+ </span><span class="s1">K</span><span class="s4">) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s1">K</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s5">2.5</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">dists </span><span class="s4">* </span><span class="s1">math</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">5</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= (</span><span class="s5">1.0 </span><span class="s4">+ </span><span class="s1">K </span><span class="s4">+ </span><span class="s1">K</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">/ </span><span class="s5">3.0</span><span class="s4">) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s1">K</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-(</span><span class="s1">dists</span><span class="s4">**</span><span class="s5">2</span><span class="s4">) / </span><span class="s5">2.0</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:  </span><span class="s2"># general case; expensive to evaluate</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">dists</span>
            <span class="s1">K</span><span class="s4">[</span><span class="s1">K </span><span class="s4">== </span><span class="s5">0.0</span><span class="s4">] += </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">float</span><span class="s4">).</span><span class="s1">eps  </span><span class="s2"># strict zeros result in nan</span>
            <span class="s1">tmp </span><span class="s4">= </span><span class="s1">math</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span><span class="s4">) * </span><span class="s1">K</span>
            <span class="s1">K</span><span class="s4">.</span><span class="s1">fill</span><span class="s4">((</span><span class="s5">2 </span><span class="s4">** (</span><span class="s5">1.0 </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span><span class="s4">)) / </span><span class="s1">gamma</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span><span class="s4">))</span>
            <span class="s1">K </span><span class="s4">*= </span><span class="s1">tmp</span><span class="s4">**</span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span>
            <span class="s1">K </span><span class="s4">*= </span><span class="s1">kv</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span><span class="s4">, </span><span class="s1">tmp</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s2"># convert from upper-triangular matrix to square matrix</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">squareform</span><span class="s4">(</span><span class="s1">K</span><span class="s4">)</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">fill_diagonal</span><span class="s4">(</span><span class="s1">K</span><span class="s4">, </span><span class="s5">1</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_length_scale</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s2"># Hyperparameter l kept fixed</span>
                <span class="s1">K_gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient</span>

            <span class="s2"># We need to recompute the pairwise dimension-wise distances</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic</span><span class="s4">:</span>
                <span class="s1">D </span><span class="s4">= (</span><span class="s1">X</span><span class="s4">[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">, :] - </span><span class="s1">X</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">, :, :]) ** </span><span class="s5">2 </span><span class="s4">/ (</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">D </span><span class="s4">= </span><span class="s1">squareform</span><span class="s4">(</span><span class="s1">dists</span><span class="s4">**</span><span class="s5">2</span><span class="s4">)[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>

            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s5">0.5</span><span class="s4">:</span>
                <span class="s1">denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">D</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s5">2</span><span class="s4">))[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
                <span class="s1">divide_result </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">D</span><span class="s4">)</span>
                <span class="s1">np</span><span class="s4">.</span><span class="s1">divide</span><span class="s4">(</span>
                    <span class="s1">D</span><span class="s4">,</span>
                    <span class="s1">denominator</span><span class="s4">,</span>
                    <span class="s1">out</span><span class="s4">=</span><span class="s1">divide_result</span><span class="s4">,</span>
                    <span class="s1">where</span><span class="s4">=</span><span class="s1">denominator </span><span class="s4">!= </span><span class="s5">0</span><span class="s4">,</span>
                <span class="s4">)</span>
                <span class="s1">K_gradient </span><span class="s4">= </span><span class="s1">K</span><span class="s4">[..., </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">] * </span><span class="s1">divide_result</span>
            <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s5">1.5</span><span class="s4">:</span>
                <span class="s1">K_gradient </span><span class="s4">= </span><span class="s5">3 </span><span class="s4">* </span><span class="s1">D </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">3 </span><span class="s4">* </span><span class="s1">D</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(-</span><span class="s5">1</span><span class="s4">)))[..., </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s5">2.5</span><span class="s4">:</span>
                <span class="s1">tmp </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s5">5 </span><span class="s4">* </span><span class="s1">D</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(-</span><span class="s5">1</span><span class="s4">))[..., </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
                <span class="s1">K_gradient </span><span class="s4">= </span><span class="s5">5.0 </span><span class="s4">/ </span><span class="s5">3.0 </span><span class="s4">* </span><span class="s1">D </span><span class="s4">* (</span><span class="s1">tmp </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">) * </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s1">tmp</span><span class="s4">)</span>
            <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu </span><span class="s4">== </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inf</span><span class="s4">:</span>
                <span class="s1">K_gradient </span><span class="s4">= </span><span class="s1">D </span><span class="s4">* </span><span class="s1">K</span><span class="s4">[..., </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s2"># approximate gradient numerically</span>
                <span class="s3">def </span><span class="s1">f</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">):  </span><span class="s2"># helper function</span>
                    <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">clone_with_theta</span><span class="s4">(</span><span class="s1">theta</span><span class="s4">)(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">)</span>

                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">_approx_fprime</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">, </span><span class="s1">f</span><span class="s4">, </span><span class="s5">1e-10</span><span class="s4">)</span>

            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient</span><span class="s4">[:, :].</span><span class="s1">sum</span><span class="s4">(-</span><span class="s5">1</span><span class="s4">)[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">anisotropic</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s6">&quot;{0}(length_scale=[{1}], nu={2:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">,</span>
                <span class="s6">&quot;, &quot;</span><span class="s4">.</span><span class="s1">join</span><span class="s4">(</span><span class="s1">map</span><span class="s4">(</span><span class="s6">&quot;{0:.3g}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)),</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span><span class="s4">,</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s6">&quot;{0}(length_scale={1:.3g}, nu={2:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">nu</span>
            <span class="s4">)</span>


<span class="s3">class </span><span class="s1">RationalQuadratic</span><span class="s4">(</span><span class="s1">StationaryKernelMixin</span><span class="s4">, </span><span class="s1">NormalizedKernelMixin</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Rational Quadratic kernel. 
 
    The RationalQuadratic kernel can be seen as a scale mixture (an infinite 
    sum) of RBF kernels with different characteristic length scales. It is 
    parameterized by a length scale parameter :math:`l&gt;0` and a scale 
    mixture parameter :math:`\\alpha&gt;0`. Only the isotropic variant 
    where length_scale :math:`l` is a scalar is supported at the moment. 
    The kernel is given by: 
 
    .. math:: 
        k(x_i, x_j) = \\left( 
        1 + \\frac{d(x_i, x_j)^2 }{ 2\\alpha  l^2}\\right)^{-\\alpha} 
 
    where :math:`\\alpha` is the scale mixture parameter, :math:`l` is 
    the length scale of the kernel and :math:`d(\\cdot,\\cdot)` is the 
    Euclidean distance. 
    For advice on how to set the parameters, see e.g. [1]_. 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    length_scale : float &gt; 0, default=1.0 
        The length scale of the kernel. 
 
    alpha : float &gt; 0, default=1.0 
        Scale mixture parameter 
 
    length_scale_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'length_scale'. 
        If set to &quot;fixed&quot;, 'length_scale' cannot be changed during 
        hyperparameter tuning. 
 
    alpha_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'alpha'. 
        If set to &quot;fixed&quot;, 'alpha' cannot be changed during 
        hyperparameter tuning. 
 
    References 
    ---------- 
    .. [1] `David Duvenaud (2014). &quot;The Kernel Cookbook: 
        Advice on Covariance functions&quot;. 
        &lt;https://www.cs.toronto.edu/~duvenaud/cookbook/&gt;`_ 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessClassifier 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import RationalQuadratic 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; kernel = RationalQuadratic(length_scale=1.0, alpha=1.5) 
    &gt;&gt;&gt; gpc = GaussianProcessClassifier(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpc.score(X, y) 
    0.9733... 
    &gt;&gt;&gt; gpc.predict_proba(X[:2,:]) 
    array([[0.8881..., 0.0566..., 0.05518...], 
            [0.8678..., 0.0707... , 0.0614...]]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">length_scale</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">,</span>
        <span class="s1">alpha</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">,</span>
        <span class="s1">length_scale_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">),</span>
        <span class="s1">alpha_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">),</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale </span><span class="s4">= </span><span class="s1">length_scale</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">alpha </span><span class="s4">= </span><span class="s1">alpha</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds </span><span class="s4">= </span><span class="s1">length_scale_bounds</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_bounds </span><span class="s4">= </span><span class="s1">alpha_bounds</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_length_scale</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;length_scale&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_alpha</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;alpha&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Y : ndarray of shape (n_samples_Y, n_features), default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            if evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims) 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when eval_gradient 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_1d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">)) &gt; </span><span class="s5">1</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">AttributeError</span><span class="s4">(</span>
                <span class="s6">&quot;RationalQuadratic kernel only supports isotropic version, &quot;</span>
                <span class="s6">&quot;please use a single scalar for length_scale&quot;</span>
            <span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">squareform</span><span class="s4">(</span><span class="s1">pdist</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;sqeuclidean&quot;</span><span class="s4">))</span>
            <span class="s1">tmp </span><span class="s4">= </span><span class="s1">dists </span><span class="s4">/ (</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2</span><span class="s4">)</span>
            <span class="s1">base </span><span class="s4">= </span><span class="s5">1 </span><span class="s4">+ </span><span class="s1">tmp</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">base</span><span class="s4">**-</span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">fill_diagonal</span><span class="s4">(</span><span class="s1">K</span><span class="s4">, </span><span class="s5">1</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">cdist</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;sqeuclidean&quot;</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= (</span><span class="s5">1 </span><span class="s4">+ </span><span class="s1">dists </span><span class="s4">/ (</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2</span><span class="s4">)) ** -</span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span>

        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s2"># gradient with respect to length_scale</span>
            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_length_scale</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s1">length_scale_gradient </span><span class="s4">= </span><span class="s1">dists </span><span class="s4">* </span><span class="s1">K </span><span class="s4">/ (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">base</span><span class="s4">)</span>
                <span class="s1">length_scale_gradient </span><span class="s4">= </span><span class="s1">length_scale_gradient</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">else</span><span class="s4">:  </span><span class="s2"># l is kept fixed</span>
                <span class="s1">length_scale_gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>

            <span class="s2"># gradient with respect to alpha</span>
            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_alpha</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s1">alpha_gradient </span><span class="s4">= </span><span class="s1">K </span><span class="s4">* (</span>
                    <span class="s4">-</span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">base</span><span class="s4">)</span>
                    <span class="s4">+ </span><span class="s1">dists </span><span class="s4">/ (</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">base</span><span class="s4">)</span>
                <span class="s4">)</span>
                <span class="s1">alpha_gradient </span><span class="s4">= </span><span class="s1">alpha_gradient</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">else</span><span class="s4">:  </span><span class="s2"># alpha is kept fixed</span>
                <span class="s1">alpha_gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>

            <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dstack</span><span class="s4">((</span><span class="s1">alpha_gradient</span><span class="s4">, </span><span class="s1">length_scale_gradient</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0}(alpha={1:.3g}, length_scale={2:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span>
        <span class="s4">)</span>


<span class="s3">class </span><span class="s1">ExpSineSquared</span><span class="s4">(</span><span class="s1">StationaryKernelMixin</span><span class="s4">, </span><span class="s1">NormalizedKernelMixin</span><span class="s4">, </span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">r&quot;&quot;&quot;Exp-Sine-Squared kernel (aka periodic kernel). 
 
    The ExpSineSquared kernel allows one to model functions which repeat 
    themselves exactly. It is parameterized by a length scale 
    parameter :math:`l&gt;0` and a periodicity parameter :math:`p&gt;0`. 
    Only the isotropic variant where :math:`l` is a scalar is 
    supported at the moment. The kernel is given by: 
 
    .. math:: 
        k(x_i, x_j) = \text{exp}\left(- 
        \frac{ 2\sin^2(\pi d(x_i, x_j)/p) }{ l^ 2} \right) 
 
    where :math:`l` is the length scale of the kernel, :math:`p` the 
    periodicity of the kernel and :math:`d(\cdot,\cdot)` is the 
    Euclidean distance. 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
 
    length_scale : float &gt; 0, default=1.0 
        The length scale of the kernel. 
 
    periodicity : float &gt; 0, default=1.0 
        The periodicity of the kernel. 
 
    length_scale_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'length_scale'. 
        If set to &quot;fixed&quot;, 'length_scale' cannot be changed during 
        hyperparameter tuning. 
 
    periodicity_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'periodicity'. 
        If set to &quot;fixed&quot;, 'periodicity' cannot be changed during 
        hyperparameter tuning. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import ExpSineSquared 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=50, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = ExpSineSquared(length_scale=1, periodicity=1) 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, alpha=5, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    0.0144... 
    &gt;&gt;&gt; gpr.predict(X[:2,:], return_std=True) 
    (array([425.6..., 457.5...]), array([0.3894..., 0.3467...])) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">length_scale</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">,</span>
        <span class="s1">periodicity</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">,</span>
        <span class="s1">length_scale_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">),</span>
        <span class="s1">periodicity_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">),</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale </span><span class="s4">= </span><span class="s1">length_scale</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">periodicity </span><span class="s4">= </span><span class="s1">periodicity</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds </span><span class="s4">= </span><span class="s1">length_scale_bounds</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">periodicity_bounds </span><span class="s4">= </span><span class="s1">periodicity_bounds</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_length_scale</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the length scale&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;length_scale&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale_bounds</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_periodicity</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;periodicity&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">periodicity_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Y : ndarray of shape (n_samples_Y, n_features), default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            if evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">squareform</span><span class="s4">(</span><span class="s1">pdist</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;euclidean&quot;</span><span class="s4">))</span>
            <span class="s1">arg </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">pi </span><span class="s4">* </span><span class="s1">dists </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">periodicity</span>
            <span class="s1">sin_of_arg </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sin</span><span class="s4">(</span><span class="s1">arg</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s5">2 </span><span class="s4">* (</span><span class="s1">sin_of_arg </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">) ** </span><span class="s5">2</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>
            <span class="s1">dists </span><span class="s4">= </span><span class="s1">cdist</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;euclidean&quot;</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span>
                <span class="s4">-</span><span class="s5">2 </span><span class="s4">* (</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sin</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">pi </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">periodicity </span><span class="s4">* </span><span class="s1">dists</span><span class="s4">) / </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">) ** </span><span class="s5">2</span>
            <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s1">cos_of_arg </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">cos</span><span class="s4">(</span><span class="s1">arg</span><span class="s4">)</span>
            <span class="s2"># gradient with respect to length_scale</span>
            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_length_scale</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s1">length_scale_gradient </span><span class="s4">= </span><span class="s5">4 </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">sin_of_arg</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">K</span>
                <span class="s1">length_scale_gradient </span><span class="s4">= </span><span class="s1">length_scale_gradient</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">else</span><span class="s4">:  </span><span class="s2"># length_scale is kept fixed</span>
                <span class="s1">length_scale_gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>
            <span class="s2"># gradient with respect to p</span>
            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_periodicity</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s1">periodicity_gradient </span><span class="s4">= (</span>
                    <span class="s5">4 </span><span class="s4">* </span><span class="s1">arg </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">cos_of_arg </span><span class="s4">* </span><span class="s1">sin_of_arg </span><span class="s4">* </span><span class="s1">K</span>
                <span class="s4">)</span>
                <span class="s1">periodicity_gradient </span><span class="s4">= </span><span class="s1">periodicity_gradient</span><span class="s4">[:, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s3">else</span><span class="s4">:  </span><span class="s2"># p is kept fixed</span>
                <span class="s1">periodicity_gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>

            <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dstack</span><span class="s4">((</span><span class="s1">length_scale_gradient</span><span class="s4">, </span><span class="s1">periodicity_gradient</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0}(length_scale={1:.3g}, periodicity={2:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">length_scale</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">periodicity</span>
        <span class="s4">)</span>


<span class="s3">class </span><span class="s1">DotProduct</span><span class="s4">(</span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">r&quot;&quot;&quot;Dot-Product kernel. 
 
    The DotProduct kernel is non-stationary and can be obtained from linear 
    regression by putting :math:`N(0, 1)` priors on the coefficients 
    of :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \sigma_0^2)` 
    on the bias. The DotProduct kernel is invariant to a rotation of 
    the coordinates about the origin, but not translations. 
    It is parameterized by a parameter sigma_0 :math:`\sigma` 
    which controls the inhomogenity of the kernel. For :math:`\sigma_0^2 =0`, 
    the kernel is called the homogeneous linear kernel, otherwise 
    it is inhomogeneous. The kernel is given by 
 
    .. math:: 
        k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j 
 
    The DotProduct kernel is commonly combined with exponentiation. 
 
    See [1]_, Chapter 4, Section 4.2, for further details regarding the 
    DotProduct kernel. 
 
    Read more in the :ref:`User Guide &lt;gp_kernels&gt;`. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    sigma_0 : float &gt;= 0, default=1.0 
        Parameter controlling the inhomogenity of the kernel. If sigma_0=0, 
        the kernel is homogeneous. 
 
    sigma_0_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'sigma_0'. 
        If set to &quot;fixed&quot;, 'sigma_0' cannot be changed during 
        hyperparameter tuning. 
 
    References 
    ---------- 
    .. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006). 
        &quot;Gaussian Processes for Machine Learning&quot;. The MIT Press. 
        &lt;http://www.gaussianprocess.org/gpml/&gt;`_ 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_friedman2 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel 
    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0) 
    &gt;&gt;&gt; kernel = DotProduct() + WhiteKernel() 
    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpr.score(X, y) 
    0.3680... 
    &gt;&gt;&gt; gpr.predict(X[:2,:], return_std=True) 
    (array([653.0..., 592.1...]), array([316.6..., 316.6...])) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">sigma_0</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">, </span><span class="s1">sigma_0_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">)):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0 </span><span class="s4">= </span><span class="s1">sigma_0</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0_bounds </span><span class="s4">= </span><span class="s1">sigma_0_bounds</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_sigma_0</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;sigma_0&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Y : ndarray of shape (n_samples_Y, n_features), default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            if evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">Y </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inner</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">X</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0</span><span class="s4">**</span><span class="s5">2</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Gradient can only be evaluated when Y is None.&quot;</span><span class="s4">)</span>
            <span class="s1">K </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">inner</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0</span><span class="s4">**</span><span class="s5">2</span>

        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">if not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_sigma_0</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s1">K_gradient </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">K</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">], </span><span class="s5">1</span><span class="s4">))</span>
                <span class="s1">K_gradient</span><span class="s4">[..., </span><span class="s5">0</span><span class="s4">] = </span><span class="s5">2 </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0</span><span class="s4">**</span><span class="s5">2</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">K_gradient</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y). 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X). 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">einsum</span><span class="s4">(</span><span class="s6">&quot;ij,ij-&gt;i&quot;</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">X</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0</span><span class="s4">**</span><span class="s5">2</span>

    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return False</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0}(sigma_0={1:.3g})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">sigma_0</span><span class="s4">)</span>


<span class="s2"># adapted from scipy/optimize/optimize.py for functions with 2d output</span>
<span class="s3">def </span><span class="s1">_approx_fprime</span><span class="s4">(</span><span class="s1">xk</span><span class="s4">, </span><span class="s1">f</span><span class="s4">, </span><span class="s1">epsilon</span><span class="s4">, </span><span class="s1">args</span><span class="s4">=()):</span>
    <span class="s1">f0 </span><span class="s4">= </span><span class="s1">f</span><span class="s4">(*((</span><span class="s1">xk</span><span class="s4">,) + </span><span class="s1">args</span><span class="s4">))</span>
    <span class="s1">grad </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">f0</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">f0</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">], </span><span class="s1">len</span><span class="s4">(</span><span class="s1">xk</span><span class="s4">)), </span><span class="s1">float</span><span class="s4">)</span>
    <span class="s1">ei </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">len</span><span class="s4">(</span><span class="s1">xk</span><span class="s4">),), </span><span class="s1">float</span><span class="s4">)</span>
    <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">xk</span><span class="s4">)):</span>
        <span class="s1">ei</span><span class="s4">[</span><span class="s1">k</span><span class="s4">] = </span><span class="s5">1.0</span>
        <span class="s1">d </span><span class="s4">= </span><span class="s1">epsilon </span><span class="s4">* </span><span class="s1">ei</span>
        <span class="s1">grad</span><span class="s4">[:, :, </span><span class="s1">k</span><span class="s4">] = (</span><span class="s1">f</span><span class="s4">(*((</span><span class="s1">xk </span><span class="s4">+ </span><span class="s1">d</span><span class="s4">,) + </span><span class="s1">args</span><span class="s4">)) - </span><span class="s1">f0</span><span class="s4">) / </span><span class="s1">d</span><span class="s4">[</span><span class="s1">k</span><span class="s4">]</span>
        <span class="s1">ei</span><span class="s4">[</span><span class="s1">k</span><span class="s4">] = </span><span class="s5">0.0</span>
    <span class="s3">return </span><span class="s1">grad</span>


<span class="s3">class </span><span class="s1">PairwiseKernel</span><span class="s4">(</span><span class="s1">Kernel</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Wrapper for kernels in sklearn.metrics.pairwise. 
 
    A thin wrapper around the functionality of the kernels in 
    sklearn.metrics.pairwise. 
 
    Note: Evaluation of eval_gradient is not analytic but numeric and all 
          kernels support only isotropic distances. The parameter gamma is 
          considered to be a hyperparameter and may be optimized. The other 
          kernel parameters are set directly at initialization and are kept 
          fixed. 
 
    .. versionadded:: 0.18 
 
    Parameters 
    ---------- 
    gamma : float, default=1.0 
        Parameter gamma of the pairwise kernel specified by metric. It should 
        be positive. 
 
    gamma_bounds : pair of floats &gt;= 0 or &quot;fixed&quot;, default=(1e-5, 1e5) 
        The lower and upper bound on 'gamma'. 
        If set to &quot;fixed&quot;, 'gamma' cannot be changed during 
        hyperparameter tuning. 
 
    metric : {&quot;linear&quot;, &quot;additive_chi2&quot;, &quot;chi2&quot;, &quot;poly&quot;, &quot;polynomial&quot;, \ 
              &quot;rbf&quot;, &quot;laplacian&quot;, &quot;sigmoid&quot;, &quot;cosine&quot;} or callable, \ 
              default=&quot;linear&quot; 
        The metric to use when calculating kernel between instances in a 
        feature array. If metric is a string, it must be one of the metrics 
        in pairwise.PAIRWISE_KERNEL_FUNCTIONS. 
        If metric is &quot;precomputed&quot;, X is assumed to be a kernel matrix. 
        Alternatively, if metric is a callable function, it is called on each 
        pair of instances (rows) and the resulting value recorded. The callable 
        should take two arrays from X as input and return a value indicating 
        the distance between them. 
 
    pairwise_kernels_kwargs : dict, default=None 
        All entries of this dict (if any) are passed as keyword arguments to 
        the pairwise kernel function. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessClassifier 
    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import PairwiseKernel 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; kernel = PairwiseKernel(metric='rbf') 
    &gt;&gt;&gt; gpc = GaussianProcessClassifier(kernel=kernel, 
    ...         random_state=0).fit(X, y) 
    &gt;&gt;&gt; gpc.score(X, y) 
    0.9733... 
    &gt;&gt;&gt; gpc.predict_proba(X[:2,:]) 
    array([[0.8880..., 0.05663..., 0.05532...], 
           [0.8676..., 0.07073..., 0.06165...]]) 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">gamma</span><span class="s4">=</span><span class="s5">1.0</span><span class="s4">,</span>
        <span class="s1">gamma_bounds</span><span class="s4">=(</span><span class="s5">1e-5</span><span class="s4">, </span><span class="s5">1e5</span><span class="s4">),</span>
        <span class="s1">metric</span><span class="s4">=</span><span class="s6">&quot;linear&quot;</span><span class="s4">,</span>
        <span class="s1">pairwise_kernels_kwargs</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">gamma </span><span class="s4">= </span><span class="s1">gamma</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">gamma_bounds </span><span class="s4">= </span><span class="s1">gamma_bounds</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">metric </span><span class="s4">= </span><span class="s1">metric</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">pairwise_kernels_kwargs </span><span class="s4">= </span><span class="s1">pairwise_kernels_kwargs</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">hyperparameter_gamma</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">Hyperparameter</span><span class="s4">(</span><span class="s6">&quot;gamma&quot;</span><span class="s4">, </span><span class="s6">&quot;numeric&quot;</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gamma_bounds</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">__call__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">eval_gradient</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return the kernel k(X, Y) and optionally its gradient. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Y : ndarray of shape (n_samples_Y, n_features), default=None 
            Right argument of the returned kernel k(X, Y). If None, k(X, X) 
            if evaluated instead. 
 
        eval_gradient : bool, default=False 
            Determines whether the gradient with respect to the log of 
            the kernel hyperparameter is computed. 
            Only supported when Y is None. 
 
        Returns 
        ------- 
        K : ndarray of shape (n_samples_X, n_samples_Y) 
            Kernel k(X, Y) 
 
        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\ 
                optional 
            The gradient of the kernel k(X, X) with respect to the log of the 
            hyperparameter of the kernel. Only returned when `eval_gradient` 
            is True. 
        &quot;&quot;&quot;</span>
        <span class="s1">pairwise_kernels_kwargs </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">pairwise_kernels_kwargs</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">pairwise_kernels_kwargs </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">pairwise_kernels_kwargs </span><span class="s4">= {}</span>

        <span class="s1">X </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">K </span><span class="s4">= </span><span class="s1">pairwise_kernels</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">Y</span><span class="s4">,</span>
            <span class="s1">metric</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">metric</span><span class="s4">,</span>
            <span class="s1">gamma</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">gamma</span><span class="s4">,</span>
            <span class="s1">filter_params</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
            <span class="s4">**</span><span class="s1">pairwise_kernels_kwargs</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">eval_gradient</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">hyperparameter_gamma</span><span class="s4">.</span><span class="s1">fixed</span><span class="s4">:</span>
                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">empty</span><span class="s4">((</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s5">0</span><span class="s4">))</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s2"># approximate gradient numerically</span>
                <span class="s3">def </span><span class="s1">f</span><span class="s4">(</span><span class="s1">gamma</span><span class="s4">):  </span><span class="s2"># helper function</span>
                    <span class="s3">return </span><span class="s1">pairwise_kernels</span><span class="s4">(</span>
                        <span class="s1">X</span><span class="s4">,</span>
                        <span class="s1">Y</span><span class="s4">,</span>
                        <span class="s1">metric</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">metric</span><span class="s4">,</span>
                        <span class="s1">gamma</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span><span class="s1">gamma</span><span class="s4">),</span>
                        <span class="s1">filter_params</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
                        <span class="s4">**</span><span class="s1">pairwise_kernels_kwargs</span><span class="s4">,</span>
                    <span class="s4">)</span>

                <span class="s3">return </span><span class="s1">K</span><span class="s4">, </span><span class="s1">_approx_fprime</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta</span><span class="s4">, </span><span class="s1">f</span><span class="s4">, </span><span class="s5">1e-10</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">K</span>

    <span class="s3">def </span><span class="s1">diag</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns the diagonal of the kernel k(X, X). 
 
        The result of this method is identical to np.diag(self(X)); however, 
        it can be evaluated more efficiently since only the diagonal is 
        evaluated. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples_X, n_features) 
            Left argument of the returned kernel k(X, Y) 
 
        Returns 
        ------- 
        K_diag : ndarray of shape (n_samples_X,) 
            Diagonal of kernel k(X, X) 
        &quot;&quot;&quot;</span>
        <span class="s2"># We have to fall back to slow way of computing diagonal</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">apply_along_axis</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s1">X</span><span class="s4">).</span><span class="s1">ravel</span><span class="s4">()</span>

    <span class="s3">def </span><span class="s1">is_stationary</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Returns whether the kernel is stationary.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">metric </span><span class="s3">in </span><span class="s4">[</span><span class="s6">&quot;rbf&quot;</span><span class="s4">]</span>

    <span class="s3">def </span><span class="s1">__repr__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s6">&quot;{0}(gamma={1}, metric={2})&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gamma</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">metric</span>
        <span class="s4">)</span>
</pre>
</body>
</html>