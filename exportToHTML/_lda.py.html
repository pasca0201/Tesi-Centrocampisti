<html>
<head>
<title>_lda.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_lda.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
 
============================================================= 
Online Latent Dirichlet Allocation with variational inference 
============================================================= 
 
This implementation is modified from Matthew D. Hoffman's onlineldavb code 
Link: https://github.com/blei-lab/onlineldavb 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Chyi-Kwei Yau</span>
<span class="s2"># Author: Matthew D. Hoffman (original onlineldavb implementation)</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">sparse </span><span class="s3">as </span><span class="s1">sp</span>
<span class="s3">from </span><span class="s1">joblib </span><span class="s3">import </span><span class="s1">effective_n_jobs</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">special </span><span class="s3">import </span><span class="s1">gammaln</span><span class="s4">, </span><span class="s1">logsumexp</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">base </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">BaseEstimator</span><span class="s4">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s4">,</span>
    <span class="s1">TransformerMixin</span><span class="s4">,</span>
    <span class="s1">_fit_context</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils </span><span class="s3">import </span><span class="s1">check_random_state</span><span class="s4">, </span><span class="s1">gen_batches</span><span class="s4">, </span><span class="s1">gen_even_slices</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s4">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s4">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span><span class="s4">, </span><span class="s1">check_non_negative</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_online_lda_fast </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_dirichlet_expectation_1d </span><span class="s3">as </span><span class="s1">cy_dirichlet_expectation_1d</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_online_lda_fast </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_dirichlet_expectation_2d</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">_online_lda_fast </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">mean_change </span><span class="s3">as </span><span class="s1">cy_mean_change</span><span class="s4">,</span>
<span class="s4">)</span>

<span class="s1">EPS </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">float</span><span class="s4">).</span><span class="s1">eps</span>


<span class="s3">def </span><span class="s1">_update_doc_distribution</span><span class="s4">(</span>
    <span class="s1">X</span><span class="s4">,</span>
    <span class="s1">exp_topic_word_distr</span><span class="s4">,</span>
    <span class="s1">doc_topic_prior</span><span class="s4">,</span>
    <span class="s1">max_doc_update_iter</span><span class="s4">,</span>
    <span class="s1">mean_change_tol</span><span class="s4">,</span>
    <span class="s1">cal_sstats</span><span class="s4">,</span>
    <span class="s1">random_state</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;E-step: update document-topic distribution. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Document word matrix. 
 
    exp_topic_word_distr : ndarray of shape (n_topics, n_features) 
        Exponential value of expectation of log topic word distribution. 
        In the literature, this is `exp(E[log(beta)])`. 
 
    doc_topic_prior : float 
        Prior of document topic distribution `theta`. 
 
    max_doc_update_iter : int 
        Max number of iterations for updating document topic distribution in 
        the E-step. 
 
    mean_change_tol : float 
        Stopping tolerance for updating document topic distribution in E-step. 
 
    cal_sstats : bool 
        Parameter that indicate to calculate sufficient statistics or not. 
        Set `cal_sstats` to `True` when we need to run M-step. 
 
    random_state : RandomState instance or None 
        Parameter that indicate how to initialize document topic distribution. 
        Set `random_state` to None will initialize document topic distribution 
        to a constant number. 
 
    Returns 
    ------- 
    (doc_topic_distr, suff_stats) : 
        `doc_topic_distr` is unnormalized topic distribution for each document. 
        In the literature, this is `gamma`. we can calculate `E[log(theta)]` 
        from it. 
        `suff_stats` is expected sufficient statistics for the M-step. 
            When `cal_sstats == False`, this will be None. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">is_sparse_x </span><span class="s4">= </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
    <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>
    <span class="s1">n_topics </span><span class="s4">= </span><span class="s1">exp_topic_word_distr</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>

    <span class="s3">if </span><span class="s1">random_state</span><span class="s4">:</span>
        <span class="s1">doc_topic_distr </span><span class="s4">= </span><span class="s1">random_state</span><span class="s4">.</span><span class="s1">gamma</span><span class="s4">(</span><span class="s5">100.0</span><span class="s4">, </span><span class="s5">0.01</span><span class="s4">, (</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_topics</span><span class="s4">)).</span><span class="s1">astype</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">doc_topic_distr </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">((</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_topics</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>

    <span class="s2"># In the literature, this is `exp(E[log(theta)])`</span>
    <span class="s1">exp_doc_topic </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span><span class="s1">_dirichlet_expectation_2d</span><span class="s4">(</span><span class="s1">doc_topic_distr</span><span class="s4">))</span>

    <span class="s2"># diff on `component_` (only calculate it when `cal_diff` is True)</span>
    <span class="s1">suff_stats </span><span class="s4">= (</span>
        <span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">exp_topic_word_distr</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">) </span><span class="s3">if </span><span class="s1">cal_sstats </span><span class="s3">else None</span>
    <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">is_sparse_x</span><span class="s4">:</span>
        <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span>
        <span class="s1">X_indices </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indices</span>
        <span class="s1">X_indptr </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indptr</span>

    <span class="s2"># These cython functions are called in a nested loop on usually very small arrays</span>
    <span class="s2"># (length=n_topics). In that case, finding the appropriate signature of the</span>
    <span class="s2"># fused-typed function can be more costly than its execution, hence the dispatch</span>
    <span class="s2"># is done outside of the loop.</span>
    <span class="s1">ctype </span><span class="s4">= </span><span class="s6">&quot;float&quot; </span><span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype </span><span class="s4">== </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32 </span><span class="s3">else </span><span class="s6">&quot;double&quot;</span>
    <span class="s1">mean_change </span><span class="s4">= </span><span class="s1">cy_mean_change</span><span class="s4">[</span><span class="s1">ctype</span><span class="s4">]</span>
    <span class="s1">dirichlet_expectation_1d </span><span class="s4">= </span><span class="s1">cy_dirichlet_expectation_1d</span><span class="s4">[</span><span class="s1">ctype</span><span class="s4">]</span>
    <span class="s1">eps </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">).</span><span class="s1">eps</span>

    <span class="s3">for </span><span class="s1">idx_d </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">is_sparse_x</span><span class="s4">:</span>
            <span class="s1">ids </span><span class="s4">= </span><span class="s1">X_indices</span><span class="s4">[</span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">] : </span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">]]</span>
            <span class="s1">cnts </span><span class="s4">= </span><span class="s1">X_data</span><span class="s4">[</span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">] : </span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">]]</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">ids </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nonzero</span><span class="s4">(</span><span class="s1">X</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, :])[</span><span class="s5">0</span><span class="s4">]</span>
            <span class="s1">cnts </span><span class="s4">= </span><span class="s1">X</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, </span><span class="s1">ids</span><span class="s4">]</span>

        <span class="s1">doc_topic_d </span><span class="s4">= </span><span class="s1">doc_topic_distr</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, :]</span>
        <span class="s2"># The next one is a copy, since the inner loop overwrites it.</span>
        <span class="s1">exp_doc_topic_d </span><span class="s4">= </span><span class="s1">exp_doc_topic</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, :].</span><span class="s1">copy</span><span class="s4">()</span>
        <span class="s1">exp_topic_word_d </span><span class="s4">= </span><span class="s1">exp_topic_word_distr</span><span class="s4">[:, </span><span class="s1">ids</span><span class="s4">]</span>

        <span class="s2"># Iterate between `doc_topic_d` and `norm_phi` until convergence</span>
        <span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">max_doc_update_iter</span><span class="s4">):</span>
            <span class="s1">last_d </span><span class="s4">= </span><span class="s1">doc_topic_d</span>

            <span class="s2"># The optimal phi_{dwk} is proportional to</span>
            <span class="s2"># exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).</span>
            <span class="s1">norm_phi </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">exp_doc_topic_d</span><span class="s4">, </span><span class="s1">exp_topic_word_d</span><span class="s4">) + </span><span class="s1">eps</span>

            <span class="s1">doc_topic_d </span><span class="s4">= </span><span class="s1">exp_doc_topic_d </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">cnts </span><span class="s4">/ </span><span class="s1">norm_phi</span><span class="s4">, </span><span class="s1">exp_topic_word_d</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
            <span class="s2"># Note: adds doc_topic_prior to doc_topic_d, in-place.</span>
            <span class="s1">dirichlet_expectation_1d</span><span class="s4">(</span><span class="s1">doc_topic_d</span><span class="s4">, </span><span class="s1">doc_topic_prior</span><span class="s4">, </span><span class="s1">exp_doc_topic_d</span><span class="s4">)</span>

            <span class="s3">if </span><span class="s1">mean_change</span><span class="s4">(</span><span class="s1">last_d</span><span class="s4">, </span><span class="s1">doc_topic_d</span><span class="s4">) &lt; </span><span class="s1">mean_change_tol</span><span class="s4">:</span>
                <span class="s3">break</span>
        <span class="s1">doc_topic_distr</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, :] = </span><span class="s1">doc_topic_d</span>

        <span class="s2"># Contribution of document d to the expected sufficient</span>
        <span class="s2"># statistics for the M step.</span>
        <span class="s3">if </span><span class="s1">cal_sstats</span><span class="s4">:</span>
            <span class="s1">norm_phi </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">exp_doc_topic_d</span><span class="s4">, </span><span class="s1">exp_topic_word_d</span><span class="s4">) + </span><span class="s1">eps</span>
            <span class="s1">suff_stats</span><span class="s4">[:, </span><span class="s1">ids</span><span class="s4">] += </span><span class="s1">np</span><span class="s4">.</span><span class="s1">outer</span><span class="s4">(</span><span class="s1">exp_doc_topic_d</span><span class="s4">, </span><span class="s1">cnts </span><span class="s4">/ </span><span class="s1">norm_phi</span><span class="s4">)</span>

    <span class="s3">return </span><span class="s4">(</span><span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">suff_stats</span><span class="s4">)</span>


<span class="s3">class </span><span class="s1">LatentDirichletAllocation</span><span class="s4">(</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s4">, </span><span class="s1">TransformerMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Latent Dirichlet Allocation with online variational Bayes algorithm. 
 
    The implementation is based on [1]_ and [2]_. 
 
    .. versionadded:: 0.17 
 
    Read more in the :ref:`User Guide &lt;LatentDirichletAllocation&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int, default=10 
        Number of topics. 
 
        .. versionchanged:: 0.19 
            ``n_topics`` was renamed to ``n_components`` 
 
    doc_topic_prior : float, default=None 
        Prior of document topic distribution `theta`. If the value is None, 
        defaults to `1 / n_components`. 
        In [1]_, this is called `alpha`. 
 
    topic_word_prior : float, default=None 
        Prior of topic word distribution `beta`. If the value is None, defaults 
        to `1 / n_components`. 
        In [1]_, this is called `eta`. 
 
    learning_method : {'batch', 'online'}, default='batch' 
        Method used to update `_component`. Only used in :meth:`fit` method. 
        In general, if the data size is large, the online update will be much 
        faster than the batch update. 
 
        Valid options: 
 
        - 'batch': Batch variational Bayes method. Use all training data in each EM 
          update. Old `components_` will be overwritten in each iteration. 
        - 'online': Online variational Bayes method. In each EM update, use mini-batch 
          of training data to update the ``components_`` variable incrementally. The 
          learning rate is controlled by the ``learning_decay`` and the 
          ``learning_offset`` parameters. 
 
        .. versionchanged:: 0.20 
            The default learning method is now ``&quot;batch&quot;``. 
 
    learning_decay : float, default=0.7 
        It is a parameter that control learning rate in the online learning 
        method. The value should be set between (0.5, 1.0] to guarantee 
        asymptotic convergence. When the value is 0.0 and batch_size is 
        ``n_samples``, the update method is same as batch learning. In the 
        literature, this is called kappa. 
 
    learning_offset : float, default=10.0 
        A (positive) parameter that downweights early iterations in online 
        learning.  It should be greater than 1.0. In the literature, this is 
        called tau_0. 
 
    max_iter : int, default=10 
        The maximum number of passes over the training data (aka epochs). 
        It only impacts the behavior in the :meth:`fit` method, and not the 
        :meth:`partial_fit` method. 
 
    batch_size : int, default=128 
        Number of documents to use in each EM iteration. Only used in online 
        learning. 
 
    evaluate_every : int, default=-1 
        How often to evaluate perplexity. Only used in `fit` method. 
        set it to 0 or negative number to not evaluate perplexity in 
        training at all. Evaluating perplexity can help you check convergence 
        in training process, but it will also increase total training time. 
        Evaluating perplexity in every iteration might increase training time 
        up to two-fold. 
 
    total_samples : int, default=1e6 
        Total number of documents. Only used in the :meth:`partial_fit` method. 
 
    perp_tol : float, default=1e-1 
        Perplexity tolerance. Only used when ``evaluate_every`` is greater than 0. 
 
    mean_change_tol : float, default=1e-3 
        Stopping tolerance for updating document topic distribution in E-step. 
 
    max_doc_update_iter : int, default=100 
        Max number of iterations for updating document topic distribution in 
        the E-step. 
 
    n_jobs : int, default=None 
        The number of jobs to use in the E-step. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    verbose : int, default=0 
        Verbosity level. 
 
    random_state : int, RandomState instance or None, default=None 
        Pass an int for reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        Variational parameters for topic word distribution. Since the complete 
        conditional for topic word distribution is a Dirichlet, 
        ``components_[i, j]`` can be viewed as pseudocount that represents the 
        number of times word `j` was assigned to topic `i`. 
        It can also be viewed as distribution over the words for each topic 
        after normalization: 
        ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``. 
 
    exp_dirichlet_component_ : ndarray of shape (n_components, n_features) 
        Exponential value of expectation of log topic word distribution. 
        In the literature, this is `exp(E[log(beta)])`. 
 
    n_batch_iter_ : int 
        Number of iterations of the EM step. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_iter_ : int 
        Number of passes over the dataset. 
 
    bound_ : float 
        Final perplexity score on training set. 
 
    doc_topic_prior_ : float 
        Prior of document topic distribution `theta`. If the value is None, 
        it is `1 / n_components`. 
 
    random_state_ : RandomState instance 
        RandomState instance that is generated either from a seed, the random 
        number generator or by `np.random`. 
 
    topic_word_prior_ : float 
        Prior of topic word distribution `beta`. If the value is None, it is 
        `1 / n_components`. 
 
    See Also 
    -------- 
    sklearn.discriminant_analysis.LinearDiscriminantAnalysis: 
        A classifier with a linear decision boundary, generated by fitting 
        class conditional densities to the data and using Bayes' rule. 
 
    References 
    ---------- 
    .. [1] &quot;Online Learning for Latent Dirichlet Allocation&quot;, Matthew D. 
           Hoffman, David M. Blei, Francis Bach, 2010 
           https://github.com/blei-lab/onlineldavb 
 
    .. [2] &quot;Stochastic Variational Inference&quot;, Matthew D. Hoffman, 
           David M. Blei, Chong Wang, John Paisley, 2013 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.decomposition import LatentDirichletAllocation 
    &gt;&gt;&gt; from sklearn.datasets import make_multilabel_classification 
    &gt;&gt;&gt; # This produces a feature matrix of token counts, similar to what 
    &gt;&gt;&gt; # CountVectorizer would produce on text. 
    &gt;&gt;&gt; X, _ = make_multilabel_classification(random_state=0) 
    &gt;&gt;&gt; lda = LatentDirichletAllocation(n_components=5, 
    ...     random_state=0) 
    &gt;&gt;&gt; lda.fit(X) 
    LatentDirichletAllocation(...) 
    &gt;&gt;&gt; # get topics for some given samples: 
    &gt;&gt;&gt; lda.transform(X[-2:]) 
    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846], 
           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s6">&quot;n_components&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;neither&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;doc_topic_prior&quot;</span><span class="s4">: [</span><span class="s3">None</span><span class="s4">, </span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;topic_word_prior&quot;</span><span class="s4">: [</span><span class="s3">None</span><span class="s4">, </span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;learning_method&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s6">&quot;batch&quot;</span><span class="s4">, </span><span class="s6">&quot;online&quot;</span><span class="s4">})],</span>
        <span class="s6">&quot;learning_decay&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s5">1</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;learning_offset&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">1.0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;max_iter&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;batch_size&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;neither&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;evaluate_every&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;neither&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;total_samples&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;neither&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;perp_tol&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;mean_change_tol&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;max_doc_update_iter&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s5">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s6">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s6">&quot;n_jobs&quot;</span><span class="s4">: [</span><span class="s3">None</span><span class="s4">, </span><span class="s1">Integral</span><span class="s4">],</span>
        <span class="s6">&quot;verbose&quot;</span><span class="s4">: [</span><span class="s6">&quot;verbose&quot;</span><span class="s4">],</span>
        <span class="s6">&quot;random_state&quot;</span><span class="s4">: [</span><span class="s6">&quot;random_state&quot;</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">n_components</span><span class="s4">=</span><span class="s5">10</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">doc_topic_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">topic_word_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">learning_method</span><span class="s4">=</span><span class="s6">&quot;batch&quot;</span><span class="s4">,</span>
        <span class="s1">learning_decay</span><span class="s4">=</span><span class="s5">0.7</span><span class="s4">,</span>
        <span class="s1">learning_offset</span><span class="s4">=</span><span class="s5">10.0</span><span class="s4">,</span>
        <span class="s1">max_iter</span><span class="s4">=</span><span class="s5">10</span><span class="s4">,</span>
        <span class="s1">batch_size</span><span class="s4">=</span><span class="s5">128</span><span class="s4">,</span>
        <span class="s1">evaluate_every</span><span class="s4">=-</span><span class="s5">1</span><span class="s4">,</span>
        <span class="s1">total_samples</span><span class="s4">=</span><span class="s5">1e6</span><span class="s4">,</span>
        <span class="s1">perp_tol</span><span class="s4">=</span><span class="s5">1e-1</span><span class="s4">,</span>
        <span class="s1">mean_change_tol</span><span class="s4">=</span><span class="s5">1e-3</span><span class="s4">,</span>
        <span class="s1">max_doc_update_iter</span><span class="s4">=</span><span class="s5">100</span><span class="s4">,</span>
        <span class="s1">n_jobs</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
        <span class="s1">random_state</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_components </span><span class="s4">= </span><span class="s1">n_components</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior </span><span class="s4">= </span><span class="s1">doc_topic_prior</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior </span><span class="s4">= </span><span class="s1">topic_word_prior</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_method </span><span class="s4">= </span><span class="s1">learning_method</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_decay </span><span class="s4">= </span><span class="s1">learning_decay</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_offset </span><span class="s4">= </span><span class="s1">learning_offset</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter </span><span class="s4">= </span><span class="s1">max_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">batch_size </span><span class="s4">= </span><span class="s1">batch_size</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">evaluate_every </span><span class="s4">= </span><span class="s1">evaluate_every</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">total_samples </span><span class="s4">= </span><span class="s1">total_samples</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">perp_tol </span><span class="s4">= </span><span class="s1">perp_tol</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">mean_change_tol </span><span class="s4">= </span><span class="s1">mean_change_tol</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">max_doc_update_iter </span><span class="s4">= </span><span class="s1">max_doc_update_iter</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs </span><span class="s4">= </span><span class="s1">n_jobs</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">= </span><span class="s1">verbose</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">random_state </span><span class="s4">= </span><span class="s1">random_state</span>

    <span class="s3">def </span><span class="s1">_init_latent_vars</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Initialize latent variables.&quot;&quot;&quot;</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">random_state_ </span><span class="s4">= </span><span class="s1">check_random_state</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">random_state</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_batch_iter_ </span><span class="s4">= </span><span class="s5">1</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_iter_ </span><span class="s4">= </span><span class="s5">0</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior_ </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior</span>

        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior_ </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior</span>

        <span class="s1">init_gamma </span><span class="s4">= </span><span class="s5">100.0</span>
        <span class="s1">init_var </span><span class="s4">= </span><span class="s5">1.0 </span><span class="s4">/ </span><span class="s1">init_gamma</span>
        <span class="s2"># In the literature, this is called `lambda`</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">random_state_</span><span class="s4">.</span><span class="s1">gamma</span><span class="s4">(</span>
            <span class="s1">init_gamma</span><span class="s4">, </span><span class="s1">init_var</span><span class="s4">, (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">)</span>
        <span class="s4">).</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

        <span class="s2"># In the literature, this is `exp(E[log(beta)])`</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">exp_dirichlet_component_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span>
            <span class="s1">_dirichlet_expectation_2d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_e_step</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">cal_sstats</span><span class="s4">, </span><span class="s1">random_init</span><span class="s4">, </span><span class="s1">parallel</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;E-step in EM update. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        cal_sstats : bool 
            Parameter that indicate whether to calculate sufficient statistics 
            or not. Set ``cal_sstats`` to True when we need to run M-step. 
 
        random_init : bool 
            Parameter that indicate whether to initialize document topic 
            distribution randomly in the E-step. Set it to True in training 
            steps. 
 
        parallel : joblib.Parallel, default=None 
            Pre-initialized instance of joblib.Parallel. 
 
        Returns 
        ------- 
        (doc_topic_distr, suff_stats) : 
            `doc_topic_distr` is unnormalized topic distribution for each 
            document. In the literature, this is called `gamma`. 
            `suff_stats` is expected sufficient statistics for the M-step. 
            When `cal_sstats == False`, it will be None. 
 
        &quot;&quot;&quot;</span>

        <span class="s2"># Run e-step in parallel</span>
        <span class="s1">random_state </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">random_state_ </span><span class="s3">if </span><span class="s1">random_init </span><span class="s3">else None</span>

        <span class="s2"># TODO: make Parallel._effective_n_jobs public instead?</span>
        <span class="s1">n_jobs </span><span class="s4">= </span><span class="s1">effective_n_jobs</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">parallel </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">parallel </span><span class="s4">= </span><span class="s1">Parallel</span><span class="s4">(</span><span class="s1">n_jobs</span><span class="s4">=</span><span class="s1">n_jobs</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">=</span><span class="s1">max</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">- </span><span class="s5">1</span><span class="s4">))</span>
        <span class="s1">results </span><span class="s4">= </span><span class="s1">parallel</span><span class="s4">(</span>
            <span class="s1">delayed</span><span class="s4">(</span><span class="s1">_update_doc_distribution</span><span class="s4">)(</span>
                <span class="s1">X</span><span class="s4">[</span><span class="s1">idx_slice</span><span class="s4">, :],</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">exp_dirichlet_component_</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior_</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">max_doc_update_iter</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">mean_change_tol</span><span class="s4">,</span>
                <span class="s1">cal_sstats</span><span class="s4">,</span>
                <span class="s1">random_state</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s3">for </span><span class="s1">idx_slice </span><span class="s3">in </span><span class="s1">gen_even_slices</span><span class="s4">(</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">], </span><span class="s1">n_jobs</span><span class="s4">)</span>
        <span class="s4">)</span>

        <span class="s2"># merge result</span>
        <span class="s1">doc_topics</span><span class="s4">, </span><span class="s1">sstats_list </span><span class="s4">= </span><span class="s1">zip</span><span class="s4">(*</span><span class="s1">results</span><span class="s4">)</span>
        <span class="s1">doc_topic_distr </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">vstack</span><span class="s4">(</span><span class="s1">doc_topics</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">cal_sstats</span><span class="s4">:</span>
            <span class="s2"># This step finishes computing the sufficient statistics for the</span>
            <span class="s2"># M-step.</span>
            <span class="s1">suff_stats </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
            <span class="s3">for </span><span class="s1">sstats </span><span class="s3">in </span><span class="s1">sstats_list</span><span class="s4">:</span>
                <span class="s1">suff_stats </span><span class="s4">+= </span><span class="s1">sstats</span>
            <span class="s1">suff_stats </span><span class="s4">*= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">exp_dirichlet_component_</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">suff_stats </span><span class="s4">= </span><span class="s3">None</span>

        <span class="s3">return </span><span class="s4">(</span><span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">suff_stats</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_em_step</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">total_samples</span><span class="s4">, </span><span class="s1">batch_update</span><span class="s4">, </span><span class="s1">parallel</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;EM update for 1 iteration. 
 
        update `_component` by batch VB or online VB. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        total_samples : int 
            Total number of documents. It is only used when 
            batch_update is `False`. 
 
        batch_update : bool 
            Parameter that controls updating method. 
            `True` for batch learning, `False` for online learning. 
 
        parallel : joblib.Parallel, default=None 
            Pre-initialized instance of joblib.Parallel 
 
        Returns 
        ------- 
        doc_topic_distr : ndarray of shape (n_samples, n_components) 
            Unnormalized document topic distribution. 
        &quot;&quot;&quot;</span>

        <span class="s2"># E-step</span>
        <span class="s1">_</span><span class="s4">, </span><span class="s1">suff_stats </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_e_step</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">cal_sstats</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">random_init</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">parallel</span><span class="s4">=</span><span class="s1">parallel</span>
        <span class="s4">)</span>

        <span class="s2"># M-step</span>
        <span class="s3">if </span><span class="s1">batch_update</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior_ </span><span class="s4">+ </span><span class="s1">suff_stats</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># online update</span>
            <span class="s2"># In the literature, the weight is `rho`</span>
            <span class="s1">weight </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">power</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">learning_offset </span><span class="s4">+ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_batch_iter_</span><span class="s4">, -</span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_decay</span>
            <span class="s4">)</span>
            <span class="s1">doc_ratio </span><span class="s4">= </span><span class="s1">float</span><span class="s4">(</span><span class="s1">total_samples</span><span class="s4">) / </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">*= </span><span class="s5">1 </span><span class="s4">- </span><span class="s1">weight</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">components_ </span><span class="s4">+= </span><span class="s1">weight </span><span class="s4">* (</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior_ </span><span class="s4">+ </span><span class="s1">doc_ratio </span><span class="s4">* </span><span class="s1">suff_stats</span>
            <span class="s4">)</span>

        <span class="s2"># update `component_` related variables</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">exp_dirichlet_component_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span>
            <span class="s1">_dirichlet_expectation_2d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">)</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_batch_iter_ </span><span class="s4">+= </span><span class="s5">1</span>
        <span class="s3">return</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span>
            <span class="s6">&quot;preserves_dtype&quot;</span><span class="s4">: [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">],</span>
            <span class="s6">&quot;requires_positive_X&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">,</span>
        <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">_check_non_neg_array</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">reset_n_features</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;check X format 
 
        check X format and make sure no negative value in X. 
 
        Parameters 
        ---------- 
        X :  array-like or sparse matrix 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dtype </span><span class="s4">= [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">] </span><span class="s3">if </span><span class="s1">reset_n_features </span><span class="s3">else </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">dtype</span>

        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">reset</span><span class="s4">=</span><span class="s1">reset_n_features</span><span class="s4">,</span>
            <span class="s1">accept_sparse</span><span class="s4">=</span><span class="s6">&quot;csr&quot;</span><span class="s4">,</span>
            <span class="s1">dtype</span><span class="s4">=</span><span class="s1">dtype</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">X</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">partial_fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Online VB with Mini-Batch update. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        self 
            Partially fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">first_time </span><span class="s4">= </span><span class="s3">not </span><span class="s1">hasattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s6">&quot;components_&quot;</span><span class="s4">)</span>

        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_non_neg_array</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">reset_n_features</span><span class="s4">=</span><span class="s1">first_time</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">=</span><span class="s6">&quot;LatentDirichletAllocation.partial_fit&quot;</span>
        <span class="s4">)</span>
        <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>
        <span class="s1">batch_size </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">batch_size</span>

        <span class="s2"># initialize parameters or check</span>
        <span class="s3">if </span><span class="s1">first_time</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_init_latent_vars</span><span class="s4">(</span><span class="s1">n_features</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_features </span><span class="s4">!= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s6">&quot;The provided data has %d dimensions while &quot;</span>
                <span class="s6">&quot;the model was trained with feature size %d.&quot;</span>
                <span class="s4">% (</span><span class="s1">n_features</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">])</span>
            <span class="s4">)</span>

        <span class="s1">n_jobs </span><span class="s4">= </span><span class="s1">effective_n_jobs</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs</span><span class="s4">)</span>
        <span class="s3">with </span><span class="s1">Parallel</span><span class="s4">(</span><span class="s1">n_jobs</span><span class="s4">=</span><span class="s1">n_jobs</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">=</span><span class="s1">max</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">- </span><span class="s5">1</span><span class="s4">)) </span><span class="s3">as </span><span class="s1">parallel</span><span class="s4">:</span>
            <span class="s3">for </span><span class="s1">idx_slice </span><span class="s3">in </span><span class="s1">gen_batches</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">batch_size</span><span class="s4">):</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">_em_step</span><span class="s4">(</span>
                    <span class="s1">X</span><span class="s4">[</span><span class="s1">idx_slice</span><span class="s4">, :],</span>
                    <span class="s1">total_samples</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">total_samples</span><span class="s4">,</span>
                    <span class="s1">batch_update</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
                    <span class="s1">parallel</span><span class="s4">=</span><span class="s1">parallel</span><span class="s4">,</span>
                <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Learn model for the data X with variational Bayes method. 
 
        When `learning_method` is 'online', use mini-batch update. 
        Otherwise, use batch update. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        self 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_non_neg_array</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">reset_n_features</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">=</span><span class="s6">&quot;LatentDirichletAllocation.fit&quot;</span>
        <span class="s4">)</span>
        <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>
        <span class="s1">max_iter </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">max_iter</span>
        <span class="s1">evaluate_every </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">evaluate_every</span>
        <span class="s1">learning_method </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">learning_method</span>

        <span class="s1">batch_size </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">batch_size</span>

        <span class="s2"># initialize parameters</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_init_latent_vars</span><span class="s4">(</span><span class="s1">n_features</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">X</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s2"># change to perplexity later</span>
        <span class="s1">last_bound </span><span class="s4">= </span><span class="s3">None</span>
        <span class="s1">n_jobs </span><span class="s4">= </span><span class="s1">effective_n_jobs</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs</span><span class="s4">)</span>
        <span class="s3">with </span><span class="s1">Parallel</span><span class="s4">(</span><span class="s1">n_jobs</span><span class="s4">=</span><span class="s1">n_jobs</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">=</span><span class="s1">max</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose </span><span class="s4">- </span><span class="s5">1</span><span class="s4">)) </span><span class="s3">as </span><span class="s1">parallel</span><span class="s4">:</span>
            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">max_iter</span><span class="s4">):</span>
                <span class="s3">if </span><span class="s1">learning_method </span><span class="s4">== </span><span class="s6">&quot;online&quot;</span><span class="s4">:</span>
                    <span class="s3">for </span><span class="s1">idx_slice </span><span class="s3">in </span><span class="s1">gen_batches</span><span class="s4">(</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">batch_size</span><span class="s4">):</span>
                        <span class="s1">self</span><span class="s4">.</span><span class="s1">_em_step</span><span class="s4">(</span>
                            <span class="s1">X</span><span class="s4">[</span><span class="s1">idx_slice</span><span class="s4">, :],</span>
                            <span class="s1">total_samples</span><span class="s4">=</span><span class="s1">n_samples</span><span class="s4">,</span>
                            <span class="s1">batch_update</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
                            <span class="s1">parallel</span><span class="s4">=</span><span class="s1">parallel</span><span class="s4">,</span>
                        <span class="s4">)</span>
                <span class="s3">else</span><span class="s4">:</span>
                    <span class="s2"># batch update</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">_em_step</span><span class="s4">(</span>
                        <span class="s1">X</span><span class="s4">, </span><span class="s1">total_samples</span><span class="s4">=</span><span class="s1">n_samples</span><span class="s4">, </span><span class="s1">batch_update</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">parallel</span><span class="s4">=</span><span class="s1">parallel</span>
                    <span class="s4">)</span>

                <span class="s2"># check perplexity</span>
                <span class="s3">if </span><span class="s1">evaluate_every </span><span class="s4">&gt; </span><span class="s5">0 </span><span class="s3">and </span><span class="s4">(</span><span class="s1">i </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">) % </span><span class="s1">evaluate_every </span><span class="s4">== </span><span class="s5">0</span><span class="s4">:</span>
                    <span class="s1">doc_topics_distr</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_e_step</span><span class="s4">(</span>
                        <span class="s1">X</span><span class="s4">, </span><span class="s1">cal_sstats</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">random_init</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">parallel</span><span class="s4">=</span><span class="s1">parallel</span>
                    <span class="s4">)</span>
                    <span class="s1">bound </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_perplexity_precomp_distr</span><span class="s4">(</span>
                        <span class="s1">X</span><span class="s4">, </span><span class="s1">doc_topics_distr</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">=</span><span class="s3">False</span>
                    <span class="s4">)</span>
                    <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                        <span class="s1">print</span><span class="s4">(</span>
                            <span class="s6">&quot;iteration: %d of max_iter: %d, perplexity: %.4f&quot;</span>
                            <span class="s4">% (</span><span class="s1">i </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">, </span><span class="s1">max_iter</span><span class="s4">, </span><span class="s1">bound</span><span class="s4">)</span>
                        <span class="s4">)</span>

                    <span class="s3">if </span><span class="s1">last_bound </span><span class="s3">and </span><span class="s1">abs</span><span class="s4">(</span><span class="s1">last_bound </span><span class="s4">- </span><span class="s1">bound</span><span class="s4">) &lt; </span><span class="s1">self</span><span class="s4">.</span><span class="s1">perp_tol</span><span class="s4">:</span>
                        <span class="s3">break</span>
                    <span class="s1">last_bound </span><span class="s4">= </span><span class="s1">bound</span>

                <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">verbose</span><span class="s4">:</span>
                    <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;iteration: %d of max_iter: %d&quot; </span><span class="s4">% (</span><span class="s1">i </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">, </span><span class="s1">max_iter</span><span class="s4">))</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">n_iter_ </span><span class="s4">+= </span><span class="s5">1</span>

        <span class="s2"># calculate final perplexity value on train set</span>
        <span class="s1">doc_topics_distr</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_e_step</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">cal_sstats</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">random_init</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">parallel</span><span class="s4">=</span><span class="s1">parallel</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">bound_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_perplexity_precomp_distr</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">doc_topics_distr</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_unnormalized_transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Transform data X according to fitted model. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        Returns 
        ------- 
        doc_topic_distr : ndarray of shape (n_samples, n_components) 
            Document topic distribution for X. 
        &quot;&quot;&quot;</span>
        <span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_e_step</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">cal_sstats</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">random_init</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">doc_topic_distr</span>

    <span class="s3">def </span><span class="s1">transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Transform data X according to the fitted model. 
 
           .. versionchanged:: 0.18 
              *doc_topic_distr* is now normalized 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        Returns 
        ------- 
        doc_topic_distr : ndarray of shape (n_samples, n_components) 
            Document topic distribution for X. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_non_neg_array</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">reset_n_features</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">=</span><span class="s6">&quot;LatentDirichletAllocation.transform&quot;</span>
        <span class="s4">)</span>
        <span class="s1">doc_topic_distr </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_unnormalized_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">doc_topic_distr </span><span class="s4">/= </span><span class="s1">doc_topic_distr</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s5">1</span><span class="s4">)[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
        <span class="s3">return </span><span class="s1">doc_topic_distr</span>

    <span class="s3">def </span><span class="s1">_approx_bound</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Estimate the variational bound. 
 
        Estimate the variational bound over &quot;all documents&quot; using only the 
        documents passed in as X. Since log-likelihood of each word cannot 
        be computed directly, we use this bound to estimate it. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        doc_topic_distr : ndarray of shape (n_samples, n_components) 
            Document topic distribution. In the literature, this is called 
            gamma. 
 
        sub_sampling : bool, default=False 
            Compensate for subsampling of documents. 
            It is used in calculate bound in online learning. 
 
        Returns 
        ------- 
        score : float 
 
        &quot;&quot;&quot;</span>

        <span class="s3">def </span><span class="s1">_loglikelihood</span><span class="s4">(</span><span class="s1">prior</span><span class="s4">, </span><span class="s1">distr</span><span class="s4">, </span><span class="s1">dirichlet_distr</span><span class="s4">, </span><span class="s1">size</span><span class="s4">):</span>
            <span class="s2"># calculate log-likelihood</span>
            <span class="s1">score </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">((</span><span class="s1">prior </span><span class="s4">- </span><span class="s1">distr</span><span class="s4">) * </span><span class="s1">dirichlet_distr</span><span class="s4">)</span>
            <span class="s1">score </span><span class="s4">+= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">gammaln</span><span class="s4">(</span><span class="s1">distr</span><span class="s4">) - </span><span class="s1">gammaln</span><span class="s4">(</span><span class="s1">prior</span><span class="s4">))</span>
            <span class="s1">score </span><span class="s4">+= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">gammaln</span><span class="s4">(</span><span class="s1">prior </span><span class="s4">* </span><span class="s1">size</span><span class="s4">) - </span><span class="s1">gammaln</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">distr</span><span class="s4">, </span><span class="s5">1</span><span class="s4">)))</span>
            <span class="s3">return </span><span class="s1">score</span>

        <span class="s1">is_sparse_x </span><span class="s4">= </span><span class="s1">sp</span><span class="s4">.</span><span class="s1">issparse</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_components </span><span class="s4">= </span><span class="s1">doc_topic_distr</span><span class="s4">.</span><span class="s1">shape</span>
        <span class="s1">n_features </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">1</span><span class="s4">]</span>
        <span class="s1">score </span><span class="s4">= </span><span class="s5">0</span>

        <span class="s1">dirichlet_doc_topic </span><span class="s4">= </span><span class="s1">_dirichlet_expectation_2d</span><span class="s4">(</span><span class="s1">doc_topic_distr</span><span class="s4">)</span>
        <span class="s1">dirichlet_component_ </span><span class="s4">= </span><span class="s1">_dirichlet_expectation_2d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">)</span>
        <span class="s1">doc_topic_prior </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">doc_topic_prior_</span>
        <span class="s1">topic_word_prior </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">topic_word_prior_</span>

        <span class="s3">if </span><span class="s1">is_sparse_x</span><span class="s4">:</span>
            <span class="s1">X_data </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">data</span>
            <span class="s1">X_indices </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indices</span>
            <span class="s1">X_indptr </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">indptr</span>

        <span class="s2"># E[log p(docs | theta, beta)]</span>
        <span class="s3">for </span><span class="s1">idx_d </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s5">0</span><span class="s4">, </span><span class="s1">n_samples</span><span class="s4">):</span>
            <span class="s3">if </span><span class="s1">is_sparse_x</span><span class="s4">:</span>
                <span class="s1">ids </span><span class="s4">= </span><span class="s1">X_indices</span><span class="s4">[</span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">] : </span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">]]</span>
                <span class="s1">cnts </span><span class="s4">= </span><span class="s1">X_data</span><span class="s4">[</span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">] : </span><span class="s1">X_indptr</span><span class="s4">[</span><span class="s1">idx_d </span><span class="s4">+ </span><span class="s5">1</span><span class="s4">]]</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">ids </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nonzero</span><span class="s4">(</span><span class="s1">X</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, :])[</span><span class="s5">0</span><span class="s4">]</span>
                <span class="s1">cnts </span><span class="s4">= </span><span class="s1">X</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, </span><span class="s1">ids</span><span class="s4">]</span>
            <span class="s1">temp </span><span class="s4">= (</span>
                <span class="s1">dirichlet_doc_topic</span><span class="s4">[</span><span class="s1">idx_d</span><span class="s4">, :, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">] + </span><span class="s1">dirichlet_component_</span><span class="s4">[:, </span><span class="s1">ids</span><span class="s4">]</span>
            <span class="s4">)</span>
            <span class="s1">norm_phi </span><span class="s4">= </span><span class="s1">logsumexp</span><span class="s4">(</span><span class="s1">temp</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s5">0</span><span class="s4">)</span>
            <span class="s1">score </span><span class="s4">+= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">cnts</span><span class="s4">, </span><span class="s1">norm_phi</span><span class="s4">)</span>

        <span class="s2"># compute E[log p(theta | alpha) - log q(theta | gamma)]</span>
        <span class="s1">score </span><span class="s4">+= </span><span class="s1">_loglikelihood</span><span class="s4">(</span>
            <span class="s1">doc_topic_prior</span><span class="s4">, </span><span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">dirichlet_doc_topic</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components</span>
        <span class="s4">)</span>

        <span class="s2"># Compensate for the subsampling of the population of documents</span>
        <span class="s3">if </span><span class="s1">sub_sampling</span><span class="s4">:</span>
            <span class="s1">doc_ratio </span><span class="s4">= </span><span class="s1">float</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">total_samples</span><span class="s4">) / </span><span class="s1">n_samples</span>
            <span class="s1">score </span><span class="s4">*= </span><span class="s1">doc_ratio</span>

        <span class="s2"># E[log p(beta | eta) - log q (beta | lambda)]</span>
        <span class="s1">score </span><span class="s4">+= </span><span class="s1">_loglikelihood</span><span class="s4">(</span>
            <span class="s1">topic_word_prior</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">, </span><span class="s1">dirichlet_component_</span><span class="s4">, </span><span class="s1">n_features</span>
        <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">score</span>

    <span class="s3">def </span><span class="s1">score</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate approximate log-likelihood as score. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        score : float 
            Use approximate bound as score. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_non_neg_array</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">reset_n_features</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">=</span><span class="s6">&quot;LatentDirichletAllocation.score&quot;</span>
        <span class="s4">)</span>

        <span class="s1">doc_topic_distr </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_unnormalized_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">score </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_approx_bound</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">score</span>

    <span class="s3">def </span><span class="s1">_perplexity_precomp_distr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">doc_topic_distr</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate approximate perplexity for data X with ability to accept 
        precomputed doc_topic_distr 
 
        Perplexity is defined as exp(-1. * log-likelihood per word) 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        doc_topic_distr : ndarray of shape (n_samples, n_components), \ 
                default=None 
            Document topic distribution. 
            If it is None, it will be generated by applying transform on X. 
 
        Returns 
        ------- 
        score : float 
            Perplexity score. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">doc_topic_distr </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">doc_topic_distr </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_unnormalized_transform</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">n_samples</span><span class="s4">, </span><span class="s1">n_components </span><span class="s4">= </span><span class="s1">doc_topic_distr</span><span class="s4">.</span><span class="s1">shape</span>
            <span class="s3">if </span><span class="s1">n_samples </span><span class="s4">!= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s6">&quot;Number of samples in X and doc_topic_distr do not match.&quot;</span>
                <span class="s4">)</span>

            <span class="s3">if </span><span class="s1">n_components </span><span class="s4">!= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_components</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s6">&quot;Number of topics does not match.&quot;</span><span class="s4">)</span>

        <span class="s1">current_samples </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
        <span class="s1">bound </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_approx_bound</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">doc_topic_distr</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">sub_sampling</span><span class="s4">:</span>
            <span class="s1">word_cnt </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">() * (</span><span class="s1">float</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">total_samples</span><span class="s4">) / </span><span class="s1">current_samples</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">word_cnt </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
        <span class="s1">perword_bound </span><span class="s4">= </span><span class="s1">bound </span><span class="s4">/ </span><span class="s1">word_cnt</span>

        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(-</span><span class="s5">1.0 </span><span class="s4">* </span><span class="s1">perword_bound</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">perplexity</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate approximate perplexity for data X. 
 
        Perplexity is defined as exp(-1. * log-likelihood per word) 
 
        .. versionchanged:: 0.19 
           *doc_topic_distr* argument has been deprecated and is ignored 
           because user no longer has access to unnormalized distribution 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Document word matrix. 
 
        sub_sampling : bool 
            Do sub-sampling or not. 
 
        Returns 
        ------- 
        score : float 
            Perplexity score. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_non_neg_array</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">reset_n_features</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">whom</span><span class="s4">=</span><span class="s6">&quot;LatentDirichletAllocation.perplexity&quot;</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_perplexity_precomp_distr</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">sub_sampling</span><span class="s4">=</span><span class="s1">sub_sampling</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">property</span>
    <span class="s3">def </span><span class="s1">_n_features_out</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">components_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s5">0</span><span class="s4">]</span>
</pre>
</body>
</html>