<html>
<head>
<title>optimize.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
optimize.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Our own implementation of the Newton algorithm 
 
Unlike the scipy.optimize version, this version of the Newton conjugate 
gradient solver uses only one function call to retrieve the 
func value, the gradient value and a callable for the Hessian matvec 
product. If the function call is very expensive (e.g. for logistic 
regression with large design matrix), this approach gives very 
significant speedups. 
&quot;&quot;&quot;</span>

<span class="s2"># This is a modified file from scipy.optimize</span>
<span class="s2"># Original authors: Travis Oliphant, Eric Jones</span>
<span class="s2"># Modifications by Gael Varoquaux, Mathieu Blondel and Tom Dupre la Tour</span>
<span class="s2"># License: BSD</span>

<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">fixes </span><span class="s3">import </span><span class="s1">line_search_wolfe1</span><span class="s4">, </span><span class="s1">line_search_wolfe2</span>


<span class="s3">class </span><span class="s1">_LineSearchError</span><span class="s4">(</span><span class="s1">RuntimeError</span><span class="s4">):</span>
    <span class="s3">pass</span>


<span class="s3">def </span><span class="s1">_line_search_wolfe12</span><span class="s4">(</span>
    <span class="s1">f</span><span class="s4">, </span><span class="s1">fprime</span><span class="s4">, </span><span class="s1">xk</span><span class="s4">, </span><span class="s1">pk</span><span class="s4">, </span><span class="s1">gfk</span><span class="s4">, </span><span class="s1">old_fval</span><span class="s4">, </span><span class="s1">old_old_fval</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">, **</span><span class="s1">kwargs</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Same as line_search_wolfe1, but fall back to line_search_wolfe2 if 
    suitable step length is not found, and raise an exception if a 
    suitable step length is not found. 
 
    Raises 
    ------ 
    _LineSearchError 
        If no suitable step size is found. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">is_verbose </span><span class="s4">= </span><span class="s1">verbose </span><span class="s4">&gt;= </span><span class="s5">2</span>
    <span class="s1">eps </span><span class="s4">= </span><span class="s5">16 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">old_fval</span><span class="s4">).</span><span class="s1">dtype</span><span class="s4">).</span><span class="s1">eps</span>
    <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
        <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;  Line Search&quot;</span><span class="s4">)</span>
        <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;    eps=16 * finfo.eps=</span><span class="s3">{</span><span class="s1">eps</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
        <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;    try line search wolfe1&quot;</span><span class="s4">)</span>

    <span class="s1">ret </span><span class="s4">= </span><span class="s1">line_search_wolfe1</span><span class="s4">(</span><span class="s1">f</span><span class="s4">, </span><span class="s1">fprime</span><span class="s4">, </span><span class="s1">xk</span><span class="s4">, </span><span class="s1">pk</span><span class="s4">, </span><span class="s1">gfk</span><span class="s4">, </span><span class="s1">old_fval</span><span class="s4">, </span><span class="s1">old_old_fval</span><span class="s4">, **</span><span class="s1">kwargs</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
        <span class="s1">_not_ </span><span class="s4">= </span><span class="s6">&quot;not &quot; </span><span class="s3">if </span><span class="s1">ret</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] </span><span class="s3">is None else </span><span class="s6">&quot;&quot;</span>
        <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;    wolfe1 line search was &quot; </span><span class="s4">+ </span><span class="s1">_not_ </span><span class="s4">+ </span><span class="s6">&quot;successful&quot;</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">ret</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s2"># Have a look at the line_search method of our NewtonSolver class. We borrow</span>
        <span class="s2"># the logic from there</span>
        <span class="s2"># Deal with relative loss differences around machine precision.</span>
        <span class="s1">args </span><span class="s4">= </span><span class="s1">kwargs</span><span class="s4">.</span><span class="s1">get</span><span class="s4">(</span><span class="s6">&quot;args&quot;</span><span class="s4">, </span><span class="s1">tuple</span><span class="s4">())</span>
        <span class="s1">fval </span><span class="s4">= </span><span class="s1">f</span><span class="s4">(</span><span class="s1">xk </span><span class="s4">+ </span><span class="s1">pk</span><span class="s4">, *</span><span class="s1">args</span><span class="s4">)</span>
        <span class="s1">tiny_loss </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">old_fval </span><span class="s4">* </span><span class="s1">eps</span><span class="s4">)</span>
        <span class="s1">loss_improvement </span><span class="s4">= </span><span class="s1">fval </span><span class="s4">- </span><span class="s1">old_fval</span>
        <span class="s1">check </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">loss_improvement</span><span class="s4">) &lt;= </span><span class="s1">tiny_loss</span>
        <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span>
                <span class="s6">&quot;    check loss |improvement| &lt;= eps * |loss_old|:&quot;</span>
                <span class="s6">f&quot; </span><span class="s3">{</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">loss_improvement</span><span class="s4">)</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">tiny_loss</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s6">&quot;</span>
            <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">check</span><span class="s4">:</span>
            <span class="s2"># 2.1 Check sum of absolute gradients as alternative condition.</span>
            <span class="s1">sum_abs_grad_old </span><span class="s4">= </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">gfk</span><span class="s4">, </span><span class="s1">ord</span><span class="s4">=</span><span class="s5">1</span><span class="s4">)</span>
            <span class="s1">grad </span><span class="s4">= </span><span class="s1">fprime</span><span class="s4">(</span><span class="s1">xk </span><span class="s4">+ </span><span class="s1">pk</span><span class="s4">, *</span><span class="s1">args</span><span class="s4">)</span>
            <span class="s1">sum_abs_grad </span><span class="s4">= </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">linalg</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">(</span><span class="s1">grad</span><span class="s4">, </span><span class="s1">ord</span><span class="s4">=</span><span class="s5">1</span><span class="s4">)</span>
            <span class="s1">check </span><span class="s4">= </span><span class="s1">sum_abs_grad </span><span class="s4">&lt; </span><span class="s1">sum_abs_grad_old</span>
            <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">&quot;    check sum(|gradient|) &lt; sum(|gradient_old|): &quot;</span>
                    <span class="s6">f&quot;</span><span class="s3">{</span><span class="s1">sum_abs_grad</span><span class="s3">} </span><span class="s6">&lt; </span><span class="s3">{</span><span class="s1">sum_abs_grad_old</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s6">&quot;</span>
                <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">check</span><span class="s4">:</span>
                <span class="s1">ret </span><span class="s4">= (</span>
                    <span class="s5">1.0</span><span class="s4">,  </span><span class="s2"># step size</span>
                    <span class="s1">ret</span><span class="s4">[</span><span class="s5">1</span><span class="s4">] + </span><span class="s5">1</span><span class="s4">,  </span><span class="s2"># number of function evaluations</span>
                    <span class="s1">ret</span><span class="s4">[</span><span class="s5">2</span><span class="s4">] + </span><span class="s5">1</span><span class="s4">,  </span><span class="s2"># number of gradient evaluations</span>
                    <span class="s1">fval</span><span class="s4">,</span>
                    <span class="s1">old_fval</span><span class="s4">,</span>
                    <span class="s1">grad</span><span class="s4">,</span>
                <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">ret</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s2"># line search failed: try different one.</span>
        <span class="s2"># TODO: It seems that the new check for the sum of absolute gradients above</span>
        <span class="s2"># catches all cases that, earlier, ended up here. In fact, our tests never</span>
        <span class="s2"># trigger this &quot;if branch&quot; here and we can consider to remove it.</span>
        <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;    last resort: try line search wolfe2&quot;</span><span class="s4">)</span>
        <span class="s1">ret </span><span class="s4">= </span><span class="s1">line_search_wolfe2</span><span class="s4">(</span>
            <span class="s1">f</span><span class="s4">, </span><span class="s1">fprime</span><span class="s4">, </span><span class="s1">xk</span><span class="s4">, </span><span class="s1">pk</span><span class="s4">, </span><span class="s1">gfk</span><span class="s4">, </span><span class="s1">old_fval</span><span class="s4">, </span><span class="s1">old_old_fval</span><span class="s4">, **</span><span class="s1">kwargs</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
            <span class="s1">_not_ </span><span class="s4">= </span><span class="s6">&quot;not &quot; </span><span class="s3">if </span><span class="s1">ret</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] </span><span class="s3">is None else </span><span class="s6">&quot;&quot;</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;    wolfe2 line search was &quot; </span><span class="s4">+ </span><span class="s1">_not_ </span><span class="s4">+ </span><span class="s6">&quot;successful&quot;</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">ret</span><span class="s4">[</span><span class="s5">0</span><span class="s4">] </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">_LineSearchError</span><span class="s4">()</span>

    <span class="s3">return </span><span class="s1">ret</span>


<span class="s3">def </span><span class="s1">_cg</span><span class="s4">(</span><span class="s1">fhess_p</span><span class="s4">, </span><span class="s1">fgrad</span><span class="s4">, </span><span class="s1">maxiter</span><span class="s4">, </span><span class="s1">tol</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Solve iteratively the linear system 'fhess_p . xsupi = fgrad' 
    with a conjugate gradient descent. 
 
    Parameters 
    ---------- 
    fhess_p : callable 
        Function that takes the gradient as a parameter and returns the 
        matrix product of the Hessian and gradient. 
 
    fgrad : ndarray of shape (n_features,) or (n_features + 1,) 
        Gradient vector. 
 
    maxiter : int 
        Number of CG iterations. 
 
    tol : float 
        Stopping criterion. 
 
    Returns 
    ------- 
    xsupi : ndarray of shape (n_features,) or (n_features + 1,) 
        Estimated solution. 
    &quot;&quot;&quot;</span>
    <span class="s1">eps </span><span class="s4">= </span><span class="s5">16 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">).</span><span class="s1">eps</span>
    <span class="s1">xsupi </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">fgrad</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">fgrad</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
    <span class="s1">ri </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">(</span><span class="s1">fgrad</span><span class="s4">)  </span><span class="s2"># residual = fgrad - fhess_p @ xsupi</span>
    <span class="s1">psupi </span><span class="s4">= -</span><span class="s1">ri</span>
    <span class="s1">i </span><span class="s4">= </span><span class="s5">0</span>
    <span class="s1">dri0 </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">ri</span><span class="s4">, </span><span class="s1">ri</span><span class="s4">)</span>
    <span class="s2"># We also keep track of |p_i|^2.</span>
    <span class="s1">psupi_norm2 </span><span class="s4">= </span><span class="s1">dri0</span>
    <span class="s1">is_verbose </span><span class="s4">= </span><span class="s1">verbose </span><span class="s4">&gt;= </span><span class="s5">2</span>

    <span class="s3">while </span><span class="s1">i </span><span class="s4">&lt;= </span><span class="s1">maxiter</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">ri</span><span class="s4">)) &lt;= </span><span class="s1">tol</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">f&quot;  Inner CG solver iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">} </span><span class="s6">stopped with</span><span class="s3">\n</span><span class="s6">&quot;</span>
                    <span class="s6">f&quot;    sum(|residuals|) &lt;= tol: </span><span class="s3">{</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">ri</span><span class="s4">))</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">tol</span><span class="s3">}</span><span class="s6">&quot;</span>
                <span class="s4">)</span>
            <span class="s3">break</span>

        <span class="s1">Ap </span><span class="s4">= </span><span class="s1">fhess_p</span><span class="s4">(</span><span class="s1">psupi</span><span class="s4">)</span>
        <span class="s2"># check curvature</span>
        <span class="s1">curv </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">psupi</span><span class="s4">, </span><span class="s1">Ap</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s5">0 </span><span class="s4">&lt;= </span><span class="s1">curv </span><span class="s4">&lt;= </span><span class="s1">eps </span><span class="s4">* </span><span class="s1">psupi_norm2</span><span class="s4">:</span>
            <span class="s2"># See https://arxiv.org/abs/1803.02924, Algo 1 Capped Conjugate Gradient.</span>
            <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                <span class="s1">print</span><span class="s4">(</span>
                    <span class="s6">f&quot;  Inner CG solver iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">} </span><span class="s6">stopped with</span><span class="s3">\n</span><span class="s6">&quot;</span>
                    <span class="s6">f&quot;    tiny_|p| = eps * ||p||^2, eps = </span><span class="s3">{</span><span class="s1">eps</span><span class="s3">}</span><span class="s6">, &quot;</span>
                    <span class="s6">f&quot;squred L2 norm ||p||^2 = </span><span class="s3">{</span><span class="s1">psupi_norm2</span><span class="s3">}\n</span><span class="s6">&quot;</span>
                    <span class="s6">f&quot;    curvature &lt;= tiny_|p|: </span><span class="s3">{</span><span class="s1">curv</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">eps </span><span class="s4">* </span><span class="s1">psupi_norm2</span><span class="s3">}</span><span class="s6">&quot;</span>
                <span class="s4">)</span>
            <span class="s3">break</span>
        <span class="s3">elif </span><span class="s1">curv </span><span class="s4">&lt; </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">i </span><span class="s4">&gt; </span><span class="s5">0</span><span class="s4">:</span>
                <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                    <span class="s1">print</span><span class="s4">(</span>
                        <span class="s6">f&quot;  Inner CG solver iteration </span><span class="s3">{</span><span class="s1">i</span><span class="s3">} </span><span class="s6">stopped with negative &quot;</span>
                        <span class="s6">f&quot;curvature, curvature = </span><span class="s3">{</span><span class="s1">curv</span><span class="s3">}</span><span class="s6">&quot;</span>
                    <span class="s4">)</span>
                <span class="s3">break</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s2"># fall back to steepest descent direction</span>
                <span class="s1">xsupi </span><span class="s4">+= </span><span class="s1">dri0 </span><span class="s4">/ </span><span class="s1">curv </span><span class="s4">* </span><span class="s1">psupi</span>
                <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
                    <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;  Inner CG solver iteration 0 fell back to steepest descent&quot;</span><span class="s4">)</span>
                <span class="s3">break</span>
        <span class="s1">alphai </span><span class="s4">= </span><span class="s1">dri0 </span><span class="s4">/ </span><span class="s1">curv</span>
        <span class="s1">xsupi </span><span class="s4">+= </span><span class="s1">alphai </span><span class="s4">* </span><span class="s1">psupi</span>
        <span class="s1">ri </span><span class="s4">+= </span><span class="s1">alphai </span><span class="s4">* </span><span class="s1">Ap</span>
        <span class="s1">dri1 </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">ri</span><span class="s4">, </span><span class="s1">ri</span><span class="s4">)</span>
        <span class="s1">betai </span><span class="s4">= </span><span class="s1">dri1 </span><span class="s4">/ </span><span class="s1">dri0</span>
        <span class="s1">psupi </span><span class="s4">= -</span><span class="s1">ri </span><span class="s4">+ </span><span class="s1">betai </span><span class="s4">* </span><span class="s1">psupi</span>
        <span class="s2"># We use  |p_i|^2 = |r_i|^2 + beta_i^2 |p_{i-1}|^2</span>
        <span class="s1">psupi_norm2 </span><span class="s4">= </span><span class="s1">dri1 </span><span class="s4">+ </span><span class="s1">betai</span><span class="s4">**</span><span class="s5">2 </span><span class="s4">* </span><span class="s1">psupi_norm2</span>
        <span class="s1">i </span><span class="s4">= </span><span class="s1">i </span><span class="s4">+ </span><span class="s5">1</span>
        <span class="s1">dri0 </span><span class="s4">= </span><span class="s1">dri1  </span><span class="s2"># update np.dot(ri,ri) for next time.</span>
    <span class="s3">if </span><span class="s1">is_verbose </span><span class="s3">and </span><span class="s1">i </span><span class="s4">&gt; </span><span class="s1">maxiter</span><span class="s4">:</span>
        <span class="s1">print</span><span class="s4">(</span>
            <span class="s6">f&quot;  Inner CG solver stopped reaching maxiter=</span><span class="s3">{</span><span class="s1">i </span><span class="s4">- </span><span class="s5">1</span><span class="s3">} </span><span class="s6">with &quot;</span>
            <span class="s6">f&quot;sum(|residuals|) = </span><span class="s3">{</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">ri</span><span class="s4">))</span><span class="s3">}</span><span class="s6">&quot;</span>
        <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">xsupi</span>


<span class="s3">def </span><span class="s1">_newton_cg</span><span class="s4">(</span>
    <span class="s1">grad_hess</span><span class="s4">,</span>
    <span class="s1">func</span><span class="s4">,</span>
    <span class="s1">grad</span><span class="s4">,</span>
    <span class="s1">x0</span><span class="s4">,</span>
    <span class="s1">args</span><span class="s4">=(),</span>
    <span class="s1">tol</span><span class="s4">=</span><span class="s5">1e-4</span><span class="s4">,</span>
    <span class="s1">maxiter</span><span class="s4">=</span><span class="s5">100</span><span class="s4">,</span>
    <span class="s1">maxinner</span><span class="s4">=</span><span class="s5">200</span><span class="s4">,</span>
    <span class="s1">line_search</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
    <span class="s1">warn</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
    <span class="s1">verbose</span><span class="s4">=</span><span class="s5">0</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Minimization of scalar function of one or more variables using the 
    Newton-CG algorithm. 
 
    Parameters 
    ---------- 
    grad_hess : callable 
        Should return the gradient and a callable returning the matvec product 
        of the Hessian. 
 
    func : callable 
        Should return the value of the function. 
 
    grad : callable 
        Should return the function value and the gradient. This is used 
        by the linesearch functions. 
 
    x0 : array of float 
        Initial guess. 
 
    args : tuple, default=() 
        Arguments passed to func_grad_hess, func and grad. 
 
    tol : float, default=1e-4 
        Stopping criterion. The iteration will stop when 
        ``max{|g_i | i = 1, ..., n} &lt;= tol`` 
        where ``g_i`` is the i-th component of the gradient. 
 
    maxiter : int, default=100 
        Number of Newton iterations. 
 
    maxinner : int, default=200 
        Number of CG iterations. 
 
    line_search : bool, default=True 
        Whether to use a line search or not. 
 
    warn : bool, default=True 
        Whether to warn when didn't converge. 
 
    Returns 
    ------- 
    xk : ndarray of float 
        Estimated minimum. 
    &quot;&quot;&quot;</span>
    <span class="s1">x0 </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">x0</span><span class="s4">).</span><span class="s1">flatten</span><span class="s4">()</span>
    <span class="s1">xk </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">(</span><span class="s1">x0</span><span class="s4">)</span>
    <span class="s1">k </span><span class="s4">= </span><span class="s5">0</span>

    <span class="s3">if </span><span class="s1">line_search</span><span class="s4">:</span>
        <span class="s1">old_fval </span><span class="s4">= </span><span class="s1">func</span><span class="s4">(</span><span class="s1">x0</span><span class="s4">, *</span><span class="s1">args</span><span class="s4">)</span>
        <span class="s1">old_old_fval </span><span class="s4">= </span><span class="s3">None</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">old_fval </span><span class="s4">= </span><span class="s5">0</span>

    <span class="s1">is_verbose </span><span class="s4">= </span><span class="s1">verbose </span><span class="s4">&gt; </span><span class="s5">0</span>

    <span class="s2"># Outer loop: our Newton iteration</span>
    <span class="s3">while </span><span class="s1">k </span><span class="s4">&lt; </span><span class="s1">maxiter</span><span class="s4">:</span>
        <span class="s2"># Compute a search direction pk by applying the CG method to</span>
        <span class="s2">#  del2 f(xk) p = - fgrad f(xk) starting from 0.</span>
        <span class="s1">fgrad</span><span class="s4">, </span><span class="s1">fhess_p </span><span class="s4">= </span><span class="s1">grad_hess</span><span class="s4">(</span><span class="s1">xk</span><span class="s4">, *</span><span class="s1">args</span><span class="s4">)</span>

        <span class="s1">absgrad </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">fgrad</span><span class="s4">)</span>
        <span class="s1">max_absgrad </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">absgrad</span><span class="s4">)</span>
        <span class="s1">check </span><span class="s4">= </span><span class="s1">max_absgrad </span><span class="s4">&lt;= </span><span class="s1">tol</span>
        <span class="s3">if </span><span class="s1">is_verbose</span><span class="s4">:</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;Newton-CG iter = </span><span class="s3">{</span><span class="s1">k</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">&quot;  Check Convergence&quot;</span><span class="s4">)</span>
            <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;    max |gradient| &lt;= tol: </span><span class="s3">{</span><span class="s1">max_absgrad</span><span class="s3">} </span><span class="s6">&lt;= </span><span class="s3">{</span><span class="s1">tol</span><span class="s3">} {</span><span class="s1">check</span><span class="s3">}</span><span class="s6">&quot;</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">check</span><span class="s4">:</span>
            <span class="s3">break</span>

        <span class="s1">maggrad </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">absgrad</span><span class="s4">)</span>
        <span class="s1">eta </span><span class="s4">= </span><span class="s1">min</span><span class="s4">([</span><span class="s5">0.5</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">maggrad</span><span class="s4">)])</span>
        <span class="s1">termcond </span><span class="s4">= </span><span class="s1">eta </span><span class="s4">* </span><span class="s1">maggrad</span>

        <span class="s2"># Inner loop: solve the Newton update by conjugate gradient, to</span>
        <span class="s2"># avoid inverting the Hessian</span>
        <span class="s1">xsupi </span><span class="s4">= </span><span class="s1">_cg</span><span class="s4">(</span><span class="s1">fhess_p</span><span class="s4">, </span><span class="s1">fgrad</span><span class="s4">, </span><span class="s1">maxiter</span><span class="s4">=</span><span class="s1">maxinner</span><span class="s4">, </span><span class="s1">tol</span><span class="s4">=</span><span class="s1">termcond</span><span class="s4">, </span><span class="s1">verbose</span><span class="s4">=</span><span class="s1">verbose</span><span class="s4">)</span>

        <span class="s1">alphak </span><span class="s4">= </span><span class="s5">1.0</span>

        <span class="s3">if </span><span class="s1">line_search</span><span class="s4">:</span>
            <span class="s3">try</span><span class="s4">:</span>
                <span class="s1">alphak</span><span class="s4">, </span><span class="s1">fc</span><span class="s4">, </span><span class="s1">gc</span><span class="s4">, </span><span class="s1">old_fval</span><span class="s4">, </span><span class="s1">old_old_fval</span><span class="s4">, </span><span class="s1">gfkp1 </span><span class="s4">= </span><span class="s1">_line_search_wolfe12</span><span class="s4">(</span>
                    <span class="s1">func</span><span class="s4">,</span>
                    <span class="s1">grad</span><span class="s4">,</span>
                    <span class="s1">xk</span><span class="s4">,</span>
                    <span class="s1">xsupi</span><span class="s4">,</span>
                    <span class="s1">fgrad</span><span class="s4">,</span>
                    <span class="s1">old_fval</span><span class="s4">,</span>
                    <span class="s1">old_old_fval</span><span class="s4">,</span>
                    <span class="s1">verbose</span><span class="s4">=</span><span class="s1">verbose</span><span class="s4">,</span>
                    <span class="s1">args</span><span class="s4">=</span><span class="s1">args</span><span class="s4">,</span>
                <span class="s4">)</span>
            <span class="s3">except </span><span class="s1">_LineSearchError</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s6">&quot;Line Search failed&quot;</span><span class="s4">)</span>
                <span class="s3">break</span>

        <span class="s1">xk </span><span class="s4">+= </span><span class="s1">alphak </span><span class="s4">* </span><span class="s1">xsupi  </span><span class="s2"># upcast if necessary</span>
        <span class="s1">k </span><span class="s4">+= </span><span class="s5">1</span>

    <span class="s3">if </span><span class="s1">warn </span><span class="s3">and </span><span class="s1">k </span><span class="s4">&gt;= </span><span class="s1">maxiter</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s4">(</span>
                <span class="s6">f&quot;newton-cg failed to converge at loss = </span><span class="s3">{</span><span class="s1">old_fval</span><span class="s3">}</span><span class="s6">. Increase the&quot;</span>
                <span class="s6">&quot; number of iterations.&quot;</span>
            <span class="s4">),</span>
            <span class="s1">ConvergenceWarning</span><span class="s4">,</span>
        <span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">is_verbose</span><span class="s4">:</span>
        <span class="s1">print</span><span class="s4">(</span><span class="s6">f&quot;  Solver did converge at loss = </span><span class="s3">{</span><span class="s1">old_fval</span><span class="s3">}</span><span class="s6">.&quot;</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">xk</span><span class="s4">, </span><span class="s1">k</span>


<span class="s3">def </span><span class="s1">_check_optimize_result</span><span class="s4">(</span><span class="s1">solver</span><span class="s4">, </span><span class="s1">result</span><span class="s4">, </span><span class="s1">max_iter</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">extra_warning_msg</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Check the OptimizeResult for successful convergence 
 
    Parameters 
    ---------- 
    solver : str 
       Solver name. Currently only `lbfgs` is supported. 
 
    result : OptimizeResult 
       Result of the scipy.optimize.minimize function. 
 
    max_iter : int, default=None 
       Expected maximum number of iterations. 
 
    extra_warning_msg : str, default=None 
        Extra warning message. 
 
    Returns 
    ------- 
    n_iter : int 
       Number of iterations. 
    &quot;&quot;&quot;</span>
    <span class="s2"># handle both scipy and scikit-learn solver names</span>
    <span class="s3">if </span><span class="s1">solver </span><span class="s4">== </span><span class="s6">&quot;lbfgs&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">result</span><span class="s4">.</span><span class="s1">status </span><span class="s4">!= </span><span class="s5">0</span><span class="s4">:</span>
            <span class="s3">try</span><span class="s4">:</span>
                <span class="s2"># The message is already decoded in scipy&gt;=1.6.0</span>
                <span class="s1">result_message </span><span class="s4">= </span><span class="s1">result</span><span class="s4">.</span><span class="s1">message</span><span class="s4">.</span><span class="s1">decode</span><span class="s4">(</span><span class="s6">&quot;latin1&quot;</span><span class="s4">)</span>
            <span class="s3">except </span><span class="s1">AttributeError</span><span class="s4">:</span>
                <span class="s1">result_message </span><span class="s4">= </span><span class="s1">result</span><span class="s4">.</span><span class="s1">message</span>
            <span class="s1">warning_msg </span><span class="s4">= (</span>
                <span class="s6">&quot;{} failed to converge (status={}):</span><span class="s3">\n</span><span class="s6">{}.</span><span class="s3">\n\n</span><span class="s6">&quot;</span>
                <span class="s6">&quot;Increase the number of iterations (max_iter) &quot;</span>
                <span class="s6">&quot;or scale the data as shown in:</span><span class="s3">\n</span><span class="s6">&quot;</span>
                <span class="s6">&quot;    https://scikit-learn.org/stable/modules/&quot;</span>
                <span class="s6">&quot;preprocessing.html&quot;</span>
            <span class="s4">).</span><span class="s1">format</span><span class="s4">(</span><span class="s1">solver</span><span class="s4">, </span><span class="s1">result</span><span class="s4">.</span><span class="s1">status</span><span class="s4">, </span><span class="s1">result_message</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">extra_warning_msg </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">warning_msg </span><span class="s4">+= </span><span class="s6">&quot;</span><span class="s3">\n</span><span class="s6">&quot; </span><span class="s4">+ </span><span class="s1">extra_warning_msg</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">warning_msg</span><span class="s4">, </span><span class="s1">ConvergenceWarning</span><span class="s4">, </span><span class="s1">stacklevel</span><span class="s4">=</span><span class="s5">2</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">max_iter </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s2"># In scipy &lt;= 1.0.0, nit may exceed maxiter for lbfgs.</span>
            <span class="s2"># See https://github.com/scipy/scipy/issues/7854</span>
            <span class="s1">n_iter_i </span><span class="s4">= </span><span class="s1">min</span><span class="s4">(</span><span class="s1">result</span><span class="s4">.</span><span class="s1">nit</span><span class="s4">, </span><span class="s1">max_iter</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">n_iter_i </span><span class="s4">= </span><span class="s1">result</span><span class="s4">.</span><span class="s1">nit</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">NotImplementedError</span>

    <span class="s3">return </span><span class="s1">n_iter_i</span>
</pre>
</body>
</html>