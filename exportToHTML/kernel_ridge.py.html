<html>
<head>
<title>kernel_ridge.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #6aab73;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
kernel_ridge.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Kernel ridge regression.&quot;&quot;&quot;</span>

<span class="s2"># Authors: Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#          Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="s2"># License: BSD 3 clause</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s4">.</span><span class="s1">base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s4">, </span><span class="s1">MultiOutputMixin</span><span class="s4">, </span><span class="s1">RegressorMixin</span><span class="s4">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">linear_model</span><span class="s4">.</span><span class="s1">_ridge </span><span class="s3">import </span><span class="s1">_solve_cholesky_kernel</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">metrics</span><span class="s4">.</span><span class="s1">pairwise </span><span class="s3">import </span><span class="s1">PAIRWISE_KERNEL_FUNCTIONS</span><span class="s4">, </span><span class="s1">pairwise_kernels</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s4">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span><span class="s4">, </span><span class="s1">check_is_fitted</span>


<span class="s3">class </span><span class="s1">KernelRidge</span><span class="s4">(</span><span class="s1">MultiOutputMixin</span><span class="s4">, </span><span class="s1">RegressorMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Kernel ridge regression. 
 
    Kernel ridge regression (KRR) combines ridge regression (linear least 
    squares with l2-norm regularization) with the kernel trick. It thus 
    learns a linear function in the space induced by the respective kernel and 
    the data. For non-linear kernels, this corresponds to a non-linear 
    function in the original space. 
 
    The form of the model learned by KRR is identical to support vector 
    regression (SVR). However, different loss functions are used: KRR uses 
    squared error loss while support vector regression uses epsilon-insensitive 
    loss, both combined with l2 regularization. In contrast to SVR, fitting a 
    KRR model can be done in closed-form and is typically faster for 
    medium-sized datasets. On the other hand, the learned model is non-sparse 
    and thus slower than SVR, which learns a sparse model for epsilon &gt; 0, at 
    prediction-time. 
 
    This estimator has built-in support for multi-variate regression 
    (i.e., when y is a 2d-array of shape [n_samples, n_targets]). 
 
    Read more in the :ref:`User Guide &lt;kernel_ridge&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float or array-like of shape (n_targets,), default=1.0 
        Regularization strength; must be a positive float. Regularization 
        improves the conditioning of the problem and reduces the variance of 
        the estimates. Larger values specify stronger regularization. 
        Alpha corresponds to ``1 / (2C)`` in other linear models such as 
        :class:`~sklearn.linear_model.LogisticRegression` or 
        :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are 
        assumed to be specific to the targets. Hence they must correspond in 
        number. See :ref:`ridge_regression` for formula. 
 
    kernel : str or callable, default=&quot;linear&quot; 
        Kernel mapping used internally. This parameter is directly passed to 
        :class:`~sklearn.metrics.pairwise.pairwise_kernels`. 
        If `kernel` is a string, it must be one of the metrics 
        in `pairwise.PAIRWISE_KERNEL_FUNCTIONS` or &quot;precomputed&quot;. 
        If `kernel` is &quot;precomputed&quot;, X is assumed to be a kernel matrix. 
        Alternatively, if `kernel` is a callable function, it is called on 
        each pair of instances (rows) and the resulting value recorded. The 
        callable should take two rows from X as input and return the 
        corresponding kernel value as a single number. This means that 
        callables from :mod:`sklearn.metrics.pairwise` are not allowed, as 
        they operate on matrices, not single samples. Use the string 
        identifying the kernel instead. 
 
    gamma : float, default=None 
        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 
        and sigmoid kernels. Interpretation of the default value is left to 
        the kernel; see the documentation for sklearn.metrics.pairwise. 
        Ignored by other kernels. 
 
    degree : float, default=3 
        Degree of the polynomial kernel. Ignored by other kernels. 
 
    coef0 : float, default=1 
        Zero coefficient for polynomial and sigmoid kernels. 
        Ignored by other kernels. 
 
    kernel_params : dict, default=None 
        Additional parameters (keyword arguments) for kernel function passed 
        as callable object. 
 
    Attributes 
    ---------- 
    dual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets) 
        Representation of weight vector(s) in kernel space 
 
    X_fit_ : {ndarray, sparse matrix} of shape (n_samples, n_features) 
        Training data, which is also required for prediction. If 
        kernel == &quot;precomputed&quot; this is instead the precomputed 
        training matrix, of shape (n_samples, n_samples). 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    sklearn.gaussian_process.GaussianProcessRegressor : Gaussian 
        Process regressor providing automatic kernel hyperparameters 
        tuning and predictions uncertainty. 
    sklearn.linear_model.Ridge : Linear ridge regression. 
    sklearn.linear_model.RidgeCV : Ridge regression with built-in 
        cross-validation. 
    sklearn.svm.SVR : Support Vector Regression accepting a large variety 
        of kernels. 
 
    References 
    ---------- 
    * Kevin P. Murphy 
      &quot;Machine Learning: A Probabilistic Perspective&quot;, The MIT Press 
      chapter 14.4.3, pp. 492-493 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.kernel_ridge import KernelRidge 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; n_samples, n_features = 10, 5 
    &gt;&gt;&gt; rng = np.random.RandomState(0) 
    &gt;&gt;&gt; y = rng.randn(n_samples) 
    &gt;&gt;&gt; X = rng.randn(n_samples, n_features) 
    &gt;&gt;&gt; krr = KernelRidge(alpha=1.0) 
    &gt;&gt;&gt; krr.fit(X, y) 
    KernelRidge(alpha=1.0) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s5">&quot;alpha&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;kernel&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">(</span><span class="s1">set</span><span class="s4">(</span><span class="s1">PAIRWISE_KERNEL_FUNCTIONS</span><span class="s4">.</span><span class="s1">keys</span><span class="s4">()) | {</span><span class="s5">&quot;precomputed&quot;</span><span class="s4">}),</span>
            <span class="s1">callable</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;gamma&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;degree&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;coef0&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;neither&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;kernel_params&quot;</span><span class="s4">: [</span><span class="s1">dict</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">alpha</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">kernel</span><span class="s4">=</span><span class="s5">&quot;linear&quot;</span><span class="s4">,</span>
        <span class="s1">gamma</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">degree</span><span class="s4">=</span><span class="s6">3</span><span class="s4">,</span>
        <span class="s1">coef0</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s1">kernel_params</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">alpha </span><span class="s4">= </span><span class="s1">alpha</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">kernel </span><span class="s4">= </span><span class="s1">kernel</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">gamma </span><span class="s4">= </span><span class="s1">gamma</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">degree </span><span class="s4">= </span><span class="s1">degree</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">coef0 </span><span class="s4">= </span><span class="s1">coef0</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">kernel_params </span><span class="s4">= </span><span class="s1">kernel_params</span>

    <span class="s3">def </span><span class="s1">_get_kernel</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">callable</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">):</span>
            <span class="s1">params </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel_params </span><span class="s3">or </span><span class="s4">{}</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">params </span><span class="s4">= {</span><span class="s5">&quot;gamma&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">gamma</span><span class="s4">, </span><span class="s5">&quot;degree&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">degree</span><span class="s4">, </span><span class="s5">&quot;coef0&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">coef0</span><span class="s4">}</span>
        <span class="s3">return </span><span class="s1">pairwise_kernels</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel</span><span class="s4">, </span><span class="s1">filter_params</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, **</span><span class="s1">params</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;pairwise&quot;</span><span class="s4">: </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel </span><span class="s4">== </span><span class="s5">&quot;precomputed&quot;</span><span class="s4">}</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Kernel Ridge regression model. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data. If kernel == &quot;precomputed&quot; this is instead 
            a precomputed kernel matrix, of shape (n_samples, n_samples). 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. 
 
        sample_weight : float or array-like of shape (n_samples,), default=None 
            Individual weights for each sample, ignored if None is passed. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s2"># Convert data</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s5">&quot;csr&quot;</span><span class="s4">, </span><span class="s5">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">multi_output</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">y_numeric</span><span class="s4">=</span><span class="s3">True</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None and not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">float</span><span class="s4">):</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>

        <span class="s1">K </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_get_kernel</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">alpha </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_1d</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span><span class="s4">)</span>

        <span class="s1">ravel </span><span class="s4">= </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">) == </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s1">y </span><span class="s4">= </span><span class="s1">y</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>
            <span class="s1">ravel </span><span class="s4">= </span><span class="s3">True</span>

        <span class="s1">copy </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">kernel </span><span class="s4">== </span><span class="s5">&quot;precomputed&quot;</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">dual_coef_ </span><span class="s4">= </span><span class="s1">_solve_cholesky_kernel</span><span class="s4">(</span><span class="s1">K</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">ravel</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">dual_coef_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">dual_coef_</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">()</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">X_fit_ </span><span class="s4">= </span><span class="s1">X</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict using the kernel ridge model. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Samples. If kernel == &quot;precomputed&quot; this is instead a 
            precomputed kernel matrix, shape = [n_samples, 
            n_samples_fitted], where n_samples_fitted is the number of 
            samples used in the fitting for this estimator. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples,) or (n_samples, n_targets) 
            Returns predicted values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=(</span><span class="s5">&quot;csr&quot;</span><span class="s4">, </span><span class="s5">&quot;csc&quot;</span><span class="s4">), </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
        <span class="s1">K </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_get_kernel</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">X_fit_</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">K</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">dual_coef_</span><span class="s4">)</span>
</pre>
</body>
</html>