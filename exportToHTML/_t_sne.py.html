<html>
<head>
<title>_t_sne.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #6aab73;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_t_sne.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Alexander Fabisch  -- &lt;afabisch@informatik.uni-bremen.de&gt;</span>
<span class="s0"># Author: Christopher Moody &lt;chrisemoody@gmail.com&gt;</span>
<span class="s0"># Author: Nick Travers &lt;nickt@squareup.com&gt;</span>
<span class="s0"># License: BSD 3 clause (C) 2014</span>

<span class="s0"># This is the exact and Barnes-Hut t-SNE implementation. There are other</span>
<span class="s0"># modifications of the algorithm:</span>
<span class="s0"># * Fast Optimization for t-SNE:</span>
<span class="s0">#   https://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/papers/vandermaaten.pdf</span>

<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>
<span class="s2">from </span><span class="s1">time </span><span class="s2">import </span><span class="s1">time</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">linalg</span>
<span class="s2">from </span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">sparse </span><span class="s2">import </span><span class="s1">csr_matrix</span><span class="s3">, </span><span class="s1">issparse</span>
<span class="s2">from </span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">spatial</span><span class="s3">.</span><span class="s1">distance </span><span class="s2">import </span><span class="s1">pdist</span><span class="s3">, </span><span class="s1">squareform</span>

<span class="s2">from </span><span class="s3">..</span><span class="s1">base </span><span class="s2">import </span><span class="s3">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s3">)</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">decomposition </span><span class="s2">import </span><span class="s1">PCA</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">metrics</span><span class="s3">.</span><span class="s1">pairwise </span><span class="s2">import </span><span class="s1">_VALID_METRICS</span><span class="s3">, </span><span class="s1">pairwise_distances</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">neighbors </span><span class="s2">import </span><span class="s1">NearestNeighbors</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">_openmp_helpers </span><span class="s2">import </span><span class="s1">_openmp_effective_n_threads</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">_param_validation </span><span class="s2">import </span><span class="s1">Hidden</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s2">from </span><span class="s3">..</span><span class="s1">utils</span><span class="s3">.</span><span class="s1">validation </span><span class="s2">import </span><span class="s1">_num_samples</span><span class="s3">, </span><span class="s1">check_non_negative</span>

<span class="s0"># mypy error: Module 'sklearn.manifold' has no attribute '_utils'</span>
<span class="s0"># mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'</span>
<span class="s2">from </span><span class="s3">. </span><span class="s2">import </span><span class="s1">_barnes_hut_tsne</span><span class="s3">, </span><span class="s1">_utils  </span><span class="s0"># type: ignore</span>

<span class="s1">MACHINE_EPSILON </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">finfo</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">double</span><span class="s3">).</span><span class="s1">eps</span>


<span class="s2">def </span><span class="s1">_joint_probabilities</span><span class="s3">(</span><span class="s1">distances</span><span class="s3">, </span><span class="s1">desired_perplexity</span><span class="s3">, </span><span class="s1">verbose</span><span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;Compute joint probabilities p_ij from distances. 
 
    Parameters 
    ---------- 
    distances : ndarray of shape (n_samples * (n_samples-1) / 2,) 
        Distances of samples are stored as condensed matrices, i.e. 
        we omit the diagonal and duplicate entries and store everything 
        in a one-dimensional array. 
 
    desired_perplexity : float 
        Desired perplexity of the joint probability distributions. 
 
    verbose : int 
        Verbosity level. 
 
    Returns 
    ------- 
    P : ndarray of shape (n_samples * (n_samples-1) / 2,) 
        Condensed joint probability matrix. 
    &quot;&quot;&quot;</span>
    <span class="s0"># Compute conditional probabilities such that they approximately match</span>
    <span class="s0"># the desired perplexity</span>
    <span class="s1">distances </span><span class="s3">= </span><span class="s1">distances</span><span class="s3">.</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">conditional_P </span><span class="s3">= </span><span class="s1">_utils</span><span class="s3">.</span><span class="s1">_binary_search_perplexity</span><span class="s3">(</span>
        <span class="s1">distances</span><span class="s3">, </span><span class="s1">desired_perplexity</span><span class="s3">, </span><span class="s1">verbose</span>
    <span class="s3">)</span>
    <span class="s1">P </span><span class="s3">= </span><span class="s1">conditional_P </span><span class="s3">+ </span><span class="s1">conditional_P</span><span class="s3">.</span><span class="s1">T</span>
    <span class="s1">sum_P </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">P</span><span class="s3">), </span><span class="s1">MACHINE_EPSILON</span><span class="s3">)</span>
    <span class="s1">P </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">(</span><span class="s1">squareform</span><span class="s3">(</span><span class="s1">P</span><span class="s3">) / </span><span class="s1">sum_P</span><span class="s3">, </span><span class="s1">MACHINE_EPSILON</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">P</span>


<span class="s2">def </span><span class="s1">_joint_probabilities_nn</span><span class="s3">(</span><span class="s1">distances</span><span class="s3">, </span><span class="s1">desired_perplexity</span><span class="s3">, </span><span class="s1">verbose</span><span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;Compute joint probabilities p_ij from distances using just nearest 
    neighbors. 
 
    This method is approximately equal to _joint_probabilities. The latter 
    is O(N), but limiting the joint probability to nearest neighbors improves 
    this substantially to O(uN). 
 
    Parameters 
    ---------- 
    distances : sparse matrix of shape (n_samples, n_samples) 
        Distances of samples to its n_neighbors nearest neighbors. All other 
        distances are left to zero (and are not materialized in memory). 
        Matrix should be of CSR format. 
 
    desired_perplexity : float 
        Desired perplexity of the joint probability distributions. 
 
    verbose : int 
        Verbosity level. 
 
    Returns 
    ------- 
    P : sparse matrix of shape (n_samples, n_samples) 
        Condensed joint probability matrix with only nearest neighbors. Matrix 
        will be of CSR format. 
    &quot;&quot;&quot;</span>
    <span class="s1">t0 </span><span class="s3">= </span><span class="s1">time</span><span class="s3">()</span>
    <span class="s0"># Compute conditional probabilities such that they approximately match</span>
    <span class="s0"># the desired perplexity</span>
    <span class="s1">distances</span><span class="s3">.</span><span class="s1">sort_indices</span><span class="s3">()</span>
    <span class="s1">n_samples </span><span class="s3">= </span><span class="s1">distances</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>
    <span class="s1">distances_data </span><span class="s3">= </span><span class="s1">distances</span><span class="s3">.</span><span class="s1">data</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, -</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s1">distances_data </span><span class="s3">= </span><span class="s1">distances_data</span><span class="s3">.</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">conditional_P </span><span class="s3">= </span><span class="s1">_utils</span><span class="s3">.</span><span class="s1">_binary_search_perplexity</span><span class="s3">(</span>
        <span class="s1">distances_data</span><span class="s3">, </span><span class="s1">desired_perplexity</span><span class="s3">, </span><span class="s1">verbose</span>
    <span class="s3">)</span>
    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">all</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">isfinite</span><span class="s3">(</span><span class="s1">conditional_P</span><span class="s3">)), </span><span class="s6">&quot;All probabilities should be finite&quot;</span>

    <span class="s0"># Symmetrize the joint probability distribution using sparse operations</span>
    <span class="s1">P </span><span class="s3">= </span><span class="s1">csr_matrix</span><span class="s3">(</span>
        <span class="s3">(</span><span class="s1">conditional_P</span><span class="s3">.</span><span class="s1">ravel</span><span class="s3">(), </span><span class="s1">distances</span><span class="s3">.</span><span class="s1">indices</span><span class="s3">, </span><span class="s1">distances</span><span class="s3">.</span><span class="s1">indptr</span><span class="s3">),</span>
        <span class="s1">shape</span><span class="s3">=(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">),</span>
    <span class="s3">)</span>
    <span class="s1">P </span><span class="s3">= </span><span class="s1">P </span><span class="s3">+ </span><span class="s1">P</span><span class="s3">.</span><span class="s1">T</span>

    <span class="s0"># Normalize the joint probability distribution</span>
    <span class="s1">sum_P </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">(</span><span class="s1">P</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(), </span><span class="s1">MACHINE_EPSILON</span><span class="s3">)</span>
    <span class="s1">P </span><span class="s3">/= </span><span class="s1">sum_P</span>

    <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">all</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">(</span><span class="s1">P</span><span class="s3">.</span><span class="s1">data</span><span class="s3">) &lt;= </span><span class="s5">1.0</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">2</span><span class="s3">:</span>
        <span class="s1">duration </span><span class="s3">= </span><span class="s1">time</span><span class="s3">() - </span><span class="s1">t0</span>
        <span class="s1">print</span><span class="s3">(</span><span class="s6">&quot;[t-SNE] Computed conditional probabilities in {:.3f}s&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s1">duration</span><span class="s3">))</span>
    <span class="s2">return </span><span class="s1">P</span>


<span class="s2">def </span><span class="s1">_kl_divergence</span><span class="s3">(</span>
    <span class="s1">params</span><span class="s3">,</span>
    <span class="s1">P</span><span class="s3">,</span>
    <span class="s1">degrees_of_freedom</span><span class="s3">,</span>
    <span class="s1">n_samples</span><span class="s3">,</span>
    <span class="s1">n_components</span><span class="s3">,</span>
    <span class="s1">skip_num_points</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">compute_error</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
<span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;t-SNE objective function: gradient of the KL divergence 
    of p_ijs and q_ijs and the absolute error. 
 
    Parameters 
    ---------- 
    params : ndarray of shape (n_params,) 
        Unraveled embedding. 
 
    P : ndarray of shape (n_samples * (n_samples-1) / 2,) 
        Condensed joint probability matrix. 
 
    degrees_of_freedom : int 
        Degrees of freedom of the Student's-t distribution. 
 
    n_samples : int 
        Number of samples. 
 
    n_components : int 
        Dimension of the embedded space. 
 
    skip_num_points : int, default=0 
        This does not compute the gradient for points with indices below 
        `skip_num_points`. This is useful when computing transforms of new 
        data where you'd like to keep the old data fixed. 
 
    compute_error: bool, default=True 
        If False, the kl_divergence is not computed and returns NaN. 
 
    Returns 
    ------- 
    kl_divergence : float 
        Kullback-Leibler divergence of p_ij and q_ij. 
 
    grad : ndarray of shape (n_params,) 
        Unraveled gradient of the Kullback-Leibler divergence with respect to 
        the embedding. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_embedded </span><span class="s3">= </span><span class="s1">params</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_components</span><span class="s3">)</span>

    <span class="s0"># Q is a heavy-tailed distribution: Student's t-distribution</span>
    <span class="s1">dist </span><span class="s3">= </span><span class="s1">pdist</span><span class="s3">(</span><span class="s1">X_embedded</span><span class="s3">, </span><span class="s6">&quot;sqeuclidean&quot;</span><span class="s3">)</span>
    <span class="s1">dist </span><span class="s3">/= </span><span class="s1">degrees_of_freedom</span>
    <span class="s1">dist </span><span class="s3">+= </span><span class="s5">1.0</span>
    <span class="s1">dist </span><span class="s3">**= (</span><span class="s1">degrees_of_freedom </span><span class="s3">+ </span><span class="s5">1.0</span><span class="s3">) / -</span><span class="s5">2.0</span>
    <span class="s1">Q </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">(</span><span class="s1">dist </span><span class="s3">/ (</span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">dist</span><span class="s3">)), </span><span class="s1">MACHINE_EPSILON</span><span class="s3">)</span>

    <span class="s0"># Optimization trick below: np.dot(x, y) is faster than</span>
    <span class="s0"># np.sum(x * y) because it calls BLAS</span>

    <span class="s0"># Objective: C (Kullback-Leibler divergence of P and Q)</span>
    <span class="s2">if </span><span class="s1">compute_error</span><span class="s3">:</span>
        <span class="s1">kl_divergence </span><span class="s3">= </span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">(</span><span class="s1">P</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">(</span><span class="s1">P</span><span class="s3">, </span><span class="s1">MACHINE_EPSILON</span><span class="s3">) / </span><span class="s1">Q</span><span class="s3">))</span>
    <span class="s2">else</span><span class="s3">:</span>
        <span class="s1">kl_divergence </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">nan</span>

    <span class="s0"># Gradient: dC/dY</span>
    <span class="s0"># pdist always returns double precision distances. Thus we need to take</span>
    <span class="s1">grad </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ndarray</span><span class="s3">((</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_components</span><span class="s3">), </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">params</span><span class="s3">.</span><span class="s1">dtype</span><span class="s3">)</span>
    <span class="s1">PQd </span><span class="s3">= </span><span class="s1">squareform</span><span class="s3">((</span><span class="s1">P </span><span class="s3">- </span><span class="s1">Q</span><span class="s3">) * </span><span class="s1">dist</span><span class="s3">)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">skip_num_points</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">):</span>
        <span class="s1">grad</span><span class="s3">[</span><span class="s1">i</span><span class="s3">] = </span><span class="s1">np</span><span class="s3">.</span><span class="s1">dot</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">ravel</span><span class="s3">(</span><span class="s1">PQd</span><span class="s3">[</span><span class="s1">i</span><span class="s3">], </span><span class="s1">order</span><span class="s3">=</span><span class="s6">&quot;K&quot;</span><span class="s3">), </span><span class="s1">X_embedded</span><span class="s3">[</span><span class="s1">i</span><span class="s3">] - </span><span class="s1">X_embedded</span><span class="s3">)</span>
    <span class="s1">grad </span><span class="s3">= </span><span class="s1">grad</span><span class="s3">.</span><span class="s1">ravel</span><span class="s3">()</span>
    <span class="s1">c </span><span class="s3">= </span><span class="s5">2.0 </span><span class="s3">* (</span><span class="s1">degrees_of_freedom </span><span class="s3">+ </span><span class="s5">1.0</span><span class="s3">) / </span><span class="s1">degrees_of_freedom</span>
    <span class="s1">grad </span><span class="s3">*= </span><span class="s1">c</span>

    <span class="s2">return </span><span class="s1">kl_divergence</span><span class="s3">, </span><span class="s1">grad</span>


<span class="s2">def </span><span class="s1">_kl_divergence_bh</span><span class="s3">(</span>
    <span class="s1">params</span><span class="s3">,</span>
    <span class="s1">P</span><span class="s3">,</span>
    <span class="s1">degrees_of_freedom</span><span class="s3">,</span>
    <span class="s1">n_samples</span><span class="s3">,</span>
    <span class="s1">n_components</span><span class="s3">,</span>
    <span class="s1">angle</span><span class="s3">=</span><span class="s5">0.5</span><span class="s3">,</span>
    <span class="s1">skip_num_points</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">verbose</span><span class="s3">=</span><span class="s2">False</span><span class="s3">,</span>
    <span class="s1">compute_error</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
    <span class="s1">num_threads</span><span class="s3">=</span><span class="s5">1</span><span class="s3">,</span>
<span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;t-SNE objective function: KL divergence of p_ijs and q_ijs. 
 
    Uses Barnes-Hut tree methods to calculate the gradient that 
    runs in O(NlogN) instead of O(N^2). 
 
    Parameters 
    ---------- 
    params : ndarray of shape (n_params,) 
        Unraveled embedding. 
 
    P : sparse matrix of shape (n_samples, n_sample) 
        Sparse approximate joint probability matrix, computed only for the 
        k nearest-neighbors and symmetrized. Matrix should be of CSR format. 
 
    degrees_of_freedom : int 
        Degrees of freedom of the Student's-t distribution. 
 
    n_samples : int 
        Number of samples. 
 
    n_components : int 
        Dimension of the embedded space. 
 
    angle : float, default=0.5 
        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. 
        'angle' is the angular size (referred to as theta in [3]) of a distant 
        node as measured from a point. If this size is below 'angle' then it is 
        used as a summary node of all points contained within it. 
        This method is not very sensitive to changes in this parameter 
        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing 
        computation time and angle greater 0.8 has quickly increasing error. 
 
    skip_num_points : int, default=0 
        This does not compute the gradient for points with indices below 
        `skip_num_points`. This is useful when computing transforms of new 
        data where you'd like to keep the old data fixed. 
 
    verbose : int, default=False 
        Verbosity level. 
 
    compute_error: bool, default=True 
        If False, the kl_divergence is not computed and returns NaN. 
 
    num_threads : int, default=1 
        Number of threads used to compute the gradient. This is set here to 
        avoid calling _openmp_effective_n_threads for each gradient step. 
 
    Returns 
    ------- 
    kl_divergence : float 
        Kullback-Leibler divergence of p_ij and q_ij. 
 
    grad : ndarray of shape (n_params,) 
        Unraveled gradient of the Kullback-Leibler divergence with respect to 
        the embedding. 
    &quot;&quot;&quot;</span>
    <span class="s1">params </span><span class="s3">= </span><span class="s1">params</span><span class="s3">.</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">X_embedded </span><span class="s3">= </span><span class="s1">params</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_components</span><span class="s3">)</span>

    <span class="s1">val_P </span><span class="s3">= </span><span class="s1">P</span><span class="s3">.</span><span class="s1">data</span><span class="s3">.</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">neighbors </span><span class="s3">= </span><span class="s1">P</span><span class="s3">.</span><span class="s1">indices</span><span class="s3">.</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">int64</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s1">indptr </span><span class="s3">= </span><span class="s1">P</span><span class="s3">.</span><span class="s1">indptr</span><span class="s3">.</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">int64</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>

    <span class="s1">grad </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">X_embedded</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">)</span>
    <span class="s1">error </span><span class="s3">= </span><span class="s1">_barnes_hut_tsne</span><span class="s3">.</span><span class="s1">gradient</span><span class="s3">(</span>
        <span class="s1">val_P</span><span class="s3">,</span>
        <span class="s1">X_embedded</span><span class="s3">,</span>
        <span class="s1">neighbors</span><span class="s3">,</span>
        <span class="s1">indptr</span><span class="s3">,</span>
        <span class="s1">grad</span><span class="s3">,</span>
        <span class="s1">angle</span><span class="s3">,</span>
        <span class="s1">n_components</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">,</span>
        <span class="s1">dof</span><span class="s3">=</span><span class="s1">degrees_of_freedom</span><span class="s3">,</span>
        <span class="s1">compute_error</span><span class="s3">=</span><span class="s1">compute_error</span><span class="s3">,</span>
        <span class="s1">num_threads</span><span class="s3">=</span><span class="s1">num_threads</span><span class="s3">,</span>
    <span class="s3">)</span>
    <span class="s1">c </span><span class="s3">= </span><span class="s5">2.0 </span><span class="s3">* (</span><span class="s1">degrees_of_freedom </span><span class="s3">+ </span><span class="s5">1.0</span><span class="s3">) / </span><span class="s1">degrees_of_freedom</span>
    <span class="s1">grad </span><span class="s3">= </span><span class="s1">grad</span><span class="s3">.</span><span class="s1">ravel</span><span class="s3">()</span>
    <span class="s1">grad </span><span class="s3">*= </span><span class="s1">c</span>

    <span class="s2">return </span><span class="s1">error</span><span class="s3">, </span><span class="s1">grad</span>


<span class="s2">def </span><span class="s1">_gradient_descent</span><span class="s3">(</span>
    <span class="s1">objective</span><span class="s3">,</span>
    <span class="s1">p0</span><span class="s3">,</span>
    <span class="s1">it</span><span class="s3">,</span>
    <span class="s1">max_iter</span><span class="s3">,</span>
    <span class="s1">n_iter_check</span><span class="s3">=</span><span class="s5">1</span><span class="s3">,</span>
    <span class="s1">n_iter_without_progress</span><span class="s3">=</span><span class="s5">300</span><span class="s3">,</span>
    <span class="s1">momentum</span><span class="s3">=</span><span class="s5">0.8</span><span class="s3">,</span>
    <span class="s1">learning_rate</span><span class="s3">=</span><span class="s5">200.0</span><span class="s3">,</span>
    <span class="s1">min_gain</span><span class="s3">=</span><span class="s5">0.01</span><span class="s3">,</span>
    <span class="s1">min_grad_norm</span><span class="s3">=</span><span class="s5">1e-7</span><span class="s3">,</span>
    <span class="s1">verbose</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">args</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
    <span class="s1">kwargs</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
<span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;Batch gradient descent with momentum and individual gains. 
 
    Parameters 
    ---------- 
    objective : callable 
        Should return a tuple of cost and gradient for a given parameter 
        vector. When expensive to compute, the cost can optionally 
        be None and can be computed every n_iter_check steps using 
        the objective_error function. 
 
    p0 : array-like of shape (n_params,) 
        Initial parameter vector. 
 
    it : int 
        Current number of iterations (this function will be called more than 
        once during the optimization). 
 
    max_iter : int 
        Maximum number of gradient descent iterations. 
 
    n_iter_check : int, default=1 
        Number of iterations before evaluating the global error. If the error 
        is sufficiently low, we abort the optimization. 
 
    n_iter_without_progress : int, default=300 
        Maximum number of iterations without progress before we abort the 
        optimization. 
 
    momentum : float within (0.0, 1.0), default=0.8 
        The momentum generates a weight for previous gradients that decays 
        exponentially. 
 
    learning_rate : float, default=200.0 
        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If 
        the learning rate is too high, the data may look like a 'ball' with any 
        point approximately equidistant from its nearest neighbours. If the 
        learning rate is too low, most points may look compressed in a dense 
        cloud with few outliers. 
 
    min_gain : float, default=0.01 
        Minimum individual gain for each parameter. 
 
    min_grad_norm : float, default=1e-7 
        If the gradient norm is below this threshold, the optimization will 
        be aborted. 
 
    verbose : int, default=0 
        Verbosity level. 
 
    args : sequence, default=None 
        Arguments to pass to objective function. 
 
    kwargs : dict, default=None 
        Keyword arguments to pass to objective function. 
 
    Returns 
    ------- 
    p : ndarray of shape (n_params,) 
        Optimum parameters. 
 
    error : float 
        Optimum. 
 
    i : int 
        Last iteration. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">args </span><span class="s2">is None</span><span class="s3">:</span>
        <span class="s1">args </span><span class="s3">= []</span>
    <span class="s2">if </span><span class="s1">kwargs </span><span class="s2">is None</span><span class="s3">:</span>
        <span class="s1">kwargs </span><span class="s3">= {}</span>

    <span class="s1">p </span><span class="s3">= </span><span class="s1">p0</span><span class="s3">.</span><span class="s1">copy</span><span class="s3">().</span><span class="s1">ravel</span><span class="s3">()</span>
    <span class="s1">update </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros_like</span><span class="s3">(</span><span class="s1">p</span><span class="s3">)</span>
    <span class="s1">gains </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ones_like</span><span class="s3">(</span><span class="s1">p</span><span class="s3">)</span>
    <span class="s1">error </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">finfo</span><span class="s3">(</span><span class="s1">float</span><span class="s3">).</span><span class="s1">max</span>
    <span class="s1">best_error </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">finfo</span><span class="s3">(</span><span class="s1">float</span><span class="s3">).</span><span class="s1">max</span>
    <span class="s1">best_iter </span><span class="s3">= </span><span class="s1">i </span><span class="s3">= </span><span class="s1">it</span>

    <span class="s1">tic </span><span class="s3">= </span><span class="s1">time</span><span class="s3">()</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">it</span><span class="s3">, </span><span class="s1">max_iter</span><span class="s3">):</span>
        <span class="s1">check_convergence </span><span class="s3">= (</span><span class="s1">i </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">) % </span><span class="s1">n_iter_check </span><span class="s3">== </span><span class="s5">0</span>
        <span class="s0"># only compute the error when needed</span>
        <span class="s1">kwargs</span><span class="s3">[</span><span class="s6">&quot;compute_error&quot;</span><span class="s3">] = </span><span class="s1">check_convergence </span><span class="s2">or </span><span class="s1">i </span><span class="s3">== </span><span class="s1">max_iter </span><span class="s3">- </span><span class="s5">1</span>

        <span class="s1">error</span><span class="s3">, </span><span class="s1">grad </span><span class="s3">= </span><span class="s1">objective</span><span class="s3">(</span><span class="s1">p</span><span class="s3">, *</span><span class="s1">args</span><span class="s3">, **</span><span class="s1">kwargs</span><span class="s3">)</span>

        <span class="s1">inc </span><span class="s3">= </span><span class="s1">update </span><span class="s3">* </span><span class="s1">grad </span><span class="s3">&lt; </span><span class="s5">0.0</span>
        <span class="s1">dec </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">invert</span><span class="s3">(</span><span class="s1">inc</span><span class="s3">)</span>
        <span class="s1">gains</span><span class="s3">[</span><span class="s1">inc</span><span class="s3">] += </span><span class="s5">0.2</span>
        <span class="s1">gains</span><span class="s3">[</span><span class="s1">dec</span><span class="s3">] *= </span><span class="s5">0.8</span>
        <span class="s1">np</span><span class="s3">.</span><span class="s1">clip</span><span class="s3">(</span><span class="s1">gains</span><span class="s3">, </span><span class="s1">min_gain</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">inf</span><span class="s3">, </span><span class="s1">out</span><span class="s3">=</span><span class="s1">gains</span><span class="s3">)</span>
        <span class="s1">grad </span><span class="s3">*= </span><span class="s1">gains</span>
        <span class="s1">update </span><span class="s3">= </span><span class="s1">momentum </span><span class="s3">* </span><span class="s1">update </span><span class="s3">- </span><span class="s1">learning_rate </span><span class="s3">* </span><span class="s1">grad</span>
        <span class="s1">p </span><span class="s3">+= </span><span class="s1">update</span>

        <span class="s2">if </span><span class="s1">check_convergence</span><span class="s3">:</span>
            <span class="s1">toc </span><span class="s3">= </span><span class="s1">time</span><span class="s3">()</span>
            <span class="s1">duration </span><span class="s3">= </span><span class="s1">toc </span><span class="s3">- </span><span class="s1">tic</span>
            <span class="s1">tic </span><span class="s3">= </span><span class="s1">toc</span>
            <span class="s1">grad_norm </span><span class="s3">= </span><span class="s1">linalg</span><span class="s3">.</span><span class="s1">norm</span><span class="s3">(</span><span class="s1">grad</span><span class="s3">)</span>

            <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">2</span><span class="s3">:</span>
                <span class="s1">print</span><span class="s3">(</span>
                    <span class="s6">&quot;[t-SNE] Iteration %d: error = %.7f,&quot;</span>
                    <span class="s6">&quot; gradient norm = %.7f&quot;</span>
                    <span class="s6">&quot; (%s iterations in %0.3fs)&quot;</span>
                    <span class="s3">% (</span><span class="s1">i </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">, </span><span class="s1">error</span><span class="s3">, </span><span class="s1">grad_norm</span><span class="s3">, </span><span class="s1">n_iter_check</span><span class="s3">, </span><span class="s1">duration</span><span class="s3">)</span>
                <span class="s3">)</span>

            <span class="s2">if </span><span class="s1">error </span><span class="s3">&lt; </span><span class="s1">best_error</span><span class="s3">:</span>
                <span class="s1">best_error </span><span class="s3">= </span><span class="s1">error</span>
                <span class="s1">best_iter </span><span class="s3">= </span><span class="s1">i</span>
            <span class="s2">elif </span><span class="s1">i </span><span class="s3">- </span><span class="s1">best_iter </span><span class="s3">&gt; </span><span class="s1">n_iter_without_progress</span><span class="s3">:</span>
                <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">2</span><span class="s3">:</span>
                    <span class="s1">print</span><span class="s3">(</span>
                        <span class="s6">&quot;[t-SNE] Iteration %d: did not make any progress &quot;</span>
                        <span class="s6">&quot;during the last %d episodes. Finished.&quot;</span>
                        <span class="s3">% (</span><span class="s1">i </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">, </span><span class="s1">n_iter_without_progress</span><span class="s3">)</span>
                    <span class="s3">)</span>
                <span class="s2">break</span>
            <span class="s2">if </span><span class="s1">grad_norm </span><span class="s3">&lt;= </span><span class="s1">min_grad_norm</span><span class="s3">:</span>
                <span class="s2">if </span><span class="s1">verbose </span><span class="s3">&gt;= </span><span class="s5">2</span><span class="s3">:</span>
                    <span class="s1">print</span><span class="s3">(</span>
                        <span class="s6">&quot;[t-SNE] Iteration %d: gradient norm %f. Finished.&quot;</span>
                        <span class="s3">% (</span><span class="s1">i </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">, </span><span class="s1">grad_norm</span><span class="s3">)</span>
                    <span class="s3">)</span>
                <span class="s2">break</span>

    <span class="s2">return </span><span class="s1">p</span><span class="s3">, </span><span class="s1">error</span><span class="s3">, </span><span class="s1">i</span>


<span class="s3">@</span><span class="s1">validate_params</span><span class="s3">(</span>
    <span class="s3">{</span>
        <span class="s6">&quot;X&quot;</span><span class="s3">: [</span><span class="s6">&quot;array-like&quot;</span><span class="s3">, </span><span class="s6">&quot;sparse matrix&quot;</span><span class="s3">],</span>
        <span class="s6">&quot;X_embedded&quot;</span><span class="s3">: [</span><span class="s6">&quot;array-like&quot;</span><span class="s3">, </span><span class="s6">&quot;sparse matrix&quot;</span><span class="s3">],</span>
        <span class="s6">&quot;n_neighbors&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;metric&quot;</span><span class="s3">: [</span><span class="s1">StrOptions</span><span class="s3">(</span><span class="s1">set</span><span class="s3">(</span><span class="s1">_VALID_METRICS</span><span class="s3">) | {</span><span class="s6">&quot;precomputed&quot;</span><span class="s3">}), </span><span class="s1">callable</span><span class="s3">],</span>
    <span class="s3">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s3">=</span><span class="s2">True</span><span class="s3">,</span>
<span class="s3">)</span>
<span class="s2">def </span><span class="s1">trustworthiness</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">X_embedded</span><span class="s3">, *, </span><span class="s1">n_neighbors</span><span class="s3">=</span><span class="s5">5</span><span class="s3">, </span><span class="s1">metric</span><span class="s3">=</span><span class="s6">&quot;euclidean&quot;</span><span class="s3">):</span>
    <span class="s4">r&quot;&quot;&quot;Indicate to what extent the local structure is retained. 
 
    The trustworthiness is within [0, 1]. It is defined as 
 
    .. math:: 
 
        T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1} 
            \sum_{j \in \mathcal{N}_{i}^{k}} \max(0, (r(i, j) - k)) 
 
    where for each sample i, :math:`\mathcal{N}_{i}^{k}` are its k nearest 
    neighbors in the output space, and every sample j is its :math:`r(i, j)`-th 
    nearest neighbor in the input space. In other words, any unexpected nearest 
    neighbors in the output space are penalised in proportion to their rank in 
    the input space. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) or \ 
        (n_samples, n_samples) 
        If the metric is 'precomputed' X must be a square distance 
        matrix. Otherwise it contains a sample per row. 
 
    X_embedded : {array-like, sparse matrix} of shape (n_samples, n_components) 
        Embedding of the training data in low-dimensional space. 
 
    n_neighbors : int, default=5 
        The number of neighbors that will be considered. Should be fewer than 
        `n_samples / 2` to ensure the trustworthiness to lies within [0, 1], as 
        mentioned in [1]_. An error will be raised otherwise. 
 
    metric : str or callable, default='euclidean' 
        Which metric to use for computing pairwise distances between samples 
        from the original input space. If metric is 'precomputed', X must be a 
        matrix of pairwise distances or squared distances. Otherwise, for a list 
        of available metrics, see the documentation of argument metric in 
        `sklearn.pairwise.pairwise_distances` and metrics listed in 
        `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the 
        &quot;cosine&quot; metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`. 
 
        .. versionadded:: 0.20 
 
    Returns 
    ------- 
    trustworthiness : float 
        Trustworthiness of the low-dimensional embedding. 
 
    References 
    ---------- 
    .. [1] Jarkko Venna and Samuel Kaski. 2001. Neighborhood 
           Preservation in Nonlinear Projection Methods: An Experimental Study. 
           In Proceedings of the International Conference on Artificial Neural Networks 
           (ICANN '01). Springer-Verlag, Berlin, Heidelberg, 485-491. 
 
    .. [2] Laurens van der Maaten. Learning a Parametric Embedding by Preserving 
           Local Structure. Proceedings of the Twelfth International Conference on 
           Artificial Intelligence and Statistics, PMLR 5:384-391, 2009. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_blobs 
    &gt;&gt;&gt; from sklearn.decomposition import PCA 
    &gt;&gt;&gt; from sklearn.manifold import trustworthiness 
    &gt;&gt;&gt; X, _ = make_blobs(n_samples=100, n_features=10, centers=3, random_state=42) 
    &gt;&gt;&gt; X_embedded = PCA(n_components=2).fit_transform(X) 
    &gt;&gt;&gt; print(f&quot;{trustworthiness(X, X_embedded, n_neighbors=5):.2f}&quot;) 
    0.92 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples </span><span class="s3">= </span><span class="s1">_num_samples</span><span class="s3">(</span><span class="s1">X</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">n_neighbors </span><span class="s3">&gt;= </span><span class="s1">n_samples </span><span class="s3">/ </span><span class="s5">2</span><span class="s3">:</span>
        <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
            <span class="s6">f&quot;n_neighbors (</span><span class="s2">{</span><span class="s1">n_neighbors</span><span class="s2">}</span><span class="s6">) should be less than n_samples / 2&quot;</span>
            <span class="s6">f&quot; (</span><span class="s2">{</span><span class="s1">n_samples </span><span class="s3">/ </span><span class="s5">2</span><span class="s2">}</span><span class="s6">)&quot;</span>
        <span class="s3">)</span>
    <span class="s1">dist_X </span><span class="s3">= </span><span class="s1">pairwise_distances</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">metric</span><span class="s3">=</span><span class="s1">metric</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">metric </span><span class="s3">== </span><span class="s6">&quot;precomputed&quot;</span><span class="s3">:</span>
        <span class="s1">dist_X </span><span class="s3">= </span><span class="s1">dist_X</span><span class="s3">.</span><span class="s1">copy</span><span class="s3">()</span>
    <span class="s0"># we set the diagonal to np.inf to exclude the points themselves from</span>
    <span class="s0"># their own neighborhood</span>
    <span class="s1">np</span><span class="s3">.</span><span class="s1">fill_diagonal</span><span class="s3">(</span><span class="s1">dist_X</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">inf</span><span class="s3">)</span>
    <span class="s1">ind_X </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">argsort</span><span class="s3">(</span><span class="s1">dist_X</span><span class="s3">, </span><span class="s1">axis</span><span class="s3">=</span><span class="s5">1</span><span class="s3">)</span>
    <span class="s0"># `ind_X[i]` is the index of sorted distances between i and other samples</span>
    <span class="s1">ind_X_embedded </span><span class="s3">= (</span>
        <span class="s1">NearestNeighbors</span><span class="s3">(</span><span class="s1">n_neighbors</span><span class="s3">=</span><span class="s1">n_neighbors</span><span class="s3">)</span>
        <span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X_embedded</span><span class="s3">)</span>
        <span class="s3">.</span><span class="s1">kneighbors</span><span class="s3">(</span><span class="s1">return_distance</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
    <span class="s3">)</span>

    <span class="s0"># We build an inverted index of neighbors in the input space: For sample i,</span>
    <span class="s0"># we define `inverted_index[i]` as the inverted index of sorted distances:</span>
    <span class="s0"># inverted_index[i][ind_X[i]] = np.arange(1, n_sample + 1)</span>
    <span class="s1">inverted_index </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">((</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">), </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">int</span><span class="s3">)</span>
    <span class="s1">ordered_indices </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">arange</span><span class="s3">(</span><span class="s1">n_samples </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">)</span>
    <span class="s1">inverted_index</span><span class="s3">[</span><span class="s1">ordered_indices</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">newaxis</span><span class="s3">], </span><span class="s1">ind_X</span><span class="s3">] = </span><span class="s1">ordered_indices</span><span class="s3">[</span><span class="s5">1</span><span class="s3">:]</span>
    <span class="s1">ranks </span><span class="s3">= (</span>
        <span class="s1">inverted_index</span><span class="s3">[</span><span class="s1">ordered_indices</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">newaxis</span><span class="s3">], </span><span class="s1">ind_X_embedded</span><span class="s3">] - </span><span class="s1">n_neighbors</span>
    <span class="s3">)</span>
    <span class="s1">t </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">ranks</span><span class="s3">[</span><span class="s1">ranks </span><span class="s3">&gt; </span><span class="s5">0</span><span class="s3">])</span>
    <span class="s1">t </span><span class="s3">= </span><span class="s5">1.0 </span><span class="s3">- </span><span class="s1">t </span><span class="s3">* (</span>
        <span class="s5">2.0 </span><span class="s3">/ (</span><span class="s1">n_samples </span><span class="s3">* </span><span class="s1">n_neighbors </span><span class="s3">* (</span><span class="s5">2.0 </span><span class="s3">* </span><span class="s1">n_samples </span><span class="s3">- </span><span class="s5">3.0 </span><span class="s3">* </span><span class="s1">n_neighbors </span><span class="s3">- </span><span class="s5">1.0</span><span class="s3">))</span>
    <span class="s3">)</span>
    <span class="s2">return </span><span class="s1">t</span>


<span class="s2">class </span><span class="s1">TSNE</span><span class="s3">(</span><span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator</span><span class="s3">):</span>
    <span class="s4">&quot;&quot;&quot;T-distributed Stochastic Neighbor Embedding. 
 
    t-SNE [1] is a tool to visualize high-dimensional data. It converts 
    similarities between data points to joint probabilities and tries 
    to minimize the Kullback-Leibler divergence between the joint 
    probabilities of the low-dimensional embedding and the 
    high-dimensional data. t-SNE has a cost function that is not convex, 
    i.e. with different initializations we can get different results. 
 
    It is highly recommended to use another dimensionality reduction 
    method (e.g. PCA for dense data or TruncatedSVD for sparse data) 
    to reduce the number of dimensions to a reasonable amount (e.g. 50) 
    if the number of features is very high. This will suppress some 
    noise and speed up the computation of pairwise distances between 
    samples. For more tips see Laurens van der Maaten's FAQ [2]. 
 
    Read more in the :ref:`User Guide &lt;t_sne&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int, default=2 
        Dimension of the embedded space. 
 
    perplexity : float, default=30.0 
        The perplexity is related to the number of nearest neighbors that 
        is used in other manifold learning algorithms. Larger datasets 
        usually require a larger perplexity. Consider selecting a value 
        between 5 and 50. Different values can result in significantly 
        different results. The perplexity must be less than the number 
        of samples. 
 
    early_exaggeration : float, default=12.0 
        Controls how tight natural clusters in the original space are in 
        the embedded space and how much space will be between them. For 
        larger values, the space between natural clusters will be larger 
        in the embedded space. Again, the choice of this parameter is not 
        very critical. If the cost function increases during initial 
        optimization, the early exaggeration factor or the learning rate 
        might be too high. 
 
    learning_rate : float or &quot;auto&quot;, default=&quot;auto&quot; 
        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If 
        the learning rate is too high, the data may look like a 'ball' with any 
        point approximately equidistant from its nearest neighbours. If the 
        learning rate is too low, most points may look compressed in a dense 
        cloud with few outliers. If the cost function gets stuck in a bad local 
        minimum increasing the learning rate may help. 
        Note that many other t-SNE implementations (bhtsne, FIt-SNE, openTSNE, 
        etc.) use a definition of learning_rate that is 4 times smaller than 
        ours. So our learning_rate=200 corresponds to learning_rate=800 in 
        those other implementations. The 'auto' option sets the learning_rate 
        to `max(N / early_exaggeration / 4, 50)` where N is the sample size, 
        following [4] and [5]. 
 
        .. versionchanged:: 1.2 
           The default value changed to `&quot;auto&quot;`. 
 
    max_iter : int, default=1000 
        Maximum number of iterations for the optimization. Should be at 
        least 250. 
 
        .. versionchanged:: 1.5 
            Parameter name changed from `n_iter` to `max_iter`. 
 
    n_iter_without_progress : int, default=300 
        Maximum number of iterations without progress before we abort the 
        optimization, used after 250 initial iterations with early 
        exaggeration. Note that progress is only checked every 50 iterations so 
        this value is rounded to the next multiple of 50. 
 
        .. versionadded:: 0.17 
           parameter *n_iter_without_progress* to control stopping criteria. 
 
    min_grad_norm : float, default=1e-7 
        If the gradient norm is below this threshold, the optimization will 
        be stopped. 
 
    metric : str or callable, default='euclidean' 
        The metric to use when calculating distance between instances in a 
        feature array. If metric is a string, it must be one of the options 
        allowed by scipy.spatial.distance.pdist for its metric parameter, or 
        a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. 
        If metric is &quot;precomputed&quot;, X is assumed to be a distance matrix. 
        Alternatively, if metric is a callable function, it is called on each 
        pair of instances (rows) and the resulting value recorded. The callable 
        should take two arrays from X as input and return a value indicating 
        the distance between them. The default is &quot;euclidean&quot; which is 
        interpreted as squared euclidean distance. 
 
    metric_params : dict, default=None 
        Additional keyword arguments for the metric function. 
 
        .. versionadded:: 1.1 
 
    init : {&quot;random&quot;, &quot;pca&quot;} or ndarray of shape (n_samples, n_components), \ 
            default=&quot;pca&quot; 
        Initialization of embedding. 
        PCA initialization cannot be used with precomputed distances and is 
        usually more globally stable than random initialization. 
 
        .. versionchanged:: 1.2 
           The default value changed to `&quot;pca&quot;`. 
 
    verbose : int, default=0 
        Verbosity level. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines the random number generator. Pass an int for reproducible 
        results across multiple function calls. Note that different 
        initializations might result in different local minima of the cost 
        function. See :term:`Glossary &lt;random_state&gt;`. 
 
    method : {'barnes_hut', 'exact'}, default='barnes_hut' 
        By default the gradient calculation algorithm uses Barnes-Hut 
        approximation running in O(NlogN) time. method='exact' 
        will run on the slower, but exact, algorithm in O(N^2) time. The 
        exact algorithm should be used when nearest-neighbor errors need 
        to be better than 3%. However, the exact method cannot scale to 
        millions of examples. 
 
        .. versionadded:: 0.17 
           Approximate optimization *method* via the Barnes-Hut. 
 
    angle : float, default=0.5 
        Only used if method='barnes_hut' 
        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. 
        'angle' is the angular size (referred to as theta in [3]) of a distant 
        node as measured from a point. If this size is below 'angle' then it is 
        used as a summary node of all points contained within it. 
        This method is not very sensitive to changes in this parameter 
        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing 
        computation time and angle greater 0.8 has quickly increasing error. 
 
    n_jobs : int, default=None 
        The number of parallel jobs to run for neighbors search. This parameter 
        has no impact when ``metric=&quot;precomputed&quot;`` or 
        (``metric=&quot;euclidean&quot;`` and ``method=&quot;exact&quot;``). 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
        .. versionadded:: 0.22 
 
    n_iter : int 
        Maximum number of iterations for the optimization. Should be at 
        least 250. 
 
        .. deprecated:: 1.5 
            `n_iter` was deprecated in version 1.5 and will be removed in 1.7. 
            Please use `max_iter` instead. 
 
    Attributes 
    ---------- 
    embedding_ : array-like of shape (n_samples, n_components) 
        Stores the embedding vectors. 
 
    kl_divergence_ : float 
        Kullback-Leibler divergence after optimization. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    learning_rate_ : float 
        Effective learning rate. 
 
        .. versionadded:: 1.2 
 
    n_iter_ : int 
        Number of iterations run. 
 
    See Also 
    -------- 
    sklearn.decomposition.PCA : Principal component analysis that is a linear 
        dimensionality reduction method. 
    sklearn.decomposition.KernelPCA : Non-linear dimensionality reduction using 
        kernels and PCA. 
    MDS : Manifold learning using multidimensional scaling. 
    Isomap : Manifold learning based on Isometric Mapping. 
    LocallyLinearEmbedding : Manifold learning using Locally Linear Embedding. 
    SpectralEmbedding : Spectral embedding for non-linear dimensionality. 
 
    Notes 
    ----- 
    For an example of using :class:`~sklearn.manifold.TSNE` in combination with 
    :class:`~sklearn.neighbors.KNeighborsTransformer` see 
    :ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`. 
 
    References 
    ---------- 
 
    [1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data 
        Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008. 
 
    [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding 
        https://lvdmaaten.github.io/tsne/ 
 
    [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms. 
        Journal of Machine Learning Research 15(Oct):3221-3245, 2014. 
        https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf 
 
    [4] Belkina, A. C., Ciccolella, C. O., Anno, R., Halpert, R., Spidlen, J., 
        &amp; Snyder-Cappione, J. E. (2019). Automated optimized parameters for 
        T-distributed stochastic neighbor embedding improve visualization 
        and analysis of large datasets. Nature Communications, 10(1), 1-12. 
 
    [5] Kobak, D., &amp; Berens, P. (2019). The art of using t-SNE for single-cell 
        transcriptomics. Nature Communications, 10(1), 1-14. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.manifold import TSNE 
    &gt;&gt;&gt; X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]) 
    &gt;&gt;&gt; X_embedded = TSNE(n_components=2, learning_rate='auto', 
    ...                   init='random', perplexity=3).fit_transform(X) 
    &gt;&gt;&gt; X_embedded.shape 
    (4, 2) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s3">: </span><span class="s1">dict </span><span class="s3">= {</span>
        <span class="s6">&quot;n_components&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;perplexity&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;neither&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;early_exaggeration&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;learning_rate&quot;</span><span class="s3">: [</span>
            <span class="s1">StrOptions</span><span class="s3">({</span><span class="s6">&quot;auto&quot;</span><span class="s3">}),</span>
            <span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;neither&quot;</span><span class="s3">),</span>
        <span class="s3">],</span>
        <span class="s6">&quot;max_iter&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Integral</span><span class="s3">, </span><span class="s5">250</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">), </span><span class="s2">None</span><span class="s3">],</span>
        <span class="s6">&quot;n_iter_without_progress&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Integral</span><span class="s3">, -</span><span class="s5">1</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;min_grad_norm&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;metric&quot;</span><span class="s3">: [</span><span class="s1">StrOptions</span><span class="s3">(</span><span class="s1">set</span><span class="s3">(</span><span class="s1">_VALID_METRICS</span><span class="s3">) | {</span><span class="s6">&quot;precomputed&quot;</span><span class="s3">}), </span><span class="s1">callable</span><span class="s3">],</span>
        <span class="s6">&quot;metric_params&quot;</span><span class="s3">: [</span><span class="s1">dict</span><span class="s3">, </span><span class="s2">None</span><span class="s3">],</span>
        <span class="s6">&quot;init&quot;</span><span class="s3">: [</span>
            <span class="s1">StrOptions</span><span class="s3">({</span><span class="s6">&quot;pca&quot;</span><span class="s3">, </span><span class="s6">&quot;random&quot;</span><span class="s3">}),</span>
            <span class="s1">np</span><span class="s3">.</span><span class="s1">ndarray</span><span class="s3">,</span>
        <span class="s3">],</span>
        <span class="s6">&quot;verbose&quot;</span><span class="s3">: [</span><span class="s6">&quot;verbose&quot;</span><span class="s3">],</span>
        <span class="s6">&quot;random_state&quot;</span><span class="s3">: [</span><span class="s6">&quot;random_state&quot;</span><span class="s3">],</span>
        <span class="s6">&quot;method&quot;</span><span class="s3">: [</span><span class="s1">StrOptions</span><span class="s3">({</span><span class="s6">&quot;barnes_hut&quot;</span><span class="s3">, </span><span class="s6">&quot;exact&quot;</span><span class="s3">})],</span>
        <span class="s6">&quot;angle&quot;</span><span class="s3">: [</span><span class="s1">Interval</span><span class="s3">(</span><span class="s1">Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;both&quot;</span><span class="s3">)],</span>
        <span class="s6">&quot;n_jobs&quot;</span><span class="s3">: [</span><span class="s2">None</span><span class="s3">, </span><span class="s1">Integral</span><span class="s3">],</span>
        <span class="s6">&quot;n_iter&quot;</span><span class="s3">: [</span>
            <span class="s1">Interval</span><span class="s3">(</span><span class="s1">Integral</span><span class="s3">, </span><span class="s5">250</span><span class="s3">, </span><span class="s2">None</span><span class="s3">, </span><span class="s1">closed</span><span class="s3">=</span><span class="s6">&quot;left&quot;</span><span class="s3">),</span>
            <span class="s1">Hidden</span><span class="s3">(</span><span class="s1">StrOptions</span><span class="s3">({</span><span class="s6">&quot;deprecated&quot;</span><span class="s3">})),</span>
        <span class="s3">],</span>
    <span class="s3">}</span>

    <span class="s0"># Control the number of exploration iterations with early_exaggeration on</span>
    <span class="s1">_EXPLORATION_MAX_ITER </span><span class="s3">= </span><span class="s5">250</span>

    <span class="s0"># Control the number of iterations between progress checks</span>
    <span class="s1">_N_ITER_CHECK </span><span class="s3">= </span><span class="s5">50</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_components</span><span class="s3">=</span><span class="s5">2</span><span class="s3">,</span>
        <span class="s3">*,</span>
        <span class="s1">perplexity</span><span class="s3">=</span><span class="s5">30.0</span><span class="s3">,</span>
        <span class="s1">early_exaggeration</span><span class="s3">=</span><span class="s5">12.0</span><span class="s3">,</span>
        <span class="s1">learning_rate</span><span class="s3">=</span><span class="s6">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,  </span><span class="s0"># TODO(1.7): set to 1000</span>
        <span class="s1">n_iter_without_progress</span><span class="s3">=</span><span class="s5">300</span><span class="s3">,</span>
        <span class="s1">min_grad_norm</span><span class="s3">=</span><span class="s5">1e-7</span><span class="s3">,</span>
        <span class="s1">metric</span><span class="s3">=</span><span class="s6">&quot;euclidean&quot;</span><span class="s3">,</span>
        <span class="s1">metric_params</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
        <span class="s1">init</span><span class="s3">=</span><span class="s6">&quot;pca&quot;</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
        <span class="s1">method</span><span class="s3">=</span><span class="s6">&quot;barnes_hut&quot;</span><span class="s3">,</span>
        <span class="s1">angle</span><span class="s3">=</span><span class="s5">0.5</span><span class="s3">,</span>
        <span class="s1">n_jobs</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
        <span class="s1">n_iter</span><span class="s3">=</span><span class="s6">&quot;deprecated&quot;</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">n_components </span><span class="s3">= </span><span class="s1">n_components</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">perplexity </span><span class="s3">= </span><span class="s1">perplexity</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">early_exaggeration </span><span class="s3">= </span><span class="s1">early_exaggeration</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate </span><span class="s3">= </span><span class="s1">learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter </span><span class="s3">= </span><span class="s1">max_iter</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter_without_progress </span><span class="s3">= </span><span class="s1">n_iter_without_progress</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">min_grad_norm </span><span class="s3">= </span><span class="s1">min_grad_norm</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">metric </span><span class="s3">= </span><span class="s1">metric</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">metric_params </span><span class="s3">= </span><span class="s1">metric_params</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">init </span><span class="s3">= </span><span class="s1">init</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">verbose </span><span class="s3">= </span><span class="s1">verbose</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">random_state </span><span class="s3">= </span><span class="s1">random_state</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">method </span><span class="s3">= </span><span class="s1">method</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">angle </span><span class="s3">= </span><span class="s1">angle</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">n_jobs </span><span class="s3">= </span><span class="s1">n_jobs</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter </span><span class="s3">= </span><span class="s1">n_iter</span>

    <span class="s2">def </span><span class="s1">_check_params_vs_input</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">):</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">perplexity </span><span class="s3">&gt;= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span><span class="s6">&quot;perplexity must be less than n_samples&quot;</span><span class="s3">)</span>

    <span class="s2">def </span><span class="s1">_fit</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">skip_num_points</span><span class="s3">=</span><span class="s5">0</span><span class="s3">):</span>
        <span class="s4">&quot;&quot;&quot;Private function to fit the model using X as training data.&quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">init</span><span class="s3">, </span><span class="s1">str</span><span class="s3">) </span><span class="s2">and </span><span class="s1">self</span><span class="s3">.</span><span class="s1">init </span><span class="s3">== </span><span class="s6">&quot;pca&quot; </span><span class="s2">and </span><span class="s1">issparse</span><span class="s3">(</span><span class="s1">X</span><span class="s3">):</span>
            <span class="s2">raise </span><span class="s1">TypeError</span><span class="s3">(</span>
                <span class="s6">&quot;PCA initialization is currently not supported &quot;</span>
                <span class="s6">&quot;with the sparse input matrix. Use &quot;</span>
                <span class="s6">'init=&quot;random&quot; instead.'</span>
            <span class="s3">)</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate </span><span class="s3">== </span><span class="s6">&quot;auto&quot;</span><span class="s3">:</span>
            <span class="s0"># See issue #18018</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate_ </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">] / </span><span class="s1">self</span><span class="s3">.</span><span class="s1">early_exaggeration </span><span class="s3">/ </span><span class="s5">4</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate_ </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">maximum</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate_</span><span class="s3">, </span><span class="s5">50</span><span class="s3">)</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate_ </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">method </span><span class="s3">== </span><span class="s6">&quot;barnes_hut&quot;</span><span class="s3">:</span>
            <span class="s1">X </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_validate_data</span><span class="s3">(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">accept_sparse</span><span class="s3">=[</span><span class="s6">&quot;csr&quot;</span><span class="s3">],</span>
                <span class="s1">ensure_min_samples</span><span class="s3">=</span><span class="s5">2</span><span class="s3">,</span>
                <span class="s1">dtype</span><span class="s3">=[</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">float64</span><span class="s3">],</span>
            <span class="s3">)</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">X </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_validate_data</span><span class="s3">(</span>
                <span class="s1">X</span><span class="s3">, </span><span class="s1">accept_sparse</span><span class="s3">=[</span><span class="s6">&quot;csr&quot;</span><span class="s3">, </span><span class="s6">&quot;csc&quot;</span><span class="s3">, </span><span class="s6">&quot;coo&quot;</span><span class="s3">], </span><span class="s1">dtype</span><span class="s3">=[</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">float64</span><span class="s3">]</span>
            <span class="s3">)</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric </span><span class="s3">== </span><span class="s6">&quot;precomputed&quot;</span><span class="s3">:</span>
            <span class="s2">if </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">init</span><span class="s3">, </span><span class="s1">str</span><span class="s3">) </span><span class="s2">and </span><span class="s1">self</span><span class="s3">.</span><span class="s1">init </span><span class="s3">== </span><span class="s6">&quot;pca&quot;</span><span class="s3">:</span>
                <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                    <span class="s6">'The parameter init=&quot;pca&quot; cannot be used with metric=&quot;precomputed&quot;.'</span>
                <span class="s3">)</span>
            <span class="s2">if </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">] != </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span><span class="s6">&quot;X should be a square distance matrix&quot;</span><span class="s3">)</span>

            <span class="s1">check_non_negative</span><span class="s3">(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s3">(</span>
                    <span class="s6">&quot;TSNE.fit(). With metric='precomputed', X &quot;</span>
                    <span class="s6">&quot;should contain positive distances.&quot;</span>
                <span class="s3">),</span>
            <span class="s3">)</span>

            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">method </span><span class="s3">== </span><span class="s6">&quot;exact&quot; </span><span class="s2">and </span><span class="s1">issparse</span><span class="s3">(</span><span class="s1">X</span><span class="s3">):</span>
                <span class="s2">raise </span><span class="s1">TypeError</span><span class="s3">(</span>
                    <span class="s6">'TSNE with method=&quot;exact&quot; does not accept sparse '</span>
                    <span class="s6">'precomputed distance matrix. Use method=&quot;barnes_hut&quot; '</span>
                    <span class="s6">&quot;or provide the dense distance matrix.&quot;</span>
                <span class="s3">)</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">method </span><span class="s3">== </span><span class="s6">&quot;barnes_hut&quot; </span><span class="s2">and </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_components </span><span class="s3">&gt; </span><span class="s5">3</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s6">&quot;'n_components' should be inferior to 4 for the &quot;</span>
                <span class="s6">&quot;barnes_hut algorithm as it relies on &quot;</span>
                <span class="s6">&quot;quad-tree or oct-tree.&quot;</span>
            <span class="s3">)</span>
        <span class="s1">random_state </span><span class="s3">= </span><span class="s1">check_random_state</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">random_state</span><span class="s3">)</span>

        <span class="s1">n_samples </span><span class="s3">= </span><span class="s1">X</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>

        <span class="s1">neighbors_nn </span><span class="s3">= </span><span class="s2">None</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">method </span><span class="s3">== </span><span class="s6">&quot;exact&quot;</span><span class="s3">:</span>
            <span class="s0"># Retrieve the distance matrix, either using the precomputed one or</span>
            <span class="s0"># computing it.</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric </span><span class="s3">== </span><span class="s6">&quot;precomputed&quot;</span><span class="s3">:</span>
                <span class="s1">distances </span><span class="s3">= </span><span class="s1">X</span>
            <span class="s2">else</span><span class="s3">:</span>
                <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">:</span>
                    <span class="s1">print</span><span class="s3">(</span><span class="s6">&quot;[t-SNE] Computing pairwise distances...&quot;</span><span class="s3">)</span>

                <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric </span><span class="s3">== </span><span class="s6">&quot;euclidean&quot;</span><span class="s3">:</span>
                    <span class="s0"># Euclidean is squared here, rather than using **= 2,</span>
                    <span class="s0"># because euclidean_distances already calculates</span>
                    <span class="s0"># squared distances, and returns np.sqrt(dist) for</span>
                    <span class="s0"># squared=False.</span>
                    <span class="s0"># Also, Euclidean is slower for n_jobs&gt;1, so don't set here</span>
                    <span class="s1">distances </span><span class="s3">= </span><span class="s1">pairwise_distances</span><span class="s3">(</span><span class="s1">X</span><span class="s3">, </span><span class="s1">metric</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric</span><span class="s3">, </span><span class="s1">squared</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)</span>
                <span class="s2">else</span><span class="s3">:</span>
                    <span class="s1">metric_params_ </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric_params </span><span class="s2">or </span><span class="s3">{}</span>
                    <span class="s1">distances </span><span class="s3">= </span><span class="s1">pairwise_distances</span><span class="s3">(</span>
                        <span class="s1">X</span><span class="s3">, </span><span class="s1">metric</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric</span><span class="s3">, </span><span class="s1">n_jobs</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_jobs</span><span class="s3">, **</span><span class="s1">metric_params_</span>
                    <span class="s3">)</span>

            <span class="s2">if </span><span class="s1">np</span><span class="s3">.</span><span class="s1">any</span><span class="s3">(</span><span class="s1">distances </span><span class="s3">&lt; </span><span class="s5">0</span><span class="s3">):</span>
                <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                    <span class="s6">&quot;All distances should be positive, the metric given is not correct&quot;</span>
                <span class="s3">)</span>

            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric </span><span class="s3">!= </span><span class="s6">&quot;euclidean&quot;</span><span class="s3">:</span>
                <span class="s1">distances </span><span class="s3">**= </span><span class="s5">2</span>

            <span class="s0"># compute the joint probability distribution for the input space</span>
            <span class="s1">P </span><span class="s3">= </span><span class="s1">_joint_probabilities</span><span class="s3">(</span><span class="s1">distances</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">perplexity</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">)</span>
            <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">all</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">isfinite</span><span class="s3">(</span><span class="s1">P</span><span class="s3">)), </span><span class="s6">&quot;All probabilities should be finite&quot;</span>
            <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">all</span><span class="s3">(</span><span class="s1">P </span><span class="s3">&gt;= </span><span class="s5">0</span><span class="s3">), </span><span class="s6">&quot;All probabilities should be non-negative&quot;</span>
            <span class="s2">assert </span><span class="s1">np</span><span class="s3">.</span><span class="s1">all</span><span class="s3">(</span>
                <span class="s1">P </span><span class="s3">&lt;= </span><span class="s5">1</span>
            <span class="s3">), </span><span class="s6">&quot;All probabilities should be less or then equal to one&quot;</span>

        <span class="s2">else</span><span class="s3">:</span>
            <span class="s0"># Compute the number of nearest neighbors to find.</span>
            <span class="s0"># LvdM uses 3 * perplexity as the number of neighbors.</span>
            <span class="s0"># In the event that we have very small # of points</span>
            <span class="s0"># set the neighbors to n - 1.</span>
            <span class="s1">n_neighbors </span><span class="s3">= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">n_samples </span><span class="s3">- </span><span class="s5">1</span><span class="s3">, </span><span class="s1">int</span><span class="s3">(</span><span class="s5">3.0 </span><span class="s3">* </span><span class="s1">self</span><span class="s3">.</span><span class="s1">perplexity </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">))</span>

            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">:</span>
                <span class="s1">print</span><span class="s3">(</span><span class="s6">&quot;[t-SNE] Computing {} nearest neighbors...&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span><span class="s1">n_neighbors</span><span class="s3">))</span>

            <span class="s0"># Find the nearest neighbors for every point</span>
            <span class="s1">knn </span><span class="s3">= </span><span class="s1">NearestNeighbors</span><span class="s3">(</span>
                <span class="s1">algorithm</span><span class="s3">=</span><span class="s6">&quot;auto&quot;</span><span class="s3">,</span>
                <span class="s1">n_jobs</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_jobs</span><span class="s3">,</span>
                <span class="s1">n_neighbors</span><span class="s3">=</span><span class="s1">n_neighbors</span><span class="s3">,</span>
                <span class="s1">metric</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric</span><span class="s3">,</span>
                <span class="s1">metric_params</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric_params</span><span class="s3">,</span>
            <span class="s3">)</span>
            <span class="s1">t0 </span><span class="s3">= </span><span class="s1">time</span><span class="s3">()</span>
            <span class="s1">knn</span><span class="s3">.</span><span class="s1">fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">)</span>
            <span class="s1">duration </span><span class="s3">= </span><span class="s1">time</span><span class="s3">() - </span><span class="s1">t0</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">:</span>
                <span class="s1">print</span><span class="s3">(</span>
                    <span class="s6">&quot;[t-SNE] Indexed {} samples in {:.3f}s...&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span>
                        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">duration</span>
                    <span class="s3">)</span>
                <span class="s3">)</span>

            <span class="s1">t0 </span><span class="s3">= </span><span class="s1">time</span><span class="s3">()</span>
            <span class="s1">distances_nn </span><span class="s3">= </span><span class="s1">knn</span><span class="s3">.</span><span class="s1">kneighbors_graph</span><span class="s3">(</span><span class="s1">mode</span><span class="s3">=</span><span class="s6">&quot;distance&quot;</span><span class="s3">)</span>
            <span class="s1">duration </span><span class="s3">= </span><span class="s1">time</span><span class="s3">() - </span><span class="s1">t0</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">:</span>
                <span class="s1">print</span><span class="s3">(</span>
                    <span class="s6">&quot;[t-SNE] Computed neighbors for {} samples in {:.3f}s...&quot;</span><span class="s3">.</span><span class="s1">format</span><span class="s3">(</span>
                        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">duration</span>
                    <span class="s3">)</span>
                <span class="s3">)</span>

            <span class="s0"># Free the memory used by the ball_tree</span>
            <span class="s2">del </span><span class="s1">knn</span>

            <span class="s0"># knn return the euclidean distance but we need it squared</span>
            <span class="s0"># to be consistent with the 'exact' method. Note that the</span>
            <span class="s0"># the method was derived using the euclidean method as in the</span>
            <span class="s0"># input space. Not sure of the implication of using a different</span>
            <span class="s0"># metric.</span>
            <span class="s1">distances_nn</span><span class="s3">.</span><span class="s1">data </span><span class="s3">**= </span><span class="s5">2</span>

            <span class="s0"># compute the joint probability distribution for the input space</span>
            <span class="s1">P </span><span class="s3">= </span><span class="s1">_joint_probabilities_nn</span><span class="s3">(</span><span class="s1">distances_nn</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">perplexity</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">)</span>

        <span class="s2">if </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">init</span><span class="s3">, </span><span class="s1">np</span><span class="s3">.</span><span class="s1">ndarray</span><span class="s3">):</span>
            <span class="s1">X_embedded </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">init</span>
        <span class="s2">elif </span><span class="s1">self</span><span class="s3">.</span><span class="s1">init </span><span class="s3">== </span><span class="s6">&quot;pca&quot;</span><span class="s3">:</span>
            <span class="s1">pca </span><span class="s3">= </span><span class="s1">PCA</span><span class="s3">(</span>
                <span class="s1">n_components</span><span class="s3">=</span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_components</span><span class="s3">,</span>
                <span class="s1">svd_solver</span><span class="s3">=</span><span class="s6">&quot;randomized&quot;</span><span class="s3">,</span>
                <span class="s1">random_state</span><span class="s3">=</span><span class="s1">random_state</span><span class="s3">,</span>
            <span class="s3">)</span>
            <span class="s0"># Always output a numpy array, no matter what is configured globally</span>
            <span class="s1">pca</span><span class="s3">.</span><span class="s1">set_output</span><span class="s3">(</span><span class="s1">transform</span><span class="s3">=</span><span class="s6">&quot;default&quot;</span><span class="s3">)</span>
            <span class="s1">X_embedded </span><span class="s3">= </span><span class="s1">pca</span><span class="s3">.</span><span class="s1">fit_transform</span><span class="s3">(</span><span class="s1">X</span><span class="s3">).</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">, </span><span class="s1">copy</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
            <span class="s0"># PCA is rescaled so that PC1 has standard deviation 1e-4 which is</span>
            <span class="s0"># the default value for random initialization. See issue #18018.</span>
            <span class="s1">X_embedded </span><span class="s3">= </span><span class="s1">X_embedded </span><span class="s3">/ </span><span class="s1">np</span><span class="s3">.</span><span class="s1">std</span><span class="s3">(</span><span class="s1">X_embedded</span><span class="s3">[:, </span><span class="s5">0</span><span class="s3">]) * </span><span class="s5">1e-4</span>
        <span class="s2">elif </span><span class="s1">self</span><span class="s3">.</span><span class="s1">init </span><span class="s3">== </span><span class="s6">&quot;random&quot;</span><span class="s3">:</span>
            <span class="s0"># The embedding is initialized with iid samples from Gaussians with</span>
            <span class="s0"># standard deviation 1e-4.</span>
            <span class="s1">X_embedded </span><span class="s3">= </span><span class="s5">1e-4 </span><span class="s3">* </span><span class="s1">random_state</span><span class="s3">.</span><span class="s1">standard_normal</span><span class="s3">(</span>
                <span class="s1">size</span><span class="s3">=(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_components</span><span class="s3">)</span>
            <span class="s3">).</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">float32</span><span class="s3">)</span>

        <span class="s0"># Degrees of freedom of the Student's t-distribution. The suggestion</span>
        <span class="s0"># degrees_of_freedom = n_components - 1 comes from</span>
        <span class="s0"># &quot;Learning a Parametric Embedding by Preserving Local Structure&quot;</span>
        <span class="s0"># Laurens van der Maaten, 2009.</span>
        <span class="s1">degrees_of_freedom </span><span class="s3">= </span><span class="s1">max</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_components </span><span class="s3">- </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s3">)</span>

        <span class="s2">return </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_tsne</span><span class="s3">(</span>
            <span class="s1">P</span><span class="s3">,</span>
            <span class="s1">degrees_of_freedom</span><span class="s3">,</span>
            <span class="s1">n_samples</span><span class="s3">,</span>
            <span class="s1">X_embedded</span><span class="s3">=</span><span class="s1">X_embedded</span><span class="s3">,</span>
            <span class="s1">neighbors</span><span class="s3">=</span><span class="s1">neighbors_nn</span><span class="s3">,</span>
            <span class="s1">skip_num_points</span><span class="s3">=</span><span class="s1">skip_num_points</span><span class="s3">,</span>
        <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">_tsne</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">P</span><span class="s3">,</span>
        <span class="s1">degrees_of_freedom</span><span class="s3">,</span>
        <span class="s1">n_samples</span><span class="s3">,</span>
        <span class="s1">X_embedded</span><span class="s3">,</span>
        <span class="s1">neighbors</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
        <span class="s1">skip_num_points</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s4">&quot;&quot;&quot;Runs t-SNE.&quot;&quot;&quot;</span>
        <span class="s0"># t-SNE minimizes the Kullback-Leiber divergence of the Gaussians P</span>
        <span class="s0"># and the Student's t-distributions Q. The optimization algorithm that</span>
        <span class="s0"># we use is batch gradient descent with two stages:</span>
        <span class="s0"># * initial optimization with early exaggeration and momentum at 0.5</span>
        <span class="s0"># * final optimization with momentum at 0.8</span>
        <span class="s1">params </span><span class="s3">= </span><span class="s1">X_embedded</span><span class="s3">.</span><span class="s1">ravel</span><span class="s3">()</span>

        <span class="s1">opt_args </span><span class="s3">= {</span>
            <span class="s6">&quot;it&quot;</span><span class="s3">: </span><span class="s5">0</span><span class="s3">,</span>
            <span class="s6">&quot;n_iter_check&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_N_ITER_CHECK</span><span class="s3">,</span>
            <span class="s6">&quot;min_grad_norm&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">min_grad_norm</span><span class="s3">,</span>
            <span class="s6">&quot;learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">learning_rate_</span><span class="s3">,</span>
            <span class="s6">&quot;verbose&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">,</span>
            <span class="s6">&quot;kwargs&quot;</span><span class="s3">: </span><span class="s1">dict</span><span class="s3">(</span><span class="s1">skip_num_points</span><span class="s3">=</span><span class="s1">skip_num_points</span><span class="s3">),</span>
            <span class="s6">&quot;args&quot;</span><span class="s3">: [</span><span class="s1">P</span><span class="s3">, </span><span class="s1">degrees_of_freedom</span><span class="s3">, </span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_components</span><span class="s3">],</span>
            <span class="s6">&quot;n_iter_without_progress&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_EXPLORATION_MAX_ITER</span><span class="s3">,</span>
            <span class="s6">&quot;max_iter&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_EXPLORATION_MAX_ITER</span><span class="s3">,</span>
            <span class="s6">&quot;momentum&quot;</span><span class="s3">: </span><span class="s5">0.5</span><span class="s3">,</span>
        <span class="s3">}</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">method </span><span class="s3">== </span><span class="s6">&quot;barnes_hut&quot;</span><span class="s3">:</span>
            <span class="s1">obj_func </span><span class="s3">= </span><span class="s1">_kl_divergence_bh</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;kwargs&quot;</span><span class="s3">][</span><span class="s6">&quot;angle&quot;</span><span class="s3">] = </span><span class="s1">self</span><span class="s3">.</span><span class="s1">angle</span>
            <span class="s0"># Repeat verbose argument for _kl_divergence_bh</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;kwargs&quot;</span><span class="s3">][</span><span class="s6">&quot;verbose&quot;</span><span class="s3">] = </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span>
            <span class="s0"># Get the number of threads for gradient computation here to</span>
            <span class="s0"># avoid recomputing it at each iteration.</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;kwargs&quot;</span><span class="s3">][</span><span class="s6">&quot;num_threads&quot;</span><span class="s3">] = </span><span class="s1">_openmp_effective_n_threads</span><span class="s3">()</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">obj_func </span><span class="s3">= </span><span class="s1">_kl_divergence</span>

        <span class="s0"># Learning schedule (part 1): do 250 iteration with lower momentum but</span>
        <span class="s0"># higher learning rate controlled via the early exaggeration parameter</span>
        <span class="s1">P </span><span class="s3">*= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">early_exaggeration</span>
        <span class="s1">params</span><span class="s3">, </span><span class="s1">kl_divergence</span><span class="s3">, </span><span class="s1">it </span><span class="s3">= </span><span class="s1">_gradient_descent</span><span class="s3">(</span><span class="s1">obj_func</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, **</span><span class="s1">opt_args</span><span class="s3">)</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">:</span>
            <span class="s1">print</span><span class="s3">(</span>
                <span class="s6">&quot;[t-SNE] KL divergence after %d iterations with early exaggeration: %f&quot;</span>
                <span class="s3">% (</span><span class="s1">it </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">, </span><span class="s1">kl_divergence</span><span class="s3">)</span>
            <span class="s3">)</span>

        <span class="s0"># Learning schedule (part 2): disable early exaggeration and finish</span>
        <span class="s0"># optimization with a higher momentum at 0.8</span>
        <span class="s1">P </span><span class="s3">/= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">early_exaggeration</span>
        <span class="s1">remaining </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_max_iter </span><span class="s3">- </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_EXPLORATION_MAX_ITER</span>
        <span class="s2">if </span><span class="s1">it </span><span class="s3">&lt; </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_EXPLORATION_MAX_ITER </span><span class="s2">or </span><span class="s1">remaining </span><span class="s3">&gt; </span><span class="s5">0</span><span class="s3">:</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;max_iter&quot;</span><span class="s3">] = </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_max_iter</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;it&quot;</span><span class="s3">] = </span><span class="s1">it </span><span class="s3">+ </span><span class="s5">1</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;momentum&quot;</span><span class="s3">] = </span><span class="s5">0.8</span>
            <span class="s1">opt_args</span><span class="s3">[</span><span class="s6">&quot;n_iter_without_progress&quot;</span><span class="s3">] = </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter_without_progress</span>
            <span class="s1">params</span><span class="s3">, </span><span class="s1">kl_divergence</span><span class="s3">, </span><span class="s1">it </span><span class="s3">= </span><span class="s1">_gradient_descent</span><span class="s3">(</span><span class="s1">obj_func</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, **</span><span class="s1">opt_args</span><span class="s3">)</span>

        <span class="s0"># Save the final number of iterations</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter_ </span><span class="s3">= </span><span class="s1">it</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">verbose</span><span class="s3">:</span>
            <span class="s1">print</span><span class="s3">(</span>
                <span class="s6">&quot;[t-SNE] KL divergence after %d iterations: %f&quot;</span>
                <span class="s3">% (</span><span class="s1">it </span><span class="s3">+ </span><span class="s5">1</span><span class="s3">, </span><span class="s1">kl_divergence</span><span class="s3">)</span>
            <span class="s3">)</span>

        <span class="s1">X_embedded </span><span class="s3">= </span><span class="s1">params</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">n_samples</span><span class="s3">, </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_components</span><span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">kl_divergence_ </span><span class="s3">= </span><span class="s1">kl_divergence</span>

        <span class="s2">return </span><span class="s1">X_embedded</span>

    <span class="s3">@</span><span class="s1">_fit_context</span><span class="s3">(</span>
        <span class="s0"># TSNE.metric is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation</span><span class="s3">=</span><span class="s2">False</span>
    <span class="s3">)</span>
    <span class="s2">def </span><span class="s1">fit_transform</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s2">None</span><span class="s3">):</span>
        <span class="s4">&quot;&quot;&quot;Fit X into an embedded space and return that transformed output. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \ 
            (n_samples, n_samples) 
            If the metric is 'precomputed' X must be a square distance 
            matrix. Otherwise it contains a sample per row. If the method 
            is 'exact', X may be a sparse matrix of type 'csr', 'csc' 
            or 'coo'. If the method is 'barnes_hut' and the metric is 
            'precomputed', X may be a precomputed sparse graph. 
 
        y : None 
            Ignored. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_components) 
            Embedding of the training data in low-dimensional space. 
        &quot;&quot;&quot;</span>
        <span class="s0"># TODO(1.7): remove</span>
        <span class="s0"># Also make sure to change `max_iter` default back to 1000 and deprecate None</span>
        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter </span><span class="s3">!= </span><span class="s6">&quot;deprecated&quot;</span><span class="s3">:</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter </span><span class="s2">is not None</span><span class="s3">:</span>
                <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                    <span class="s6">&quot;Both 'n_iter' and 'max_iter' attributes were set. Attribute&quot;</span>
                    <span class="s6">&quot; 'n_iter' was deprecated in version 1.5 and will be removed in&quot;</span>
                    <span class="s6">&quot; 1.7. To avoid this error, only set the 'max_iter' attribute.&quot;</span>
                <span class="s3">)</span>
            <span class="s1">warnings</span><span class="s3">.</span><span class="s1">warn</span><span class="s3">(</span>
                <span class="s3">(</span>
                    <span class="s6">&quot;'n_iter' was renamed to 'max_iter' in version 1.5 and &quot;</span>
                    <span class="s6">&quot;will be removed in 1.7.&quot;</span>
                <span class="s3">),</span>
                <span class="s1">FutureWarning</span><span class="s3">,</span>
            <span class="s3">)</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">_max_iter </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">n_iter</span>
        <span class="s2">elif </span><span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter </span><span class="s2">is None</span><span class="s3">:</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">_max_iter </span><span class="s3">= </span><span class="s5">1000</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">_max_iter </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">max_iter</span>

        <span class="s1">self</span><span class="s3">.</span><span class="s1">_check_params_vs_input</span><span class="s3">(</span><span class="s1">X</span><span class="s3">)</span>
        <span class="s1">embedding </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_fit</span><span class="s3">(</span><span class="s1">X</span><span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">embedding_ </span><span class="s3">= </span><span class="s1">embedding</span>
        <span class="s2">return </span><span class="s1">self</span><span class="s3">.</span><span class="s1">embedding_</span>

    <span class="s3">@</span><span class="s1">_fit_context</span><span class="s3">(</span>
        <span class="s0"># TSNE.metric is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation</span><span class="s3">=</span><span class="s2">False</span>
    <span class="s3">)</span>
    <span class="s2">def </span><span class="s1">fit</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">=</span><span class="s2">None</span><span class="s3">):</span>
        <span class="s4">&quot;&quot;&quot;Fit X into an embedded space. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \ 
            (n_samples, n_samples) 
            If the metric is 'precomputed' X must be a square distance 
            matrix. Otherwise it contains a sample per row. If the method 
            is 'exact', X may be a sparse matrix of type 'csr', 'csc' 
            or 'coo'. If the method is 'barnes_hut' and the metric is 
            'precomputed', X may be a precomputed sparse graph. 
 
        y : None 
            Ignored. 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">fit_transform</span><span class="s3">(</span><span class="s1">X</span><span class="s3">)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s3">@</span><span class="s1">property</span>
    <span class="s2">def </span><span class="s1">_n_features_out</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s4">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self</span><span class="s3">.</span><span class="s1">embedding_</span><span class="s3">.</span><span class="s1">shape</span><span class="s3">[</span><span class="s5">1</span><span class="s3">]</span>

    <span class="s2">def </span><span class="s1">_more_tags</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span><span class="s6">&quot;pairwise&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">metric </span><span class="s3">== </span><span class="s6">&quot;precomputed&quot;</span><span class="s3">}</span>
</pre>
</body>
</html>