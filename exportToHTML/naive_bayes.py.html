<html>
<head>
<title>naive_bayes.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #6aab73;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
naive_bayes.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Naive Bayes algorithms. 
 
These are supervised learning methods based on applying Bayes' theorem with strong 
(naive) feature independence assumptions. 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Vincent Michel &lt;vincent.michel@inria.fr&gt;</span>
<span class="s2">#         Minor fixes by Fabian Pedregosa</span>
<span class="s2">#         Amit Aides &lt;amitibo@tx.technion.ac.il&gt;</span>
<span class="s2">#         Yehuda Finkelstein &lt;yehudaf@tx.technion.ac.il&gt;</span>
<span class="s2">#         Lars Buitinck</span>
<span class="s2">#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="s2">#         (parts based on earlier work by Mathieu Blondel)</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABCMeta</span><span class="s4">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">special </span><span class="s3">import </span><span class="s1">logsumexp</span>

<span class="s3">from </span><span class="s4">.</span><span class="s1">base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s4">, </span><span class="s1">ClassifierMixin</span><span class="s4">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">preprocessing </span><span class="s3">import </span><span class="s1">LabelBinarizer</span><span class="s4">, </span><span class="s1">binarize</span><span class="s4">, </span><span class="s1">label_binarize</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s1">Interval</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">extmath </span><span class="s3">import </span><span class="s1">safe_sparse_dot</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">multiclass </span><span class="s3">import </span><span class="s1">_check_partial_fit_first_call</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span><span class="s4">, </span><span class="s1">check_is_fitted</span><span class="s4">, </span><span class="s1">check_non_negative</span>

<span class="s1">__all__ </span><span class="s4">= [</span>
    <span class="s5">&quot;BernoulliNB&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;GaussianNB&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;MultinomialNB&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;ComplementNB&quot;</span><span class="s4">,</span>
    <span class="s5">&quot;CategoricalNB&quot;</span><span class="s4">,</span>
<span class="s4">]</span>


<span class="s3">class </span><span class="s1">_BaseNB</span><span class="s4">(</span><span class="s1">ClassifierMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span><span class="s4">, </span><span class="s1">metaclass</span><span class="s4">=</span><span class="s1">ABCMeta</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Abstract base class for naive Bayes estimators&quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute the unnormalized posterior log probability of X 
 
        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of 
        shape (n_samples, n_classes). 
 
        Public methods predict, predict_proba, predict_log_proba, and 
        predict_joint_log_proba pass the input through _check_X before handing it 
        over to _joint_log_likelihood. The term &quot;joint log likelihood&quot; is used 
        interchangibly with &quot;joint log probability&quot;. 
        &quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;To be overridden in subclasses with the actual checks. 
 
        Only used in predict* methods. 
        &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">predict_joint_log_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Return joint log probability estimates for the test vector X. 
 
        For each row x of X and class y, the joint log probability is given by 
        ``log P(x, y) = log P(y) + log P(x|y),`` 
        where ``log P(y)`` is the class prior probability and ``log P(x|y)`` is 
        the class-conditional probability. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples, n_classes) 
            Returns the joint log-probability of the samples for each class in 
            the model. The columns correspond to the classes in sorted 
            order, as they appear in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">predict</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Perform classification on an array of test vectors X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples,) 
            Predicted target values for X. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">jll </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">argmax</span><span class="s4">(</span><span class="s1">jll</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)]</span>

    <span class="s3">def </span><span class="s1">predict_log_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return log-probability estimates for the test vector X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        C : array-like of shape (n_samples, n_classes) 
            Returns the log-probability of the samples for each class in 
            the model. The columns correspond to the classes in sorted 
            order, as they appear in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">jll </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s2"># normalize by P(x) = P(f_1, ..., f_n)</span>
        <span class="s1">log_prob_x </span><span class="s4">= </span><span class="s1">logsumexp</span><span class="s4">(</span><span class="s1">jll</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">jll </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">log_prob_x</span><span class="s4">).</span><span class="s1">T</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return probability estimates for the test vector X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        C : array-like of shape (n_samples, n_classes) 
            Returns the probability of the samples for each class in 
            the model. The columns correspond to the classes in sorted 
            order, as they appear in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">predict_log_proba</span><span class="s4">(</span><span class="s1">X</span><span class="s4">))</span>


<span class="s3">class </span><span class="s1">GaussianNB</span><span class="s4">(</span><span class="s1">_BaseNB</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Gaussian Naive Bayes (GaussianNB). 
 
    Can perform online updates to model parameters via :meth:`partial_fit`. 
    For details on algorithm used to update feature means and variance online, 
    see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque: 
 
        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf 
 
    Read more in the :ref:`User Guide &lt;gaussian_naive_bayes&gt;`. 
 
    Parameters 
    ---------- 
    priors : array-like of shape (n_classes,), default=None 
        Prior probabilities of the classes. If specified, the priors are not 
        adjusted according to the data. 
 
    var_smoothing : float, default=1e-9 
        Portion of the largest variance of all features that is added to 
        variances for calculation stability. 
 
        .. versionadded:: 0.20 
 
    Attributes 
    ---------- 
    class_count_ : ndarray of shape (n_classes,) 
        number of training samples observed in each class. 
 
    class_prior_ : ndarray of shape (n_classes,) 
        probability of each class. 
 
    classes_ : ndarray of shape (n_classes,) 
        class labels known to the classifier. 
 
    epsilon_ : float 
        absolute additive value to variances. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    var_ : ndarray of shape (n_classes, n_features) 
        Variance of each feature per class. 
 
        .. versionadded:: 1.0 
 
    theta_ : ndarray of shape (n_classes, n_features) 
        mean of each feature per class. 
 
    See Also 
    -------- 
    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models. 
    CategoricalNB : Naive Bayes classifier for categorical features. 
    ComplementNB : Complement Naive Bayes classifier. 
    MultinomialNB : Naive Bayes classifier for multinomial models. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) 
    &gt;&gt;&gt; Y = np.array([1, 1, 1, 2, 2, 2]) 
    &gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB 
    &gt;&gt;&gt; clf = GaussianNB() 
    &gt;&gt;&gt; clf.fit(X, Y) 
    GaussianNB() 
    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]])) 
    [1] 
    &gt;&gt;&gt; clf_pf = GaussianNB() 
    &gt;&gt;&gt; clf_pf.partial_fit(X, Y, np.unique(Y)) 
    GaussianNB() 
    &gt;&gt;&gt; print(clf_pf.predict([[-0.8, -1]])) 
    [1] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s5">&quot;priors&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;var_smoothing&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, *, </span><span class="s1">priors</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">var_smoothing</span><span class="s4">=</span><span class="s6">1e-9</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">priors </span><span class="s4">= </span><span class="s1">priors</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">var_smoothing </span><span class="s4">= </span><span class="s1">var_smoothing</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Gaussian Naive Bayes according to X, y. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples 
            and `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
            .. versionadded:: 0.17 
               Gaussian Naive Bayes supports fitting with *sample_weight*. 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span><span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_partial_fit</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y</span><span class="s4">), </span><span class="s1">_refit</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Validate X, used only in predict* methods.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

    <span class="s4">@</span><span class="s1">staticmethod</span>
    <span class="s3">def </span><span class="s1">_update_mean_variance</span><span class="s4">(</span><span class="s1">n_past</span><span class="s4">, </span><span class="s1">mu</span><span class="s4">, </span><span class="s1">var</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Compute online update of Gaussian mean and variance. 
 
        Given starting sample count, mean, and variance, a new set of 
        points X, and optionally sample weights, return the updated mean and 
        variance. (NB - each dimension (column) in X is treated as independent 
        -- you get variance, not covariance). 
 
        Can take scalar mean and variance, or vector mean and variance to 
        simultaneously update a number of independent Gaussians. 
 
        See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque: 
 
        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf 
 
        Parameters 
        ---------- 
        n_past : int 
            Number of samples represented in old mean and variance. If sample 
            weights were given, this should contain the sum of sample 
            weights represented in old mean and variance. 
 
        mu : array-like of shape (number of Gaussians,) 
            Means for Gaussians in original set. 
 
        var : array-like of shape (number of Gaussians,) 
            Variances for Gaussians in original set. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
        Returns 
        ------- 
        total_mu : array-like of shape (number of Gaussians,) 
            Updated mean for each Gaussian over the combined set. 
 
        total_var : array-like of shape (number of Gaussians,) 
            Updated variance for each Gaussian over the combined set. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] == </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">mu</span><span class="s4">, </span><span class="s1">var</span>

        <span class="s2"># Compute (potentially weighted) mean and variance of new datapoints</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">n_new </span><span class="s4">= </span><span class="s1">float</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">())</span>
            <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isclose</span><span class="s4">(</span><span class="s1">n_new</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">):</span>
                <span class="s3">return </span><span class="s1">mu</span><span class="s4">, </span><span class="s1">var</span>
            <span class="s1">new_mu </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
            <span class="s1">new_var </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">((</span><span class="s1">X </span><span class="s4">- </span><span class="s1">new_mu</span><span class="s4">) ** </span><span class="s6">2</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">n_new </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>
            <span class="s1">new_var </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">var</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>
            <span class="s1">new_mu </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_past </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">new_mu</span><span class="s4">, </span><span class="s1">new_var</span>

        <span class="s1">n_total </span><span class="s4">= </span><span class="s1">float</span><span class="s4">(</span><span class="s1">n_past </span><span class="s4">+ </span><span class="s1">n_new</span><span class="s4">)</span>

        <span class="s2"># Combine mean of old and new data, taking into consideration</span>
        <span class="s2"># (weighted) number of observations</span>
        <span class="s1">total_mu </span><span class="s4">= (</span><span class="s1">n_new </span><span class="s4">* </span><span class="s1">new_mu </span><span class="s4">+ </span><span class="s1">n_past </span><span class="s4">* </span><span class="s1">mu</span><span class="s4">) / </span><span class="s1">n_total</span>

        <span class="s2"># Combine variance of old and new data, taking into consideration</span>
        <span class="s2"># (weighted) number of observations. This is achieved by combining</span>
        <span class="s2"># the sum-of-squared-differences (ssd)</span>
        <span class="s1">old_ssd </span><span class="s4">= </span><span class="s1">n_past </span><span class="s4">* </span><span class="s1">var</span>
        <span class="s1">new_ssd </span><span class="s4">= </span><span class="s1">n_new </span><span class="s4">* </span><span class="s1">new_var</span>
        <span class="s1">total_ssd </span><span class="s4">= </span><span class="s1">old_ssd </span><span class="s4">+ </span><span class="s1">new_ssd </span><span class="s4">+ (</span><span class="s1">n_new </span><span class="s4">* </span><span class="s1">n_past </span><span class="s4">/ </span><span class="s1">n_total</span><span class="s4">) * (</span><span class="s1">mu </span><span class="s4">- </span><span class="s1">new_mu</span><span class="s4">) ** </span><span class="s6">2</span>
        <span class="s1">total_var </span><span class="s4">= </span><span class="s1">total_ssd </span><span class="s4">/ </span><span class="s1">n_total</span>

        <span class="s3">return </span><span class="s1">total_mu</span><span class="s4">, </span><span class="s1">total_var</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">partial_fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Incremental fit on a batch of samples. 
 
        This method is expected to be called several times consecutively 
        on different chunks of a dataset so as to implement out-of-core 
        or online learning. 
 
        This is especially useful when the whole dataset is too big to fit in 
        memory at once. 
 
        This method has some performance and numerical stability overhead, 
        hence it is better to call partial_fit on chunks of data that are 
        as large as possible (as long as fitting in the memory budget) to 
        hide the overhead. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        classes : array-like of shape (n_classes,), default=None 
            List of all the classes that can possibly appear in the y vector. 
 
            Must be provided at the first call to partial_fit, can be omitted 
            in subsequent calls. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
            .. versionadded:: 0.17 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_partial_fit</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">, </span><span class="s1">_refit</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_partial_fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">_refit</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Actual implementation of Gaussian NB fitting. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        classes : array-like of shape (n_classes,), default=None 
            List of all the classes that can possibly appear in the y vector. 
 
            Must be provided at the first call to partial_fit, can be omitted 
            in subsequent calls. 
 
        _refit : bool, default=False 
            If true, act as though this were the first time we called 
            _partial_fit (ie, throw away any past fitting and start over). 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
        Returns 
        ------- 
        self : object 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">_refit</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_ </span><span class="s4">= </span><span class="s3">None</span>

        <span class="s1">first_call </span><span class="s4">= </span><span class="s1">_check_partial_fit_first_call</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">)</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s1">first_call</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>

        <span class="s2"># If the ratio of data variance between dimensions is too small, it</span>
        <span class="s2"># will cause numerical errors. To address this, we artificially</span>
        <span class="s2"># boost the variance by epsilon, a small fraction of the standard</span>
        <span class="s2"># deviation of the largest dimension.</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">epsilon_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">var_smoothing </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">var</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">).</span><span class="s1">max</span><span class="s4">()</span>

        <span class="s3">if </span><span class="s1">first_call</span><span class="s4">:</span>
            <span class="s2"># This is the first call to partial_fit:</span>
            <span class="s2"># initialize various cumulative counters</span>
            <span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>
            <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">theta_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">))</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">var_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">))</span>

            <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>

            <span class="s2"># Initialise the class prior</span>
            <span class="s2"># Take into account the priors</span>
            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">priors </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">priors </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">priors</span><span class="s4">)</span>
                <span class="s2"># Check that the provided prior matches the number of classes</span>
                <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">priors</span><span class="s4">) != </span><span class="s1">n_classes</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Number of priors must match number of classes.&quot;</span><span class="s4">)</span>
                <span class="s2"># Check that the sum is 1</span>
                <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isclose</span><span class="s4">(</span><span class="s1">priors</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(), </span><span class="s6">1.0</span><span class="s4">):</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;The sum of the priors should be 1.&quot;</span><span class="s4">)</span>
                <span class="s2"># Check that the priors are non-negative</span>
                <span class="s3">if </span><span class="s4">(</span><span class="s1">priors </span><span class="s4">&lt; </span><span class="s6">0</span><span class="s4">).</span><span class="s1">any</span><span class="s4">():</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Priors must be non-negative.&quot;</span><span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior_ </span><span class="s4">= </span><span class="s1">priors</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s2"># Initialize the priors to zeros for each class</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] != </span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]:</span>
                <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;Number of features %d does not match previous data %d.&quot;</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s1">msg </span><span class="s4">% (</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]))</span>
            <span class="s2"># Put epsilon back in each time</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">var_</span><span class="s4">[:, :] -= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">epsilon_</span>

        <span class="s1">classes </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span>

        <span class="s1">unique_y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
        <span class="s1">unique_y_in_classes </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isin</span><span class="s4">(</span><span class="s1">unique_y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">)</span>

        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">unique_y_in_classes</span><span class="s4">):</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;The target label(s) %s in y do not exist in the initial classes %s&quot;</span>
                <span class="s4">% (</span><span class="s1">unique_y</span><span class="s4">[~</span><span class="s1">unique_y_in_classes</span><span class="s4">], </span><span class="s1">classes</span><span class="s4">)</span>
            <span class="s4">)</span>

        <span class="s3">for </span><span class="s1">y_i </span><span class="s3">in </span><span class="s1">unique_y</span><span class="s4">:</span>
            <span class="s1">i </span><span class="s4">= </span><span class="s1">classes</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">y_i</span><span class="s4">)</span>
            <span class="s1">X_i </span><span class="s4">= </span><span class="s1">X</span><span class="s4">[</span><span class="s1">y </span><span class="s4">== </span><span class="s1">y_i</span><span class="s4">, :]</span>

            <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
                <span class="s1">sw_i </span><span class="s4">= </span><span class="s1">sample_weight</span><span class="s4">[</span><span class="s1">y </span><span class="s4">== </span><span class="s1">y_i</span><span class="s4">]</span>
                <span class="s1">N_i </span><span class="s4">= </span><span class="s1">sw_i</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">sw_i </span><span class="s4">= </span><span class="s3">None</span>
                <span class="s1">N_i </span><span class="s4">= </span><span class="s1">X_i</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>

            <span class="s1">new_theta</span><span class="s4">, </span><span class="s1">new_sigma </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_update_mean_variance</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">var_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :], </span><span class="s1">X_i</span><span class="s4">, </span><span class="s1">sw_i</span>
            <span class="s4">)</span>

            <span class="s1">self</span><span class="s4">.</span><span class="s1">theta_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :] = </span><span class="s1">new_theta</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">var_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :] = </span><span class="s1">new_sigma</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] += </span><span class="s1">N_i</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">var_</span><span class="s4">[:, :] += </span><span class="s1">self</span><span class="s4">.</span><span class="s1">epsilon_</span>

        <span class="s2"># Update if only no priors is provided</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">priors </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s2"># Empirical prior, with sample_weight taken into account</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">/ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s1">joint_log_likelihood </span><span class="s4">= []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">size</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)):</span>
            <span class="s1">jointi </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">])</span>
            <span class="s1">n_ij </span><span class="s4">= -</span><span class="s6">0.5 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s6">2.0 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">pi </span><span class="s4">* </span><span class="s1">self</span><span class="s4">.</span><span class="s1">var_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :]))</span>
            <span class="s1">n_ij </span><span class="s4">-= </span><span class="s6">0.5 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(((</span><span class="s1">X </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">theta_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :]) ** </span><span class="s6">2</span><span class="s4">) / (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">var_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">, :]), </span><span class="s6">1</span><span class="s4">)</span>
            <span class="s1">joint_log_likelihood</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">jointi </span><span class="s4">+ </span><span class="s1">n_ij</span><span class="s4">)</span>

        <span class="s1">joint_log_likelihood </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">joint_log_likelihood</span><span class="s4">).</span><span class="s1">T</span>
        <span class="s3">return </span><span class="s1">joint_log_likelihood</span>


<span class="s3">class </span><span class="s1">_BaseDiscreteNB</span><span class="s4">(</span><span class="s1">_BaseNB</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Abstract base class for naive Bayes on discrete/categorical data 
 
    Any estimator based on this class should provide: 
 
    __init__ 
    _joint_log_likelihood(X) as per _BaseNB 
    _update_feature_log_prob(alpha) 
    _count(X, Y) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s5">&quot;alpha&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">), </span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;fit_prior&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;class_prior&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;force_alpha&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">, </span><span class="s1">fit_prior</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">class_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">force_alpha</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">alpha </span><span class="s4">= </span><span class="s1">alpha</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">fit_prior </span><span class="s4">= </span><span class="s1">fit_prior</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior </span><span class="s4">= </span><span class="s1">class_prior</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">force_alpha </span><span class="s4">= </span><span class="s1">force_alpha</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">_count</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Update counts that are used to calculate probabilities. 
 
        The counts make up a sufficient statistic extracted from the data. 
        Accordingly, this method is called each time `fit` or `partial_fit` 
        update the model. `class_count_` and `feature_count_` must be updated 
        here along with any model specific counts. 
 
        Parameters 
        ---------- 
        X : {ndarray, sparse matrix} of shape (n_samples, n_features) 
            The input samples. 
        Y : ndarray of shape (n_samples, n_classes) 
            Binarized class labels. 
        &quot;&quot;&quot;</span>

    <span class="s4">@</span><span class="s1">abstractmethod</span>
    <span class="s3">def </span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Update feature log probabilities based on counts. 
 
        This method is called each time `fit` or `partial_fit` update the 
        model. 
 
        Parameters 
        ---------- 
        alpha : float 
            smoothing parameter. See :meth:`_check_alpha`. 
        &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Validate X, used only in predict* methods.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s5">&quot;csr&quot;</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_check_X_y</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Validate X and y in fit methods.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s5">&quot;csr&quot;</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s1">reset</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_update_class_log_prior</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">class_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Update class log priors. 
 
        The class log priors are based on `class_prior`, class count or the 
        number of classes. This method is called each time `fit` or 
        `partial_fit` update the model. 
        &quot;&quot;&quot;</span>
        <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">class_prior </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">class_prior</span><span class="s4">) != </span><span class="s1">n_classes</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;Number of priors must match number of classes.&quot;</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">class_prior</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">self</span><span class="s4">.</span><span class="s1">fit_prior</span><span class="s4">:</span>
            <span class="s3">with </span><span class="s1">warnings</span><span class="s4">.</span><span class="s1">catch_warnings</span><span class="s4">():</span>
                <span class="s2"># silence the warning when count is 0 because class was not yet</span>
                <span class="s2"># observed</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">simplefilter</span><span class="s4">(</span><span class="s5">&quot;ignore&quot;</span><span class="s4">, </span><span class="s1">RuntimeWarning</span><span class="s4">)</span>
                <span class="s1">log_class_count </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">)</span>

            <span class="s2"># empirical prior, with sample_weight taken into account</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_ </span><span class="s4">= </span><span class="s1">log_class_count </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">())</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">, -</span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">))</span>

    <span class="s3">def </span><span class="s1">_check_alpha</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s1">alpha </span><span class="s4">= (</span>
            <span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span><span class="s4">) </span><span class="s3">if not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span><span class="s4">, </span><span class="s1">Real</span><span class="s4">) </span><span class="s3">else </span><span class="s1">self</span><span class="s4">.</span><span class="s1">alpha</span>
        <span class="s4">)</span>
        <span class="s1">alpha_min </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">min</span><span class="s4">(</span><span class="s1">alpha</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">alpha</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ndarray</span><span class="s4">):</span>
            <span class="s3">if not </span><span class="s1">alpha</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] == </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;When alpha is an array, it should contains `n_features`. &quot;</span>
                    <span class="s5">f&quot;Got </span><span class="s3">{</span><span class="s1">alpha</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span><span class="s3">} </span><span class="s5">elements instead of </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s3">}</span><span class="s5">.&quot;</span>
                <span class="s4">)</span>
            <span class="s2"># check that all alpha are positive</span>
            <span class="s3">if </span><span class="s1">alpha_min </span><span class="s4">&lt; </span><span class="s6">0</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;All values in alpha must be greater than 0.&quot;</span><span class="s4">)</span>
        <span class="s1">alpha_lower_bound </span><span class="s4">= </span><span class="s6">1e-10</span>
        <span class="s3">if </span><span class="s1">alpha_min </span><span class="s4">&lt; </span><span class="s1">alpha_lower_bound </span><span class="s3">and not </span><span class="s1">self</span><span class="s4">.</span><span class="s1">force_alpha</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s5">&quot;alpha too small will result in numeric errors, setting alpha =&quot;</span>
                <span class="s5">f&quot; </span><span class="s3">{</span><span class="s1">alpha_lower_bound</span><span class="s3">:</span><span class="s5">.1e</span><span class="s3">}</span><span class="s5">. Use `force_alpha=True` to keep alpha&quot;</span>
                <span class="s5">&quot; unchanged.&quot;</span>
            <span class="s4">)</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">maximum</span><span class="s4">(</span><span class="s1">alpha</span><span class="s4">, </span><span class="s1">alpha_lower_bound</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">alpha</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">partial_fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Incremental fit on a batch of samples. 
 
        This method is expected to be called several times consecutively 
        on different chunks of a dataset so as to implement out-of-core 
        or online learning. 
 
        This is especially useful when the whole dataset is too big to fit in 
        memory at once. 
 
        This method has some performance overhead hence it is better to call 
        partial_fit on chunks of data that are as large as possible 
        (as long as fitting in the memory budget) to hide the overhead. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        classes : array-like of shape (n_classes,), default=None 
            List of all the classes that can possibly appear in the y vector. 
 
            Must be provided at the first call to partial_fit, can be omitted 
            in subsequent calls. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">first_call </span><span class="s4">= </span><span class="s3">not </span><span class="s1">hasattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s5">&quot;classes_&quot;</span><span class="s4">)</span>

        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_X_y</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s1">first_call</span><span class="s4">)</span>
        <span class="s1">_</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>

        <span class="s3">if </span><span class="s1">_check_partial_fit_first_call</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">):</span>
            <span class="s2"># This is the first call to partial_fit:</span>
            <span class="s2"># initialize various cumulative counters</span>
            <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">_init_counters</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">)</span>

        <span class="s1">Y </span><span class="s4">= </span><span class="s1">label_binarize</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] == </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">) == </span><span class="s6">2</span><span class="s4">:</span>
                <span class="s1">Y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">concatenate</span><span class="s4">((</span><span class="s6">1 </span><span class="s4">- </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">), </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:  </span><span class="s2"># degenerate case: just one class</span>
                <span class="s1">Y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones_like</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">)</span>

        <span class="s3">if </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] != </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]:</span>
            <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;X.shape[0]=%d and y.shape[0]=%d are incompatible.&quot;</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s1">msg </span><span class="s4">% (</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]))</span>

        <span class="s2"># label_binarize() returns arrays with dtype=np.int64.</span>
        <span class="s2"># We convert it to np.float64 to support sample_weight consistently</span>
        <span class="s1">Y </span><span class="s4">= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>
            <span class="s1">Y </span><span class="s4">*= </span><span class="s1">sample_weight</span><span class="s4">.</span><span class="s1">T</span>

        <span class="s1">class_prior </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior</span>

        <span class="s2"># Count raw events from data before updating the class log prior</span>
        <span class="s2"># and feature log probas</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_count</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">)</span>

        <span class="s2"># XXX: OPTIM: we could introduce a public finalization method to</span>
        <span class="s2"># be called by the user explicitly just once after several consecutive</span>
        <span class="s2"># calls to partial_fit and prior any call to predict[_[log_]proba]</span>
        <span class="s2"># to avoid computing the smooth log probas at each call to partial fit</span>
        <span class="s1">alpha </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_alpha</span><span class="s4">()</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">alpha</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_update_class_log_prior</span><span class="s4">(</span><span class="s1">class_prior</span><span class="s4">=</span><span class="s1">class_prior</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span><span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Naive Bayes classifier according to X, y. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_X_y</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">)</span>
        <span class="s1">_</span><span class="s4">, </span><span class="s1">n_features </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span>

        <span class="s1">labelbin </span><span class="s4">= </span><span class="s1">LabelBinarizer</span><span class="s4">()</span>
        <span class="s1">Y </span><span class="s4">= </span><span class="s1">labelbin</span><span class="s4">.</span><span class="s1">fit_transform</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_ </span><span class="s4">= </span><span class="s1">labelbin</span><span class="s4">.</span><span class="s1">classes_</span>
        <span class="s3">if </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] == </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">) == </span><span class="s6">2</span><span class="s4">:</span>
                <span class="s1">Y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">concatenate</span><span class="s4">((</span><span class="s6">1 </span><span class="s4">- </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">), </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:  </span><span class="s2"># degenerate case: just one class</span>
                <span class="s1">Y </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones_like</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">)</span>

        <span class="s2"># LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.</span>
        <span class="s2"># We convert it to np.float64 to support sample_weight consistently;</span>
        <span class="s2"># this means we also don't have to cast X to floating point</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">Y </span><span class="s4">= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">copy</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">atleast_2d</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>
            <span class="s1">Y </span><span class="s4">*= </span><span class="s1">sample_weight</span><span class="s4">.</span><span class="s1">T</span>

        <span class="s1">class_prior </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_prior</span>

        <span class="s2"># Count raw events from data before updating the class log prior</span>
        <span class="s2"># and feature log probas</span>
        <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_init_counters</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_count</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">)</span>
        <span class="s1">alpha </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_check_alpha</span><span class="s4">()</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">alpha</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_update_class_log_prior</span><span class="s4">(</span><span class="s1">class_prior</span><span class="s4">=</span><span class="s1">class_prior</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_init_counters</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;poor_score&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">}</span>


<span class="s3">class </span><span class="s1">MultinomialNB</span><span class="s4">(</span><span class="s1">_BaseDiscreteNB</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Naive Bayes classifier for multinomial models. 
 
    The multinomial Naive Bayes classifier is suitable for classification with 
    discrete features (e.g., word counts for text classification). The 
    multinomial distribution normally requires integer feature counts. However, 
    in practice, fractional counts such as tf-idf may also work. 
 
    Read more in the :ref:`User Guide &lt;multinomial_naive_bayes&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float or array-like of shape (n_features,), default=1.0 
        Additive (Laplace/Lidstone) smoothing parameter 
        (set alpha=0 and force_alpha=True, for no smoothing). 
 
    force_alpha : bool, default=True 
        If False and alpha is less than 1e-10, it will set alpha to 
        1e-10. If True, alpha will remain unchanged. This may cause 
        numerical errors if alpha is too close to 0. 
 
        .. versionadded:: 1.2 
        .. versionchanged:: 1.4 
           The default value of `force_alpha` changed to `True`. 
 
    fit_prior : bool, default=True 
        Whether to learn class prior probabilities or not. 
        If false, a uniform prior will be used. 
 
    class_prior : array-like of shape (n_classes,), default=None 
        Prior probabilities of the classes. If specified, the priors are not 
        adjusted according to the data. 
 
    Attributes 
    ---------- 
    class_count_ : ndarray of shape (n_classes,) 
        Number of samples encountered for each class during fitting. This 
        value is weighted by the sample weight when provided. 
 
    class_log_prior_ : ndarray of shape (n_classes,) 
        Smoothed empirical log probability for each class. 
 
    classes_ : ndarray of shape (n_classes,) 
        Class labels known to the classifier 
 
    feature_count_ : ndarray of shape (n_classes, n_features) 
        Number of samples encountered for each (class, feature) 
        during fitting. This value is weighted by the sample weight when 
        provided. 
 
    feature_log_prob_ : ndarray of shape (n_classes, n_features) 
        Empirical log probability of features 
        given a class, ``P(x_i|y)``. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models. 
    CategoricalNB : Naive Bayes classifier for categorical features. 
    ComplementNB : Complement Naive Bayes classifier. 
    GaussianNB : Gaussian Naive Bayes. 
 
    References 
    ---------- 
    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to 
    Information Retrieval. Cambridge University Press, pp. 234-265. 
    https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; rng = np.random.RandomState(1) 
    &gt;&gt;&gt; X = rng.randint(5, size=(6, 100)) 
    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5, 6]) 
    &gt;&gt;&gt; from sklearn.naive_bayes import MultinomialNB 
    &gt;&gt;&gt; clf = MultinomialNB() 
    &gt;&gt;&gt; clf.fit(X, y) 
    MultinomialNB() 
    &gt;&gt;&gt; print(clf.predict(X[2:3])) 
    [3] 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">, *, </span><span class="s1">alpha</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">, </span><span class="s1">force_alpha</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">fit_prior</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">class_prior</span><span class="s4">=</span><span class="s3">None</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">alpha</span><span class="s4">=</span><span class="s1">alpha</span><span class="s4">,</span>
            <span class="s1">fit_prior</span><span class="s4">=</span><span class="s1">fit_prior</span><span class="s4">,</span>
            <span class="s1">class_prior</span><span class="s4">=</span><span class="s1">class_prior</span><span class="s4">,</span>
            <span class="s1">force_alpha</span><span class="s4">=</span><span class="s1">force_alpha</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;requires_positive_X&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">}</span>

    <span class="s3">def </span><span class="s1">_count</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Count and smooth feature occurrences.&quot;&quot;&quot;</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s5">&quot;MultinomialNB (input X)&quot;</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_ </span><span class="s4">+= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">+= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Apply smoothing to raw counts and recompute log probabilities&quot;&quot;&quot;</span>
        <span class="s1">smoothed_fc </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_ </span><span class="s4">+ </span><span class="s1">alpha</span>
        <span class="s1">smoothed_cc </span><span class="s4">= </span><span class="s1">smoothed_fc</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">smoothed_fc</span><span class="s4">) - </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span>
            <span class="s1">smoothed_cc</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate the posterior log probability of the samples X&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_</span><span class="s4">.</span><span class="s1">T</span><span class="s4">) + </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_</span>


<span class="s3">class </span><span class="s1">ComplementNB</span><span class="s4">(</span><span class="s1">_BaseDiscreteNB</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;The Complement Naive Bayes classifier described in Rennie et al. (2003). 
 
    The Complement Naive Bayes classifier was designed to correct the &quot;severe 
    assumptions&quot; made by the standard Multinomial Naive Bayes classifier. It is 
    particularly suited for imbalanced data sets. 
 
    Read more in the :ref:`User Guide &lt;complement_naive_bayes&gt;`. 
 
    .. versionadded:: 0.20 
 
    Parameters 
    ---------- 
    alpha : float or array-like of shape (n_features,), default=1.0 
        Additive (Laplace/Lidstone) smoothing parameter 
        (set alpha=0 and force_alpha=True, for no smoothing). 
 
    force_alpha : bool, default=True 
        If False and alpha is less than 1e-10, it will set alpha to 
        1e-10. If True, alpha will remain unchanged. This may cause 
        numerical errors if alpha is too close to 0. 
 
        .. versionadded:: 1.2 
        .. versionchanged:: 1.4 
           The default value of `force_alpha` changed to `True`. 
 
    fit_prior : bool, default=True 
        Only used in edge case with a single class in the training set. 
 
    class_prior : array-like of shape (n_classes,), default=None 
        Prior probabilities of the classes. Not used. 
 
    norm : bool, default=False 
        Whether or not a second normalization of the weights is performed. The 
        default behavior mirrors the implementations found in Mahout and Weka, 
        which do not follow the full algorithm described in Table 9 of the 
        paper. 
 
    Attributes 
    ---------- 
    class_count_ : ndarray of shape (n_classes,) 
        Number of samples encountered for each class during fitting. This 
        value is weighted by the sample weight when provided. 
 
    class_log_prior_ : ndarray of shape (n_classes,) 
        Smoothed empirical log probability for each class. Only used in edge 
        case with a single class in the training set. 
 
    classes_ : ndarray of shape (n_classes,) 
        Class labels known to the classifier 
 
    feature_all_ : ndarray of shape (n_features,) 
        Number of samples encountered for each feature during fitting. This 
        value is weighted by the sample weight when provided. 
 
    feature_count_ : ndarray of shape (n_classes, n_features) 
        Number of samples encountered for each (class, feature) during fitting. 
        This value is weighted by the sample weight when provided. 
 
    feature_log_prob_ : ndarray of shape (n_classes, n_features) 
        Empirical weights for class complements. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models. 
    CategoricalNB : Naive Bayes classifier for categorical features. 
    GaussianNB : Gaussian Naive Bayes. 
    MultinomialNB : Naive Bayes classifier for multinomial models. 
 
    References 
    ---------- 
    Rennie, J. D., Shih, L., Teevan, J., &amp; Karger, D. R. (2003). 
    Tackling the poor assumptions of naive bayes text classifiers. In ICML 
    (Vol. 3, pp. 616-623). 
    https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; rng = np.random.RandomState(1) 
    &gt;&gt;&gt; X = rng.randint(5, size=(6, 100)) 
    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5, 6]) 
    &gt;&gt;&gt; from sklearn.naive_bayes import ComplementNB 
    &gt;&gt;&gt; clf = ComplementNB() 
    &gt;&gt;&gt; clf.fit(X, y) 
    ComplementNB() 
    &gt;&gt;&gt; print(clf.predict(X[2:3])) 
    [3] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">_BaseDiscreteNB</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s5">&quot;norm&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">alpha</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">,</span>
        <span class="s1">force_alpha</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">fit_prior</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">class_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">norm</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">alpha</span><span class="s4">=</span><span class="s1">alpha</span><span class="s4">,</span>
            <span class="s1">force_alpha</span><span class="s4">=</span><span class="s1">force_alpha</span><span class="s4">,</span>
            <span class="s1">fit_prior</span><span class="s4">=</span><span class="s1">fit_prior</span><span class="s4">,</span>
            <span class="s1">class_prior</span><span class="s4">=</span><span class="s1">class_prior</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">norm </span><span class="s4">= </span><span class="s1">norm</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;requires_positive_X&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">}</span>

    <span class="s3">def </span><span class="s1">_count</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Count feature occurrences.&quot;&quot;&quot;</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s5">&quot;ComplementNB (input X)&quot;</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_ </span><span class="s4">+= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">+= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_all_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Apply smoothing to raw counts and compute the weights.&quot;&quot;&quot;</span>
        <span class="s1">comp_count </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_all_ </span><span class="s4">+ </span><span class="s1">alpha </span><span class="s4">- </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_</span>
        <span class="s1">logged </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">comp_count </span><span class="s4">/ </span><span class="s1">comp_count</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">keepdims</span><span class="s4">=</span><span class="s3">True</span><span class="s4">))</span>
        <span class="s2"># _BaseNB.predict uses argmax, but ComplementNB operates with argmin.</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">norm</span><span class="s4">:</span>
            <span class="s1">summed </span><span class="s4">= </span><span class="s1">logged</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">keepdims</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
            <span class="s1">feature_log_prob </span><span class="s4">= </span><span class="s1">logged </span><span class="s4">/ </span><span class="s1">summed</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">feature_log_prob </span><span class="s4">= -</span><span class="s1">logged</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_ </span><span class="s4">= </span><span class="s1">feature_log_prob</span>

    <span class="s3">def </span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate the class scores for the samples in X.&quot;&quot;&quot;</span>
        <span class="s1">jll </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">) == </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s1">jll </span><span class="s4">+= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_</span>
        <span class="s3">return </span><span class="s1">jll</span>


<span class="s3">class </span><span class="s1">BernoulliNB</span><span class="s4">(</span><span class="s1">_BaseDiscreteNB</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Naive Bayes classifier for multivariate Bernoulli models. 
 
    Like MultinomialNB, this classifier is suitable for discrete data. The 
    difference is that while MultinomialNB works with occurrence counts, 
    BernoulliNB is designed for binary/boolean features. 
 
    Read more in the :ref:`User Guide &lt;bernoulli_naive_bayes&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float or array-like of shape (n_features,), default=1.0 
        Additive (Laplace/Lidstone) smoothing parameter 
        (set alpha=0 and force_alpha=True, for no smoothing). 
 
    force_alpha : bool, default=True 
        If False and alpha is less than 1e-10, it will set alpha to 
        1e-10. If True, alpha will remain unchanged. This may cause 
        numerical errors if alpha is too close to 0. 
 
        .. versionadded:: 1.2 
        .. versionchanged:: 1.4 
           The default value of `force_alpha` changed to `True`. 
 
    binarize : float or None, default=0.0 
        Threshold for binarizing (mapping to booleans) of sample features. 
        If None, input is presumed to already consist of binary vectors. 
 
    fit_prior : bool, default=True 
        Whether to learn class prior probabilities or not. 
        If false, a uniform prior will be used. 
 
    class_prior : array-like of shape (n_classes,), default=None 
        Prior probabilities of the classes. If specified, the priors are not 
        adjusted according to the data. 
 
    Attributes 
    ---------- 
    class_count_ : ndarray of shape (n_classes,) 
        Number of samples encountered for each class during fitting. This 
        value is weighted by the sample weight when provided. 
 
    class_log_prior_ : ndarray of shape (n_classes,) 
        Log probability of each class (smoothed). 
 
    classes_ : ndarray of shape (n_classes,) 
        Class labels known to the classifier 
 
    feature_count_ : ndarray of shape (n_classes, n_features) 
        Number of samples encountered for each (class, feature) 
        during fitting. This value is weighted by the sample weight when 
        provided. 
 
    feature_log_prob_ : ndarray of shape (n_classes, n_features) 
        Empirical log probability of features given a class, P(x_i|y). 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    CategoricalNB : Naive Bayes classifier for categorical features. 
    ComplementNB : The Complement Naive Bayes classifier 
        described in Rennie et al. (2003). 
    GaussianNB : Gaussian Naive Bayes (GaussianNB). 
    MultinomialNB : Naive Bayes classifier for multinomial models. 
 
    References 
    ---------- 
    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to 
    Information Retrieval. Cambridge University Press, pp. 234-265. 
    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html 
 
    A. McCallum and K. Nigam (1998). A comparison of event models for naive 
    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for 
    Text Categorization, pp. 41-48. 
 
    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with 
    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; rng = np.random.RandomState(1) 
    &gt;&gt;&gt; X = rng.randint(5, size=(6, 100)) 
    &gt;&gt;&gt; Y = np.array([1, 2, 3, 4, 4, 5]) 
    &gt;&gt;&gt; from sklearn.naive_bayes import BernoulliNB 
    &gt;&gt;&gt; clf = BernoulliNB() 
    &gt;&gt;&gt; clf.fit(X, Y) 
    BernoulliNB() 
    &gt;&gt;&gt; print(clf.predict(X[2:3])) 
    [3] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">_BaseDiscreteNB</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s5">&quot;binarize&quot;</span><span class="s4">: [</span><span class="s3">None</span><span class="s4">, </span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">alpha</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">,</span>
        <span class="s1">force_alpha</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">binarize</span><span class="s4">=</span><span class="s6">0.0</span><span class="s4">,</span>
        <span class="s1">fit_prior</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">class_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">alpha</span><span class="s4">=</span><span class="s1">alpha</span><span class="s4">,</span>
            <span class="s1">fit_prior</span><span class="s4">=</span><span class="s1">fit_prior</span><span class="s4">,</span>
            <span class="s1">class_prior</span><span class="s4">=</span><span class="s1">class_prior</span><span class="s4">,</span>
            <span class="s1">force_alpha</span><span class="s4">=</span><span class="s1">force_alpha</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">binarize </span><span class="s4">= </span><span class="s1">binarize</span>

    <span class="s3">def </span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Validate X, used only in predict* methods.&quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">super</span><span class="s4">().</span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">binarize </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">X </span><span class="s4">= </span><span class="s1">binarize</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">threshold</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">binarize</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">X</span>

    <span class="s3">def </span><span class="s1">_check_X_y</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">super</span><span class="s4">().</span><span class="s1">_check_X_y</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s1">reset</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">binarize </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">X </span><span class="s4">= </span><span class="s1">binarize</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">threshold</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">binarize</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span>

    <span class="s3">def </span><span class="s1">_count</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Count and smooth feature occurrences.&quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_ </span><span class="s4">+= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">Y</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">+= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Apply smoothing to raw counts and recompute log probabilities&quot;&quot;&quot;</span>
        <span class="s1">smoothed_fc </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_count_ </span><span class="s4">+ </span><span class="s1">alpha</span>
        <span class="s1">smoothed_cc </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">+ </span><span class="s1">alpha </span><span class="s4">* </span><span class="s6">2</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">smoothed_fc</span><span class="s4">) - </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span>
            <span class="s1">smoothed_cc</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate the posterior log probability of the samples X&quot;&quot;&quot;</span>
        <span class="s1">n_features </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>
        <span class="s1">n_features_X </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>

        <span class="s3">if </span><span class="s1">n_features_X </span><span class="s4">!= </span><span class="s1">n_features</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Expected input with %d features, got %d instead&quot;</span>
                <span class="s4">% (</span><span class="s1">n_features</span><span class="s4">, </span><span class="s1">n_features_X</span><span class="s4">)</span>
            <span class="s4">)</span>

        <span class="s1">neg_prob </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s6">1 </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">exp</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_</span><span class="s4">))</span>
        <span class="s2"># Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob</span>
        <span class="s1">jll </span><span class="s4">= </span><span class="s1">safe_sparse_dot</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, (</span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_ </span><span class="s4">- </span><span class="s1">neg_prob</span><span class="s4">).</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s1">jll </span><span class="s4">+= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_ </span><span class="s4">+ </span><span class="s1">neg_prob</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">jll</span>


<span class="s3">class </span><span class="s1">CategoricalNB</span><span class="s4">(</span><span class="s1">_BaseDiscreteNB</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Naive Bayes classifier for categorical features. 
 
    The categorical Naive Bayes classifier is suitable for classification with 
    discrete features that are categorically distributed. The categories of 
    each feature are drawn from a categorical distribution. 
 
    Read more in the :ref:`User Guide &lt;categorical_naive_bayes&gt;`. 
 
    Parameters 
    ---------- 
    alpha : float, default=1.0 
        Additive (Laplace/Lidstone) smoothing parameter 
        (set alpha=0 and force_alpha=True, for no smoothing). 
 
    force_alpha : bool, default=True 
        If False and alpha is less than 1e-10, it will set alpha to 
        1e-10. If True, alpha will remain unchanged. This may cause 
        numerical errors if alpha is too close to 0. 
 
        .. versionadded:: 1.2 
        .. versionchanged:: 1.4 
           The default value of `force_alpha` changed to `True`. 
 
    fit_prior : bool, default=True 
        Whether to learn class prior probabilities or not. 
        If false, a uniform prior will be used. 
 
    class_prior : array-like of shape (n_classes,), default=None 
        Prior probabilities of the classes. If specified, the priors are not 
        adjusted according to the data. 
 
    min_categories : int or array-like of shape (n_features,), default=None 
        Minimum number of categories per feature. 
 
        - integer: Sets the minimum number of categories per feature to 
          `n_categories` for each features. 
        - array-like: shape (n_features,) where `n_categories[i]` holds the 
          minimum number of categories for the ith column of the input. 
        - None (default): Determines the number of categories automatically 
          from the training data. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    category_count_ : list of arrays of shape (n_features,) 
        Holds arrays of shape (n_classes, n_categories of respective feature) 
        for each feature. Each array provides the number of samples 
        encountered for each class and category of the specific feature. 
 
    class_count_ : ndarray of shape (n_classes,) 
        Number of samples encountered for each class during fitting. This 
        value is weighted by the sample weight when provided. 
 
    class_log_prior_ : ndarray of shape (n_classes,) 
        Smoothed empirical log probability for each class. 
 
    classes_ : ndarray of shape (n_classes,) 
        Class labels known to the classifier 
 
    feature_log_prob_ : list of arrays of shape (n_features,) 
        Holds arrays of shape (n_classes, n_categories of respective feature) 
        for each feature. Each array provides the empirical log probability 
        of categories given the respective feature and class, ``P(x_i|y)``. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_categories_ : ndarray of shape (n_features,), dtype=np.int64 
        Number of categories for each feature. This value is 
        inferred from the data or set by the minimum number of categories. 
 
        .. versionadded:: 0.24 
 
    See Also 
    -------- 
    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models. 
    ComplementNB : Complement Naive Bayes classifier. 
    GaussianNB : Gaussian Naive Bayes. 
    MultinomialNB : Naive Bayes classifier for multinomial models. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; rng = np.random.RandomState(1) 
    &gt;&gt;&gt; X = rng.randint(5, size=(6, 100)) 
    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5, 6]) 
    &gt;&gt;&gt; from sklearn.naive_bayes import CategoricalNB 
    &gt;&gt;&gt; clf = CategoricalNB() 
    &gt;&gt;&gt; clf.fit(X, y) 
    CategoricalNB() 
    &gt;&gt;&gt; print(clf.predict(X[2:3])) 
    [3] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s4">**</span><span class="s1">_BaseDiscreteNB</span><span class="s4">.</span><span class="s1">_parameter_constraints</span><span class="s4">,</span>
        <span class="s5">&quot;min_categories&quot;</span><span class="s4">: [</span>
            <span class="s3">None</span><span class="s4">,</span>
            <span class="s5">&quot;array-like&quot;</span><span class="s4">,</span>
            <span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">),</span>
        <span class="s4">],</span>
        <span class="s5">&quot;alpha&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">alpha</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">,</span>
        <span class="s1">force_alpha</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">fit_prior</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">class_prior</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">min_categories</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">super</span><span class="s4">().</span><span class="s1">__init__</span><span class="s4">(</span>
            <span class="s1">alpha</span><span class="s4">=</span><span class="s1">alpha</span><span class="s4">,</span>
            <span class="s1">force_alpha</span><span class="s4">=</span><span class="s1">force_alpha</span><span class="s4">,</span>
            <span class="s1">fit_prior</span><span class="s4">=</span><span class="s1">fit_prior</span><span class="s4">,</span>
            <span class="s1">class_prior</span><span class="s4">=</span><span class="s1">class_prior</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">min_categories </span><span class="s4">= </span><span class="s1">min_categories</span>

    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fit Naive Bayes classifier according to X, y. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. Here, each feature of X is 
            assumed to be from a different categorical distribution. 
            It is further assumed that all categories of each feature are 
            represented by the numbers 0, ..., n - 1, where n refers to the 
            total number of categories for the given feature. This can, for 
            instance, be achieved with the help of OrdinalEncoder. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">super</span><span class="s4">().</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">partial_fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Incremental fit on a batch of samples. 
 
        This method is expected to be called several times consecutively 
        on different chunks of a dataset so as to implement out-of-core 
        or online learning. 
 
        This is especially useful when the whole dataset is too big to fit in 
        memory at once. 
 
        This method has some performance overhead hence it is better to call 
        partial_fit on chunks of data that are as large as possible 
        (as long as fitting in the memory budget) to hide the overhead. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vectors, where `n_samples` is the number of samples and 
            `n_features` is the number of features. Here, each feature of X is 
            assumed to be from a different categorical distribution. 
            It is further assumed that all categories of each feature are 
            represented by the numbers 0, ..., n - 1, where n refers to the 
            total number of categories for the given feature. This can, for 
            instance, be achieved with the help of OrdinalEncoder. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        classes : array-like of shape (n_classes,), default=None 
            List of all the classes that can possibly appear in the y vector. 
 
            Must be provided at the first call to partial_fit, can be omitted 
            in subsequent calls. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Weights applied to individual samples (1. for unweighted). 
 
        Returns 
        ------- 
        self : object 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">super</span><span class="s4">().</span><span class="s1">partial_fit</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span><span class="s5">&quot;requires_positive_X&quot;</span><span class="s4">: </span><span class="s3">True</span><span class="s4">}</span>

    <span class="s3">def </span><span class="s1">_check_X</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Validate X, used only in predict* methods.&quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s5">&quot;int&quot;</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">force_all_finite</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span>
        <span class="s4">)</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s5">&quot;CategoricalNB (input X)&quot;</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">X</span>

    <span class="s3">def </span><span class="s1">_check_X_y</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">True</span><span class="s4">):</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_data</span><span class="s4">(</span>
            <span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s5">&quot;int&quot;</span><span class="s4">, </span><span class="s1">accept_sparse</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">force_all_finite</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s1">reset</span>
        <span class="s4">)</span>
        <span class="s1">check_non_negative</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s5">&quot;CategoricalNB (input X)&quot;</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span>

    <span class="s3">def </span><span class="s1">_init_counters</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_features</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">category_count_ </span><span class="s4">= [</span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s6">0</span><span class="s4">)) </span><span class="s3">for </span><span class="s1">_ </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">n_features</span><span class="s4">)]</span>

    <span class="s4">@</span><span class="s1">staticmethod</span>
    <span class="s3">def </span><span class="s1">_validate_n_categories</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">min_categories</span><span class="s4">):</span>
        <span class="s2"># rely on max for n_categories categories are encoded between 0...n-1</span>
        <span class="s1">n_categories_X </span><span class="s4">= </span><span class="s1">X</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">) + </span><span class="s6">1</span>
        <span class="s1">min_categories_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">min_categories</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">min_categories </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">issubdtype</span><span class="s4">(</span><span class="s1">min_categories_</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">signedinteger</span><span class="s4">):</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;'min_categories' should have integral type. Got &quot;</span>
                    <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">min_categories_</span><span class="s4">.</span><span class="s1">dtype</span><span class="s3">} </span><span class="s5">instead.&quot;</span>
                <span class="s4">)</span>
            <span class="s1">n_categories_ </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">maximum</span><span class="s4">(</span><span class="s1">n_categories_X</span><span class="s4">, </span><span class="s1">min_categories_</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">int64</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">n_categories_</span><span class="s4">.</span><span class="s1">shape </span><span class="s4">!= </span><span class="s1">n_categories_X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">f&quot;'min_categories' should have shape (</span><span class="s3">{</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span><span class="s3">}</span><span class="s5">,&quot;</span>
                    <span class="s5">&quot;) when an array-like is provided. Got&quot;</span>
                    <span class="s5">f&quot; </span><span class="s3">{</span><span class="s1">min_categories_</span><span class="s4">.</span><span class="s1">shape</span><span class="s3">} </span><span class="s5">instead.&quot;</span>
                <span class="s4">)</span>
            <span class="s3">return </span><span class="s1">n_categories_</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">n_categories_X</span>

    <span class="s3">def </span><span class="s1">_count</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">):</span>
        <span class="s3">def </span><span class="s1">_update_cat_count_dims</span><span class="s4">(</span><span class="s1">cat_count</span><span class="s4">, </span><span class="s1">highest_feature</span><span class="s4">):</span>
            <span class="s1">diff </span><span class="s4">= </span><span class="s1">highest_feature </span><span class="s4">+ </span><span class="s6">1 </span><span class="s4">- </span><span class="s1">cat_count</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>
            <span class="s3">if </span><span class="s1">diff </span><span class="s4">&gt; </span><span class="s6">0</span><span class="s4">:</span>
                <span class="s2"># we append a column full of zeros for each new category</span>
                <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">pad</span><span class="s4">(</span><span class="s1">cat_count</span><span class="s4">, [(</span><span class="s6">0</span><span class="s4">, </span><span class="s6">0</span><span class="s4">), (</span><span class="s6">0</span><span class="s4">, </span><span class="s1">diff</span><span class="s4">)], </span><span class="s5">&quot;constant&quot;</span><span class="s4">)</span>
            <span class="s3">return </span><span class="s1">cat_count</span>

        <span class="s3">def </span><span class="s1">_update_cat_count</span><span class="s4">(</span><span class="s1">X_feature</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">cat_count</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">):</span>
            <span class="s3">for </span><span class="s1">j </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">):</span>
                <span class="s1">mask </span><span class="s4">= </span><span class="s1">Y</span><span class="s4">[:, </span><span class="s1">j</span><span class="s4">].</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">bool</span><span class="s4">)</span>
                <span class="s3">if </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">.</span><span class="s1">type </span><span class="s4">== </span><span class="s1">np</span><span class="s4">.</span><span class="s1">int64</span><span class="s4">:</span>
                    <span class="s1">weights </span><span class="s4">= </span><span class="s3">None</span>
                <span class="s3">else</span><span class="s4">:</span>
                    <span class="s1">weights </span><span class="s4">= </span><span class="s1">Y</span><span class="s4">[</span><span class="s1">mask</span><span class="s4">, </span><span class="s1">j</span><span class="s4">]</span>
                <span class="s1">counts </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">X_feature</span><span class="s4">[</span><span class="s1">mask</span><span class="s4">], </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">weights</span><span class="s4">)</span>
                <span class="s1">indices </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nonzero</span><span class="s4">(</span><span class="s1">counts</span><span class="s4">)[</span><span class="s6">0</span><span class="s4">]</span>
                <span class="s1">cat_count</span><span class="s4">[</span><span class="s1">j</span><span class="s4">, </span><span class="s1">indices</span><span class="s4">] += </span><span class="s1">counts</span><span class="s4">[</span><span class="s1">indices</span><span class="s4">]</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_ </span><span class="s4">+= </span><span class="s1">Y</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_categories_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_n_categories</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">min_categories</span><span class="s4">)</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">):</span>
            <span class="s1">X_feature </span><span class="s4">= </span><span class="s1">X</span><span class="s4">[:, </span><span class="s1">i</span><span class="s4">]</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">category_count_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] = </span><span class="s1">_update_cat_count_dims</span><span class="s4">(</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">category_count_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_categories_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] - </span><span class="s6">1</span>
            <span class="s4">)</span>
            <span class="s1">_update_cat_count</span><span class="s4">(</span>
                <span class="s1">X_feature</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">category_count_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>
            <span class="s4">)</span>

    <span class="s3">def </span><span class="s1">_update_feature_log_prob</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">alpha</span><span class="s4">):</span>
        <span class="s1">feature_log_prob </span><span class="s4">= []</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">):</span>
            <span class="s1">smoothed_cat_count </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">category_count_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">] + </span><span class="s1">alpha</span>
            <span class="s1">smoothed_class_count </span><span class="s4">= </span><span class="s1">smoothed_cat_count</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
            <span class="s1">feature_log_prob</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span>
                <span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">smoothed_cat_count</span><span class="s4">) - </span><span class="s1">np</span><span class="s4">.</span><span class="s1">log</span><span class="s4">(</span><span class="s1">smoothed_class_count</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">))</span>
            <span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_ </span><span class="s4">= </span><span class="s1">feature_log_prob</span>

    <span class="s3">def </span><span class="s1">_joint_log_likelihood</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">_check_n_features</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">reset</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
        <span class="s1">jll </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">X</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_count_</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]))</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_</span><span class="s4">):</span>
            <span class="s1">indices </span><span class="s4">= </span><span class="s1">X</span><span class="s4">[:, </span><span class="s1">i</span><span class="s4">]</span>
            <span class="s1">jll </span><span class="s4">+= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">feature_log_prob_</span><span class="s4">[</span><span class="s1">i</span><span class="s4">][:, </span><span class="s1">indices</span><span class="s4">].</span><span class="s1">T</span>
        <span class="s1">total_ll </span><span class="s4">= </span><span class="s1">jll </span><span class="s4">+ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">class_log_prior_</span>
        <span class="s3">return </span><span class="s1">total_ll</span>
</pre>
</body>
</html>