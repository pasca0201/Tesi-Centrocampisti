<html>
<head>
<title>calibration.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #6aab73;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
calibration.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Methods for calibrating predicted probabilities.&quot;&quot;&quot;</span>

<span class="s2"># Author: Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="s2">#         Balazs Kegl &lt;balazs.kegl@gmail.com&gt;</span>
<span class="s2">#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="s2">#         Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">inspect </span><span class="s3">import </span><span class="s1">signature</span>
<span class="s3">from </span><span class="s1">math </span><span class="s3">import </span><span class="s1">log</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">optimize </span><span class="s3">import </span><span class="s1">minimize</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">special </span><span class="s3">import </span><span class="s1">expit</span>

<span class="s3">from </span><span class="s1">sklearn</span><span class="s4">.</span><span class="s1">utils </span><span class="s3">import </span><span class="s1">Bunch</span>

<span class="s3">from </span><span class="s4">.</span><span class="s1">_loss </span><span class="s3">import </span><span class="s1">HalfBinomialLoss</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">base </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">BaseEstimator</span><span class="s4">,</span>
    <span class="s1">ClassifierMixin</span><span class="s4">,</span>
    <span class="s1">MetaEstimatorMixin</span><span class="s4">,</span>
    <span class="s1">RegressorMixin</span><span class="s4">,</span>
    <span class="s1">_fit_context</span><span class="s4">,</span>
    <span class="s1">clone</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">isotonic </span><span class="s3">import </span><span class="s1">IsotonicRegression</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">model_selection </span><span class="s3">import </span><span class="s1">LeaveOneOut</span><span class="s4">, </span><span class="s1">check_cv</span><span class="s4">, </span><span class="s1">cross_val_predict</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">preprocessing </span><span class="s3">import </span><span class="s1">LabelEncoder</span><span class="s4">, </span><span class="s1">label_binarize</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">svm </span><span class="s3">import </span><span class="s1">LinearSVC</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_safe_indexing</span><span class="s4">,</span>
    <span class="s1">column_or_1d</span><span class="s4">,</span>
    <span class="s1">indexable</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">HasMethods</span><span class="s4">,</span>
    <span class="s1">Interval</span><span class="s4">,</span>
    <span class="s1">StrOptions</span><span class="s4">,</span>
    <span class="s1">validate_params</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_plotting </span><span class="s3">import </span><span class="s1">_BinaryClassifierCurveDisplayMixin</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_response </span><span class="s3">import </span><span class="s1">_get_response_values</span><span class="s4">, </span><span class="s1">_process_predict_proba</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">metadata_routing </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">MetadataRouter</span><span class="s4">,</span>
    <span class="s1">MethodMapping</span><span class="s4">,</span>
    <span class="s1">_routing_enabled</span><span class="s4">,</span>
    <span class="s1">process_routing</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s4">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s4">.</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_check_method_params</span><span class="s4">,</span>
    <span class="s1">_check_pos_label_consistency</span><span class="s4">,</span>
    <span class="s1">_check_response_method</span><span class="s4">,</span>
    <span class="s1">_check_sample_weight</span><span class="s4">,</span>
    <span class="s1">_num_samples</span><span class="s4">,</span>
    <span class="s1">check_consistent_length</span><span class="s4">,</span>
    <span class="s1">check_is_fitted</span><span class="s4">,</span>
<span class="s4">)</span>


<span class="s3">class </span><span class="s1">CalibratedClassifierCV</span><span class="s4">(</span><span class="s1">ClassifierMixin</span><span class="s4">, </span><span class="s1">MetaEstimatorMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Probability calibration with isotonic regression or logistic regression. 
 
    This class uses cross-validation to both estimate the parameters of a 
    classifier and subsequently calibrate a classifier. With default 
    `ensemble=True`, for each cv split it 
    fits a copy of the base estimator to the training subset, and calibrates it 
    using the testing subset. For prediction, predicted probabilities are 
    averaged across these individual calibrated classifiers. When 
    `ensemble=False`, cross-validation is used to obtain unbiased predictions, 
    via :func:`~sklearn.model_selection.cross_val_predict`, which are then 
    used for calibration. For prediction, the base estimator, trained using all 
    the data, is used. This is the prediction method implemented when 
    `probabilities=True` for :class:`~sklearn.svm.SVC` and :class:`~sklearn.svm.NuSVC` 
    estimators (see :ref:`User Guide &lt;scores_probabilities&gt;` for details). 
 
    Already fitted classifiers can be calibrated via the parameter 
    `cv=&quot;prefit&quot;`. In this case, no cross-validation is used and all provided 
    data is used for calibration. The user has to take care manually that data 
    for model fitting and calibration are disjoint. 
 
    The calibration is based on the :term:`decision_function` method of the 
    `estimator` if it exists, else on :term:`predict_proba`. 
 
    Read more in the :ref:`User Guide &lt;calibration&gt;`. 
    In order to learn more on the CalibratedClassifierCV class, see the 
    following calibration examples: 
    :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`, 
    :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`, and 
    :ref:`sphx_glr_auto_examples_calibration_plot_calibration_multiclass.py`. 
 
    Parameters 
    ---------- 
    estimator : estimator instance, default=None 
        The classifier whose output need to be calibrated to provide more 
        accurate `predict_proba` outputs. The default classifier is 
        a :class:`~sklearn.svm.LinearSVC`. 
 
        .. versionadded:: 1.2 
 
    method : {'sigmoid', 'isotonic'}, default='sigmoid' 
        The method to use for calibration. Can be 'sigmoid' which 
        corresponds to Platt's method (i.e. a logistic regression model) or 
        'isotonic' which is a non-parametric approach. It is not advised to 
        use isotonic calibration with too few calibration samples 
        ``(&lt;&lt;1000)`` since it tends to overfit. 
 
    cv : int, cross-validation generator, iterable or &quot;prefit&quot;, \ 
            default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - integer, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For integer/None inputs, if ``y`` is binary or multiclass, 
        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is 
        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold` 
        is used. 
 
        Refer to the :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        If &quot;prefit&quot; is passed, it is assumed that `estimator` has been 
        fitted already and all data is used for calibration. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    n_jobs : int, default=None 
        Number of jobs to run in parallel. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. 
 
        Base estimator clones are fitted in parallel across cross-validation 
        iterations. Therefore parallelism happens only when `cv != &quot;prefit&quot;`. 
 
        See :term:`Glossary &lt;n_jobs&gt;` for more details. 
 
        .. versionadded:: 0.24 
 
    ensemble : bool, default=True 
        Determines how the calibrator is fitted when `cv` is not `'prefit'`. 
        Ignored if `cv='prefit'`. 
 
        If `True`, the `estimator` is fitted using training data, and 
        calibrated using testing data, for each `cv` fold. The final estimator 
        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where 
        `n_cv` is the number of cross-validation folds. The output is the 
        average predicted probabilities of all pairs. 
 
        If `False`, `cv` is used to compute unbiased predictions, via 
        :func:`~sklearn.model_selection.cross_val_predict`, which are then 
        used for calibration. At prediction time, the classifier used is the 
        `estimator` trained on all the data. 
        Note that this method is also internally implemented  in 
        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter. 
 
        .. versionadded:: 0.24 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes,) 
        The class labels. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. Only defined if the 
        underlying estimator exposes such an attribute when fit. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Only defined if the 
        underlying estimator exposes such an attribute when fit. 
 
        .. versionadded:: 1.0 
 
    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv=&quot;prefit&quot;` \ 
            or `ensemble=False`) 
        The list of classifier and calibrator pairs. 
 
        - When `cv=&quot;prefit&quot;`, the fitted `estimator` and fitted 
          calibrator. 
        - When `cv` is not &quot;prefit&quot; and `ensemble=True`, `n_cv` fitted 
          `estimator` and calibrator pairs. `n_cv` is the number of 
          cross-validation folds. 
        - When `cv` is not &quot;prefit&quot; and `ensemble=False`, the `estimator`, 
          fitted on all the data, and fitted calibrator. 
 
        .. versionchanged:: 0.24 
            Single calibrated classifier case when `ensemble=False`. 
 
    See Also 
    -------- 
    calibration_curve : Compute true and predicted probabilities 
        for a calibration curve. 
 
    References 
    ---------- 
    .. [1] Obtaining calibrated probability estimates from decision trees 
           and naive Bayesian classifiers, B. Zadrozny &amp; C. Elkan, ICML 2001 
 
    .. [2] Transforming Classifier Scores into Accurate Multiclass 
           Probability Estimates, B. Zadrozny &amp; C. Elkan, (KDD 2002) 
 
    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to 
           Regularized Likelihood Methods, J. Platt, (1999) 
 
    .. [4] Predicting Good Probabilities with Supervised Learning, 
           A. Niculescu-Mizil &amp; R. Caruana, ICML 2005 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_classification 
    &gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB 
    &gt;&gt;&gt; from sklearn.calibration import CalibratedClassifierCV 
    &gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=2, 
    ...                            n_redundant=0, random_state=42) 
    &gt;&gt;&gt; base_clf = GaussianNB() 
    &gt;&gt;&gt; calibrated_clf = CalibratedClassifierCV(base_clf, cv=3) 
    &gt;&gt;&gt; calibrated_clf.fit(X, y) 
    CalibratedClassifierCV(...) 
    &gt;&gt;&gt; len(calibrated_clf.calibrated_classifiers_) 
    3 
    &gt;&gt;&gt; calibrated_clf.predict_proba(X)[:5, :] 
    array([[0.110..., 0.889...], 
           [0.072..., 0.927...], 
           [0.928..., 0.071...], 
           [0.928..., 0.071...], 
           [0.071..., 0.928...]]) 
    &gt;&gt;&gt; from sklearn.model_selection import train_test_split 
    &gt;&gt;&gt; X, y = make_classification(n_samples=100, n_features=2, 
    ...                            n_redundant=0, random_state=42) 
    &gt;&gt;&gt; X_train, X_calib, y_train, y_calib = train_test_split( 
    ...        X, y, random_state=42 
    ... ) 
    &gt;&gt;&gt; base_clf = GaussianNB() 
    &gt;&gt;&gt; base_clf.fit(X_train, y_train) 
    GaussianNB() 
    &gt;&gt;&gt; calibrated_clf = CalibratedClassifierCV(base_clf, cv=&quot;prefit&quot;) 
    &gt;&gt;&gt; calibrated_clf.fit(X_calib, y_calib) 
    CalibratedClassifierCV(...) 
    &gt;&gt;&gt; len(calibrated_clf.calibrated_classifiers_) 
    1 
    &gt;&gt;&gt; calibrated_clf.predict_proba([[-0.5, 0.5]]) 
    array([[0.936..., 0.063...]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints</span><span class="s4">: </span><span class="s1">dict </span><span class="s4">= {</span>
        <span class="s5">&quot;estimator&quot;</span><span class="s4">: [</span>
            <span class="s1">HasMethods</span><span class="s4">([</span><span class="s5">&quot;fit&quot;</span><span class="s4">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">]),</span>
            <span class="s1">HasMethods</span><span class="s4">([</span><span class="s5">&quot;fit&quot;</span><span class="s4">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s4">]),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;method&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;isotonic&quot;</span><span class="s4">, </span><span class="s5">&quot;sigmoid&quot;</span><span class="s4">})],</span>
        <span class="s5">&quot;cv&quot;</span><span class="s4">: [</span><span class="s5">&quot;cv_object&quot;</span><span class="s4">, </span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;prefit&quot;</span><span class="s4">})],</span>
        <span class="s5">&quot;n_jobs&quot;</span><span class="s4">: [</span><span class="s1">Integral</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;ensemble&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">}</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">,</span>
        <span class="s1">estimator</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">method</span><span class="s4">=</span><span class="s5">&quot;sigmoid&quot;</span><span class="s4">,</span>
        <span class="s1">cv</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">n_jobs</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ensemble</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">estimator </span><span class="s4">= </span><span class="s1">estimator</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">method </span><span class="s4">= </span><span class="s1">method</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">cv </span><span class="s4">= </span><span class="s1">cv</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs </span><span class="s4">= </span><span class="s1">n_jobs</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ensemble </span><span class="s4">= </span><span class="s1">ensemble</span>

    <span class="s3">def </span><span class="s1">_get_estimator</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Resolve which estimator to return (default is LinearSVC)&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">estimator </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s2"># we want all classifiers that don't expose a random_state</span>
            <span class="s2"># to be deterministic (and we don't want to expose this one).</span>
            <span class="s1">estimator </span><span class="s4">= </span><span class="s1">LinearSVC</span><span class="s4">(</span><span class="s1">random_state</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">_routing_enabled</span><span class="s4">():</span>
                <span class="s1">estimator</span><span class="s4">.</span><span class="s1">set_fit_request</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">estimator </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">estimator</span>

        <span class="s3">return </span><span class="s1">estimator</span>

    <span class="s4">@</span><span class="s1">_fit_context</span><span class="s4">(</span>
        <span class="s2"># CalibratedClassifierCV.estimator is not validated yet</span>
        <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">False</span>
    <span class="s4">)</span>
    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, **</span><span class="s1">fit_params</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the calibrated model. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. 
 
        **fit_params : dict 
            Parameters to pass to the `fit` method of the underlying 
            classifier. 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of self. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_classification_targets</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">indexable</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">X</span><span class="s4">)</span>

        <span class="s1">estimator </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_get_estimator</span><span class="s4">()</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_ </span><span class="s4">= []</span>
        <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv </span><span class="s4">== </span><span class="s5">&quot;prefit&quot;</span><span class="s4">:</span>
            <span class="s2"># `classes_` should be consistent with that of estimator</span>
            <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">, </span><span class="s1">attributes</span><span class="s4">=[</span><span class="s5">&quot;classes_&quot;</span><span class="s4">])</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">classes_</span>

            <span class="s1">predictions</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">_get_response_values</span><span class="s4">(</span>
                <span class="s1">estimator</span><span class="s4">,</span>
                <span class="s1">X</span><span class="s4">,</span>
                <span class="s1">response_method</span><span class="s4">=[</span><span class="s5">&quot;decision_function&quot;</span><span class="s4">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">],</span>
            <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
                <span class="s2"># Reshape binary output from `(n_samples,)` to `(n_samples, 1)`</span>
                <span class="s1">predictions </span><span class="s4">= </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>

            <span class="s1">calibrated_classifier </span><span class="s4">= </span><span class="s1">_fit_calibrator</span><span class="s4">(</span>
                <span class="s1">estimator</span><span class="s4">,</span>
                <span class="s1">predictions</span><span class="s4">,</span>
                <span class="s1">y</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">,</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">method</span><span class="s4">,</span>
                <span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">calibrated_classifier</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># Set `classes_` using all `y`</span>
            <span class="s1">label_encoder_ </span><span class="s4">= </span><span class="s1">LabelEncoder</span><span class="s4">().</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_ </span><span class="s4">= </span><span class="s1">label_encoder_</span><span class="s4">.</span><span class="s1">classes_</span>

            <span class="s3">if </span><span class="s1">_routing_enabled</span><span class="s4">():</span>
                <span class="s1">routed_params </span><span class="s4">= </span><span class="s1">process_routing</span><span class="s4">(</span>
                    <span class="s1">self</span><span class="s4">,</span>
                    <span class="s5">&quot;fit&quot;</span><span class="s4">,</span>
                    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
                    <span class="s4">**</span><span class="s1">fit_params</span><span class="s4">,</span>
                <span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s2"># sample_weight checks</span>
                <span class="s1">fit_parameters </span><span class="s4">= </span><span class="s1">signature</span><span class="s4">(</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">).</span><span class="s1">parameters</span>
                <span class="s1">supports_sw </span><span class="s4">= </span><span class="s5">&quot;sample_weight&quot; </span><span class="s3">in </span><span class="s1">fit_parameters</span>
                <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None and not </span><span class="s1">supports_sw</span><span class="s4">:</span>
                    <span class="s1">estimator_name </span><span class="s4">= </span><span class="s1">type</span><span class="s4">(</span><span class="s1">estimator</span><span class="s4">).</span><span class="s1">__name__</span>
                    <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                        <span class="s5">f&quot;Since </span><span class="s3">{</span><span class="s1">estimator_name</span><span class="s3">} </span><span class="s5">does not appear to accept&quot;</span>
                        <span class="s5">&quot; sample_weight, sample weights will only be used for the&quot;</span>
                        <span class="s5">&quot; calibration itself. This can be caused by a limitation of&quot;</span>
                        <span class="s5">&quot; the current scikit-learn API. See the following issue for&quot;</span>
                        <span class="s5">&quot; more details:&quot;</span>
                        <span class="s5">&quot; https://github.com/scikit-learn/scikit-learn/issues/21134.&quot;</span>
                        <span class="s5">&quot; Be warned that the result of the calibration is likely to be&quot;</span>
                        <span class="s5">&quot; incorrect.&quot;</span>
                    <span class="s4">)</span>
                <span class="s1">routed_params </span><span class="s4">= </span><span class="s1">Bunch</span><span class="s4">()</span>
                <span class="s1">routed_params</span><span class="s4">.</span><span class="s1">splitter </span><span class="s4">= </span><span class="s1">Bunch</span><span class="s4">(</span><span class="s1">split</span><span class="s4">={})  </span><span class="s2"># no routing for splitter</span>
                <span class="s1">routed_params</span><span class="s4">.</span><span class="s1">estimator </span><span class="s4">= </span><span class="s1">Bunch</span><span class="s4">(</span><span class="s1">fit</span><span class="s4">=</span><span class="s1">fit_params</span><span class="s4">)</span>
                <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None and </span><span class="s1">supports_sw</span><span class="s4">:</span>
                    <span class="s1">routed_params</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">[</span><span class="s5">&quot;sample_weight&quot;</span><span class="s4">] = </span><span class="s1">sample_weight</span>

            <span class="s2"># Check that each cross-validation fold can have at least one</span>
            <span class="s2"># example per class</span>
            <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span><span class="s4">, </span><span class="s1">int</span><span class="s4">):</span>
                <span class="s1">n_folds </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span>
            <span class="s3">elif </span><span class="s1">hasattr</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span><span class="s4">, </span><span class="s5">&quot;n_splits&quot;</span><span class="s4">):</span>
                <span class="s1">n_folds </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span><span class="s4">.</span><span class="s1">n_splits</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">n_folds </span><span class="s4">= </span><span class="s3">None</span>
            <span class="s3">if </span><span class="s1">n_folds </span><span class="s3">and </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">return_counts</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)[</span><span class="s6">1</span><span class="s4">] &lt; </span><span class="s1">n_folds</span><span class="s4">):</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">f&quot;Requesting </span><span class="s3">{</span><span class="s1">n_folds</span><span class="s3">}</span><span class="s5">-fold &quot;</span>
                    <span class="s5">&quot;cross-validation but provided less than &quot;</span>
                    <span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">n_folds</span><span class="s3">} </span><span class="s5">examples for at least one class.&quot;</span>
                <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span><span class="s4">, </span><span class="s1">LeaveOneOut</span><span class="s4">):</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;LeaveOneOut cross-validation does not allow&quot;</span>
                    <span class="s5">&quot;all classes to be present in test splits. &quot;</span>
                    <span class="s5">&quot;Please use a cross-validation generator that allows &quot;</span>
                    <span class="s5">&quot;all classes to appear in every test and train split.&quot;</span>
                <span class="s4">)</span>
            <span class="s1">cv </span><span class="s4">= </span><span class="s1">check_cv</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classifier</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>

            <span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">ensemble</span><span class="s4">:</span>
                <span class="s1">parallel </span><span class="s4">= </span><span class="s1">Parallel</span><span class="s4">(</span><span class="s1">n_jobs</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs</span><span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_ </span><span class="s4">= </span><span class="s1">parallel</span><span class="s4">(</span>
                    <span class="s1">delayed</span><span class="s4">(</span><span class="s1">_fit_classifier_calibrator_pair</span><span class="s4">)(</span>
                        <span class="s1">clone</span><span class="s4">(</span><span class="s1">estimator</span><span class="s4">),</span>
                        <span class="s1">X</span><span class="s4">,</span>
                        <span class="s1">y</span><span class="s4">,</span>
                        <span class="s1">train</span><span class="s4">=</span><span class="s1">train</span><span class="s4">,</span>
                        <span class="s1">test</span><span class="s4">=</span><span class="s1">test</span><span class="s4">,</span>
                        <span class="s1">method</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">method</span><span class="s4">,</span>
                        <span class="s1">classes</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">,</span>
                        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
                        <span class="s1">fit_params</span><span class="s4">=</span><span class="s1">routed_params</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">,</span>
                    <span class="s4">)</span>
                    <span class="s3">for </span><span class="s1">train</span><span class="s4">, </span><span class="s1">test </span><span class="s3">in </span><span class="s1">cv</span><span class="s4">.</span><span class="s1">split</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, **</span><span class="s1">routed_params</span><span class="s4">.</span><span class="s1">splitter</span><span class="s4">.</span><span class="s1">split</span><span class="s4">)</span>
                <span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">this_estimator </span><span class="s4">= </span><span class="s1">clone</span><span class="s4">(</span><span class="s1">estimator</span><span class="s4">)</span>
                <span class="s1">method_name </span><span class="s4">= </span><span class="s1">_check_response_method</span><span class="s4">(</span>
                    <span class="s1">this_estimator</span><span class="s4">,</span>
                    <span class="s4">[</span><span class="s5">&quot;decision_function&quot;</span><span class="s4">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">],</span>
                <span class="s4">).</span><span class="s1">__name__</span>
                <span class="s1">predictions </span><span class="s4">= </span><span class="s1">cross_val_predict</span><span class="s4">(</span>
                    <span class="s1">estimator</span><span class="s4">=</span><span class="s1">this_estimator</span><span class="s4">,</span>
                    <span class="s1">X</span><span class="s4">=</span><span class="s1">X</span><span class="s4">,</span>
                    <span class="s1">y</span><span class="s4">=</span><span class="s1">y</span><span class="s4">,</span>
                    <span class="s1">cv</span><span class="s4">=</span><span class="s1">cv</span><span class="s4">,</span>
                    <span class="s1">method</span><span class="s4">=</span><span class="s1">method_name</span><span class="s4">,</span>
                    <span class="s1">n_jobs</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">n_jobs</span><span class="s4">,</span>
                    <span class="s1">params</span><span class="s4">=</span><span class="s1">routed_params</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">,</span>
                <span class="s4">)</span>
                <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">) == </span><span class="s6">2</span><span class="s4">:</span>
                    <span class="s2"># Ensure shape (n_samples, 1) in the binary case</span>
                    <span class="s3">if </span><span class="s1">method_name </span><span class="s4">== </span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">:</span>
                        <span class="s2"># Select the probability column of the postive class</span>
                        <span class="s1">predictions </span><span class="s4">= </span><span class="s1">_process_predict_proba</span><span class="s4">(</span>
                            <span class="s1">y_pred</span><span class="s4">=</span><span class="s1">predictions</span><span class="s4">,</span>
                            <span class="s1">target_type</span><span class="s4">=</span><span class="s5">&quot;binary&quot;</span><span class="s4">,</span>
                            <span class="s1">classes</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">,</span>
                            <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s6">1</span><span class="s4">],</span>
                        <span class="s4">)</span>
                    <span class="s1">predictions </span><span class="s4">= </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>

                <span class="s1">this_estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, **</span><span class="s1">routed_params</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">)</span>
                <span class="s2"># Note: Here we don't pass on fit_params because the supported</span>
                <span class="s2"># calibrators don't support fit_params anyway</span>
                <span class="s1">calibrated_classifier </span><span class="s4">= </span><span class="s1">_fit_calibrator</span><span class="s4">(</span>
                    <span class="s1">this_estimator</span><span class="s4">,</span>
                    <span class="s1">predictions</span><span class="s4">,</span>
                    <span class="s1">y</span><span class="s4">,</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">,</span>
                    <span class="s1">self</span><span class="s4">.</span><span class="s1">method</span><span class="s4">,</span>
                    <span class="s1">sample_weight</span><span class="s4">,</span>
                <span class="s4">)</span>
                <span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">calibrated_classifier</span><span class="s4">)</span>

        <span class="s1">first_clf </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">].</span><span class="s1">estimator</span>
        <span class="s3">if </span><span class="s1">hasattr</span><span class="s4">(</span><span class="s1">first_clf</span><span class="s4">, </span><span class="s5">&quot;n_features_in_&quot;</span><span class="s4">):</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">n_features_in_ </span><span class="s4">= </span><span class="s1">first_clf</span><span class="s4">.</span><span class="s1">n_features_in_</span>
        <span class="s3">if </span><span class="s1">hasattr</span><span class="s4">(</span><span class="s1">first_clf</span><span class="s4">, </span><span class="s5">&quot;feature_names_in_&quot;</span><span class="s4">):</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">feature_names_in_ </span><span class="s4">= </span><span class="s1">first_clf</span><span class="s4">.</span><span class="s1">feature_names_in_</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calibrated probabilities of classification. 
 
        This function returns calibrated probabilities of classification 
        according to each class on an array of test vectors X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The samples, as accepted by `estimator.predict_proba`. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples, n_classes) 
            The predicted probas. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s2"># Compute the arithmetic mean of the predictions of the calibrated</span>
        <span class="s2"># classifiers</span>
        <span class="s1">mean_proba </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)))</span>
        <span class="s3">for </span><span class="s1">calibrated_classifier </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_</span><span class="s4">:</span>
            <span class="s1">proba </span><span class="s4">= </span><span class="s1">calibrated_classifier</span><span class="s4">.</span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
            <span class="s1">mean_proba </span><span class="s4">+= </span><span class="s1">proba</span>

        <span class="s1">mean_proba </span><span class="s4">/= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">calibrated_classifiers_</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">mean_proba</span>

    <span class="s3">def </span><span class="s1">predict</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict the target of new samples. 
 
        The predicted class is the class that has the highest probability, 
        and can thus be different from the prediction of the uncalibrated classifier. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The samples, as accepted by `estimator.predict`. 
 
        Returns 
        ------- 
        C : ndarray of shape (n_samples,) 
            The predicted class. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">argmax</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)]</span>

    <span class="s3">def </span><span class="s1">get_metadata_routing</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Get metadata routing of this object. 
 
        Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing 
        mechanism works. 
 
        Returns 
        ------- 
        routing : MetadataRouter 
            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating 
            routing information. 
        &quot;&quot;&quot;</span>
        <span class="s1">router </span><span class="s4">= (</span>
            <span class="s1">MetadataRouter</span><span class="s4">(</span><span class="s1">owner</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">__class__</span><span class="s4">.</span><span class="s1">__name__</span><span class="s4">)</span>
            <span class="s4">.</span><span class="s1">add_self_request</span><span class="s4">(</span><span class="s1">self</span><span class="s4">)</span>
            <span class="s4">.</span><span class="s1">add</span><span class="s4">(</span>
                <span class="s1">estimator</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">_get_estimator</span><span class="s4">(),</span>
                <span class="s1">method_mapping</span><span class="s4">=</span><span class="s1">MethodMapping</span><span class="s4">().</span><span class="s1">add</span><span class="s4">(</span><span class="s1">caller</span><span class="s4">=</span><span class="s5">&quot;fit&quot;</span><span class="s4">, </span><span class="s1">callee</span><span class="s4">=</span><span class="s5">&quot;fit&quot;</span><span class="s4">),</span>
            <span class="s4">)</span>
            <span class="s4">.</span><span class="s1">add</span><span class="s4">(</span>
                <span class="s1">splitter</span><span class="s4">=</span><span class="s1">self</span><span class="s4">.</span><span class="s1">cv</span><span class="s4">,</span>
                <span class="s1">method_mapping</span><span class="s4">=</span><span class="s1">MethodMapping</span><span class="s4">().</span><span class="s1">add</span><span class="s4">(</span><span class="s1">caller</span><span class="s4">=</span><span class="s5">&quot;fit&quot;</span><span class="s4">, </span><span class="s1">callee</span><span class="s4">=</span><span class="s5">&quot;split&quot;</span><span class="s4">),</span>
            <span class="s4">)</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">router</span>

    <span class="s3">def </span><span class="s1">_more_tags</span><span class="s4">(</span><span class="s1">self</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s4">{</span>
            <span class="s5">&quot;_xfail_checks&quot;</span><span class="s4">: {</span>
                <span class="s5">&quot;check_sample_weights_invariance&quot;</span><span class="s4">: (</span>
                    <span class="s5">&quot;Due to the cross-validation and sample ordering, removing a sample&quot;</span>
                    <span class="s5">&quot; is not strictly equal to putting is weight to zero. Specific unit&quot;</span>
                    <span class="s5">&quot; tests are added for CalibratedClassifierCV specifically.&quot;</span>
                <span class="s4">),</span>
            <span class="s4">}</span>
        <span class="s4">}</span>


<span class="s3">def </span><span class="s1">_fit_classifier_calibrator_pair</span><span class="s4">(</span>
    <span class="s1">estimator</span><span class="s4">,</span>
    <span class="s1">X</span><span class="s4">,</span>
    <span class="s1">y</span><span class="s4">,</span>
    <span class="s1">train</span><span class="s4">,</span>
    <span class="s1">test</span><span class="s4">,</span>
    <span class="s1">method</span><span class="s4">,</span>
    <span class="s1">classes</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">fit_params</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Fit a classifier/calibration pair on a given train/test split. 
 
    Fit the classifier on the train set, compute its predictions on the test 
    set and use the predictions as input to fit the calibrator along with the 
    test labels. 
 
    Parameters 
    ---------- 
    estimator : estimator instance 
        Cloned base estimator. 
 
    X : array-like, shape (n_samples, n_features) 
        Sample data. 
 
    y : array-like, shape (n_samples,) 
        Targets. 
 
    train : ndarray, shape (n_train_indices,) 
        Indices of the training subset. 
 
    test : ndarray, shape (n_test_indices,) 
        Indices of the testing subset. 
 
    method : {'sigmoid', 'isotonic'} 
        Method to use for calibration. 
 
    classes : ndarray, shape (n_classes,) 
        The target classes. 
 
    sample_weight : array-like, default=None 
        Sample weights for `X`. 
 
    fit_params : dict, default=None 
        Parameters to pass to the `fit` method of the underlying 
        classifier. 
 
    Returns 
    ------- 
    calibrated_classifier : _CalibratedClassifier instance 
    &quot;&quot;&quot;</span>
    <span class="s1">fit_params_train </span><span class="s4">= </span><span class="s1">_check_method_params</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">params</span><span class="s4">=</span><span class="s1">fit_params</span><span class="s4">, </span><span class="s1">indices</span><span class="s4">=</span><span class="s1">train</span><span class="s4">)</span>
    <span class="s1">X_train</span><span class="s4">, </span><span class="s1">y_train </span><span class="s4">= </span><span class="s1">_safe_indexing</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">train</span><span class="s4">), </span><span class="s1">_safe_indexing</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">train</span><span class="s4">)</span>
    <span class="s1">X_test</span><span class="s4">, </span><span class="s1">y_test </span><span class="s4">= </span><span class="s1">_safe_indexing</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">test</span><span class="s4">), </span><span class="s1">_safe_indexing</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">test</span><span class="s4">)</span>

    <span class="s1">estimator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">X_train</span><span class="s4">, </span><span class="s1">y_train</span><span class="s4">, **</span><span class="s1">fit_params_train</span><span class="s4">)</span>

    <span class="s1">predictions</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">_get_response_values</span><span class="s4">(</span>
        <span class="s1">estimator</span><span class="s4">,</span>
        <span class="s1">X_test</span><span class="s4">,</span>
        <span class="s1">response_method</span><span class="s4">=[</span><span class="s5">&quot;decision_function&quot;</span><span class="s4">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">],</span>
    <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s2"># Reshape binary output from `(n_samples,)` to `(n_samples, 1)`</span>
        <span class="s1">predictions </span><span class="s4">= </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>

    <span class="s1">sw_test </span><span class="s4">= </span><span class="s3">None if </span><span class="s1">sample_weight </span><span class="s3">is None else </span><span class="s1">_safe_indexing</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">test</span><span class="s4">)</span>
    <span class="s1">calibrated_classifier </span><span class="s4">= </span><span class="s1">_fit_calibrator</span><span class="s4">(</span>
        <span class="s1">estimator</span><span class="s4">, </span><span class="s1">predictions</span><span class="s4">, </span><span class="s1">y_test</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">, </span><span class="s1">method</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sw_test</span>
    <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">calibrated_classifier</span>


<span class="s3">def </span><span class="s1">_fit_calibrator</span><span class="s4">(</span><span class="s1">clf</span><span class="s4">, </span><span class="s1">predictions</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">, </span><span class="s1">method</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Fit calibrator(s) and return a `_CalibratedClassifier` 
    instance. 
 
    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted. 
    However, if `n_classes` equals 2, one calibrator is fitted. 
 
    Parameters 
    ---------- 
    clf : estimator instance 
        Fitted classifier. 
 
    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \ 
                    when binary. 
        Raw predictions returned by the un-calibrated base classifier. 
 
    y : array-like, shape (n_samples,) 
        The targets. 
 
    classes : ndarray, shape (n_classes,) 
        All the prediction classes. 
 
    method : {'sigmoid', 'isotonic'} 
        The method to use for calibration. 
 
    sample_weight : ndarray, shape (n_samples,), default=None 
        Sample weights. If None, then samples are equally weighted. 
 
    Returns 
    ------- 
    pipeline : _CalibratedClassifier instance 
    &quot;&quot;&quot;</span>
    <span class="s1">Y </span><span class="s4">= </span><span class="s1">label_binarize</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s1">classes</span><span class="s4">)</span>
    <span class="s1">label_encoder </span><span class="s4">= </span><span class="s1">LabelEncoder</span><span class="s4">().</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">classes</span><span class="s4">)</span>
    <span class="s1">pos_class_indices </span><span class="s4">= </span><span class="s1">label_encoder</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">clf</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>
    <span class="s1">calibrators </span><span class="s4">= []</span>
    <span class="s3">for </span><span class="s1">class_idx</span><span class="s4">, </span><span class="s1">this_pred </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">pos_class_indices</span><span class="s4">, </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">T</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">method </span><span class="s4">== </span><span class="s5">&quot;isotonic&quot;</span><span class="s4">:</span>
            <span class="s1">calibrator </span><span class="s4">= </span><span class="s1">IsotonicRegression</span><span class="s4">(</span><span class="s1">out_of_bounds</span><span class="s4">=</span><span class="s5">&quot;clip&quot;</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:  </span><span class="s2"># &quot;sigmoid&quot;</span>
            <span class="s1">calibrator </span><span class="s4">= </span><span class="s1">_SigmoidCalibration</span><span class="s4">()</span>
        <span class="s1">calibrator</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">this_pred</span><span class="s4">, </span><span class="s1">Y</span><span class="s4">[:, </span><span class="s1">class_idx</span><span class="s4">], </span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s1">calibrators</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s1">calibrator</span><span class="s4">)</span>

    <span class="s1">pipeline </span><span class="s4">= </span><span class="s1">_CalibratedClassifier</span><span class="s4">(</span><span class="s1">clf</span><span class="s4">, </span><span class="s1">calibrators</span><span class="s4">, </span><span class="s1">method</span><span class="s4">=</span><span class="s1">method</span><span class="s4">, </span><span class="s1">classes</span><span class="s4">=</span><span class="s1">classes</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">pipeline</span>


<span class="s3">class </span><span class="s1">_CalibratedClassifier</span><span class="s4">:</span>
    <span class="s0">&quot;&quot;&quot;Pipeline-like chaining a fitted classifier and its fitted calibrators. 
 
    Parameters 
    ---------- 
    estimator : estimator instance 
        Fitted classifier. 
 
    calibrators : list of fitted estimator instances 
        List of fitted calibrators (either 'IsotonicRegression' or 
        '_SigmoidCalibration'). The number of calibrators equals the number of 
        classes. However, if there are 2 classes, the list contains only one 
        fitted calibrator. 
 
    classes : array-like of shape (n_classes,) 
        All the prediction classes. 
 
    method : {'sigmoid', 'isotonic'}, default='sigmoid' 
        The method to use for calibration. Can be 'sigmoid' which 
        corresponds to Platt's method or 'isotonic' which is a 
        non-parametric approach based on isotonic regression. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">estimator</span><span class="s4">, </span><span class="s1">calibrators</span><span class="s4">, *, </span><span class="s1">classes</span><span class="s4">, </span><span class="s1">method</span><span class="s4">=</span><span class="s5">&quot;sigmoid&quot;</span><span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">estimator </span><span class="s4">= </span><span class="s1">estimator</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">calibrators </span><span class="s4">= </span><span class="s1">calibrators</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">classes </span><span class="s4">= </span><span class="s1">classes</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">method </span><span class="s4">= </span><span class="s1">method</span>

    <span class="s3">def </span><span class="s1">predict_proba</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Calculate calibrated probabilities. 
 
        Calculates classification calibrated probabilities 
        for each class, in a one-vs-all manner, for `X`. 
 
        Parameters 
        ---------- 
        X : ndarray of shape (n_samples, n_features) 
            The sample data. 
 
        Returns 
        ------- 
        proba : array, shape (n_samples, n_classes) 
            The predicted probabilities. Can be exact zeros. 
        &quot;&quot;&quot;</span>
        <span class="s1">predictions</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">_get_response_values</span><span class="s4">(</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">,</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">response_method</span><span class="s4">=[</span><span class="s5">&quot;decision_function&quot;</span><span class="s4">, </span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">],</span>
        <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s2"># Reshape binary output from `(n_samples,)` to `(n_samples, 1)`</span>
            <span class="s1">predictions </span><span class="s4">= </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">)</span>

        <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes</span><span class="s4">)</span>

        <span class="s1">label_encoder </span><span class="s4">= </span><span class="s1">LabelEncoder</span><span class="s4">().</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">classes</span><span class="s4">)</span>
        <span class="s1">pos_class_indices </span><span class="s4">= </span><span class="s1">label_encoder</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">estimator</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>

        <span class="s1">proba </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">X</span><span class="s4">), </span><span class="s1">n_classes</span><span class="s4">))</span>
        <span class="s3">for </span><span class="s1">class_idx</span><span class="s4">, </span><span class="s1">this_pred</span><span class="s4">, </span><span class="s1">calibrator </span><span class="s3">in </span><span class="s1">zip</span><span class="s4">(</span>
            <span class="s1">pos_class_indices</span><span class="s4">, </span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">T</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">calibrators</span>
        <span class="s4">):</span>
            <span class="s3">if </span><span class="s1">n_classes </span><span class="s4">== </span><span class="s6">2</span><span class="s4">:</span>
                <span class="s2"># When binary, `predictions` consists only of predictions for</span>
                <span class="s2"># clf.classes_[1] but `pos_class_indices` = 0</span>
                <span class="s1">class_idx </span><span class="s4">+= </span><span class="s6">1</span>
            <span class="s1">proba</span><span class="s4">[:, </span><span class="s1">class_idx</span><span class="s4">] = </span><span class="s1">calibrator</span><span class="s4">.</span><span class="s1">predict</span><span class="s4">(</span><span class="s1">this_pred</span><span class="s4">)</span>

        <span class="s2"># Normalize the probabilities</span>
        <span class="s3">if </span><span class="s1">n_classes </span><span class="s4">== </span><span class="s6">2</span><span class="s4">:</span>
            <span class="s1">proba</span><span class="s4">[:, </span><span class="s6">0</span><span class="s4">] = </span><span class="s6">1.0 </span><span class="s4">- </span><span class="s1">proba</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">]</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">proba</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
            <span class="s2"># In the edge case where for each class calibrator returns a null</span>
            <span class="s2"># probability for a given sample, use the uniform distribution</span>
            <span class="s2"># instead.</span>
            <span class="s1">uniform_proba </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">full_like</span><span class="s4">(</span><span class="s1">proba</span><span class="s4">, </span><span class="s6">1 </span><span class="s4">/ </span><span class="s1">n_classes</span><span class="s4">)</span>
            <span class="s1">proba </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">divide</span><span class="s4">(</span>
                <span class="s1">proba</span><span class="s4">, </span><span class="s1">denominator</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">uniform_proba</span><span class="s4">, </span><span class="s1">where</span><span class="s4">=</span><span class="s1">denominator </span><span class="s4">!= </span><span class="s6">0</span>
            <span class="s4">)</span>

        <span class="s2"># Deal with cases where the predicted probability minimally exceeds 1.0</span>
        <span class="s1">proba</span><span class="s4">[(</span><span class="s6">1.0 </span><span class="s4">&lt; </span><span class="s1">proba</span><span class="s4">) &amp; (</span><span class="s1">proba </span><span class="s4">&lt;= </span><span class="s6">1.0 </span><span class="s4">+ </span><span class="s6">1e-5</span><span class="s4">)] = </span><span class="s6">1.0</span>

        <span class="s3">return </span><span class="s1">proba</span>


<span class="s2"># The max_abs_prediction_threshold was approximated using</span>
<span class="s2"># logit(np.finfo(np.float64).eps) which is about -36</span>
<span class="s3">def </span><span class="s1">_sigmoid_calibration</span><span class="s4">(</span>
    <span class="s1">predictions</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">max_abs_prediction_threshold</span><span class="s4">=</span><span class="s6">30</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Probability Calibration with sigmoid method (Platt 2000) 
 
    Parameters 
    ---------- 
    predictions : ndarray of shape (n_samples,) 
        The decision function or predict proba for the samples. 
 
    y : ndarray of shape (n_samples,) 
        The targets. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. If None, then samples are equally weighted. 
 
    Returns 
    ------- 
    a : float 
        The slope. 
 
    b : float 
        The intercept. 
 
    References 
    ---------- 
    Platt, &quot;Probabilistic Outputs for Support Vector Machines&quot; 
    &quot;&quot;&quot;</span>
    <span class="s1">predictions </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">predictions</span><span class="s4">)</span>
    <span class="s1">y </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>

    <span class="s1">F </span><span class="s4">= </span><span class="s1">predictions  </span><span class="s2"># F follows Platt's notations</span>

    <span class="s1">scale_constant </span><span class="s4">= </span><span class="s6">1.0</span>
    <span class="s1">max_prediction </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">F</span><span class="s4">))</span>

    <span class="s2"># If the predictions have large values we scale them in order to bring</span>
    <span class="s2"># them within a suitable range. This has no effect on the final</span>
    <span class="s2"># (prediction) result because linear models like Logisitic Regression</span>
    <span class="s2"># without a penalty are invariant to multiplying the features by a</span>
    <span class="s2"># constant.</span>
    <span class="s3">if </span><span class="s1">max_prediction </span><span class="s4">&gt;= </span><span class="s1">max_abs_prediction_threshold</span><span class="s4">:</span>
        <span class="s1">scale_constant </span><span class="s4">= </span><span class="s1">max_prediction</span>
        <span class="s2"># We rescale the features in a copy: inplace rescaling could confuse</span>
        <span class="s2"># the caller and make the code harder to reason about.</span>
        <span class="s1">F </span><span class="s4">= </span><span class="s1">F </span><span class="s4">/ </span><span class="s1">scale_constant</span>

    <span class="s2"># Bayesian priors (see Platt end of section 2.2):</span>
    <span class="s2"># It corresponds to the number of samples, taking into account the</span>
    <span class="s2"># `sample_weight`.</span>
    <span class="s1">mask_negative_samples </span><span class="s4">= </span><span class="s1">y </span><span class="s4">&lt;= </span><span class="s6">0</span>
    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">prior0 </span><span class="s4">= (</span><span class="s1">sample_weight</span><span class="s4">[</span><span class="s1">mask_negative_samples</span><span class="s4">]).</span><span class="s1">sum</span><span class="s4">()</span>
        <span class="s1">prior1 </span><span class="s4">= (</span><span class="s1">sample_weight</span><span class="s4">[~</span><span class="s1">mask_negative_samples</span><span class="s4">]).</span><span class="s1">sum</span><span class="s4">()</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">prior0 </span><span class="s4">= </span><span class="s1">float</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">mask_negative_samples</span><span class="s4">))</span>
        <span class="s1">prior1 </span><span class="s4">= </span><span class="s1">y</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] - </span><span class="s1">prior0</span>
    <span class="s1">T </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros_like</span><span class="s4">(</span><span class="s1">y</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
    <span class="s1">T</span><span class="s4">[</span><span class="s1">y </span><span class="s4">&gt; </span><span class="s6">0</span><span class="s4">] = (</span><span class="s1">prior1 </span><span class="s4">+ </span><span class="s6">1.0</span><span class="s4">) / (</span><span class="s1">prior1 </span><span class="s4">+ </span><span class="s6">2.0</span><span class="s4">)</span>
    <span class="s1">T</span><span class="s4">[</span><span class="s1">y </span><span class="s4">&lt;= </span><span class="s6">0</span><span class="s4">] = </span><span class="s6">1.0 </span><span class="s4">/ (</span><span class="s1">prior0 </span><span class="s4">+ </span><span class="s6">2.0</span><span class="s4">)</span>

    <span class="s1">bin_loss </span><span class="s4">= </span><span class="s1">HalfBinomialLoss</span><span class="s4">()</span>

    <span class="s3">def </span><span class="s1">loss_grad</span><span class="s4">(</span><span class="s1">AB</span><span class="s4">):</span>
        <span class="s2"># .astype below is needed to ensure y_true and raw_prediction have the</span>
        <span class="s2"># same dtype. With result = np.float64(0) * np.array([1, 2], dtype=np.float32)</span>
        <span class="s2"># - in Numpy 2, result.dtype is float64</span>
        <span class="s2"># - in Numpy&lt;2, result.dtype is float32</span>
        <span class="s1">raw_prediction </span><span class="s4">= -(</span><span class="s1">AB</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] * </span><span class="s1">F </span><span class="s4">+ </span><span class="s1">AB</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]).</span><span class="s1">astype</span><span class="s4">(</span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">predictions</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">)</span>
        <span class="s1">l</span><span class="s4">, </span><span class="s1">g </span><span class="s4">= </span><span class="s1">bin_loss</span><span class="s4">.</span><span class="s1">loss_gradient</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">=</span><span class="s1">T</span><span class="s4">,</span>
            <span class="s1">raw_prediction</span><span class="s4">=</span><span class="s1">raw_prediction</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">loss </span><span class="s4">= </span><span class="s1">l</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
        <span class="s2"># TODO: Remove casting to np.float64 when minimum supported SciPy is 1.11.2</span>
        <span class="s2"># With SciPy &gt;= 1.11.2, the LBFGS implementation will cast to float64</span>
        <span class="s2"># https://github.com/scipy/scipy/pull/18825.</span>
        <span class="s2"># Here we cast to float64 to support SciPy &lt; 1.11.2</span>
        <span class="s1">grad </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">([-</span><span class="s1">g </span><span class="s4">@ </span><span class="s1">F</span><span class="s4">, -</span><span class="s1">g</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()], </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">loss</span><span class="s4">, </span><span class="s1">grad</span>

    <span class="s1">AB0 </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s6">0.0</span><span class="s4">, </span><span class="s1">log</span><span class="s4">((</span><span class="s1">prior0 </span><span class="s4">+ </span><span class="s6">1.0</span><span class="s4">) / (</span><span class="s1">prior1 </span><span class="s4">+ </span><span class="s6">1.0</span><span class="s4">))])</span>

    <span class="s1">opt_result </span><span class="s4">= </span><span class="s1">minimize</span><span class="s4">(</span>
        <span class="s1">loss_grad</span><span class="s4">,</span>
        <span class="s1">AB0</span><span class="s4">,</span>
        <span class="s1">method</span><span class="s4">=</span><span class="s5">&quot;L-BFGS-B&quot;</span><span class="s4">,</span>
        <span class="s1">jac</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">options</span><span class="s4">={</span>
            <span class="s5">&quot;gtol&quot;</span><span class="s4">: </span><span class="s6">1e-6</span><span class="s4">,</span>
            <span class="s5">&quot;ftol&quot;</span><span class="s4">: </span><span class="s6">64 </span><span class="s4">* </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">float</span><span class="s4">).</span><span class="s1">eps</span><span class="s4">,</span>
        <span class="s4">},</span>
    <span class="s4">)</span>
    <span class="s1">AB_ </span><span class="s4">= </span><span class="s1">opt_result</span><span class="s4">.</span><span class="s1">x</span>

    <span class="s2"># The tuned multiplicative parameter is converted back to the original</span>
    <span class="s2"># input feature scale. The offset parameter does not need rescaling since</span>
    <span class="s2"># we did not rescale the outcome variable.</span>
    <span class="s3">return </span><span class="s1">AB_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] / </span><span class="s1">scale_constant</span><span class="s4">, </span><span class="s1">AB_</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]</span>


<span class="s3">class </span><span class="s1">_SigmoidCalibration</span><span class="s4">(</span><span class="s1">RegressorMixin</span><span class="s4">, </span><span class="s1">BaseEstimator</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Sigmoid regression model. 
 
    Attributes 
    ---------- 
    a_ : float 
        The slope. 
 
    b_ : float 
        The intercept. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">fit</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model using X, y as training data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples,) 
            Training data. 
 
        y : array-like of shape (n_samples,) 
            Training target. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. If None, then samples are equally weighted. 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of self. 
        &quot;&quot;&quot;</span>
        <span class="s1">X </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">X</span><span class="s4">)</span>
        <span class="s1">y </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y</span><span class="s4">)</span>
        <span class="s1">X</span><span class="s4">, </span><span class="s1">y </span><span class="s4">= </span><span class="s1">indexable</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">)</span>

        <span class="s1">self</span><span class="s4">.</span><span class="s1">a_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">b_ </span><span class="s4">= </span><span class="s1">_sigmoid_calibration</span><span class="s4">(</span><span class="s1">X</span><span class="s4">, </span><span class="s1">y</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, </span><span class="s1">T</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Predict new data by linear interpolation. 
 
        Parameters 
        ---------- 
        T : array-like of shape (n_samples,) 
            Data to predict from. 
 
        Returns 
        ------- 
        T_ : ndarray of shape (n_samples,) 
            The predicted data. 
        &quot;&quot;&quot;</span>
        <span class="s1">T </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">expit</span><span class="s4">(-(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">a_ </span><span class="s4">* </span><span class="s1">T </span><span class="s4">+ </span><span class="s1">self</span><span class="s4">.</span><span class="s1">b_</span><span class="s4">))</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_prob&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;n_bins&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;strategy&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;uniform&quot;</span><span class="s4">, </span><span class="s5">&quot;quantile&quot;</span><span class="s4">})],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">calibration_curve</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_prob</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">n_bins</span><span class="s4">=</span><span class="s6">5</span><span class="s4">,</span>
    <span class="s1">strategy</span><span class="s4">=</span><span class="s5">&quot;uniform&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute true and predicted probabilities for a calibration curve. 
 
    The method assumes the inputs come from a binary classifier, and 
    discretize the [0, 1] interval into bins. 
 
    Calibration curves may also be referred to as reliability diagrams. 
 
    Read more in the :ref:`User Guide &lt;calibration&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True targets. 
 
    y_prob : array-like of shape (n_samples,) 
        Probabilities of the positive class. 
 
    pos_label : int, float, bool or str, default=None 
        The label of the positive class. 
 
        .. versionadded:: 1.1 
 
    n_bins : int, default=5 
        Number of bins to discretize the [0, 1] interval. A bigger number 
        requires more data. Bins with no samples (i.e. without 
        corresponding values in `y_prob`) will not be returned, thus the 
        returned arrays may have less than `n_bins` values. 
 
    strategy : {'uniform', 'quantile'}, default='uniform' 
        Strategy used to define the widths of the bins. 
 
        uniform 
            The bins have identical widths. 
        quantile 
            The bins have the same number of samples and depend on `y_prob`. 
 
    Returns 
    ------- 
    prob_true : ndarray of shape (n_bins,) or smaller 
        The proportion of samples whose class is the positive class, in each 
        bin (fraction of positives). 
 
    prob_pred : ndarray of shape (n_bins,) or smaller 
        The mean predicted probability in each bin. 
 
    References 
    ---------- 
    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good 
    Probabilities With Supervised Learning, in Proceedings of the 22nd 
    International Conference on Machine Learning (ICML). 
    See section 4 (Qualitative Analysis of Predictions). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.calibration import calibration_curve 
    &gt;&gt;&gt; y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]) 
    &gt;&gt;&gt; y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.]) 
    &gt;&gt;&gt; prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3) 
    &gt;&gt;&gt; prob_true 
    array([0. , 0.5, 1. ]) 
    &gt;&gt;&gt; prob_pred 
    array([0.2  , 0.525, 0.85 ]) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">y_prob </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_prob</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_prob</span><span class="s4">)</span>
    <span class="s1">pos_label </span><span class="s4">= </span><span class="s1">_check_pos_label_consistency</span><span class="s4">(</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_prob</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() &lt; </span><span class="s6">0 </span><span class="s3">or </span><span class="s1">y_prob</span><span class="s4">.</span><span class="s1">max</span><span class="s4">() &gt; </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;y_prob has values outside [0, 1].&quot;</span><span class="s4">)</span>

    <span class="s1">labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">) &gt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">f&quot;Only binary classification is supported. Provided labels </span><span class="s3">{</span><span class="s1">labels</span><span class="s3">}</span><span class="s5">.&quot;</span>
        <span class="s4">)</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">y_true </span><span class="s4">== </span><span class="s1">pos_label</span>

    <span class="s3">if </span><span class="s1">strategy </span><span class="s4">== </span><span class="s5">&quot;quantile&quot;</span><span class="s4">:  </span><span class="s2"># Determine bin edges by distribution of data</span>
        <span class="s1">quantiles </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">linspace</span><span class="s4">(</span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s1">n_bins </span><span class="s4">+ </span><span class="s6">1</span><span class="s4">)</span>
        <span class="s1">bins </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">percentile</span><span class="s4">(</span><span class="s1">y_prob</span><span class="s4">, </span><span class="s1">quantiles </span><span class="s4">* </span><span class="s6">100</span><span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">strategy </span><span class="s4">== </span><span class="s5">&quot;uniform&quot;</span><span class="s4">:</span>
        <span class="s1">bins </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">linspace</span><span class="s4">(</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">, </span><span class="s1">n_bins </span><span class="s4">+ </span><span class="s6">1</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Invalid entry to 'strategy' input. Strategy &quot;</span>
            <span class="s5">&quot;must be either 'quantile' or 'uniform'.&quot;</span>
        <span class="s4">)</span>

    <span class="s1">binids </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">bins</span><span class="s4">[</span><span class="s6">1</span><span class="s4">:-</span><span class="s6">1</span><span class="s4">], </span><span class="s1">y_prob</span><span class="s4">)</span>

    <span class="s1">bin_sums </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">binids</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">y_prob</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">bins</span><span class="s4">))</span>
    <span class="s1">bin_true </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">binids</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">bins</span><span class="s4">))</span>
    <span class="s1">bin_total </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">binids</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">bins</span><span class="s4">))</span>

    <span class="s1">nonzero </span><span class="s4">= </span><span class="s1">bin_total </span><span class="s4">!= </span><span class="s6">0</span>
    <span class="s1">prob_true </span><span class="s4">= </span><span class="s1">bin_true</span><span class="s4">[</span><span class="s1">nonzero</span><span class="s4">] / </span><span class="s1">bin_total</span><span class="s4">[</span><span class="s1">nonzero</span><span class="s4">]</span>
    <span class="s1">prob_pred </span><span class="s4">= </span><span class="s1">bin_sums</span><span class="s4">[</span><span class="s1">nonzero</span><span class="s4">] / </span><span class="s1">bin_total</span><span class="s4">[</span><span class="s1">nonzero</span><span class="s4">]</span>

    <span class="s3">return </span><span class="s1">prob_true</span><span class="s4">, </span><span class="s1">prob_pred</span>


<span class="s3">class </span><span class="s1">CalibrationDisplay</span><span class="s4">(</span><span class="s1">_BinaryClassifierCurveDisplayMixin</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Calibration curve (also known as reliability diagram) visualization. 
 
    It is recommended to use 
    :func:`~sklearn.calibration.CalibrationDisplay.from_estimator` or 
    :func:`~sklearn.calibration.CalibrationDisplay.from_predictions` 
    to create a `CalibrationDisplay`. All parameters are stored as attributes. 
 
    Read more about calibration in the :ref:`User Guide &lt;calibration&gt;` and 
    more about the scikit-learn visualization API in :ref:`visualizations`. 
 
    For an example on how to use the visualization, see 
    :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`. 
 
    .. versionadded:: 1.0 
 
    Parameters 
    ---------- 
    prob_true : ndarray of shape (n_bins,) 
        The proportion of samples whose class is the positive class (fraction 
        of positives), in each bin. 
 
    prob_pred : ndarray of shape (n_bins,) 
        The mean predicted probability in each bin. 
 
    y_prob : ndarray of shape (n_samples,) 
        Probability estimates for the positive class, for each sample. 
 
    estimator_name : str, default=None 
        Name of estimator. If None, the estimator name is not shown. 
 
    pos_label : int, float, bool or str, default=None 
        The positive class when computing the calibration curve. 
        By default, `pos_label` is set to `estimators.classes_[1]` when using 
        `from_estimator` and set to 1 when using `from_predictions`. 
 
        .. versionadded:: 1.1 
 
    Attributes 
    ---------- 
    line_ : matplotlib Artist 
        Calibration curve. 
 
    ax_ : matplotlib Axes 
        Axes with calibration curve. 
 
    figure_ : matplotlib Figure 
        Figure containing the curve. 
 
    See Also 
    -------- 
    calibration_curve : Compute true and predicted probabilities for a 
        calibration curve. 
    CalibrationDisplay.from_predictions : Plot calibration curve using true 
        and predicted labels. 
    CalibrationDisplay.from_estimator : Plot calibration curve using an 
        estimator and data. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import make_classification 
    &gt;&gt;&gt; from sklearn.model_selection import train_test_split 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
    &gt;&gt;&gt; from sklearn.calibration import calibration_curve, CalibrationDisplay 
    &gt;&gt;&gt; X, y = make_classification(random_state=0) 
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split( 
    ...     X, y, random_state=0) 
    &gt;&gt;&gt; clf = LogisticRegression(random_state=0) 
    &gt;&gt;&gt; clf.fit(X_train, y_train) 
    LogisticRegression(random_state=0) 
    &gt;&gt;&gt; y_prob = clf.predict_proba(X_test)[:, 1] 
    &gt;&gt;&gt; prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10) 
    &gt;&gt;&gt; disp = CalibrationDisplay(prob_true, prob_pred, y_prob) 
    &gt;&gt;&gt; disp.plot() 
    &lt;...&gt; 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">__init__</span><span class="s4">(</span>
        <span class="s1">self</span><span class="s4">, </span><span class="s1">prob_true</span><span class="s4">, </span><span class="s1">prob_pred</span><span class="s4">, </span><span class="s1">y_prob</span><span class="s4">, *, </span><span class="s1">estimator_name</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span>
    <span class="s4">):</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">prob_true </span><span class="s4">= </span><span class="s1">prob_true</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">prob_pred </span><span class="s4">= </span><span class="s1">prob_pred</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">y_prob </span><span class="s4">= </span><span class="s1">y_prob</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">estimator_name </span><span class="s4">= </span><span class="s1">estimator_name</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">pos_label </span><span class="s4">= </span><span class="s1">pos_label</span>

    <span class="s3">def </span><span class="s1">plot</span><span class="s4">(</span><span class="s1">self</span><span class="s4">, *, </span><span class="s1">ax</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">name</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">ref_line</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, **</span><span class="s1">kwargs</span><span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Plot visualization. 
 
        Extra keyword arguments will be passed to 
        :func:`matplotlib.pyplot.plot`. 
 
        Parameters 
        ---------- 
        ax : Matplotlib Axes, default=None 
            Axes object to plot on. If `None`, a new figure and axes is 
            created. 
 
        name : str, default=None 
            Name for labeling curve. If `None`, use `estimator_name` if 
            not `None`, otherwise no labeling is shown. 
 
        ref_line : bool, default=True 
            If `True`, plots a reference line representing a perfectly 
            calibrated classifier. 
 
        **kwargs : dict 
            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`. 
 
        Returns 
        ------- 
        display : :class:`~sklearn.calibration.CalibrationDisplay` 
            Object that stores computed values. 
        &quot;&quot;&quot;</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ax_</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">figure_</span><span class="s4">, </span><span class="s1">name </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">_validate_plot_params</span><span class="s4">(</span><span class="s1">ax</span><span class="s4">=</span><span class="s1">ax</span><span class="s4">, </span><span class="s1">name</span><span class="s4">=</span><span class="s1">name</span><span class="s4">)</span>

        <span class="s1">info_pos_label </span><span class="s4">= (</span>
            <span class="s5">f&quot;(Positive class: </span><span class="s3">{</span><span class="s1">self</span><span class="s4">.</span><span class="s1">pos_label</span><span class="s3">}</span><span class="s5">)&quot; </span><span class="s3">if </span><span class="s1">self</span><span class="s4">.</span><span class="s1">pos_label </span><span class="s3">is not None else </span><span class="s5">&quot;&quot;</span>
        <span class="s4">)</span>

        <span class="s1">line_kwargs </span><span class="s4">= {</span><span class="s5">&quot;marker&quot;</span><span class="s4">: </span><span class="s5">&quot;s&quot;</span><span class="s4">, </span><span class="s5">&quot;linestyle&quot;</span><span class="s4">: </span><span class="s5">&quot;-&quot;</span><span class="s4">}</span>
        <span class="s3">if </span><span class="s1">name </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">line_kwargs</span><span class="s4">[</span><span class="s5">&quot;label&quot;</span><span class="s4">] = </span><span class="s1">name</span>
        <span class="s1">line_kwargs</span><span class="s4">.</span><span class="s1">update</span><span class="s4">(**</span><span class="s1">kwargs</span><span class="s4">)</span>

        <span class="s1">ref_line_label </span><span class="s4">= </span><span class="s5">&quot;Perfectly calibrated&quot;</span>
        <span class="s1">existing_ref_line </span><span class="s4">= </span><span class="s1">ref_line_label </span><span class="s3">in </span><span class="s1">self</span><span class="s4">.</span><span class="s1">ax_</span><span class="s4">.</span><span class="s1">get_legend_handles_labels</span><span class="s4">()[</span><span class="s6">1</span><span class="s4">]</span>
        <span class="s3">if </span><span class="s1">ref_line </span><span class="s3">and not </span><span class="s1">existing_ref_line</span><span class="s4">:</span>
            <span class="s1">self</span><span class="s4">.</span><span class="s1">ax_</span><span class="s4">.</span><span class="s1">plot</span><span class="s4">([</span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">], [</span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">], </span><span class="s5">&quot;k:&quot;</span><span class="s4">, </span><span class="s1">label</span><span class="s4">=</span><span class="s1">ref_line_label</span><span class="s4">)</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">line_ </span><span class="s4">= </span><span class="s1">self</span><span class="s4">.</span><span class="s1">ax_</span><span class="s4">.</span><span class="s1">plot</span><span class="s4">(</span><span class="s1">self</span><span class="s4">.</span><span class="s1">prob_pred</span><span class="s4">, </span><span class="s1">self</span><span class="s4">.</span><span class="s1">prob_true</span><span class="s4">, **</span><span class="s1">line_kwargs</span><span class="s4">)[</span><span class="s6">0</span><span class="s4">]</span>

        <span class="s2"># We always have to show the legend for at least the reference line</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ax_</span><span class="s4">.</span><span class="s1">legend</span><span class="s4">(</span><span class="s1">loc</span><span class="s4">=</span><span class="s5">&quot;lower right&quot;</span><span class="s4">)</span>

        <span class="s1">xlabel </span><span class="s4">= </span><span class="s5">f&quot;Mean predicted probability </span><span class="s3">{</span><span class="s1">info_pos_label</span><span class="s3">}</span><span class="s5">&quot;</span>
        <span class="s1">ylabel </span><span class="s4">= </span><span class="s5">f&quot;Fraction of positives </span><span class="s3">{</span><span class="s1">info_pos_label</span><span class="s3">}</span><span class="s5">&quot;</span>
        <span class="s1">self</span><span class="s4">.</span><span class="s1">ax_</span><span class="s4">.</span><span class="s1">set</span><span class="s4">(</span><span class="s1">xlabel</span><span class="s4">=</span><span class="s1">xlabel</span><span class="s4">, </span><span class="s1">ylabel</span><span class="s4">=</span><span class="s1">ylabel</span><span class="s4">)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s4">@</span><span class="s1">classmethod</span>
    <span class="s3">def </span><span class="s1">from_estimator</span><span class="s4">(</span>
        <span class="s1">cls</span><span class="s4">,</span>
        <span class="s1">estimator</span><span class="s4">,</span>
        <span class="s1">X</span><span class="s4">,</span>
        <span class="s1">y</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">n_bins</span><span class="s4">=</span><span class="s6">5</span><span class="s4">,</span>
        <span class="s1">strategy</span><span class="s4">=</span><span class="s5">&quot;uniform&quot;</span><span class="s4">,</span>
        <span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">name</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ref_line</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">ax</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s4">**</span><span class="s1">kwargs</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Plot calibration curve using a binary classifier and data. 
 
        A calibration curve, also known as a reliability diagram, uses inputs 
        from a binary classifier and plots the average predicted probability 
        for each bin against the fraction of positive classes, on the 
        y-axis. 
 
        Extra keyword arguments will be passed to 
        :func:`matplotlib.pyplot.plot`. 
 
        Read more about calibration in the :ref:`User Guide &lt;calibration&gt;` and 
        more about the scikit-learn visualization API in :ref:`visualizations`. 
 
        .. versionadded:: 1.0 
 
        Parameters 
        ---------- 
        estimator : estimator instance 
            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline` 
            in which the last estimator is a classifier. The classifier must 
            have a :term:`predict_proba` method. 
 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Input values. 
 
        y : array-like of shape (n_samples,) 
            Binary target values. 
 
        n_bins : int, default=5 
            Number of bins to discretize the [0, 1] interval into when 
            calculating the calibration curve. A bigger number requires more 
            data. 
 
        strategy : {'uniform', 'quantile'}, default='uniform' 
            Strategy used to define the widths of the bins. 
 
            - `'uniform'`: The bins have identical widths. 
            - `'quantile'`: The bins have the same number of samples and depend 
              on predicted probabilities. 
 
        pos_label : int, float, bool or str, default=None 
            The positive class when computing the calibration curve. 
            By default, `estimators.classes_[1]` is considered as the 
            positive class. 
 
            .. versionadded:: 1.1 
 
        name : str, default=None 
            Name for labeling curve. If `None`, the name of the estimator is 
            used. 
 
        ref_line : bool, default=True 
            If `True`, plots a reference line representing a perfectly 
            calibrated classifier. 
 
        ax : matplotlib axes, default=None 
            Axes object to plot on. If `None`, a new figure and axes is 
            created. 
 
        **kwargs : dict 
            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`. 
 
        Returns 
        ------- 
        display : :class:`~sklearn.calibration.CalibrationDisplay`. 
            Object that stores computed values. 
 
        See Also 
        -------- 
        CalibrationDisplay.from_predictions : Plot calibration curve using true 
            and predicted labels. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import matplotlib.pyplot as plt 
        &gt;&gt;&gt; from sklearn.datasets import make_classification 
        &gt;&gt;&gt; from sklearn.model_selection import train_test_split 
        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
        &gt;&gt;&gt; from sklearn.calibration import CalibrationDisplay 
        &gt;&gt;&gt; X, y = make_classification(random_state=0) 
        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split( 
        ...     X, y, random_state=0) 
        &gt;&gt;&gt; clf = LogisticRegression(random_state=0) 
        &gt;&gt;&gt; clf.fit(X_train, y_train) 
        LogisticRegression(random_state=0) 
        &gt;&gt;&gt; disp = CalibrationDisplay.from_estimator(clf, X_test, y_test) 
        &gt;&gt;&gt; plt.show() 
        &quot;&quot;&quot;</span>
        <span class="s1">y_prob</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">name </span><span class="s4">= </span><span class="s1">cls</span><span class="s4">.</span><span class="s1">_validate_and_get_response_values</span><span class="s4">(</span>
            <span class="s1">estimator</span><span class="s4">,</span>
            <span class="s1">X</span><span class="s4">,</span>
            <span class="s1">y</span><span class="s4">,</span>
            <span class="s1">response_method</span><span class="s4">=</span><span class="s5">&quot;predict_proba&quot;</span><span class="s4">,</span>
            <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">,</span>
            <span class="s1">name</span><span class="s4">=</span><span class="s1">name</span><span class="s4">,</span>
        <span class="s4">)</span>

        <span class="s3">return </span><span class="s1">cls</span><span class="s4">.</span><span class="s1">from_predictions</span><span class="s4">(</span>
            <span class="s1">y</span><span class="s4">,</span>
            <span class="s1">y_prob</span><span class="s4">,</span>
            <span class="s1">n_bins</span><span class="s4">=</span><span class="s1">n_bins</span><span class="s4">,</span>
            <span class="s1">strategy</span><span class="s4">=</span><span class="s1">strategy</span><span class="s4">,</span>
            <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">,</span>
            <span class="s1">name</span><span class="s4">=</span><span class="s1">name</span><span class="s4">,</span>
            <span class="s1">ref_line</span><span class="s4">=</span><span class="s1">ref_line</span><span class="s4">,</span>
            <span class="s1">ax</span><span class="s4">=</span><span class="s1">ax</span><span class="s4">,</span>
            <span class="s4">**</span><span class="s1">kwargs</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s4">@</span><span class="s1">classmethod</span>
    <span class="s3">def </span><span class="s1">from_predictions</span><span class="s4">(</span>
        <span class="s1">cls</span><span class="s4">,</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_prob</span><span class="s4">,</span>
        <span class="s4">*,</span>
        <span class="s1">n_bins</span><span class="s4">=</span><span class="s6">5</span><span class="s4">,</span>
        <span class="s1">strategy</span><span class="s4">=</span><span class="s5">&quot;uniform&quot;</span><span class="s4">,</span>
        <span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">name</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">ref_line</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
        <span class="s1">ax</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s4">**</span><span class="s1">kwargs</span><span class="s4">,</span>
    <span class="s4">):</span>
        <span class="s0">&quot;&quot;&quot;Plot calibration curve using true labels and predicted probabilities. 
 
        Calibration curve, also known as reliability diagram, uses inputs 
        from a binary classifier and plots the average predicted probability 
        for each bin against the fraction of positive classes, on the 
        y-axis. 
 
        Extra keyword arguments will be passed to 
        :func:`matplotlib.pyplot.plot`. 
 
        Read more about calibration in the :ref:`User Guide &lt;calibration&gt;` and 
        more about the scikit-learn visualization API in :ref:`visualizations`. 
 
        .. versionadded:: 1.0 
 
        Parameters 
        ---------- 
        y_true : array-like of shape (n_samples,) 
            True labels. 
 
        y_prob : array-like of shape (n_samples,) 
            The predicted probabilities of the positive class. 
 
        n_bins : int, default=5 
            Number of bins to discretize the [0, 1] interval into when 
            calculating the calibration curve. A bigger number requires more 
            data. 
 
        strategy : {'uniform', 'quantile'}, default='uniform' 
            Strategy used to define the widths of the bins. 
 
            - `'uniform'`: The bins have identical widths. 
            - `'quantile'`: The bins have the same number of samples and depend 
              on predicted probabilities. 
 
        pos_label : int, float, bool or str, default=None 
            The positive class when computing the calibration curve. 
            By default `pos_label` is set to 1. 
 
            .. versionadded:: 1.1 
 
        name : str, default=None 
            Name for labeling curve. 
 
        ref_line : bool, default=True 
            If `True`, plots a reference line representing a perfectly 
            calibrated classifier. 
 
        ax : matplotlib axes, default=None 
            Axes object to plot on. If `None`, a new figure and axes is 
            created. 
 
        **kwargs : dict 
            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`. 
 
        Returns 
        ------- 
        display : :class:`~sklearn.calibration.CalibrationDisplay`. 
            Object that stores computed values. 
 
        See Also 
        -------- 
        CalibrationDisplay.from_estimator : Plot calibration curve using an 
            estimator and data. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import matplotlib.pyplot as plt 
        &gt;&gt;&gt; from sklearn.datasets import make_classification 
        &gt;&gt;&gt; from sklearn.model_selection import train_test_split 
        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
        &gt;&gt;&gt; from sklearn.calibration import CalibrationDisplay 
        &gt;&gt;&gt; X, y = make_classification(random_state=0) 
        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split( 
        ...     X, y, random_state=0) 
        &gt;&gt;&gt; clf = LogisticRegression(random_state=0) 
        &gt;&gt;&gt; clf.fit(X_train, y_train) 
        LogisticRegression(random_state=0) 
        &gt;&gt;&gt; y_prob = clf.predict_proba(X_test)[:, 1] 
        &gt;&gt;&gt; disp = CalibrationDisplay.from_predictions(y_test, y_prob) 
        &gt;&gt;&gt; plt.show() 
        &quot;&quot;&quot;</span>
        <span class="s1">pos_label_validated</span><span class="s4">, </span><span class="s1">name </span><span class="s4">= </span><span class="s1">cls</span><span class="s4">.</span><span class="s1">_validate_from_predictions_params</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_prob</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">name</span><span class="s4">=</span><span class="s1">name</span>
        <span class="s4">)</span>

        <span class="s1">prob_true</span><span class="s4">, </span><span class="s1">prob_pred </span><span class="s4">= </span><span class="s1">calibration_curve</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_prob</span><span class="s4">, </span><span class="s1">n_bins</span><span class="s4">=</span><span class="s1">n_bins</span><span class="s4">, </span><span class="s1">strategy</span><span class="s4">=</span><span class="s1">strategy</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span>
        <span class="s4">)</span>

        <span class="s1">disp </span><span class="s4">= </span><span class="s1">cls</span><span class="s4">(</span>
            <span class="s1">prob_true</span><span class="s4">=</span><span class="s1">prob_true</span><span class="s4">,</span>
            <span class="s1">prob_pred</span><span class="s4">=</span><span class="s1">prob_pred</span><span class="s4">,</span>
            <span class="s1">y_prob</span><span class="s4">=</span><span class="s1">y_prob</span><span class="s4">,</span>
            <span class="s1">estimator_name</span><span class="s4">=</span><span class="s1">name</span><span class="s4">,</span>
            <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label_validated</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s3">return </span><span class="s1">disp</span><span class="s4">.</span><span class="s1">plot</span><span class="s4">(</span><span class="s1">ax</span><span class="s4">=</span><span class="s1">ax</span><span class="s4">, </span><span class="s1">ref_line</span><span class="s4">=</span><span class="s1">ref_line</span><span class="s4">, **</span><span class="s1">kwargs</span><span class="s4">)</span>
</pre>
</body>
</html>